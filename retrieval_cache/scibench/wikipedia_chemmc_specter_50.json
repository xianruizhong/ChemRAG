[
    {
        "id": "test_0",
        "question": "Calculate the de Broglie wavelength for (a) an electron with a kinetic energy of $100 \\mathrm{eV}$",
        "golden_answers": [
            " 0.123"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 5205018,
                    "contents": "Compton wavelength\nIt appears in the Dirac equation (the following is an explicitly covariant form employing the Einstein summation convention): The reduced Compton wavelength also appears in Schrödinger's equation, although its presence is obscured in traditional representations of the equation. The following is the traditional representation of Schrödinger's equation for an electron in a hydrogen-like atom: Dividing through by , and rewriting in terms of the fine structure constant, one obtains:",
                    "score": 0.926673173904419
                },
                {
                    "id": 7748007,
                    "contents": "Introduction to quantum mechanics\nwhere , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit. The energy of the electron can also be calculated, and is given by . Thus Bohr's assumption that angular momentum is quantized means that an electron can inhabit only certain orbits around the nucleus and that it can have only certain energies. A consequence of these constraints is that the electron does not crash into the nucleus: it cannot continuously emit energy, and it cannot come closer to the nucleus than a0 (the Bohr radius). An electron loses energy by jumping instantaneously from its original orbit to a lower orbit; the extra energy is emitted in the form of a photon. Conversely, an electron that absorbs a photon gains energy, hence it jumps to an orbit that is farther from the nucleus.",
                    "score": 0.9114158153533936
                },
                {
                    "id": 6114470,
                    "contents": "Davisson–Germer experiment\nAccording to the de Broglie relation, electrons with kinetic energy of have a wavelength of . The experimental outcome was via Bragg's law, which closely matched the predictions. As Davisson and Germer state in their 1928 follow-up paper to their Nobel prize winning paper, \"These results, including the failure of the data to satisfy the Bragg formula, are in accord with those previously obtained in our experiments on electron diffraction. The reflection data fail to satisfy the Bragg relation for the same reason that the electron diffraction beams fail to coincide with their Laue beam analogues.\" However, they add, \"The calculated wave-lengths are in excellent agreement with the theoretical values of h/mv as shown in the accompanying table.\" So although electron energy diffraction does not follow the Bragg law, it did confirm de Broglie's theory that particles behave like waves. However, the experiments did not follow the de Broglie calculations which led to attempts by Carl Eckart,",
                    "score": 0.9049333333969116
                },
                {
                    "id": 154982,
                    "contents": "Rydberg formula\nBut the Rydberg formula also provides correct wavelengths for distant electrons, where the effective nuclear charge can be estimated as the same as that for hydrogen, since all but one of the nuclear charges have been screened by other electrons, and the core of the atom has an effective positive charge of +1.",
                    "score": 0.9041628241539001
                },
                {
                    "id": 10045498,
                    "contents": "Kaufmann–Bucherer–Neumann experiments\nKaufmann also made a calculation mistake in deriving the deflection curves. Those errors were corrected by him in 1902. In 1902 and 1903 Kaufmann performed another series of tests with updated and improved experimental techniques. The results were interpreted by him as a confirmation of Abraham's theory and of the assumption that the electron's mass is completely of electromagnetic origin. Hermann Starke conducted similar measurements in 1903, although he used cathode rays limited to 0.3c. The results that he obtained were interpreted by him as being in agreement with those of Kaufmann. Competing theories In 1902, Max Abraham published a theory based on the assumption that the electron was a rigid, perfect sphere, with its charge being distributed evenly on its surface. As explained above, he introduced the so-called \"transverse electromagnetic mass\" besides the \"longitudinal electromagnetic mass\", and argued that the entire electron mass is of electromagnetic origin.",
                    "score": 0.9030439853668213
                },
                {
                    "id": 6114471,
                    "contents": "Davisson–Germer experiment\nfollow the Bragg law, it did confirm de Broglie's theory that particles behave like waves. However, the experiments did not follow the de Broglie calculations which led to attempts by Carl Eckart, A.L. Patterson, and Fritz Zwicky to make an examination of the possible ways of interpreting the systematic differences between observed and de Broglie calculated electron wavelengths by a contraction factor or index of contraction.",
                    "score": 0.9012709856033325
                },
                {
                    "id": 5205017,
                    "contents": "Compton wavelength\nThe CODATA 2018 value for the Compton wavelength of the electron is . Other particles have different Compton wavelengths. Reduced Compton wavelength When the Compton wavelength is divided by , one obtains the \"reduced\" Compton wavelength (barred lambda), i.e. the Compton wavelength for radian instead of radians: where is the \"reduced\" Planck constant. Role in equations for massive particles The inverse reduced Compton wavelength is a natural representation for mass on the quantum scale, and as such, it appears in many of the fundamental equations of quantum mechanics. The reduced Compton wavelength appears in the relativistic Klein–Gordon equation for a free particle: It appears in the Dirac equation (the following is an explicitly covariant form employing the Einstein summation convention):",
                    "score": 0.90096116065979
                },
                {
                    "id": 5205016,
                    "contents": "Compton wavelength\nThe Compton wavelength is a quantum mechanical property of a particle. The Compton wavelength of a particle is equal to the wavelength of a photon whose energy is the same as the mass of that particle (see mass–energy equivalence). It was introduced by Arthur Compton in his explanation of the scattering of photons by electrons (a process known as Compton scattering). The standard Compton wavelength, , of a particle is given by, while its frequency is given by, where is the Planck constant, is the particle's rest mass, and is the speed of light. The significance of this formula is shown in the derivation of the Compton shift formula. It is equivalent to the de Broglie wavelength with . The CODATA 2018 value for the Compton wavelength of the electron is . Other particles have different Compton wavelengths.",
                    "score": 0.899310827255249
                },
                {
                    "id": 5205019,
                    "contents": "Compton wavelength\nDividing through by , and rewriting in terms of the fine structure constant, one obtains: Distinction between reduced and non-reduced The reduced Compton wavelength is a natural representation of mass on the quantum scale. Equations that pertain to inertial mass like Klein-Gordon and Schrödinger's, use the reduced Compton wavelength. The non-reduced Compton wavelength is a natural representation for mass that has been converted into energy. Equations that pertain to the conversion of mass into energy, or to the wavelengths of photons interacting with mass, use the non-reduced Compton wavelength. A particle of mass has a rest energy of . The non-reduced Compton wavelength for this particle is the wavelength of a photon of the same energy. For photons of frequency , energy is given by which yields the non-reduced or standard Compton wavelength formula if solved for .",
                    "score": 0.8977645635604858
                },
                {
                    "id": 8430393,
                    "contents": "Heinrich Kayser\nHeinrich Gustav Johannes Kayser ForMemRS (; 16 March 1853 – 14 October 1940) was a German physicist and spectroscopist. Biography Kayser was born at Bingen am Rhein. Kayser's early work was concerned with the characteristics of acoustic waves. He discovered the occurrence of helium in the Earth's atmosphere in 1868 during a solar eclipse when he detected a new spectral line in the solar spectrum. In 1881 Kayser coined the word “adsorption”. Together with Carl Runge, he examined the spectra of chemical elements. In 1905, he wrote a paper on electron theory. The kayser unit, associated with wavenumber, of the CGS system was named after him. He died at Bonn in 1940. Works Lehrbuch der Physik für Studierende . Enke, Stuttgart 3rd ed. 1900 Digital edition by the University and State Library Düsseldorf References External links",
                    "score": 0.8960238695144653
                },
                {
                    "id": 7747995,
                    "contents": "Introduction to quantum mechanics\nEinstein explained the effect by postulating that a beam of light is a stream of particles (\"photons\") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron. To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the work function and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:",
                    "score": 0.8956377506256104
                },
                {
                    "id": 7748052,
                    "contents": "Introduction to quantum mechanics\nThe Lamb shift is an example of a quantum electrodynamics prediction that has been experimentally verified. It is an effect whereby the quantum nature of the electromagnetic field makes the energy levels in an atom or ion deviate slightly from what they would otherwise be. As a result, spectral lines may shift or split. Similarly, within a freely propagating electromagnetic wave, the current can also be just an abstract displacement current, instead of involving charge carriers. In QED, its full description makes essential use of short-lived virtual particles. There, QED again validates an earlier, rather mysterious concept. Standard Model",
                    "score": 0.8947248458862305
                },
                {
                    "id": 3882728,
                    "contents": "Klein–Nishina formula\nThe Klein–Nishina formula gives the differential cross section of photons scattered from a single free electron in lowest order of quantum electrodynamics. At low frequencies (e.g., visible light) this yields Thomson scattering; at higher frequencies (e.g., x-rays and gamma-rays) this yields Compton scattering. For an incident unpolarized photon of energy , the differential cross section is: where is a differential cross section, is an infinitesimal solid angle element, is the fine structure constant (~1/137.04), is the scattering angle; is the \"reduced\" Compton wave length of the electron (~0.38616 pm); is the mass of an electron (~511 keV); and is the ratio of photon energy after and before the collision: Note that this result may also be expressed in terms of the classical electron radius :",
                    "score": 0.8941885828971863
                },
                {
                    "id": 5205023,
                    "contents": "Compton wavelength\nThus the uncertainty in position must be greater than half of the reduced Compton wavelength . The Compton wavelength can be contrasted with the de Broglie wavelength, which depends on the momentum of a particle and determines the cutoff between particle and wave behavior in quantum mechanics. Notably, de Broglie's derivation of the de Broglie wavelength is based on the assumption that an observed particle is associated with a periodic phenomenon of the particle's Compton frequency. Relationship to other constants Typical atomic lengths, wave numbers, and areas in physics can be related to the reduced Compton wavelength for the electron () and the electromagnetic fine structure constant (). The Bohr radius is related to the Compton wavelength by: The classical electron radius is about 3 times larger than the proton radius, and is written: The Rydberg constant, having dimensions of linear wavenumber, is written: This yields the sequence: .",
                    "score": 0.8940173387527466
                },
                {
                    "id": 9980766,
                    "contents": "Kramers–Heisenberg formula\nThe Kramers–Heisenberg dispersion formula is an expression for the cross section for scattering of a photon by an atomic electron. It was derived before the advent of quantum mechanics by Hendrik Kramers and Werner Heisenberg in 1925, based on the correspondence principle applied to the classical dispersion formula for light. The quantum mechanical derivation was given by Paul Dirac in 1927. The Kramers–Heisenberg formula was an important achievement when it was published, explaining the notion of \"negative absorption\" (stimulated emission), the Thomas–Reiche–Kuhn sum rule, and inelastic scattering — where the energy of the scattered photon may be larger or smaller than that of the incident photon — thereby anticipating the discovery of the Raman effect. Equation The Kramers–Heisenberg (KH) formula for second order processes is",
                    "score": 0.8939170241355896
                },
                {
                    "id": 7748006,
                    "contents": "Introduction to quantum mechanics\nSome fundamental assumptions of the Bohr model were soon proven wrong—but the key result that the discrete lines in emission spectra are due to some property of the electrons in atoms being quantized is correct. The way that the electrons actually behave is strikingly different from Bohr's atom, and from what we see in the world of our everyday experience; this modern quantum mechanical model of the atom is discussed below. Bohr theorized that the angular momentum, , of an electron is quantized: where is an integer and is the Planck constant. Starting from this assumption, Coulomb's law and the equations of circular motion show that an electron with units of angular momentum orbits a proton at a distance given by , where is the Coulomb constant, is the mass of an electron, and is the charge on an electron. For simplicity this is written as where , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit.",
                    "score": 0.893451988697052
                },
                {
                    "id": 552406,
                    "contents": "Wavenumber\nwhich remains essentially the same in air, and so the spectroscopic wavenumber is directly related to the angles of light scattered from diffraction gratings and the distance between fringes in interferometers, when those instruments are operated in air or vacuum. Such wavenumbers were first used in the calculations of Johannes Rydberg in the 1880s. The Rydberg–Ritz combination principle of 1908 was also formulated in terms of wavenumbers. A few years later spectral lines could be understood in quantum theory as differences between energy levels, energy being proportional to wavenumber, or frequency. However, spectroscopic data kept being tabulated in terms of spectroscopic wavenumber rather than frequency or energy. For example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula:",
                    "score": 0.8929492235183716
                },
                {
                    "id": 11719110,
                    "contents": "History of quantum mechanics\nFounding experiments Thomas Young's double-slit experiment demonstrating the wave nature of light. (c. 1801) Henri Becquerel discovers radioactivity. (1896) J. J. Thomson's cathode ray tube experiments (discovers the electron and its negative charge). (1897) The study of black-body radiation between 1850 and 1900, which could not be explained without quantum concepts. The photoelectric effect: Einstein explained this in 1905 (and later received a Nobel prize for it) using the concept of photons, particles of light with quantized energy. Robert Millikan's oil-drop experiment, which showed that electric charge occurs as quanta (whole units). (1909) Ernest Rutherford's gold foil experiment disproved the plum pudding model of the atom which suggested that the mass and positive charge of the atom are almost uniformly distributed. This led to the planetary model of the atom (1911).",
                    "score": 0.8928794860839844
                },
                {
                    "id": 552407,
                    "contents": "Wavenumber\nFor example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula: where R is the Rydberg constant, and ni and nf are the principal quantum numbers of the initial and final levels respectively (ni is greater than nf for emission). A spectroscopic wavenumber can be converted into energy per photon E by Planck's relation: It can also be converted into wavelength of light: where n is the refractive index of the medium. Note that the wavelength of light changes as it passes through different media, however, the spectroscopic wavenumber (i.e., frequency) remains constant. Conventionally, inverse centimeter (cm−1) units are used for , so often that such spatial frequencies are stated by some authors \"in wavenumbers\", incorrectly transferring the name of the quantity to the CGS unit cm−1 itself. See also Spatial frequency Refractive index Zonal wavenumber References Wave mechanics Physical quantities Units of frequency",
                    "score": 0.8926674723625183
                },
                {
                    "id": 10943259,
                    "contents": "Theoretical and experimental justification for the Schrödinger equation\nAlso, it was noted that excited atoms emit radiation with discrete frequencies. Einstein used this fact to interpret discrete energy packets of light as, in fact, real particles. If these real particles are emitted from atoms in discrete energy packets, however, must the emitters, the electrons, also change energy in discrete energy packets? There is nothing in Newtonian mechanics that explains this. The de Broglie hypothesis helped explain these phenomena by noting that the only allowed states for an electron orbiting an atom are those that allow for standing waves associated with each electron. Balmer series The Balmer series identifies those frequencies of light that can be emitted from an excited hydrogen atom: where R is known as the Rydberg constant and is equal to 13.6 electron volts. Assumptions of the Bohr model The Bohr model, introduced in 1913, was an attempt to provide a theoretical basis for the Balmer series. The assumptions of the model are:",
                    "score": 0.8919475674629211
                },
                {
                    "id": 21198260,
                    "contents": "Double ionization\nSee also List of laser articles Nonlinear optics Photoionization Ionization High harmonic generation Above threshold ionization References Atomic Quantum mechanics Nonlinear optics",
                    "score": 0.8914008140563965
                },
                {
                    "id": 4788815,
                    "contents": "Lamb shift\nThis particular difference is a one-loop effect of quantum electrodynamics, and can be interpreted as the influence of virtual photons that have been emitted and re-absorbed by the atom. In quantum electrodynamics the electromagnetic field is quantized and, like the harmonic oscillator in quantum mechanics, its lowest state is not zero. Thus, there exist small zero-point oscillations that cause the electron to execute rapid oscillatory motions. The electron is \"smeared out\" and each radius value is changed from r to r + δr (a small but finite perturbation). The Coulomb potential is therefore perturbed by a small amount and the degeneracy of the two energy levels is removed. The new potential can be approximated (using atomic units) as follows: The Lamb shift itself is given by with k(n, 0) around 13 varying slightly with n, and with log(k(n,)) a small number (approx. -0.05) making k(n,) close to unity. For a derivation of ΔELamb see for example:",
                    "score": 0.8910428881645203
                },
                {
                    "id": 6598017,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path (IMFP) is an index of how far an electron on average travels through a solid before losing energy. If a monochromatic primary beam of electrons is incident on a solid surface, the majority of incident electrons lose their energy because they interact strongly with matter, leading to plasmon excitation, electron-hole pair formation, and vibrational excitation. The intensity of the primary electrons, , is damped as a function of the distance, d, into the solid. The intensity decay can be expressed as follows: where is the intensity after the primary electron beam has traveled through the solid to a distance . The parameter , termed the inelastic mean free path (IMFP), is defined as the distance an electron beam can travel before its intensity decays to of its initial value. (Note that this is equation is closely related to the Beer-Lambert law.)",
                    "score": 0.8909120559692383
                },
                {
                    "id": 632879,
                    "contents": "Bohr radius\nRelated units The Bohr radius of the electron is one of a trio of related units of length, the other two being the Compton wavelength of the electron and the classical electron radius . The Bohr radius is built from the electron mass , Planck's constant and the electron charge . The Compton wavelength is built from , and the speed of light . The classical electron radius is built from , and . Any one of these three lengths can be written in terms of any other using the fine-structure constant : The Bohr radius is about 19,000 times bigger than the classical electron radius (i.e. the common scale of atoms is angstrom, while the scale of particles is femtometer). The electron's Compton wavelength is about 20 times smaller than the Bohr radius, and the classical electron radius is about 1000 times smaller than the electron's Compton wavelength. Hydrogen atom and similar systems The Bohr radius including the effect of reduced mass in the hydrogen atom is given by",
                    "score": 0.8907198905944824
                },
                {
                    "id": 154975,
                    "contents": "Rydberg formula\nIn atomic physics, the Rydberg formula calculates the wavelengths of a spectral line in many chemical elements. The formula was primarily presented as a generalization of the Balmer series for all atomic electron transitions of hydrogen. It was first empirically stated in 1888 by the Swedish physicist Johannes Rydberg, then theoretically by Niels Bohr in 1913, who used a primitive form of quantum mechanics. The formula directly generalizes the equations used to calculate the wavelengths of the hydrogen spectral series.",
                    "score": 0.8905200362205505
                },
                {
                    "id": 7748002,
                    "contents": "Introduction to quantum mechanics\nIn 1885 the Swiss mathematician Johann Balmer discovered that each wavelength (lambda) in the visible spectrum of hydrogen is related to some integer by the equation where is a constant Balmer determined is equal to 364.56 nm. In 1888 Johannes Rydberg generalized and greatly increased the explanatory utility of Balmer's formula. He predicted that is related to two integers and according to what is now known as the Rydberg formula: where R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m.",
                    "score": 0.8902007341384888
                },
                {
                    "id": 6598018,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path of electrons can roughly be described by a universal curve that is the same for all materials. See also Scattering theory Beer-Lambert law References Atomic, molecular, and optical physics",
                    "score": 0.889944851398468
                },
                {
                    "id": 7747993,
                    "contents": "Introduction to quantum mechanics\nIn 1887, Heinrich Hertz observed that when light with sufficient frequency hits a metallic surface, the surface emits electrons. In 1902, Philipp Lenard discovered that the maximum possible energy of an ejected electron is related to the frequency of the light, not to its intensity: if the frequency is too low, no electrons are ejected regardless of the intensity. Strong beams of light toward the red end of the spectrum might produce no electrical potential at all, while weak beams of light toward the violet end of the spectrum would produce higher and higher voltages. The lowest frequency of light that can cause electrons to be emitted, called the threshold frequency, is different for different metals. This observation is at odds with classical electromagnetism, which predicts that the electron's energy should be proportional to the intensity of the incident radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a",
                    "score": 0.8896628618240356
                },
                {
                    "id": 7747994,
                    "contents": "Introduction to quantum mechanics\nelectron's energy should be proportional to the intensity of the incident radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a higher intensity of light would produce a higher voltage from the photoelectric device.",
                    "score": 0.8894189596176147
                },
                {
                    "id": 1694117,
                    "contents": "Electron\nDe Broglie's prediction of a wave nature for electrons led Erwin Schrödinger to postulate a wave equation for electrons moving under the influence of the nucleus in the atom. In 1926, this equation, the Schrödinger equation, successfully described how electron waves propagated. Rather than yielding a solution that determined the location of an electron over time, this wave equation also could be used to predict the probability of finding an electron near a position, especially a position near where the electron was bound in space, for which the electron wave equations did not change in time. This approach led to a second formulation of quantum mechanics (the first by Heisenberg in 1925), and solutions of Schrödinger's equation, like Heisenberg's, provided derivations of the energy states of an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction",
                    "score": 0.8893923759460449
                },
                {
                    "id": 8830572,
                    "contents": "Electron spectroscopy\nHistory The development of electron spectroscopy can be considered to have begun in 1887 when the German physicist Heinrich Rudolf Hertz discovered the photoelectric effect but was unable to explain it. In 1900, Max Planck (1918 Nobel Prize in Physics) suggested that energy carried by electromagnetic waves could only be released in \"packets\" of energy. In 1905 Albert Einstein (1921 Nobel Prize of Physics) explained Planck's discovery and the photoelectric effect. He presented the hypothesis that light energy is carried in discrete quantized packets (photons), each with energy hν to explain the experimental dobservations. Two years after this publication, in 1907, P. D. Innes recorded the first XPS spectrum.",
                    "score": 0.8893890380859375
                },
                {
                    "id": 15698732,
                    "contents": "Classical mechanics\nThe classical approximation to quantum mechanics The ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is where h is Planck's constant and p is the momentum. Again, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.",
                    "score": 0.8890774846076965
                },
                {
                    "id": 16680977,
                    "contents": "Timeline of quantum mechanics\n1801 – Thomas Young establishes that light made up of waves with his Double-slit experiment. 1859 – Gustav Kirchhoff introduces the concept of a blackbody and proves that its emission spectrum depends only on its temperature. 1860-1900 – Ludwig Eduard Boltzmann, James Clerk Maxwell and others develop the theory of statistical mechanics. Boltzmann argues that entropy is a measure of disorder. 1877 – Boltzmann suggests that the energy levels of a physical system could be discrete based on statistical mechanics and mathematical arguments; also produces the first circle diagram representation, or atomic model of a molecule (such as an iodine gas molecule) in terms of the overlapping terms α and β, later (in 1928) called molecular orbitals, of the constituting atoms. 1885 – Johann Jakob Balmer discovers a numerical relationship between visible spectral lines of hydrogen, the Balmer series.",
                    "score": 0.889043927192688
                },
                {
                    "id": 7747996,
                    "contents": "Introduction to quantum mechanics\nIf is greater than , the energy is enough to remove an electron. The ejected electron has a kinetic energy, , which is, at most, equal to the photon's energy minus the energy needed to dislodge the electron from the metal:",
                    "score": 0.8887400031089783
                },
                {
                    "id": 16992313,
                    "contents": "Electron mass\nThe electron relative atomic mass is an adjusted parameter in the CODATA set of fundamental physical constants, while the electron rest mass in kilograms is calculated from the values of the Planck constant, the fine-structure constant and the Rydberg constant, as detailed above. Relationship to other physical constants The electron mass is used to calculate the Avogadro constant NA: Hence it is also related to the atomic mass constant mu: where Mu is the molar mass constant (defined in SI) and Ar(e) is a directly measured quantity, the relative atomic mass of the electron. Note that mu is defined in terms of Ar(e), and not the other way round, and so the name \"electron mass in atomic mass units\" for Ar(e) involves a circular definition (at least in terms of practical measurements).",
                    "score": 0.8886905908584595
                },
                {
                    "id": 1694091,
                    "contents": "Electron\nThe electron is a subatomic particle (denoted by the symbol or ) whose electric charge is negative one elementary charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, ħ. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence",
                    "score": 0.8883715271949768
                },
                {
                    "id": 4757586,
                    "contents": "QED: The Strange Theory of Light and Matter\nThe four lectures 1. Photons - Corpuscles of Light In the first lecture, which acts as a gentle lead-in to the subject of quantum electrodynamics, Feynman describes the basic properties of photons. He discusses how to measure the probability that a photon will reflect or transmit through a partially reflective piece of glass. 2. Fits of Reflection and Transmission - Quantum Behaviour In the second lecture, Feynman looks at the different paths a photon can take as it travels from one point to another and how this affects phenomena like reflection and diffraction. 3. Electrons and Their interactions The third lecture describes quantum phenomena such as the famous double-slit experiment and Werner Heisenberg's uncertainty principle, thus describing the transmission and reflection of photons. It also introduces his famous \"Feynman diagrams\" and how quantum electrodynamics describes the interactions of subatomic particles. 4. New Queries",
                    "score": 0.8882015943527222
                },
                {
                    "id": 200651,
                    "contents": "Electron diffraction\nThen the relativistic velocity is given by the equation Substitution of the De Broglie equation to the above expression of energy gives which leads to the final expression for the relativistic wavelength The wavelength of the electrons in a 10 kV SEM is then 12.2 × 10−12 m (12.2 pm) while in a 200 kV TEM the wavelength is 2.5 pm. In comparison, the wavelength of X-rays usually used in X-ray diffraction is in the order of 100 pm (Cu Kα: λ=154 pm). Diffraction on atomic lattice Wavelength of the electron beam used in a typical electron microscope is sufficiently small, that crystal lattice acts as a diffraction grating. Therefore a diffraction pattern can be formed with beams diffracted under certain angles and intensities. Diffraction angles",
                    "score": 0.8877964019775391
                },
                {
                    "id": 7747990,
                    "contents": "Introduction to quantum mechanics\nFor centuries, scientists had debated between two possible theories of light: was it a wave or did it instead comprise a stream of tiny particles? By the 19th century, the debate was generally considered to have been settled in favor of the wave theory, as it was able to explain observed effects such as refraction, diffraction, interference, and polarization. James Clerk Maxwell had shown that electricity, magnetism, and light are all manifestations of the same phenomenon: the electromagnetic field. Maxwell's equations, which are the complete set of laws of classical electromagnetism, describe light as waves: a combination of oscillating electric and magnetic fields. Because of the preponderance of evidence in favor of the wave theory, Einstein's ideas were met initially with great skepticism. Eventually, however, the photon model became favored. One of the most significant pieces of evidence in its favor was its ability to explain several puzzling properties of the photoelectric",
                    "score": 0.8873182535171509
                },
                {
                    "id": 7421971,
                    "contents": "Extreme ultraviolet\nStrictly speaking, photoelectrons, Auger electrons and secondary electrons are all accompanied by positively charged holes (ions which can be neutralized by pulling electrons from nearby molecules) in order to preserve charge neutrality. An electron-hole pair is often referred to as an exciton. For highly energetic electrons, the electron-hole separation can be quite large and the binding energy is correspondingly low, but at lower energy, the electron and hole can be closer to each other. The exciton itself diffuses quite a large distance (>10 nm). As the name implies, an exciton is an excited state; only when it disappears as the electron and hole recombine, can stable chemical reaction products form.",
                    "score": 0.8871515989303589
                },
                {
                    "id": 1565315,
                    "contents": "Atomic orbital\nWith de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schrödinger equation treatment of hydrogen-like atoms, a Bohr electron \"wavelength\" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength. The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.",
                    "score": 0.8869803547859192
                },
                {
                    "id": 1696331,
                    "contents": "Electronvolt\nIt is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics, and high-energy astrophysics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion (109) electronvolts; it is equivalent to the GeV. Definition An electronvolt is the amount of kinetic energy gained or lost by a single electron accelerating from rest through an electric potential difference of one volt in vacuum. Hence, it has a value of one volt, , multiplied by the electron's elementary charge e, Therefore, one electronvolt is equal to The electronvolt, as opposed to the volt, is not an SI unit. The electronvolt (eV) is a unit of energy whereas the volt (V) is the derived SI unit of electric potential. The SI unit for energy is the joule (J).",
                    "score": 0.8869526386260986
                },
                {
                    "id": 154978,
                    "contents": "Rydberg formula\nAs stressed by Niels Bohr, expressing results in terms of wavenumber, not wavelength, was the key to Rydberg's discovery. The fundamental role of wavenumbers was also emphasized by the Rydberg-Ritz combination principle of 1908. The fundamental reason for this lies in quantum mechanics. Light's wavenumber is proportional to frequency , and therefore also proportional to light's quantum energy E. Thus, (in this formula the h represents Planck's constant). Modern understanding is that Rydberg's findings were a reflection of the underlying simplicity of the behavior of spectral lines, in terms of fixed (quantized) energy differences between electron orbitals in atoms. Rydberg's 1888 classical expression for the form of the spectral series was not accompanied by a physical explanation. Walther Ritz's pre-quantum 1908 explanation for the mechanism underlying the spectral series was that atomic electrons behaved like magnets and that the magnets could vibrate with respect to the atomic",
                    "score": 0.8866490125656128
                },
                {
                    "id": 1169136,
                    "contents": "Photoelectric effect\nThe experimental results disagree with classical electromagnetism, which predicts that continuous light waves transfer energy to electrons, which would then be emitted when they accumulate enough energy. An alteration in the intensity of light would theoretically change the kinetic energy of the emitted electrons, with sufficiently dim light resulting in a delayed emission. The experimental results instead show that electrons are dislodged only when the light exceeds a certain frequency—regardless of the light's intensity or duration of exposure. Because a low-frequency beam at a high intensity could not build up the energy required to produce photoelectrons, as it would have if light's energy were coming from a continuous wave, Albert Einstein proposed that a beam of light is not a wave propagating through space, but a swarm of discrete energy packets, known as photons.",
                    "score": 0.8865336179733276
                },
                {
                    "id": 14511009,
                    "contents": "Felix Ehrenhaft\nEven while controversy raged on sub-electronic charges, Ehrenhaft made important and substantial contributions to physics including the demonstration of photophoresis and other effects on the interaction of particles with light. Some of these effects have subsequently been explained in terms of existing phenomena, but some still remain poorly understood. He became professor of experimental physics at Vienna in 1920 and was known as a conscientious researcher and effective lecturer though single-minded to the point of absurdity. Albert Einstein was a frequent visitor to his home. Following the Anschluss in 1938, Ehrenhaft emigrated, first to England, then to the U.S. where he became a citizen.",
                    "score": 0.8862675428390503
                },
                {
                    "id": 15413671,
                    "contents": "Centre wavelength\nThe centre wavelength is the power-weighted mean wavelength: And the total power is: where is the power spectral density, for example in W/nm. The above integrals theoretically extend over the entire spectrum, however it is usually sufficient to perform the integral over the spectrum where the spectral density is higher than a fraction of its maximum. See also peak wavelength dominant wavelength Waves",
                    "score": 0.8860936760902405
                },
                {
                    "id": 16680993,
                    "contents": "Timeline of quantum mechanics\n1916 – To account for the Zeeman effect (1896), i.e. that atomic absorption or emission spectral lines change when the light source is subjected to a magnetic field, Arnold Sommerfeld suggests there might be \"elliptical orbits\" in atoms in addition to spherical orbits. 1918 – Sir Ernest Rutherford notices that, when alpha particles are shot into nitrogen gas, his scintillation detectors shows the signatures of hydrogen nuclei. Rutherford determines that the only place this hydrogen could have come from was the nitrogen, and therefore nitrogen must contain hydrogen nuclei. He thus suggests that the hydrogen nucleus, which is known to have an atomic number of 1, is an elementary particle, which he decides must be the protons hypothesized by Eugen Goldstein.",
                    "score": 0.8860561847686768
                },
                {
                    "id": 1106388,
                    "contents": "Timeline of electromagnetism and classical optics\n1838 – Michael Faraday uses Volta's battery to discover cathode rays. 1839 – Alexandre Edmond Becquerel observes the photoelectric effect with an electrode in a conductive solution exposed to light. 1840 – James Prescott Joule formulates Joule's Law (sometimes called the Joule-Lenz law) quantifying the amount of heat produced in a circuit as proportional to the product of the time duration, the resistance, and the square of the current passing through it. 1845 – Michael Faraday discovers that light propagation in a material can be influenced by external magnetic fields (Faraday effect) 1849 – Hippolyte Fizeau and Jean-Bernard Foucault measure the speed of light to be about 298,000 km/s",
                    "score": 0.8860281109809875
                },
                {
                    "id": 7421970,
                    "contents": "Extreme ultraviolet\nPoint of absorption: EUV photon energy = 92 eV, = Electron binding energy + photoelectron initial kinetic energy Within 3 mean free paths of photoelectron (1–2 nm): Reduction of photoelectron kinetic energy = ionization potential + secondary electron kinetic energy; Within 3 mean free paths of secondary electron (~30 nm): Reduction of secondary electron kinetic energy = ionization potential + tertiary electron kinetic energy mNth generation electron slows down aside from ionization by heating (phonon generation) Final generation electron kinetic energy ~ 0 eV => dissociative electron attachment + heat, where the ionization potential is typically 7–9 eV for organic materials and 4–5 eV for metals. The photoelectron subsequently causes the emission of secondary electrons through the process of impact ionization. Sometimes, an Auger transition is also possible, resulting in the emission of two electrons with the absorption of a single photon.",
                    "score": 0.8857753276824951
                },
                {
                    "id": 1694106,
                    "contents": "Electron\nThe German-born British physicist Arthur Schuster expanded upon Crookes's experiments by placing metal plates parallel to the cathode rays and applying an electric potential between the plates. The field deflected the rays toward the positively charged plate, providing further evidence that the rays carried negative charge. By measuring the amount of deflection for a given level of current, in 1890 Schuster was able to estimate the charge-to-mass ratio of the ray components. However, this produced a value that was more than a thousand times greater than what was expected, so little credence was given to his calculations at the time. This is because it was assumed that the charge carriers were much heavier hydrogen or nitrogen atoms. Schuster's estimates would subsequently turn out to be largely correct. In 1892 Hendrik Lorentz suggested that the mass of these particles (electrons) could be a consequence of their electric charge.",
                    "score": 0.88564532995224
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_1",
        "question": "The threshold wavelength for potassium metal is $564 \\mathrm{~nm}$. What is its work function? \r\n",
        "golden_answers": [
            " 3.52"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 18302694,
                    "contents": "LeRoy Apker\nApker found that in addition to visible radiation, near-ultraviolet radiation also produces photoconductitivty. Deeper into the ultraviolet spectrum, however, potassium iodide has a strong absorption line due to the formation of chargeless particles called excitons. These excitons transfer energy to the electrons in the F-Centers with remarkably high efficiency, and these excited electrons are excited from the crystals in exciton-induced photoemission. Apker observed the same sort of behavior in other crystals such as barium oxide. Legacy In 1978, Apker's wife and colleague Jean Dickey Apker established the LeRoy Apker Award of the American Physical Society in memory of Apker. The award is presented to two college undergraduates each year. Bibliography References 1915 births 1970 suicides Experimental physicists Scientists from Rochester, New York University of Rochester alumni Oliver E. Buckley Condensed Matter Prize winners Suicides by firearm in New York (state)",
                    "score": 0.8516349196434021
                },
                {
                    "id": 1160605,
                    "contents": "Potassium\nPotassium is a chemical element with the symbol K (from Neo-Latin kalium) and atomic number19. Potassium is a silvery-white metal that is soft enough to be cut with a knife with little force. Potassium metal reacts rapidly with atmospheric oxygen to form flaky white potassium peroxide in only seconds of exposure. It was first isolated from potash, the ashes of plants, from which its name derives. In the periodic table, potassium is one of the alkali metals, all of which have a single valence electron in the outer electron shell, that is easily removed to create an ion with a positive charge – a cation, that combines with anions to form salts. Potassium in nature occurs only in ionic salts. Elemental potassium reacts vigorously with water, generating sufficient heat to ignite hydrogen emitted in the reaction, and burning with a lilac-colored flame. It is found dissolved in sea water (which is 0.04% potassium by weight), and occurs in many minerals such as orthoclase, a common",
                    "score": 0.8501822352409363
                },
                {
                    "id": 1160611,
                    "contents": "Potassium\nProperties Physical Potassium is the second least dense metal after lithium. It is a soft solid with a low melting point, and can be easily cut with a knife. Freshly cut potassium is silvery in appearance, but it begins to tarnish toward gray immediately on exposure to air. In a flame test, potassium and its compounds emit a lilac color with a peak emission wavelength of 766.5 nanometers. Neutral potassium atoms have 19 electrons, one more than the configuration of the noble gas argon. Because of its low first ionization energy of 418.8kJ/mol, the potassium atom is much more likely to lose the last electron and acquire a positive charge, although negatively charged alkalide ions are not impossible. In contrast, the second ionization energy is very high (3052kJ/mol).",
                    "score": 0.8484677672386169
                },
                {
                    "id": 21321664,
                    "contents": "Potassium niobate\nPotassium niobate (KNbO3) is an inorganic compound with the formula KNbO3. A colorless solid, it is classified as a perovskite ferroelectric material. It exhibits nonlinear optical properties, and is a component of some lasers. Nanowires of potassium niobate have been used to produce tunable coherent light. Structure On cooling from high temperature, KNbO3 undergoes a series of structural phase transitions. At 435 °C, the crystal symmetry changes from cubic centrosymmetric (Pmm) to tetragonal non-centrosymmetric (P4mm). On further cooling, at 225 °C the crystal symmetry changes from tetragonal (P4mm) to orthorhombic (Amm2) and at −50 °C from orthorhombic (Amm2) to rhombohedral (R3m). Applications and research In addition to research in electronic memory storage, potassium niobate is used in resonant doubling. This technique allows small infrared lasers to convert output into blue light, a critical technology for the production of blue lasers and technology dependent upon them.",
                    "score": 0.8475852608680725
                },
                {
                    "id": 18302693,
                    "contents": "LeRoy Apker\nPotassium iodide Apker followed up his work on the photoelectric effect with an investigation of the photoelectric properties of the alkali halides, particularly potassium iodide. In potassium iodide, an ionic crystal, some iodide ions can be removed and their vacant places will be filled by electrons. Called \"F-Centers,\" these defects absorb visible and ultraviolet light, coloring the crystals at photon energies where they are usually transparent. Additionally, the absorption of visible radiation can free trapped electrons inside the crystal and produce photoconductivity.",
                    "score": 0.8437552452087402
                },
                {
                    "id": 1160655,
                    "contents": "Potassium\nAnother example is potassium cobaltinitrite, , which is used as artist's pigment under the name of Aureolin or Cobalt Yellow. The stable isotopes of potassium can be laser cooled and used to probe fundamental and technological problems in quantum physics. The two bosonic isotopes possess convenient Feshbach resonances to enable studies requiring tunable interactions, while 40K is one of only two stable fermions amongst the alkali metals. Laboratory uses An alloy of sodium and potassium, NaK is a liquid used as a heat-transfer medium and a desiccant for producing dry and air-free solvents. It can also be used in reactive distillation. The ternary alloy of 12% Na, 47% K and 41% Cs has the lowest melting point of −78°C of any metallic compound. Metallic potassium is used in several types of magnetometers. Precautions Potassium metal can react violently with water producing potassium hydroxide (KOH) and hydrogen gas. 2 K (s) + 2 (l) → 2 KOH (aq) + ↑ (g)",
                    "score": 0.842043936252594
                },
                {
                    "id": 1160614,
                    "contents": "Potassium\nOrganopotassium compounds illustrate nonionic compounds of potassium. They feature highly polar covalent K---C bonds. Examples include benzyl potassium. Potassium intercalates into graphite to give a variety of compounds, including KC8. Isotopes",
                    "score": 0.8403635621070862
                },
                {
                    "id": 188814,
                    "contents": "Theodore Maiman\nTheir work was stimulated by a 1958 paper by Arthur L. Schawlow and Charles H. Townes offering theoretical analysis and a proposal for a gaseous system using potassium vapor excited by a potassium lamp. However Maiman identified multiple flaws in the Schawlow-Townes proposal and the reason for their rejection of a solid-state design, including a significant difference in the band-gap nature of pink rubies and red rubies, and pursued his own vision: \"I was the only one that analyzed ruby in enough detail to have the confidence to stick with it.\" His successful design used synthetic pink ruby crystal grown by the Linde Division of Union Carbide as the active laser medium and a helical xenon flash lamp as the excitation source. As Townes later wrote, \"Maiman's laser had several aspects not considered in our theoretical paper, nor discussed by others before the ruby demonstration.\" One piece of evidence that convinced Maiman (and later the world) that he had lased pink ruby was that \"when",
                    "score": 0.8399716019630432
                },
                {
                    "id": 8827534,
                    "contents": "Franz–Keldysh effect\nSee also Quantum-confined Stark effect Notes References W. Franz, Einfluß eines elektrischen Feldes auf eine optische Absorptionskante, Z. Naturforschung 13a (1958) 484–489. L. V. Keldysh, Behaviour of Non-Metallic Crystals in Strong Electric Fields, J. Exptl. Theoret. Phys. (USSR) 33 (1957) 994–1003, translation: Soviet Physics JETP 6 (1958) 763–770. L. V. Keldysh, Ionization in the Field of a Strong Electromagnetic Wave, J. Exptl. Theoret. Phys. (USSR) 47 (1964) 1945–1957, translation: Soviet Physics JETP 20 (1965) 1307–1314. J. I. Pankove, Optical Processes in Semiconductors, Dover Publications Inc. New York (1971). H. Haug and S. W. Koch, \"Quantum Theory of the Optical and Electronic Properties of Semiconductors\", World Scientific (1994). C. Kittel, \"Introduction to Solid State Physics\", Wiley (1996). Optoelectronics Electronic engineering",
                    "score": 0.8388234972953796
                },
                {
                    "id": 25930685,
                    "contents": "Mark Thompson (chemist)\nResearch Thompson's multidisciplinary research focuses on solving problems related to energy inefficiency of existing light-generating sources. His research is primarily focused on organic light-emitting diodes, organic photovoltaics and device interfaces.",
                    "score": 0.837648868560791
                },
                {
                    "id": 9748926,
                    "contents": "Karl Baedeker (scientist)\nKarl Wilhelm Sali Baedeker (3 February 1877 – 6 August 1914) was a German physicist, and a professor at the University of Jena. He was the grandson of Karl Baedeker, the founder of the eponymous travel guide publishing house, and the son of Fritz Baedeker (1844 - 1925), who ran the same company from 1869 until his death in 1925. One of his scientific discoveries was that the resistivity of cuprous iodide (CuI) depended on its stoichiometry. Thin films of the material became much more conductive when exposed to iodine vapor; the effect was reversible. This was the first example of doping a semiconductor to change its properties and also the discovery of transparent conductive materials.",
                    "score": 0.8373456597328186
                },
                {
                    "id": 21321665,
                    "contents": "Potassium niobate\nPotassium niobate has been found useful in many different areas of materials science, including properties of lasers, quantum teleportation, and it has been used to study the optical properties of particulate composite materials. Safety The for potassium niobate is 3000 mg/kg (oral, rat). References Potassium compounds Niobates Nonlinear optical materials Ferroelectric materials Perovskites Piezoelectric materials",
                    "score": 0.8369771838188171
                },
                {
                    "id": 10411461,
                    "contents": "Harry Kroger\nHarry Kroger is an American physicist and electrical engineer. He used to be a Bartle professor of electrical engineering at Binghamton University, a part of the State University of New York (SUNY) system. He has been a member of the Institute of Electrical and Electronics Engineers (IEEE) since 1964 and became a Life Fellow of the IEEE in 2001. He has now retired to Florida. Family Dr. Kroger is married to Mrs. Sandra Vought Kroger for over 50 years now. They have three children; Charles Kroger (b 1960), John Kroger (b 1962), and Carolyn Kroger Estes (b 1964). He has ten grandchildren and one great grandchild. Education and career Dr. Kroger received his B.S. degree from the University of Rochester in 1957 and the Ph.D. degree from Cornell University in 1962. Both of his degrees are in physics. The title of his doctoral dissertation was \"Photon absorption by valence electrons in magnesium, chromium, iron and cobalt\".",
                    "score": 0.8349635601043701
                },
                {
                    "id": 1106216,
                    "contents": "Heike Kamerlingh Onnes\nSuperconductivity In 1911 Kamerlingh Onnes measured the electrical conductivity of pure metals (mercury, and later tin and lead) at very low temperatures. Some scientists, such as William Thomson (Lord Kelvin), believed that electrons flowing through a conductor would come to a complete halt or, in other words, metal resistivity would become infinitely large at absolute zero. Others, including Kamerlingh Onnes, felt that a conductor's electrical resistance would steadily decrease and drop to nil. Augustus Matthiessen said that when the temperature decreases, the metal conductivity usually improves or in other words, the electrical resistivity usually decreases with a decrease of temperature.",
                    "score": 0.834446907043457
                },
                {
                    "id": 10822180,
                    "contents": "Gordon Kidd Teal\nGordon Kidd Teal (January 10, 1907 – January 7, 2003) was an American engineer. He invented a method of applying the Czochralski method to produce extremely pure germanium single crystals used in making greatly improved transistors. He, together with Morgan Sparks, invented a modification of the process that produced the configuration necessary for the fabrication of bipolar junction transistors. He is most remembered for developing the first silicon transistor while at Texas Instruments.",
                    "score": 0.8318256139755249
                },
                {
                    "id": 6751385,
                    "contents": "Potassium titanyl phosphate\nOperational aspects Crystals of KTP are highly transparent for wavelengths between 350–2700 nm with a reduced transmission out to 4500 nm where the crystal is effectively opaque. Its second-harmonic generation (SHG) coefficient is about three times higher than KDP. It has a Mohs hardness of about 5. KTP is also used as an optical parametric oscillator for near IR generation up to 4 µm. It is particularly suited to high power operation as an optical parametric oscillator due to its high damage threshold and large crystal aperture. The high degree of birefringent walk-off between the pump signal and idler beams present in this material limit its use as an optical parametric oscillator for very low power applications.",
                    "score": 0.8308194875717163
                },
                {
                    "id": 3303178,
                    "contents": "Klaus von Klitzing\nResearch and career During his career Klitzing has worked at the Clarendon Laboratory at the University of Oxford and the Grenoble High Magnetic Field Laboratory in France (now LNCMI), where he continued to work until becoming a professor at the Technical University of Munich in 1980. He has been a director of the Max Planck Institute for Solid State Research in Stuttgart since 1985. The von Klitzing constant, RK = h/e2 = is named in honor of Klaus von Klitzing's discovery of the quantum Hall effect, and is listed in the National Institute of Standards and Technology Reference on Constants, Units, and Uncertainty. The inverse of the constant is equal to half the value of the conductance quantum. More recently, Klitzing's research focuses on the properties of low-dimensional electronic systems, typically in low temperatures and in high magnetic fields. Honours and awards Von Klitzing has won numerous awards and honours including:",
                    "score": 0.8306298851966858
                },
                {
                    "id": 26593088,
                    "contents": "Kagome metal\nIn the Kagome structure, atoms are arranged into layered sets of overlapping triangles so that there exist large empty hexagonal spaces. Electrons in the metal experience a \"three-dimensional cousin of the quantum Hall effect\". The inherent magnetism of the metal and the quantum-mechanical magnetism induce electrons to flow around the edges of the triangular crystals, akin to superconductivity. Unlike superconductivity, this structure and behavior is stable at room temperature. Other structures were shown to exhibit the quantum hall effect at very low temperatures with an external magnetic field as high as 1 million times the strength that of the earth. By building metal out of a ferromagnetic material, that exterior magnetic field was no longer necessary, and the quantum Hall effect persists into room temperature.",
                    "score": 0.8299919366836548
                },
                {
                    "id": 1797086,
                    "contents": "John Bardeen\nOther awards In addition to being awarded the Nobel prize twice, Bardeen has numerous other awards including: 1952 Franklin Institute's Stuart Ballantine Medal. 1959 elected a Fellow of the American Academy of Arts and Sciences 1965 National Medal of Science. 1971 IEEE Medal of Honor for \"his profound contributions to the understanding of the conductivity of solids, to the invention of the transistor, and to the microscopic theory of superconductivity.\" Elected a Foreign Member of the Royal Society (ForMemRS) in 1973. 1975 Franklin Medal. On January 10, 1977, John Bardeen was presented with the Presidential Medal of Freedom by President Gerald Ford. He was represented at the ceremony by his son, William Bardeen. Bardeen was one of 11 recipients given the Third Century Award from President George H. W. Bush in 1990 for \"exceptional contributions to American society\" and was granted a gold medal from the Soviet Academy of Sciences in 1988.",
                    "score": 0.829858124256134
                },
                {
                    "id": 1749750,
                    "contents": "Gilbert N. Lewis\nIn 1926, he coined the term \"photon\" for the smallest unit of radiant energy (light). Actually, the outcome of his letter to Nature was not what he had intended. In the letter, he proposed a photon being a structural element, not energy. He insisted on the need for a new variable, the number of photons. Although his theory differed from the quantum theory of light introduced by Albert Einstein in 1905, his name was adopted for what Einstein had called a light quantum (Lichtquant in German). Other achievements In 1921, Lewis was the first to propose an empirical equation describing the failure of strong electrolytes to obey the law of mass action, a problem that had perplexed physical chemists for twenty years. His empirical equations for what he called ionic strength were later confirmed to be in accord with the Debye–Hückel equation for strong electrolytes, published in 1923.",
                    "score": 0.8295294642448425
                },
                {
                    "id": 17051272,
                    "contents": "Roberto Merlin\nIn 1985 he was promoted to associate professor, and then professor in 1989. From 1993 to 1996, Merlin served as Associate Chair for Research and Facilities of the Department of Physics. In 2000, he received a joint appointment to the Department of Electrical Engineering and Computer Science. He is now the director of the Optical Physics Interdisciplinary Laboratory. Merlin is a fellow of the American Physical Society, the Optical Society of America, the von Humboldt Foundation, and the John Simon Guggenheim Memorial Foundation. In 2006 he received the Frank Isakson Prize for Optical Effects in Solids from the American Physical Society. Merlin does a variety of interdisciplinary work, mostly related to condensed matter physics. He has done research on Raman spectroscopy, rare-earth magnet semiconductors, superconductors, superlattices, ultrafast lasers, intercalated graphite, and negative refraction. Selected publications",
                    "score": 0.8286120891571045
                },
                {
                    "id": 15370254,
                    "contents": "II-VI\nII-VI may refer to: II-VI Incorporated, a manufacturer of optical materials and semiconductors II-VI semiconductor compound, a material composed of a metal from either group 2 or 12 of the periodic table and a nonmetal from group 16",
                    "score": 0.826158344745636
                },
                {
                    "id": 11232202,
                    "contents": "Thallium halides\nThe thallium halides include monohalides, where thallium has oxidation state +1, trihalides in which thallium generally has oxidation state +3, and some intermediate halides containing thallium with mixed +1 and +3 oxidation states. These materials find use in specialized optical settings, such as focusing elements in research spectrophotometers. Compared to the more common zinc selenide-based optics, materials such as thallium bromoiodide enable transmission at longer wavelengths. In the infrared, this allows for measurements as low as 350 cm−1 (28 μm), whereas zinc selenide is opaque by 21.5 μm, and ZnSe optics are generally only usable to 650 cm−1 (15 μm). Monohalides The monohalides all contain thallium with oxidation state +1. Parallels can be drawn between the thallium(I) halides and their corresponding silver salts; for example, thallium(I) chloride and bromide are light-sensitive, and thallium(I) fluoride is more soluble in water than the chloride and bromide.",
                    "score": 0.8261485695838928
                },
                {
                    "id": 1201116,
                    "contents": "Rubidium\nOther potential or current uses of rubidium include a working fluid in vapor turbines, as a getter in vacuum tubes, and as a photocell component. Rubidium is also used as an ingredient in special types of glass, in the production of superoxide by burning in oxygen, in the study of potassium ion channels in biology, and as the vapor in atomic magnetometers. In particular, 87Rb is used with other alkali metals in the development of spin-exchange relaxation-free (SERF) magnetometers.",
                    "score": 0.8254052996635437
                },
                {
                    "id": 699043,
                    "contents": "Electrical resistivity and conductivity\nAt high metal temperatures, the Wiedemann–Franz law holds: where is the thermal conductivity of the metal, is Boltzmann's constant, is the electron charge, is temperature, and is the electrical conductivity coefficient. Semiconductors In general, intrinsic semiconductor resistivity decreases with increasing temperature. The electrons are bumped to the conduction energy band by thermal energy, where they flow freely, and in doing so leave behind holes in the valence band, which also flow freely. The electric resistance of a typical intrinsic (non doped) semiconductor decreases exponentially with temperature: An even better approximation of the temperature dependence of the resistivity of a semiconductor is given by the Steinhart–Hart equation: where , and are the so-called Steinhart–Hart coefficients. This equation is used to calibrate thermistors.",
                    "score": 0.8250261545181274
                },
                {
                    "id": 26593091,
                    "contents": "Kagome metal\nThe photo-electronic structure of this metal was mapped at Lawrence Berkeley National Lab's Advanced Light Source beamlines 7.0.2 MAESTRO and 4.0.3 MERLIN. The measurements taken here mapped the band structures of the metal under a current and showed “the double-Dirac-cone structure corresponding to Dirac Fermions”. A 30 meV gap between cones was shown, which is indicative of the Hall effect and massive Dirac fermions. The experiment was published in Nature on March 19, 2018.",
                    "score": 0.8247900009155273
                },
                {
                    "id": 28284219,
                    "contents": "Gilles Holst\nGilles Holst (20 March 1886 – 11 October 1968) was a Dutch physicist, known worldwide for his invention in 1932 of the low-pressure sodium lamp. Early life His father was a manager of a shipyard. In 1904 he went to ETH Zurich to study mechanical engineering, changing after a year to mathematics and physics. Career He worked with Balthasar van der Pol, known for the Van der Pol oscillator, and Frans Michel Penning, known for Penning ionization and the Penning mixture. In 1908 he became a geprüfter Fachlehrer, or qualified teacher. And most important, he became the science director of the Philips Physics Laboratory in Eindhoven. In 1909 he became an assistant to Heike Kamerlingh Onnes at Leiden University. At Leiden, it is believed that he was the first to witness the phenomenon of superconductivity. The Gilles Holst Award was first awarded in 1939. Personal life He died in Holland at the age of 82. References External links Holst Centre",
                    "score": 0.8247105479240417
                },
                {
                    "id": 2051948,
                    "contents": "Thomas Johann Seebeck\nOther achievements In 1808, Seebeck was first to produce and describe the amalgam of potassium. In 1810, he observed the magnetic properties of nickel and cobalt. In 1818, Seebeck discovered the optical activity of the solutions of sugar. See also List of Baltic German scientists References Further reading Magie, W. M. (1963). A Source Book in Physics. Harvard: Cambridge MA. pp. 461–464. Partial translation of Seebeck's \"Magnetische Polarisation der Metalle und Erze durch Temperatur-Differenz.\" External links A Biography of Seebeck, includes references 1770 births 1831 deaths People from Tallinn People from the Governorate of Estonia Baltic-German people 19th-century German physicists University of Göttingen alumni Members of the Prussian Academy of Sciences",
                    "score": 0.8239242434501648
                },
                {
                    "id": 11502631,
                    "contents": "Willard Gibbs Award\nNicholas Turro 2000 \"For pioneering and interdisciplinary research on the interaction of light and organic molecules, for the invention of novel and general methods for investigation of organic reactions of supra molecular systems, and for the development of organic systems whose reactivity is extremely sensitive to the application of weak magnetic fields.\" Tobin J. Marks 2001 \"For highly original research that has had a major, lasting impact on important areas of chemical science, including f-element coordination and organometallic chemistry, homogeneous small molecule and polymerization catalysis, molecule-based photonics materials, low-dimensional electronic conductors, oxide chemical vapor deposition, high temperature superconductors, and metallocene anti-tumor agents.\"",
                    "score": 0.8235756158828735
                },
                {
                    "id": 2051945,
                    "contents": "Thomas Johann Seebeck\nAfter the discovery of the electron and its fundamental charge, it was quickly realized that Seebeck's effect was an electric current that is induced, which by Ampere's law deflects the magnet. More specifically, the temperature difference produces an electric potential (voltage) which can drive an electric current in a closed circuit. Today, this effect is known as the Peltier–Seebeck effect. The voltage produced is proportional to the temperature difference between the two junctions. The proportionality constant (a) is known as the Seebeck coefficient, and often referred to as the thermoelectric power or thermopower. The Seebeck voltage does not depend on the distribution of temperature along the metals between the junctions. This effect is the physical basis for a thermocouple, which is used often for temperature measurement.",
                    "score": 0.8234030604362488
                },
                {
                    "id": 19909623,
                    "contents": "John N. Shive\nJohn Northrup Shive (February 22, 1913 – June 1, 1984) was an American physicist and inventor. He made notable contributions in electronic engineering and solid-state physics during the early days of transistor development at Bell Laboratories. In particular, he produced experimental evidence that holes could diffuse through bulk germanium, and not just along the surface as previously thought. This paved the way from Bardeen and Brattain's point-contact transistor to Shockley's more-robust junction transistor. Shive is best known for inventing the phototransistor in 1948 (a device that combines the sensitivity to light of a photodiode and the current gain of a transistor), and for the Shive wave machine in 1959 (an educational apparatus used to illustrate wave motion).",
                    "score": 0.8233251571655273
                },
                {
                    "id": 29789992,
                    "contents": "Richard Allan Ferrell\nAs postdocs in 1956, Michael Tinkham and Rolf Eldridge Glover III demonstrated \"the existence of a superconducting \"energy gap\" by showing that light below a certain frequency was transmitted much more readily through a superconducting film than through a normal metal film. This counter-intuitive result was a landmark confirmation of the famous Bardeen-Cooper-Schrieffer theory of superconductivity.\" Subsequently, Glover and Tinkham collaborated with Ferrell on the theory of their discovery. Selected publications (over 500 citations) (over 500 citations) (over 450 citations) (over 850 citations) (over 3750 citations) 1967 (over 550 citations) References 1926 births 2005 deaths 20th-century American physicists 21st-century American physicists Condensed matter physicists Theoretical physicists People from Santa Ana, California California Institute of Technology alumni Princeton University alumni University System of Maryland faculty",
                    "score": 0.8231741189956665
                },
                {
                    "id": 25291565,
                    "contents": "Wilhelm Klemm\nIn 1936, Wilhelm Klemm and Anna Neuber published research on the magnetic properties of triphenylchromium compounds. Their magnetic susceptibility (approx. 1.73 Bohr magnetons) was found to be inconsistent with the structure determination proposed by Franz Hein for penta-, tetra- and triphenylchromium compounds. In 1934, Wilhelm Klemm and Heinrich Bommer were the first to achieve pure erbium, by heating erbium chloride with potassium. In 1936, Wilhelm Klemm and Heinrich Bommer were the first to isolate elemental ytterbium by reducing ytterbium (III) chloride with potassium at 250 °C. They also determined the crystal structure and magnetic properties of the metal. Klemm's work on transition metal oxides, fluorides and lanthanides was interrupted in 1939 by World War II.",
                    "score": 0.8230767846107483
                },
                {
                    "id": 27371640,
                    "contents": "Rubin Braunstein\nIn 1964 Braunstein became a professor of physics at University of California, Los Angeles (UCLA), where he remained for the rest of his career. His research there continued his RCA work with optoelectronic properties of semiconductors as well as contributions related to the optical properties of highly transparent materials such as tungstate glasses. Some of Braunstein's work was theoretical, including the proposal that neutral atoms could be scattered by a sufficiently intense standing wave of light. Since light is an electromagnetic wave, it had long been known that charged particles like electrons would be scattered. The effect with neutral atoms is much weaker, but was finally observed nearly 20 years after the proposal of Braunstein and his co-authors. Braunstein was selected as a Fellow of the American Physical Society in 1964. See also Light-emitting diode#History List of Syracuse University people References Further reading",
                    "score": 0.8224910497665405
                },
                {
                    "id": 1278292,
                    "contents": "Thallium\nOptics Thallium(I) bromide and thallium(I) iodide crystals have been used as infrared optical materials, because they are harder than other common infrared optics, and because they have transmission at significantly longer wavelengths. The trade name KRS-5 refers to this material. Thallium(I) oxide has been used to manufacture glasses that have a high index of refraction. Combined with sulfur or selenium and arsenic, thallium has been used in the production of high-density glasses that have low melting points in the range of 125 and 150 °C. These glasses have room temperature properties that are similar to ordinary glasses and are durable, insoluble in water and have unique refractive indices. Electronics",
                    "score": 0.8221571445465088
                },
                {
                    "id": 5497943,
                    "contents": "Lene Hau\nLene Vestergaard Hau, Quantum Optics: Slowing single photons Brian Murphy and Lene Vestergaard Hau, Electro-optical nanotraps for neutral atoms, Lene Vestergaard Hau, Optical information processing in Bose–Einstein condensates, Lene Vestergaard Hau, Quantum physics – Tangled memories, Lene Vestergaard Hau, Nonlinear optics: Shocking superfluids, Christopher Slowe, Laurent Vernac, Lene Vestergaard Hau, A High Flux Source of Cold Rubidium Christopher Slowe, Naomi S. Ginsberg, Trygve Ristroph, Anne Goodsell, and Lene Vestergaard Hau, Ultraslow Light & Bose–Einstein Condensates:Two-way Control with Coherent Light & Atom Fields Marin Soljacic, Elefterios Lidorikis, J. D. Joannopoulos, Lene Vestergaard Hau, Ultra Low-Power All-Optical Switching Trygve Ristroph, Anne Goodsell, J. A. Golovchenko, and Lene Vestergaard Hau, Detection and quantized conductance of neutral atoms near a charged carbon nanotube",
                    "score": 0.8214542865753174
                },
                {
                    "id": 29751136,
                    "contents": "Urbach energy\nHistory and name The Urbach Energy is defined by an exponential increase in absorbance with energy. While an exponential dependence of absorbance had been observed previously in photographic materials, it was Franz Urbach that evaluated this property systematically in crystals. He used silver bromide for his study while working at the Kodak Company in 1953. Definition Absorption in semiconductors is known to increase exponentially near the onset of absorption, spanning several orders of magnitude. Absorption as a function of energy can be described by the following equation: where and are fitting parameters with dimensions of inverse length and energy, respectively, and is the Urbach Energy. This equation is only valid when . The Urbach Energy is temperature-dependent. Room temperature values of for hydrogenated amorphous silicon are typically between 50 meV and 150 meV.",
                    "score": 0.8211504817008972
                },
                {
                    "id": 16858681,
                    "contents": "List of German inventors and discoverers\nWolfgang Ketterle: German-American physicist who developed an \"atom laser\", amongst other breakthroughs. Nobel laureate 2001. Erhard Kietz: Pioneer discoverer of video technology. Gustav Kirchhoff: Discovery of the principles upon which spectroscopy is founded. He contributed to the fundamental understanding of electrical circuits, spectroscopy, and the emission of black-body radiation by heated objects. He coined the term \"black body\" radiation in 1862, and two different sets of concepts (one in circuit theory, and one in spectroscopy) are named \"Kirchhoff's laws\" after him; there is also a Kirchhoff's Law in thermochemistry. The Bunsen–Kirchhoff Award for spectroscopy is named after him and his colleague, Robert Bunsen, who both invented the spectrometer in 1859. Martin Heinrich Klaproth: Discovered the element Uranium. Klaus von Klitzing: Physicist, known for discovery of the integer quantum Hall effect, 1985 Nobel Prize in Physics.",
                    "score": 0.8210381865501404
                },
                {
                    "id": 29751138,
                    "contents": "Urbach energy\nMeasurement To evaluate the Urbach Energy, the absorption coefficient needs to be measured over several orders of magnitude. For this reason, high precision techniques such as the constant photocurrent method (CPM) or photothermal deflection spectroscopy are used. References Materials science Semiconductors",
                    "score": 0.8207617402076721
                },
                {
                    "id": 1739252,
                    "contents": "Gustav Kirchhoff\nKirchhoff formulated his circuit laws, which are now ubiquitous in electrical engineering, in 1845, while still a student. He completed this study as a seminar exercise; it later became his doctoral dissertation. He was called to the University of Heidelberg in 1854, where he collaborated in spectroscopic work with Robert Bunsen. In 1857 he calculated that an electric signal in a resistanceless wire travels along the wire at the speed of light. He proposed his law of thermal radiation in 1859, and gave a proof in 1861. Together Kirchhoff and Bunsen invented the spectroscope, which Kirchhoff used to pioneer the identification of the elements in the Sun, showing in 1859 that the Sun contains sodium. He and Bunsen discovered caesium and rubidium in 1861. At Heidelberg he ran a mathematico-physical seminar, modelled on Franz Ernst Neumann's, with the mathematician Leo Koenigsberger. Among those who attended this seminar were Arthur Schuster and Sofia Kovalevskaya.",
                    "score": 0.8205993175506592
                },
                {
                    "id": 1852761,
                    "contents": "Metal\nThe elemental metals have electrical conductivity values of from 6.9 × 103 S/cm for manganese to 6.3 × 105 S/cm for silver. In contrast, a semiconducting metalloid such as boron has an electrical conductivity 1.5 × 10−6 S/cm. With one exception, metallic elements reduce their electrical conductivity when heated. Plutonium increases its electrical conductivity when heated in the temperature range of around −175 to +125 °C. Metals are relatively good conductors of heat. The electrons in a metal's electron cloud are highly mobile and easily able to pass on heat-induced vibrational energy.",
                    "score": 0.8203181624412537
                },
                {
                    "id": 16525214,
                    "contents": "List of German inventions and discoveries\n1958: Discovery of the Mössbauer effect by Rudolf Mössbauer 1959: Penning trap by Hans Georg Dehmelt 1961: Bark scale by Eberhard Zwicker 1963: Proposition of heterojunction by Herbert Kroemer 1980: Quantum Hall effect by Klaus von Klitzing 1980s: Atomic force microscope and the scanning tunneling microscope by Gerd Binnig 1988: Discovery of giant magnetoresistance by Peter Grünberg 1994: STED microscopy by Stefan Hell and Jan Wichmann 1998: Frequency comb by Theodor W. Hänsch",
                    "score": 0.8201708793640137
                },
                {
                    "id": 27210422,
                    "contents": "S. C. Jain\nDuring his doctoral studies, Jain assisted his mentor, K. S. Krishnan, the co-discoverer of the Raman Effect, on the thermal conductivity of solids. It was during this time, the duo developed a methodology for the measurement of thermal conductivity in solids at high temperatures which was published by them in an article, Thermionic Constants of Metals and Semiconductors. II. Metals of the First Transition Group in 1952 in Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences and later explained further by way of another article, Determination of thermal conductivities at high temperatures in British Journal of Applied Physics in 1954. The measurement protocol later came to be known as the \"Jain-Krishnan Method\". Polar crystals, thin films and semiconductor devices are some of the other areas he worked on and he was credited with developing experimental techniques in these disciplines which are in use around the world. He also contributed to the",
                    "score": 0.8200525045394897
                },
                {
                    "id": 18302689,
                    "contents": "LeRoy Apker\nLeRoy W. Apker (June 11, 1915 – July 5, 1970) was an American experimental physicist. Along with his colleagues E. A. Taft and Jean Dickey, he studied the photoelectric emission of electrons from semiconductors and discovered the phenomenon of exciton-induced photoemission in potassium iodide. In 1955, he received the Oliver E. Buckley Condensed Matter Prize of the American Physical Society for his work.",
                    "score": 0.8200482130050659
                },
                {
                    "id": 17011590,
                    "contents": "Keith Schwab\nKeith Schwab (born May 18, 1968) is an American physicist born in St. Louis, Missouri. His contributions are in the areas of nanoscience, ultra-low temperature physics, and quantum effects.",
                    "score": 0.8195379972457886
                },
                {
                    "id": 10833317,
                    "contents": "John Hall Gladstone\nOptical phenomena and the properties of elements and compounds in relation to light have always been a major interest for Gladstone. This comes out quite early in his career, and in a variety of forms. Thus in 1854 he lectured at the Royal Institution on \"Chromatic Phenomena exhibited by Transmitted Light.\" In 1855 there were \"Notes on some substances which exhibit the Phenomena of Fluorescence\", and in 1856 on \"Some Dichromatic Phenomena among Solutions\". In 1858 he drew attention to the use of the prism in qualitative analysis (Quart. Journ. Chem. Soc., 1O, 79), and discovered distinct lines in the absorption spectrum of didymium, a substance long afterwards resolved by Auer von Welsbach into the two elements known as praseodymium and neodymium. A little later he studied the absorption spectrum of the atmosphere, and found that the Fraunhofer lines varied according to the time of day, and that the change must be due to some constituents of the earth's atmosphere. In this research he",
                    "score": 0.8195065855979919
                },
                {
                    "id": 1979281,
                    "contents": "John Tyndall\nTyndall, J. (1870), Notes of a Course of Nine Lectures on Light (80 pages) Tyndall, J. (1870), Notes of a Course of Seven Lectures on Electrical Phenomena and Theories (50 pages) Tyndall, J. (1870), Researches on diamagnetism and magne-crystallic action: including the question of diamagnetic polarity, (a compilation of 1850s research reports), Longmans, Green, London Tyndall, J. (1871), Hours of exercise in the Alps, Longmans, Green, and Co., London Tyndall, J. (1871), Fragments of Science: A Series of Detached Essays, Lectures, and Reviews, (1872 edition), Longmans, Green, London Tyndall, J. (1872), Contributions to Molecular Physics in the Domain of Radiant Heat, (a compilation of 1860s research reports), (1873 edition), D. Appleton and Company, New York Tyndall, J. (1873), The forms of water in clouds & rivers, ice & glaciers, H. S. King & Co., London Tyndall, J. (1873), Six Lectures on Light (290 pages)",
                    "score": 0.8191896080970764
                },
                {
                    "id": 8612295,
                    "contents": "Potassium aluminium borate\nPotassium aluminium borate (K2Al2B2O7) is an ionic compound composed of potassium ions, aluminium ions, and borate ions. Its crystal form exhibits nonlinear optical properties. The ultraviolet beam at 266 nm can be obtained by fourth harmonic generation (FGH) of 1064 nm Nd:YAG laser radiation through a nonlinear crystal K2Al2B2O7 (KABO). References Nonlinear optical materials Borates Potassium compounds Aluminium compounds Crystals",
                    "score": 0.8191807866096497
                },
                {
                    "id": 28284220,
                    "contents": "Gilles Holst\nThe Gilles Holst Award was first awarded in 1939. Personal life He died in Holland at the age of 82. References External links Holst Centre 1886 births 1968 deaths 20th-century Dutch physicists ETH Zurich alumni Leiden University faculty Members of the Royal Netherlands Academy of Arts and Sciences Superconductivity",
                    "score": 0.8189833760261536
                },
                {
                    "id": 9924270,
                    "contents": "Lothar Wolfgang Nordheim\nLothar Wolfgang Nordheim (November 7, 1899, Munich – October 5, 1985, La Jolla, California) was a German born Jewish American theoretical physicist. He was a pioneer in the applications of quantum mechanics to solid-state problems, such as thermionic emission, work function of metals, field electron emission, rectification in metal-semiconductor contacts and electrical resistance in metals and alloys. He also worked in the mathematical foundations of quantum mechanics, cosmic rays and in nuclear physics. Life He obtained his PhD in 1923, under the supervision of Max Born in the University of Göttingen. He also worked with Edward Teller on the muon, sparkling his interest in cosmic rays. As a \"physical assistant\" to David Hilbert (like his teacher Born before him), he worked with him John von Neumann and Eugene Wigner on the mathematical formulation of quantum mechanics in 1928.",
                    "score": 0.8188019394874573
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_2",
        "question": "Evaluate the series\r\n$$\r\nS=\\sum_{n=0}^{\\infty} \\frac{1}{3^n}\r\n$$",
        "golden_answers": [
            " 3 / 2"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 12548116,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nSince the sum of an infinite series is defined as the limit of its partial sums, Notes References Page images at HTML with figures and commentary at Geometric series Proof without words",
                    "score": 0.9262202978134155
                },
                {
                    "id": 11678088,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nAfter Christian Wolff read Leibniz's treatment of Grandi's series in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Briefly, if one expresses a partial sum of this series as a function of the penultimate term, one obtains either or . The mean of these values is , and assuming that at infinity yields as the value of the series. Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. The arithmetic means of neighboring partial sums do not converge to any particular value, and for all finite cases one has , not . Generally, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series. Leibniz counsels Wolff to reconsider so that he \"might produce something worthy of science and himself.\" Modern methods",
                    "score": 0.9237600564956665
                },
                {
                    "id": 11678087,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nLeibniz did not quite assert that the series had a sum, but he did infer an association with following Mercator's method. The attitude that a series could equal some finite quantity without actually adding up to it as a sum would be commonplace in the 18th century, although no distinction is made in modern mathematics.",
                    "score": 0.9223804473876953
                },
                {
                    "id": 17472486,
                    "contents": "List of mathematical series\nThis list of mathematical series contains formulae for finite and infinite sums. It can be used in conjunction with other tools for evaluating sums. Here, is taken to have the value denotes the fractional part of is a Bernoulli polynomial. is a Bernoulli number, and here, is an Euler number. is the Riemann zeta function. is the gamma function. is a polygamma function. is a polylogarithm. is binomial coefficient denotes exponential of Sums of powers See Faulhaber's formula. The first few values are: See zeta constants. The first few values are: (the Basel problem) Power series Low-order polylogarithms Finite sums: , (geometric series) Infinite sums, valid for (see polylogarithm): The following is a useful property to calculate low-integer-order polylogarithms recursively in closed form: Exponential function (cf. mean of Poisson distribution) (cf. second moment of Poisson distribution) where is the Touchard polynomials.",
                    "score": 0.9198772311210632
                },
                {
                    "id": 5898948,
                    "contents": "Convergent series\nIn mathematics, a series is the sum of the terms of an infinite sequence of numbers. More precisely, an infinite sequence defines a series that is denoted The th partial sum is the sum of the first terms of the sequence; that is, A series is convergent (or converges) if the sequence of its partial sums tends to a limit; that means that, when adding one after the other in the order given by the indices, one gets partial sums that become closer and closer to a given number. More precisely, a series converges, if there exists a number such that for every arbitrarily small positive number , there is a (sufficiently large) integer such that for all , If the series is convergent, the (necessarily unique) number is called the sum of the series. The same notation",
                    "score": 0.9196469783782959
                },
                {
                    "id": 14058441,
                    "contents": "Florence Marie Mears\nMears specialized in the findings of definitions and values assigned to various infinite series of numbers. An \"infinite series\" is an endless series of numbers, each succeeding the other that is a certain amount lesser or greater than the proceeding one. An example set of an infinite series includes is 1 + ½ + ¼ etc. in which the definition of the series can be defined as the number two. As a result, Mears created several theorems about these definitions, many of which provided truth for many practicing mathematicians, engineers, chemists, physicists, and astronomers. One of her most popular theorems, called the Norlund Mean can be explained through absolute regularity, the summability of Cauchy products, and inverse properties.",
                    "score": 0.9195469617843628
                },
                {
                    "id": 13740952,
                    "contents": "Sums of powers\nThe sum of the terms in the geometric series is",
                    "score": 0.9191503524780273
                },
                {
                    "id": 20560519,
                    "contents": "Isidore Isaac Hirschman Jr.\nSelected publications Articles } Books Hirschman, I. (1962). Infinite Series. New York: Holt, Rinehart & Winston. – A textbook for advanced undergraduate and graduate mathematics. Hirschman, Isidore Isaac; Widder, David Vernon (1955). The Convolution Transform. New York: Princeton University Press; now available from Dover Publications. References http://mathdl.maa.org/mathDL/46/?pa=content&sa=viewDocument&nodeId=3801&bodyId=4189 20th-century American mathematicians Harvard University alumni Washington University in St. Louis faculty Washington University in St. Louis mathematicians",
                    "score": 0.9190704822540283
                },
                {
                    "id": 10313842,
                    "contents": "Summation of Grandi's series\nThe Cesàro sum of 1 + 0 − 1 + 1 + 0 − 1 + · · · is 2⁄3. So the Cesàro sum of a series can be altered by inserting infinitely many 0s as well as infinitely many brackets. The series can also be summed by the more general fractional (C, a) methods. Abel sum Abel summation is similar to Euler's attempted definition of sums of divergent series, but it avoids Callet's and N. Bernoulli's objections by precisely constructing the function to use. In fact, Euler likely meant to limit his definition to power series, and in practice he used it almost exclusively in a form now known as Abel's method. Given a series a0 + a1 + a2 + · · ·, one forms a new series a0 + a1x + a2x2 + · · ·. If the latter series converges for 0 < x < 1 to a function with a limit as x tends to 1, then this limit is called the Abel sum of the original series, after Abel's theorem which guarantees that the procedure is consistent with ordinary summation. For Grandi's series one has",
                    "score": 0.916985034942627
                },
                {
                    "id": 11678089,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nModern methods Geometric series Any summation method possessing the properties of regularity, linearity, and stability will sum a geometric series In this case a = 1 and r = −2, so the sum is . Euler summation In his 1755 Institutiones, Leonhard Euler effectively took what is now called the Euler transform of , arriving at the convergent series . Since the latter sums to , Euler concluded that . His ideas on infinite series do not quite follow the modern approach; today one says that is Euler summable and that its Euler sum is . The Euler transform begins with the sequence of positive terms: a0 = 1, a1 = 2, a2 = 4, a3 = 8,... The sequence of forward differences is then Δa0 = a1 − a0 = 2 − 1 = 1, Δa1 = a2 − a1 = 4 − 2 = 2, Δa2 = a3 − a2 = 8 − 4 = 4, Δa3 = a4 − a3 = 16 − 8 = 8,... which is just the same sequence. Hence the iterated forward difference sequences all start with for every n. The Euler transform is the series",
                    "score": 0.9169330596923828
                },
                {
                    "id": 11646640,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe infinite series whose terms are the natural numbers is a divergent series. The nth partial sum of the series is the triangular number which increases without bound as n goes to infinity. Because the sequence of partial sums fails to converge to a finite limit, the series does not have a sum.",
                    "score": 0.9164787530899048
                },
                {
                    "id": 10049357,
                    "contents": "History of Grandi's series\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all x less than 1, it should hold for x equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as f(1) of some function's power series was thought to be the most natural strategy. Leibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0: 1 − 1 = 1 − 1 + 1 − 1 = 1 − 1 + 1 − 1 + 1 − 1 = 0. Taking an odd number of terms, the last term is +1 and the sum is 1: 1 = 1 − 1 + 1 = 1 − 1 + 1 − 1 + 1 = 1.",
                    "score": 0.915829062461853
                },
                {
                    "id": 3559416,
                    "contents": "Sinc function\nThe sum of the squares also equals : When the signs of the addends alternate and begin with +, the sum equals : The alternating sums of the squares and cubes also equal : Series expansion The Taylor series of the unnormalized function can be obtained from that of the sine: The series converges for all . The normalized version follows easily: Euler famously compared this series to the expansion of the infinite product form to solve the Basel problem.",
                    "score": 0.915488600730896
                },
                {
                    "id": 1169612,
                    "contents": "Pi\nAs individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of . An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is: Note that (n − 1)n(n + 1) = n3 − n. The following table compares the convergence rates of these two series: After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term. Irrationality and transcendence",
                    "score": 0.915069580078125
                },
                {
                    "id": 1790041,
                    "contents": "Series (mathematics)\nBasic properties An infinite series or simply a series is an infinite sum, represented by an infinite expression of the form where is any ordered sequence of terms, such as numbers, functions, or anything else that can be added (an abelian group). This is an expression that is obtained from the list of terms by laying them side by side, and conjoining them with the symbol \"+\". A series may also be represented by using summation notation, such as If an abelian group of terms has a concept of limit (e.g., if it is a metric space), then some series, the convergent series, can be interpreted as having a value in , called the sum of the series. This includes the common cases from calculus, in which the group is the field of real numbers or the field of complex numbers. Given a series , its th partial sum is By definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes",
                    "score": 0.9138367772102356
                },
                {
                    "id": 1169606,
                    "contents": "Pi\nThe calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his Tantrasamgraha, around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, Yuktibhāṣā, from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite",
                    "score": 0.9133221507072449
                },
                {
                    "id": 11646645,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nHeuristics Srinivasa Ramanujan presented two derivations of \"\" in chapter 8 of his first notebook. The simpler, less rigorous derivation proceeds in two steps, as follows. The first key insight is that the series of positive numbers closely resembles the alternating series . The latter series is also divergent, but it is much easier to work with; there are several classical methods that assign it a value, which have been explored since the 18th century. In order to transform the series into , one can subtract 4 from the second term, 8 from the fourth term, 12 from the sixth term, and so on. The total amount to be subtracted is , which is 4 times the original series. These relationships can be expressed using algebra. Whatever the \"sum\" of the series might be, call it Then multiply this equation by 4 and subtract the second equation from the first:",
                    "score": 0.9132533669471741
                },
                {
                    "id": 10049358,
                    "contents": "History of Grandi's series\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is Eli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"",
                    "score": 0.9126628041267395
                },
                {
                    "id": 12548083,
                    "contents": "1/2 + 1/4 + 1/8 + 1/16 + ⋯\nIn mathematics, the infinite series is an elementary example of a geometric series that converges absolutely. The sum of the series is 1. In summation notation, this may be expressed as The series is related to philosophical questions considered in antiquity, particularly to Zeno's paradoxes. Proof As with any infinite series, the sum is defined to mean the limit of the partial sum of the first terms as approaches infinity. By various arguments, one can show that this finite sum is equal to As approaches infinity, the term approaches 0 and so tends to 1. History",
                    "score": 0.9126095175743103
                },
                {
                    "id": 1216477,
                    "contents": "Real analysis\nAn example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes: In contrast, the harmonic series has been known since the Middle Ages to be a divergent series: (Here, \"\" is merely a notational convention to indicate that the partial sums of the series grow without bound.) A series is said to converge absolutely if is convergent. A convergent series for which diverges is said to converge non-absolutely. It is easily shown that absolute convergence of a series implies its convergence. On the other hand, an example of a series that converges non-absolutely is Taylor series The Taylor series of a real or complex-valued function ƒ(x) that is infinitely differentiable at a real or complex number a is the power series : which can be written in the more compact sigma notation as",
                    "score": 0.9122607707977295
                },
                {
                    "id": 2195776,
                    "contents": "List of real analysis topics\nConvergent series – a series whose sequence of partial sums converges Divergent series – a series whose sequence of partial sums diverges Power series – a series of the form Taylor series – a series of the form Maclaurin series – see Taylor seriesBinomial series – the Maclaurin series of the function f given by f(x) = (1 + x) α Telescoping series Alternating series Geometric series Divergent geometric series Harmonic series Fourier series Lambert series",
                    "score": 0.9121854305267334
                },
                {
                    "id": 23465699,
                    "contents": "List of sums of reciprocals\nInfinitely many terms Convergent series",
                    "score": 0.9118424654006958
                },
                {
                    "id": 3863129,
                    "contents": "Divergent series\nSum of a series Cauchy's classical definition of the sum of a series defines the sum to be the limit of the sequence of partial sums . This is the default definition of convergence of a sequence. Nørlund means Suppose pn is a sequence of positive terms, starting from p0. Suppose also that If now we transform a sequence s by using p to give weighted means, setting then the limit of tn as n goes to infinity is an average called the Nørlund mean Np(s). The Nørlund mean is regular, linear, and stable. Moreover, any two Nørlund means are consistent. Cesàro summation The most significant of the Nørlund means are the Cesàro sums. Here, if we define the sequence pk by then the Cesàro sum Ck is defined by Cesàro sums are Nørlund means if , and hence are regular, linear, stable, and consistent. C0 is ordinary summation, and C1 is ordinary Cesàro summation. Cesàro sums have the property that if then Ch is stronger than Ck.",
                    "score": 0.9115577936172485
                },
                {
                    "id": 24331221,
                    "contents": "Fundamental series\nthe frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and noticed that this series had a simpler formula than the others and thus called it the \"fundamental series\" and used the letter F.",
                    "score": 0.9114827513694763
                },
                {
                    "id": 4580091,
                    "contents": "Series expansion\nIn mathematics, a series expansion is an expansion of a function into a series, or infinite sum. It is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division). The resulting so-called series often can be limited to a finite number of terms, thus yielding an approximation of the function. The fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions. There are several kinds of series expansions, such as:",
                    "score": 0.9099294543266296
                },
                {
                    "id": 12162084,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nIn mathematics, is the infinite series whose terms are the successive powers of two. As a geometric series, it is characterized by its first term, 1, and its common ratio, 2. As a series of real numbers it diverges to infinity, so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the 2-adic metric. Summation The partial sums of are since these diverge to infinity, so does the series.",
                    "score": 0.9096724390983582
                },
                {
                    "id": 680910,
                    "contents": "Power series\nThe power series expansion of the inverse function of an analytic function can be determined using the Lagrange inversion theorem. Behavior near the boundary The sum of a power series with a positive radius of convergence is an analytic function at every point in the interior of the disc of convergence. However, different behavior can occur at points on the boundary of that disc. For example:",
                    "score": 0.909320592880249
                },
                {
                    "id": 12548113,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nIn mathematics, the infinite series is an example of one of the first infinite series to be summed in the history of mathematics; it was used by Archimedes circa 250–200 BC. As it is a geometric series with first term and common ratio , its sum is Visual demonstrations The series lends itself to some particularly simple visual demonstrations because a square and a triangle both divide into four similar pieces, each of which contains the area of the original. In the figure on the left, if the large square is taken to have area 1, then the largest black square has area × = . Likewise, the second largest black square has area , and the third largest black square has area . The area taken up by all of the black squares together is therefore , and this is also the area taken up by the gray squares and the white squares. Since these three areas cover the unit square, the figure demonstrates that",
                    "score": 0.9091456532478333
                },
                {
                    "id": 5898954,
                    "contents": "Convergent series\nIf the series converges, then the series is absolutely convergent. The Maclaurin series of the exponential function is absolutely convergent for every complex value of the variable. If the series converges but the series diverges, then the series is conditionally convergent. The Maclaurin series of the logarithm function is conditionally convergent for . The Riemann series theorem states that if a series converges conditionally, it is possible to rearrange the terms of the series in such a way that the series converges to any value, or even diverges. Uniform convergence Let be a sequence of functions. The series is said to converge uniformly to f if the sequence of partial sums defined by converges uniformly to f. There is an analogue of the comparison test for infinite series of functions called the Weierstrass M-test. Cauchy convergence criterion The Cauchy convergence criterion states that a series",
                    "score": 0.9090754985809326
                },
                {
                    "id": 21053061,
                    "contents": "Humbert series\nIn mathematics, Humbert series are a set of seven hypergeometric series Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, Ξ2 of two variables that generalize Kummer's confluent hypergeometric series 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable. The first of these double series was introduced by . Definitions The Humbert series Φ1 is defined for |x| < 1 by the double series: where the Pochhammer symbol (q)n represents the rising factorial: where the second equality is true for all complex except . For other values of x the function Φ1 can be defined by analytic continuation. The Humbert series Φ1 can also be written as a one-dimensional Euler-type integral: This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration. Similarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series:",
                    "score": 0.9085017442703247
                },
                {
                    "id": 1280599,
                    "contents": "Taylor series\nHere is the th finite difference operator with step size . The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function is analytic at , the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series. In general, for any infinite sequence , the following power series identity holds: So in particular, The series on the right is the expectation value of , where is a Poisson-distributed random variable that takes the value with probability . Hence, The law of large numbers implies that the identity holds. List of Maclaurin series of some common functions Several important Maclaurin series expansions follow. All these expansions are valid for complex arguments . Exponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm",
                    "score": 0.9083563089370728
                },
                {
                    "id": 11637875,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nEuler and Borel Euler applied another technique to the series: the Euler transform, one of his own inventions. To compute the Euler transform, one begins with the sequence of positive terms that makes up the alternating series—in this case The first element of this sequence is labeled a0. Next one needs the sequence of forward differences among ; this is just The first element of this sequence is labeled Δa0. The Euler transform also depends on differences of differences, and higher iterations, but all the forward differences among are 0. The Euler transform of is then defined as In modern terminology, one says that is Euler summable to . The Euler summability also implies Borel summability, with the same summation value, as it does in general.",
                    "score": 0.9082189798355103
                },
                {
                    "id": 3863128,
                    "contents": "Divergent series\nClassical summation methods The two classical summation methods for series, ordinary convergence and absolute convergence, define the sum as a limit of certain partial sums. These are included only for completeness; strictly speaking they are not true summation methods for divergent series since, by definition, a series is divergent only if these methods do not work. Most but not all summation methods for divergent series extend these methods to a larger class of sequences. Absolute convergence Absolute convergence defines the sum of a sequence (or set) of numbers to be the limit of the net of all partial sums , if it exists. It does not depend on the order of the elements of the sequence, and a classical theorem says that a sequence is absolutely convergent if and only if the sequence of absolute values is convergent in the standard sense. Sum of a series",
                    "score": 0.9082118272781372
                },
                {
                    "id": 1790042,
                    "contents": "Series (mathematics)\nBy definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes A series is said to be convergent if it converges to some limit, or divergent when it does not. The value of this limit, if it exists, is then the value of the series. Convergent series A series is said to converge or to be convergent when the sequence of partial sums has a finite limit. If the limit of is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the value (or sum) of the series An easy way that an infinite series can converge is if all the are zero for sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense. Working out the properties of the series that converge, even if infinitely many terms are nonzero, is the essence of the study of series. Consider the example",
                    "score": 0.9080818295478821
                },
                {
                    "id": 11646646,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe second key insight is that the alternating series is the formal power series expansion of the function but with x defined as 1. Accordingly, Ramanujan writes Dividing both sides by −3, one gets c = . Generally speaking, it is incorrect to manipulate infinite series as if they were finite sums. For example, if zeroes are inserted into arbitrary positions of a divergent series, it is possible to arrive at results that are not self-consistent, let alone consistent with other methods. In particular, the step is not justified by the additive identity law alone. For an extreme example, appending a single zero to the front of the series can lead to a different result.",
                    "score": 0.9077460765838623
                },
                {
                    "id": 1216476,
                    "contents": "Real analysis\nGiven an (infinite) sequence , we can define an associated series as the formal mathematical object sometimes simply written as . The partial sums of a series are the numbers . A series is said to be convergent if the sequence consisting of its partial sums, , is convergent; otherwise it is divergent. The sum of a convergent series is defined as the number The word \"sum\" is used here in a metaphorical sense as a shorthand for taking the limit of a sequence of partial sums and should not be interpreted as simply \"adding\" an infinite number of terms. For instance, in contrast to the behavior of finite sums, rearranging the terms of an infinite series may result in convergence to a different number (see the article on the Riemann rearrangement theorem for further discussion). An example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes:",
                    "score": 0.9072840213775635
                },
                {
                    "id": 11637866,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nIn mathematics, 1 − 2 + 3 − 4 + ··· is an infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first m terms of the series can be expressed as The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation: A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts. Many of these summability methods easily assign to a \"value\" of . Cesàro summation is one of the few methods that do not sum , so the series is an example where a slightly stronger method, such as Abel summation, is required.",
                    "score": 0.9072226285934448
                },
                {
                    "id": 17337406,
                    "contents": "Appell series\nIn mathematics, Appell series are a set of four hypergeometric series F1, F2, F3, F4 of two variables that were introduced by and that generalize Gauss's hypergeometric series 2F1 of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable. Definitions The Appell series F1 is defined for |x| < 1, |y| < 1 by the double series where is the Pochhammer symbol. For other values of x and y the function F1 can be defined by analytic continuation. It can be shown that Similarly, the function F2 is defined for |x| + |y| < 1 by the series and it can be shown that Also the function F3 for |x| < 1, |y| < 1 can be defined by the series and the function F4 for |x|½ + |y|½ < 1 by the series",
                    "score": 0.9069940447807312
                },
                {
                    "id": 12162087,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nThe above manipulation might be called on to produce −1 outside the context of a sufficiently powerful summation procedure. For the most well-known and straightforward sum concepts, including the fundamental convergent one, it is absurd that a series of positive terms could have a negative value. A similar phenomenon occurs with the divergent geometric series (Grandi's series), where a series of integers appears to have the non-integer sum These examples illustrate the potential danger in applying similar arguments to the series implied by such recurring decimals as and most notably . The arguments are ultimately justified for these convergent series, implying that and but the underlying proofs demand careful thinking about the interpretation of endless sums.",
                    "score": 0.906586229801178
                },
                {
                    "id": 1790061,
                    "contents": "Series (mathematics)\nLaurent series Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence. Dirichlet series A Dirichlet series is one of the form where s is a complex number. For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function",
                    "score": 0.9061896204948425
                },
                {
                    "id": 1790082,
                    "contents": "Series (mathematics)\nSee also Continued fraction Convergence tests Convergent series Divergent series Infinite compositions of analytic functions Infinite expression Infinite product Iterated binary operation List of mathematical series Prefix sum Sequence transformation Series expansion References Bibliography Bromwich, T. J. An Introduction to the Theory of Infinite Series MacMillan & Co. 1908, revised 1926, reprinted 1939, 1942, 1949, 1955, 1959, 1965. Walter Rudin, Principles of Mathematical Analysis (McGraw-Hill: New York, 1964). External links Infinite Series Tutorial Calculus",
                    "score": 0.9060359001159668
                },
                {
                    "id": 11646653,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nRamanujan summation is a method to isolate the constant term in the Euler–Maclaurin formula for the partial sums of a series. For a function f, the classical Ramanujan sum of the series is defined as where f(2k−1) is the (2k − 1)-th derivative of f and B2k is the 2k-th Bernoulli number: , , and so on. Setting , the first derivative of f is 1, and every other term vanishes, so",
                    "score": 0.9057778716087341
                },
                {
                    "id": 12162085,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nSummation The partial sums of are since these diverge to infinity, so does the series. Therefore, any totally regular summation method gives a sum of infinity, including the Cesàro sum and Abel sum. On the other hand, there is at least one generally useful method that sums to the finite value of −1. The associated power series has a radius of convergence around 0 of only so it does not converge at Nonetheless, the so-defined function has a unique analytic continuation to the complex plane with the point deleted, and it is given by the same rule Since the original series is said to be summable (E) to −1, and −1 is the (E) sum of the series. (The notation is due to G. H. Hardy in reference to Leonhard Euler's approach to divergent series). An almost identical approach (the one taken by Euler himself) is to consider the power series whose coefficients are all 1, that is, and plugging in These two series are related by the substitution",
                    "score": 0.9055444002151489
                },
                {
                    "id": 1790056,
                    "contents": "Series (mathematics)\ncan be made minimal independently of x by choosing a sufficiently large N. Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the ƒn are integrable on a closed and bounded interval I and converge uniformly, then the series is also integrable on I and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.",
                    "score": 0.9054824113845825
                },
                {
                    "id": 10049374,
                    "contents": "History of Grandi's series\nFrobenius' short paper, barely two pages, begins by quoting from Leibniz's treatment of 1 − 1 + 1 − 1 + · · ·. He infers that Leibniz was actually stating a generalization of Abel's Theorem. The result, now known as Frobenius' theorem, has a simple statement in modern terms: any series that is Cesàro summable is also Abel summable to the same sum. Historian Giovanni Ferraro emphasizes that Frobenius did not actually state the theorem in such terms, and Leibniz did not state it at all. Leibniz was defending the association of the divergent series with the value 1⁄2, while Frobenius' theorem is stated in terms of convergent sequences and the epsilon-delta formulation of the limit of a function.",
                    "score": 0.905381977558136
                },
                {
                    "id": 1574967,
                    "contents": "Archimedes\nArchimedes gives the value of the square root of 3 as lying between (approximately 1.7320261) and (approximately 1.7320512) in Measurement of a Circle. The actual value is approximately 1.7320508, making this a very accurate estimate. He introduced this result without offering any explanation of how he had obtained it. This aspect of the work of Archimedes caused John Wallis to remark that he was: \"as it were of set purpose to have covered up the traces of his investigation as if he had grudged posterity the secret of his method of inquiry while he wished to extort from them assent to his results.\" It is possible that he used an iterative procedure to calculate these values. The infinite series",
                    "score": 0.9052610397338867
                },
                {
                    "id": 11646654,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nwhere f(2k−1) is the (2k − 1)-th derivative of f and B2k is the 2k-th Bernoulli number: , , and so on. Setting , the first derivative of f is 1, and every other term vanishes, so To avoid inconsistencies, the modern theory of Ramanujan summation requires that f is \"regular\" in the sense that the higher-order derivatives of f decay quickly enough for the remainder terms in the Euler–Maclaurin formula to tend to 0. Ramanujan tacitly assumed this property. The regularity requirement prevents the use of Ramanujan summation upon spaced-out series like , because no regular function takes those values. Instead, such a series must be interpreted by zeta function regularization. For this reason, Hardy recommends \"great caution\" when applying the Ramanujan sums of known series to find the sums of related series.",
                    "score": 0.9050127267837524
                },
                {
                    "id": 1790058,
                    "contents": "Series (mathematics)\nis the Taylor series of at the origin and converges to it for every x. Unless it converges only at x=c, such a series converges on a certain open disc of convergence centered at the point c in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients an. The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets. Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required. Formal power series",
                    "score": 0.904932975769043
                },
                {
                    "id": 1790055,
                    "contents": "Series (mathematics)\nSeries of functions A series of real- or complex-valued functions converges pointwise on a set E, if the series converges for each x in E as an ordinary series of real or complex numbers. Equivalently, the partial sums converge to ƒ(x) as N → ∞ for each x ∈ E. A stronger notion of convergence of a series of functions is the uniform convergence. A series converges uniformly if it converges pointwise to the function ƒ(x), and the error in approximating the limit by the Nth partial sum, can be made minimal independently of x by choosing a sufficiently large N.",
                    "score": 0.9048910140991211
                },
                {
                    "id": 680906,
                    "contents": "Power series\nFor , there is no general statement on the convergence of the series. However, Abel's theorem states that if the series is convergent for some value such that , then the sum of the series for is the limit of the sum of the series for where is a real variable less than that tends to . Operations on power series Addition and subtraction When two functions f and g are decomposed into power series around the same center c, the power series of the sum or difference of the functions can be obtained by termwise addition and subtraction. That is, if and then It is not true that if two power series and have the same radius of convergence, then also has this radius of convergence. If and , then both series have the same radius of convergence of 1, but the series has a radius of convergence of 3. Multiplication and division With the same definitions for and , the power series of the product and quotient of the functions can be obtained as follows:",
                    "score": 0.904827892780304
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_3",
        "question": "Evaluate the series\r\n$$\r\nS=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{2^n}\r\n$$",
        "golden_answers": [
            " 1"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 12548116,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nSince the sum of an infinite series is defined as the limit of its partial sums, Notes References Page images at HTML with figures and commentary at Geometric series Proof without words",
                    "score": 0.9241862297058105
                },
                {
                    "id": 11678088,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nAfter Christian Wolff read Leibniz's treatment of Grandi's series in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Briefly, if one expresses a partial sum of this series as a function of the penultimate term, one obtains either or . The mean of these values is , and assuming that at infinity yields as the value of the series. Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. The arithmetic means of neighboring partial sums do not converge to any particular value, and for all finite cases one has , not . Generally, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series. Leibniz counsels Wolff to reconsider so that he \"might produce something worthy of science and himself.\" Modern methods",
                    "score": 0.9235342144966125
                },
                {
                    "id": 5898948,
                    "contents": "Convergent series\nIn mathematics, a series is the sum of the terms of an infinite sequence of numbers. More precisely, an infinite sequence defines a series that is denoted The th partial sum is the sum of the first terms of the sequence; that is, A series is convergent (or converges) if the sequence of its partial sums tends to a limit; that means that, when adding one after the other in the order given by the indices, one gets partial sums that become closer and closer to a given number. More precisely, a series converges, if there exists a number such that for every arbitrarily small positive number , there is a (sufficiently large) integer such that for all , If the series is convergent, the (necessarily unique) number is called the sum of the series. The same notation",
                    "score": 0.9210975170135498
                },
                {
                    "id": 14058441,
                    "contents": "Florence Marie Mears\nMears specialized in the findings of definitions and values assigned to various infinite series of numbers. An \"infinite series\" is an endless series of numbers, each succeeding the other that is a certain amount lesser or greater than the proceeding one. An example set of an infinite series includes is 1 + ½ + ¼ etc. in which the definition of the series can be defined as the number two. As a result, Mears created several theorems about these definitions, many of which provided truth for many practicing mathematicians, engineers, chemists, physicists, and astronomers. One of her most popular theorems, called the Norlund Mean can be explained through absolute regularity, the summability of Cauchy products, and inverse properties.",
                    "score": 0.9207783937454224
                },
                {
                    "id": 17472486,
                    "contents": "List of mathematical series\nThis list of mathematical series contains formulae for finite and infinite sums. It can be used in conjunction with other tools for evaluating sums. Here, is taken to have the value denotes the fractional part of is a Bernoulli polynomial. is a Bernoulli number, and here, is an Euler number. is the Riemann zeta function. is the gamma function. is a polygamma function. is a polylogarithm. is binomial coefficient denotes exponential of Sums of powers See Faulhaber's formula. The first few values are: See zeta constants. The first few values are: (the Basel problem) Power series Low-order polylogarithms Finite sums: , (geometric series) Infinite sums, valid for (see polylogarithm): The following is a useful property to calculate low-integer-order polylogarithms recursively in closed form: Exponential function (cf. mean of Poisson distribution) (cf. second moment of Poisson distribution) where is the Touchard polynomials.",
                    "score": 0.920464277267456
                },
                {
                    "id": 11678087,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nLeibniz did not quite assert that the series had a sum, but he did infer an association with following Mercator's method. The attitude that a series could equal some finite quantity without actually adding up to it as a sum would be commonplace in the 18th century, although no distinction is made in modern mathematics.",
                    "score": 0.9201837778091431
                },
                {
                    "id": 13740952,
                    "contents": "Sums of powers\nThe sum of the terms in the geometric series is",
                    "score": 0.9177367091178894
                },
                {
                    "id": 1790041,
                    "contents": "Series (mathematics)\nBasic properties An infinite series or simply a series is an infinite sum, represented by an infinite expression of the form where is any ordered sequence of terms, such as numbers, functions, or anything else that can be added (an abelian group). This is an expression that is obtained from the list of terms by laying them side by side, and conjoining them with the symbol \"+\". A series may also be represented by using summation notation, such as If an abelian group of terms has a concept of limit (e.g., if it is a metric space), then some series, the convergent series, can be interpreted as having a value in , called the sum of the series. This includes the common cases from calculus, in which the group is the field of real numbers or the field of complex numbers. Given a series , its th partial sum is By definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes",
                    "score": 0.9167125225067139
                },
                {
                    "id": 20560519,
                    "contents": "Isidore Isaac Hirschman Jr.\nSelected publications Articles } Books Hirschman, I. (1962). Infinite Series. New York: Holt, Rinehart & Winston. – A textbook for advanced undergraduate and graduate mathematics. Hirschman, Isidore Isaac; Widder, David Vernon (1955). The Convolution Transform. New York: Princeton University Press; now available from Dover Publications. References http://mathdl.maa.org/mathDL/46/?pa=content&sa=viewDocument&nodeId=3801&bodyId=4189 20th-century American mathematicians Harvard University alumni Washington University in St. Louis faculty Washington University in St. Louis mathematicians",
                    "score": 0.9166955947875977
                },
                {
                    "id": 10313842,
                    "contents": "Summation of Grandi's series\nThe Cesàro sum of 1 + 0 − 1 + 1 + 0 − 1 + · · · is 2⁄3. So the Cesàro sum of a series can be altered by inserting infinitely many 0s as well as infinitely many brackets. The series can also be summed by the more general fractional (C, a) methods. Abel sum Abel summation is similar to Euler's attempted definition of sums of divergent series, but it avoids Callet's and N. Bernoulli's objections by precisely constructing the function to use. In fact, Euler likely meant to limit his definition to power series, and in practice he used it almost exclusively in a form now known as Abel's method. Given a series a0 + a1 + a2 + · · ·, one forms a new series a0 + a1x + a2x2 + · · ·. If the latter series converges for 0 < x < 1 to a function with a limit as x tends to 1, then this limit is called the Abel sum of the original series, after Abel's theorem which guarantees that the procedure is consistent with ordinary summation. For Grandi's series one has",
                    "score": 0.9164338111877441
                },
                {
                    "id": 1169612,
                    "contents": "Pi\nAs individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of . An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is: Note that (n − 1)n(n + 1) = n3 − n. The following table compares the convergence rates of these two series: After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term. Irrationality and transcendence",
                    "score": 0.9163182973861694
                },
                {
                    "id": 11646640,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe infinite series whose terms are the natural numbers is a divergent series. The nth partial sum of the series is the triangular number which increases without bound as n goes to infinity. Because the sequence of partial sums fails to converge to a finite limit, the series does not have a sum.",
                    "score": 0.9154461026191711
                },
                {
                    "id": 3559416,
                    "contents": "Sinc function\nThe sum of the squares also equals : When the signs of the addends alternate and begin with +, the sum equals : The alternating sums of the squares and cubes also equal : Series expansion The Taylor series of the unnormalized function can be obtained from that of the sine: The series converges for all . The normalized version follows easily: Euler famously compared this series to the expansion of the infinite product form to solve the Basel problem.",
                    "score": 0.915013313293457
                },
                {
                    "id": 11646645,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nHeuristics Srinivasa Ramanujan presented two derivations of \"\" in chapter 8 of his first notebook. The simpler, less rigorous derivation proceeds in two steps, as follows. The first key insight is that the series of positive numbers closely resembles the alternating series . The latter series is also divergent, but it is much easier to work with; there are several classical methods that assign it a value, which have been explored since the 18th century. In order to transform the series into , one can subtract 4 from the second term, 8 from the fourth term, 12 from the sixth term, and so on. The total amount to be subtracted is , which is 4 times the original series. These relationships can be expressed using algebra. Whatever the \"sum\" of the series might be, call it Then multiply this equation by 4 and subtract the second equation from the first:",
                    "score": 0.9145689010620117
                },
                {
                    "id": 11678089,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nModern methods Geometric series Any summation method possessing the properties of regularity, linearity, and stability will sum a geometric series In this case a = 1 and r = −2, so the sum is . Euler summation In his 1755 Institutiones, Leonhard Euler effectively took what is now called the Euler transform of , arriving at the convergent series . Since the latter sums to , Euler concluded that . His ideas on infinite series do not quite follow the modern approach; today one says that is Euler summable and that its Euler sum is . The Euler transform begins with the sequence of positive terms: a0 = 1, a1 = 2, a2 = 4, a3 = 8,... The sequence of forward differences is then Δa0 = a1 − a0 = 2 − 1 = 1, Δa1 = a2 − a1 = 4 − 2 = 2, Δa2 = a3 − a2 = 8 − 4 = 4, Δa3 = a4 − a3 = 16 − 8 = 8,... which is just the same sequence. Hence the iterated forward difference sequences all start with for every n. The Euler transform is the series",
                    "score": 0.9142256379127502
                },
                {
                    "id": 10049357,
                    "contents": "History of Grandi's series\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all x less than 1, it should hold for x equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as f(1) of some function's power series was thought to be the most natural strategy. Leibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0: 1 − 1 = 1 − 1 + 1 − 1 = 1 − 1 + 1 − 1 + 1 − 1 = 0. Taking an odd number of terms, the last term is +1 and the sum is 1: 1 = 1 − 1 + 1 = 1 − 1 + 1 − 1 + 1 = 1.",
                    "score": 0.9135644435882568
                },
                {
                    "id": 24331221,
                    "contents": "Fundamental series\nthe frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and noticed that this series had a simpler formula than the others and thus called it the \"fundamental series\" and used the letter F.",
                    "score": 0.9132291078567505
                },
                {
                    "id": 3863129,
                    "contents": "Divergent series\nSum of a series Cauchy's classical definition of the sum of a series defines the sum to be the limit of the sequence of partial sums . This is the default definition of convergence of a sequence. Nørlund means Suppose pn is a sequence of positive terms, starting from p0. Suppose also that If now we transform a sequence s by using p to give weighted means, setting then the limit of tn as n goes to infinity is an average called the Nørlund mean Np(s). The Nørlund mean is regular, linear, and stable. Moreover, any two Nørlund means are consistent. Cesàro summation The most significant of the Nørlund means are the Cesàro sums. Here, if we define the sequence pk by then the Cesàro sum Ck is defined by Cesàro sums are Nørlund means if , and hence are regular, linear, stable, and consistent. C0 is ordinary summation, and C1 is ordinary Cesàro summation. Cesàro sums have the property that if then Ch is stronger than Ck.",
                    "score": 0.9129878282546997
                },
                {
                    "id": 1216477,
                    "contents": "Real analysis\nAn example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes: In contrast, the harmonic series has been known since the Middle Ages to be a divergent series: (Here, \"\" is merely a notational convention to indicate that the partial sums of the series grow without bound.) A series is said to converge absolutely if is convergent. A convergent series for which diverges is said to converge non-absolutely. It is easily shown that absolute convergence of a series implies its convergence. On the other hand, an example of a series that converges non-absolutely is Taylor series The Taylor series of a real or complex-valued function ƒ(x) that is infinitely differentiable at a real or complex number a is the power series : which can be written in the more compact sigma notation as",
                    "score": 0.9129157066345215
                },
                {
                    "id": 12548083,
                    "contents": "1/2 + 1/4 + 1/8 + 1/16 + ⋯\nIn mathematics, the infinite series is an elementary example of a geometric series that converges absolutely. The sum of the series is 1. In summation notation, this may be expressed as The series is related to philosophical questions considered in antiquity, particularly to Zeno's paradoxes. Proof As with any infinite series, the sum is defined to mean the limit of the partial sum of the first terms as approaches infinity. By various arguments, one can show that this finite sum is equal to As approaches infinity, the term approaches 0 and so tends to 1. History",
                    "score": 0.9123423099517822
                },
                {
                    "id": 4580091,
                    "contents": "Series expansion\nIn mathematics, a series expansion is an expansion of a function into a series, or infinite sum. It is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division). The resulting so-called series often can be limited to a finite number of terms, thus yielding an approximation of the function. The fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions. There are several kinds of series expansions, such as:",
                    "score": 0.9123281836509705
                },
                {
                    "id": 680910,
                    "contents": "Power series\nThe power series expansion of the inverse function of an analytic function can be determined using the Lagrange inversion theorem. Behavior near the boundary The sum of a power series with a positive radius of convergence is an analytic function at every point in the interior of the disc of convergence. However, different behavior can occur at points on the boundary of that disc. For example:",
                    "score": 0.9122111201286316
                },
                {
                    "id": 10049358,
                    "contents": "History of Grandi's series\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is Eli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"",
                    "score": 0.9116055965423584
                },
                {
                    "id": 5898954,
                    "contents": "Convergent series\nIf the series converges, then the series is absolutely convergent. The Maclaurin series of the exponential function is absolutely convergent for every complex value of the variable. If the series converges but the series diverges, then the series is conditionally convergent. The Maclaurin series of the logarithm function is conditionally convergent for . The Riemann series theorem states that if a series converges conditionally, it is possible to rearrange the terms of the series in such a way that the series converges to any value, or even diverges. Uniform convergence Let be a sequence of functions. The series is said to converge uniformly to f if the sequence of partial sums defined by converges uniformly to f. There is an analogue of the comparison test for infinite series of functions called the Weierstrass M-test. Cauchy convergence criterion The Cauchy convergence criterion states that a series",
                    "score": 0.9106811285018921
                },
                {
                    "id": 1169606,
                    "contents": "Pi\nThe calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his Tantrasamgraha, around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, Yuktibhāṣā, from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite",
                    "score": 0.9103835225105286
                },
                {
                    "id": 12162084,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nIn mathematics, is the infinite series whose terms are the successive powers of two. As a geometric series, it is characterized by its first term, 1, and its common ratio, 2. As a series of real numbers it diverges to infinity, so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the 2-adic metric. Summation The partial sums of are since these diverge to infinity, so does the series.",
                    "score": 0.9101392030715942
                },
                {
                    "id": 1790042,
                    "contents": "Series (mathematics)\nBy definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes A series is said to be convergent if it converges to some limit, or divergent when it does not. The value of this limit, if it exists, is then the value of the series. Convergent series A series is said to converge or to be convergent when the sequence of partial sums has a finite limit. If the limit of is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the value (or sum) of the series An easy way that an infinite series can converge is if all the are zero for sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense. Working out the properties of the series that converge, even if infinitely many terms are nonzero, is the essence of the study of series. Consider the example",
                    "score": 0.9099109768867493
                },
                {
                    "id": 1216476,
                    "contents": "Real analysis\nGiven an (infinite) sequence , we can define an associated series as the formal mathematical object sometimes simply written as . The partial sums of a series are the numbers . A series is said to be convergent if the sequence consisting of its partial sums, , is convergent; otherwise it is divergent. The sum of a convergent series is defined as the number The word \"sum\" is used here in a metaphorical sense as a shorthand for taking the limit of a sequence of partial sums and should not be interpreted as simply \"adding\" an infinite number of terms. For instance, in contrast to the behavior of finite sums, rearranging the terms of an infinite series may result in convergence to a different number (see the article on the Riemann rearrangement theorem for further discussion). An example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes:",
                    "score": 0.90971440076828
                },
                {
                    "id": 2195776,
                    "contents": "List of real analysis topics\nConvergent series – a series whose sequence of partial sums converges Divergent series – a series whose sequence of partial sums diverges Power series – a series of the form Taylor series – a series of the form Maclaurin series – see Taylor seriesBinomial series – the Maclaurin series of the function f given by f(x) = (1 + x) α Telescoping series Alternating series Geometric series Divergent geometric series Harmonic series Fourier series Lambert series",
                    "score": 0.9093843102455139
                },
                {
                    "id": 1790056,
                    "contents": "Series (mathematics)\ncan be made minimal independently of x by choosing a sufficiently large N. Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the ƒn are integrable on a closed and bounded interval I and converge uniformly, then the series is also integrable on I and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.",
                    "score": 0.9091151356697083
                },
                {
                    "id": 12548113,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nIn mathematics, the infinite series is an example of one of the first infinite series to be summed in the history of mathematics; it was used by Archimedes circa 250–200 BC. As it is a geometric series with first term and common ratio , its sum is Visual demonstrations The series lends itself to some particularly simple visual demonstrations because a square and a triangle both divide into four similar pieces, each of which contains the area of the original. In the figure on the left, if the large square is taken to have area 1, then the largest black square has area × = . Likewise, the second largest black square has area , and the third largest black square has area . The area taken up by all of the black squares together is therefore , and this is also the area taken up by the gray squares and the white squares. Since these three areas cover the unit square, the figure demonstrates that",
                    "score": 0.9087969064712524
                },
                {
                    "id": 23465699,
                    "contents": "List of sums of reciprocals\nInfinitely many terms Convergent series",
                    "score": 0.9086902141571045
                },
                {
                    "id": 11637875,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nEuler and Borel Euler applied another technique to the series: the Euler transform, one of his own inventions. To compute the Euler transform, one begins with the sequence of positive terms that makes up the alternating series—in this case The first element of this sequence is labeled a0. Next one needs the sequence of forward differences among ; this is just The first element of this sequence is labeled Δa0. The Euler transform also depends on differences of differences, and higher iterations, but all the forward differences among are 0. The Euler transform of is then defined as In modern terminology, one says that is Euler summable to . The Euler summability also implies Borel summability, with the same summation value, as it does in general.",
                    "score": 0.9084164500236511
                },
                {
                    "id": 3863128,
                    "contents": "Divergent series\nClassical summation methods The two classical summation methods for series, ordinary convergence and absolute convergence, define the sum as a limit of certain partial sums. These are included only for completeness; strictly speaking they are not true summation methods for divergent series since, by definition, a series is divergent only if these methods do not work. Most but not all summation methods for divergent series extend these methods to a larger class of sequences. Absolute convergence Absolute convergence defines the sum of a sequence (or set) of numbers to be the limit of the net of all partial sums , if it exists. It does not depend on the order of the elements of the sequence, and a classical theorem says that a sequence is absolutely convergent if and only if the sequence of absolute values is convergent in the standard sense. Sum of a series",
                    "score": 0.9083706736564636
                },
                {
                    "id": 21053061,
                    "contents": "Humbert series\nIn mathematics, Humbert series are a set of seven hypergeometric series Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, Ξ2 of two variables that generalize Kummer's confluent hypergeometric series 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable. The first of these double series was introduced by . Definitions The Humbert series Φ1 is defined for |x| < 1 by the double series: where the Pochhammer symbol (q)n represents the rising factorial: where the second equality is true for all complex except . For other values of x the function Φ1 can be defined by analytic continuation. The Humbert series Φ1 can also be written as a one-dimensional Euler-type integral: This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration. Similarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series:",
                    "score": 0.9082565307617188
                },
                {
                    "id": 1280599,
                    "contents": "Taylor series\nHere is the th finite difference operator with step size . The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function is analytic at , the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series. In general, for any infinite sequence , the following power series identity holds: So in particular, The series on the right is the expectation value of , where is a Poisson-distributed random variable that takes the value with probability . Hence, The law of large numbers implies that the identity holds. List of Maclaurin series of some common functions Several important Maclaurin series expansions follow. All these expansions are valid for complex arguments . Exponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm",
                    "score": 0.9078887701034546
                },
                {
                    "id": 11646646,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe second key insight is that the alternating series is the formal power series expansion of the function but with x defined as 1. Accordingly, Ramanujan writes Dividing both sides by −3, one gets c = . Generally speaking, it is incorrect to manipulate infinite series as if they were finite sums. For example, if zeroes are inserted into arbitrary positions of a divergent series, it is possible to arrive at results that are not self-consistent, let alone consistent with other methods. In particular, the step is not justified by the additive identity law alone. For an extreme example, appending a single zero to the front of the series can lead to a different result.",
                    "score": 0.9078435897827148
                },
                {
                    "id": 1790055,
                    "contents": "Series (mathematics)\nSeries of functions A series of real- or complex-valued functions converges pointwise on a set E, if the series converges for each x in E as an ordinary series of real or complex numbers. Equivalently, the partial sums converge to ƒ(x) as N → ∞ for each x ∈ E. A stronger notion of convergence of a series of functions is the uniform convergence. A series converges uniformly if it converges pointwise to the function ƒ(x), and the error in approximating the limit by the Nth partial sum, can be made minimal independently of x by choosing a sufficiently large N.",
                    "score": 0.9076903462409973
                },
                {
                    "id": 17337406,
                    "contents": "Appell series\nIn mathematics, Appell series are a set of four hypergeometric series F1, F2, F3, F4 of two variables that were introduced by and that generalize Gauss's hypergeometric series 2F1 of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable. Definitions The Appell series F1 is defined for |x| < 1, |y| < 1 by the double series where is the Pochhammer symbol. For other values of x and y the function F1 can be defined by analytic continuation. It can be shown that Similarly, the function F2 is defined for |x| + |y| < 1 by the series and it can be shown that Also the function F3 for |x| < 1, |y| < 1 can be defined by the series and the function F4 for |x|½ + |y|½ < 1 by the series",
                    "score": 0.9076500535011292
                },
                {
                    "id": 680906,
                    "contents": "Power series\nFor , there is no general statement on the convergence of the series. However, Abel's theorem states that if the series is convergent for some value such that , then the sum of the series for is the limit of the sum of the series for where is a real variable less than that tends to . Operations on power series Addition and subtraction When two functions f and g are decomposed into power series around the same center c, the power series of the sum or difference of the functions can be obtained by termwise addition and subtraction. That is, if and then It is not true that if two power series and have the same radius of convergence, then also has this radius of convergence. If and , then both series have the same radius of convergence of 1, but the series has a radius of convergence of 3. Multiplication and division With the same definitions for and , the power series of the product and quotient of the functions can be obtained as follows:",
                    "score": 0.9073991775512695
                },
                {
                    "id": 1790039,
                    "contents": "Series (mathematics)\nIn modern terminology, any (ordered) infinite sequence of terms (that is, numbers, functions, or anything that can be added) defines a series, which is the operation of adding the one after the other. To emphasize that there are an infinite number of terms, a series may be called an infinite series. Such a series is represented (or denoted) by an expression like or, using the summation sign, The infinite sequence of additions implied by a series cannot be effectively carried on (at least in a finite amount of time). However, if the set to which the terms and their finite sums belong has a notion of limit, it is sometimes possible to assign a value to a series, called the sum of the series. This value is the limit as tends to infinity (if the limit exists) of the finite sums of the first terms of the series, which are called the th partial sums of the series. That is,",
                    "score": 0.9071587324142456
                },
                {
                    "id": 12162085,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nSummation The partial sums of are since these diverge to infinity, so does the series. Therefore, any totally regular summation method gives a sum of infinity, including the Cesàro sum and Abel sum. On the other hand, there is at least one generally useful method that sums to the finite value of −1. The associated power series has a radius of convergence around 0 of only so it does not converge at Nonetheless, the so-defined function has a unique analytic continuation to the complex plane with the point deleted, and it is given by the same rule Since the original series is said to be summable (E) to −1, and −1 is the (E) sum of the series. (The notation is due to G. H. Hardy in reference to Leonhard Euler's approach to divergent series). An almost identical approach (the one taken by Euler himself) is to consider the power series whose coefficients are all 1, that is, and plugging in These two series are related by the substitution",
                    "score": 0.9070538282394409
                },
                {
                    "id": 1790071,
                    "contents": "Series (mathematics)\nGeneralizations Asymptotic series Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge, but they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers. Divergent series",
                    "score": 0.9068697094917297
                },
                {
                    "id": 1790058,
                    "contents": "Series (mathematics)\nis the Taylor series of at the origin and converges to it for every x. Unless it converges only at x=c, such a series converges on a certain open disc of convergence centered at the point c in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients an. The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets. Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required. Formal power series",
                    "score": 0.9068129658699036
                },
                {
                    "id": 11637866,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nIn mathematics, 1 − 2 + 3 − 4 + ··· is an infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first m terms of the series can be expressed as The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation: A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts. Many of these summability methods easily assign to a \"value\" of . Cesàro summation is one of the few methods that do not sum , so the series is an example where a slightly stronger method, such as Abel summation, is required.",
                    "score": 0.9063588380813599
                },
                {
                    "id": 1744069,
                    "contents": "Geometric series\nThe convergence of the geometric series depends on the value of the common ratio r: If |r| < 1, the terms of the series approach zero in the limit (becoming smaller and smaller in magnitude), and the series converges to the sum a / (1 - r). If |r| = 1, the series does not converge. When r = 1, all of the terms of the series are the same and the series is infinite. When r = −1, the terms take two values alternately (for example, 2, −2, 2, −2, 2,... ). The sum of the terms oscillates between two values (for example, 2, 0, 2, 0, 2,... ). This is a different type of divergence. See for example Grandi's series: 1 − 1 + 1 − 1 + ···. If |r| > 1, the terms of the series become larger and larger in magnitude. The sum of the terms also gets larger and larger, and the series does not converge to a sum. (The series diverges.)",
                    "score": 0.9062271118164062
                },
                {
                    "id": 1790061,
                    "contents": "Series (mathematics)\nLaurent series Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence. Dirichlet series A Dirichlet series is one of the form where s is a complex number. For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function",
                    "score": 0.9061805605888367
                },
                {
                    "id": 1790040,
                    "contents": "Series (mathematics)\nWhen this limit exists, one says that the series is convergent or summable, or that the sequence is summable. In this case, the limit is called the sum of the series. Otherwise, the series is said to be divergent. The notation denotes both the series—that is the implicit process of adding the terms one after the other indefinitely—and, if the series is convergent, the sum of the series—the result of the process. This is a generalization of the similar convention of denoting by both the addition—the process of adding—and its result—the sum of and . Generally, the terms of a series come from a ring, often the field of the real numbers or the field of the complex numbers. In this case, the set of all series is itself a ring (and even an associative algebra), in which the addition consists of adding the series term by term, and the multiplication is the Cauchy product. Basic properties",
                    "score": 0.905998706817627
                },
                {
                    "id": 1790045,
                    "contents": "Series (mathematics)\nA geometric series is one where each successive term is produced by multiplying the previous term by a constant number (called the common ratio in this context). For example: In general, the geometric series converges if and only if , in which case it converges to . The harmonic series is the series The harmonic series is divergent. An alternating series is a series where terms alternate signs. Examples: (alternating harmonic series) and A telescoping series converges if the sequence bn converges to a limit L—as n goes to infinity. The value of the series is then b1 − L. An arithmetico-geometric series is a generalization of the geometric series, which has coefficients of the common ratio equal to the terms in an arithmetic sequence. Example: The p-series converges if p > 1 and diverges for p ≤ 1, which can be shown with the integral criterion described below in convergence tests. As a function of p, the sum of this series is Riemann's zeta function.",
                    "score": 0.9056720733642578
                },
                {
                    "id": 19430505,
                    "contents": "Series multisection\nIn mathematics, a multisection of a power series is a new power series composed of equally spaced terms extracted unaltered from the original series. Formally, if one is given a power series then its multisection is a power series of the form where p, q are integers, with 0 ≤ p < q. Multisection of analytic functions A multisection of the series of an analytic function has a closed-form expression in terms of the function : where is a primitive q-th root of unity. This solution was first discovered by Thomas Simpson. This expression is especially useful in that it can convert an infinite sum into a finite sum. It is used, for example, in a key step of a standard proof of Gauss's digamma theorem, which gives a closed-form solution to the digamma function evaluated at rational values p/q. Examples Bisection In general, the bisections of a series are the even and odd parts of the series. Geometric series Consider the geometric series",
                    "score": 0.9056352972984314
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.6
            }
        }
    },
    {
        "id": "test_4",
        "question": "The relationship introduced in Problem $1-48$ has been interpreted to mean that a particle of mass $m\\left(E=m c^2\\right)$ can materialize from nothing provided that it returns to nothing within a time $\\Delta t \\leq h / m c^2$. Particles that last for time $\\Delta t$ or more are called real particles; particles that last less than time $\\Delta t$ are called virtual particles. The mass of the charged pion, a subatomic particle, is $2.5 \\times 10^{-28} \\mathrm{~kg}$. What is the minimum lifetime if the pion is to be considered a real particle?",
        "golden_answers": [
            " 2.9"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 28126397,
                    "contents": "Raymond Droz\nproblems, but still played occasional sessions. He also worked with Heinz Kretzschmar (1960), Bill Ramsey (1965), Buck Clayton (1966), Rex Stewart (1966), Othella Dallas (1967), Gabriela Schaaf (1979) and New Zurich Jazztett (1981).",
                    "score": 0.8858914375305176
                },
                {
                    "id": 24092966,
                    "contents": "If You're Not Part of the Solution, You're Part of the Problem\nTrack listing Original Reissue Personnel George Cables – electronic piano Joe Henderson – tenor saxophone Ron McClure – double bass, electric bass Woody Shaw – flugelhorn, trumpet Tony Waters – congas Lenny White – drums References Joe Henderson live albums Albums produced by Orrin Keepnews 1970 live albums Milestone Records live albums",
                    "score": 0.8849961757659912
                },
                {
                    "id": 18094536,
                    "contents": "Problem book\nThe casebook for law and other non-technical fields can provide a similar function. Notable problem books in mathematics Paul Halmos (1982) A Hilbert Space Problem Book () Frederick Mosteller (1965,1987) Fifty Challenging Problems in Probability with Solutions () Arthur Engel (1997) Problem-Solving Strategies () Notable problem books in physics V. V. Batygin and I. N. Toptygin (1964,1978) Problems in Electrodynamics (ASIN B003X6BPSE) Kyriakos Tamvakis (2005) Problems and Solutions in Quantum Mechanics () A.P. Lightman, W.H. Press, R.H. Price, and S.A. Teukolsky (1979) Problem Book in Relativity and Gravitation () W-H. Steeb (2006) Problems And Solutions in Quantum Computing And Quantum Information ()",
                    "score": 0.8844562768936157
                },
                {
                    "id": 2444429,
                    "contents": "Hilbert's problems\nPaul Erdős posed hundreds, if not thousands, of mathematical problems, many of them profound. Erdős often offered monetary rewards; the size of the reward depended on the perceived difficulty of the problem. The end of the millennium, which was also the centennial of Hilbert's announcement of his problems, provided a natural occasion to propose \"a new set of Hilbert problems.\" Several mathematicians accepted the challenge, notably Fields Medalist Steve Smale, who responded to a request by Vladimir Arnold to propose a list of 18 problems.",
                    "score": 0.88423752784729
                },
                {
                    "id": 1463638,
                    "contents": "Early life of Isaac Newton\nNext November, Newton redeemed his promise to Halley by sending him, by the hand of Mr. Paget, a fellow of Trinity College and mathematical master of Christ's Hospital, a copy of his demonstration; and very soon afterward Halley again visited Cambridge to confer with Newton about the problem. On his return to London on 10 December 1684, he informed the Royal Society \"that he had lately seen Mr. Newton at Cambridge, who had shown him a curious treatise De Motu\", which at Halley's desire he promised to send to the Society to be entered upon their register. \"Mr. Halley was desired to put Mr. Newton in mind of his promise for the securing this invention to himself, till he could be at leisure to publish it\", and Paget was desired to join with Halley in urging Newton to do so. By the middle of February Newton had sent his paper to Aston, one of the secretaries of the Society, and in a letter to Aston dated 23 February 1685, Newton thanked him for \"having entered on the register his notions",
                    "score": 0.88170325756073
                },
                {
                    "id": 10981135,
                    "contents": "Alexander Fetter\nAlexander Fetter retired from full-time as a professor in November 2007, but continues to work half time there. Personal life Alexander Fetter was married to Jean Fetter (who is now married to Steven Chu, Alexander's former colleague and a former Secretary of Energy under President Obama) and had 2 children with her (Anne L. Fetter, and Andrew J. Fetter), and is currently married to Lynn Bunim. His sister Ann (\"Nan\") Fetter Friedlaender was the first woman Dean at MIT (Economics Department). Fetter has 6 grandchildren the first of whom was born in 1995 and the most recent in 2007. Selected publications Quantum Theory of Many-Particle Systems, Dover Publications, 2003, Theoretical Mechanics of Particles and Continua, Dover Publications, 2003, Nonlinear Mechanics: A Supplement to Theoretical Mechanics of Particles and Continua, Dover Publications, 2006, Nonuniform states of an imperfect bose gas, Annals of Physics, 1972 References",
                    "score": 0.8816389441490173
                },
                {
                    "id": 3468122,
                    "contents": "Hugh Everett III\nHowever, while in Copenhagen, in his hotel, he started work on a new idea to use generalized Lagrange multipliers for mathematical optimization. Everett's theorem, published in 1963, relates the Lagrangian bidual to the primal problem. In 1962 Everett accepted an invitation to present the relative-state formulation (as it was still called) at a conference on the foundations of quantum mechanics held at Xavier University in Cincinnati. In his exposition Everett presented his derivation of probability and also stated explicitly that observers in all branches of the wavefunction were equally \"real.\" He also agreed with an observation from the floor that the number of branches of the universal wavefunction was an uncountable infinity.",
                    "score": 0.881538987159729
                },
                {
                    "id": 6854195,
                    "contents": "Hilbert's twentieth problem\nProblem statement The original problem statement in its entirety is as follows:",
                    "score": 0.8812321424484253
                },
                {
                    "id": 10827324,
                    "contents": "Meet the Quagmires\nfor a do-over. When Peter finally gets it right, he forgets a few hours later and parties with Cleveland instead of keeping the date. He asks Death for another chance but Death, fed up with Peter's continued blunders, tells him that he will have to fix the problem on his own.",
                    "score": 0.8811858892440796
                },
                {
                    "id": 9649939,
                    "contents": "History of variational principles in physics\nA translation of this passage reads: \"Let the mass of the projectile be M, and let its squared velocity resulting from its height be while being moved over a distance ds. The body will have a momentum that, when multiplied by the distance ds, will give , the momentum of the body integrated over the distance ds. Now I assert that the curve thus described by the body to be the curve (from among all other curves connecting the same endpoints) that minimizes or, provided that M is constant, .\"",
                    "score": 0.8811613917350769
                },
                {
                    "id": 10291409,
                    "contents": "Archibald Cary Coolidge\n9) The Philippine Problem",
                    "score": 0.8809230327606201
                },
                {
                    "id": 22490636,
                    "contents": "Paul J. Nahin\nWhen Least Is Best: How Mathematicians Discovered Many Clever Ways to Make Things as Small (or as Large) as Possible (2011); Number-Crunching: Taming Unruly Computational Problems from Mathematical Physics to Science Fiction (2011) Dr. Euler's Fabulous Formula: Cures Many Mathematical Ills (2011) 2017 pbk edition Digital dice: computational solutions to practical probability problems (2008); 2013 pbk edition Time Travel: A Writer's Guide to the Real Science of Plausible Time Travel (1997); 2011 pbk edition Mrs. Perkins's Electric Quilt: And Other Intriguing Stories of Mathematical Physics (2009) Chases and Escapes: The Mathematics of Pursuit and Evasion (2007; reprinted in paperback 2012) Time Machines: Time Travel in Physics, Metaphysics, and Science Fiction (2001) The Science of Radio: With MATLAB and Electronics Workbench Demonstrations, 2nd Edition (2001) Duelling Idiots and Other Probability Puzzlers (2000); 2012 pbk edition",
                    "score": 0.8808408379554749
                },
                {
                    "id": 9094979,
                    "contents": "Eduard Schulte\nIn 1942, Schulte learned about the Final Solution concept, and in July 1942 he told Isidor Koppelman who relayed the information to Gerhart M. Riegner, the Swiss representative of World Jewish Congress. In August 1942, the Riegner Telegram notified the Allies, but they largely ignored the information which stated the estimated number of 3.5 to 4 million Jews, and the planned use of hydrogen cyanide. In 1943, the Gestapo noticed his activities, and Schulte had to permanently flee to Switzerland with his wife, while his two sons had to remain under German control to fight in the Wehrmacht. One son was killed in the war. After the war, Schulte remained silent. Riegner always refused to acknowledge who had supplied him with the information as this was \"the one request he ever made of me\". References \"Who Was the 'Mysterious Messenger'?\" by Richard Breitman, Commentary, pp. October 1, 1983",
                    "score": 0.8800444602966309
                },
                {
                    "id": 9649938,
                    "contents": "History of variational principles in physics\nLeonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the Additamentum 2 to his \"Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes\". He begins the second paragraph: \"Sit massa corporis projecti ==M, ejusque, dum spatiolum == ds emetitur, celeritas debita altitudini == v; erit quantitas motus corporis in hoc loco == ; quae per ipsum spatiolum ds multiplicata, dabit motum corporis collectivum per spatiolum ds. Iam dico lineam a corpore descriptam ita fore comparatam, ut, inter omnes alias lineas iisdem terminis contentas, sit , seu, ob M constans, minimum.\" A translation of this passage reads:",
                    "score": 0.8797284364700317
                },
                {
                    "id": 15171527,
                    "contents": "How Do You Solve a Problem Like Maria? (Canadian TV series)\nReferences External links Official Program Website at cbc.ca TV, eh? 2000s Canadian reality television series 2008 Canadian television series debuts 2008 Canadian television series endings CBC Television original programming Music competitions in Canada Singing talent shows The Sound of Music Television series by Temple Street Productions",
                    "score": 0.879703164100647
                },
                {
                    "id": 3216796,
                    "contents": "Hilbert's sixth problem\nHilbert gave the further explanation of this problem and its possible specific forms: \"As to the axioms of the theory of probabilities, it seems to me desirable that their logical investigation should be accompanied by a rigorous and satisfactory development of the method of mean values in mathematical physics, and in particular in the kinetic theory of gases. ... Boltzmann's work on the principles of mechanics suggests the problem of developing mathematically the limiting processes, there merely indicated, which lead from the atomistic view to the laws of motion of continua.\" History David Hilbert himself devoted much of his research to the sixth problem; in particular, he worked in those fields of physics that arose after he stated the problem. In the 1910s, celestial mechanics evolved into general relativity. Hilbert and Emmy Noether corresponded extensively with Albert Einstein on the formulation of the theory.",
                    "score": 0.8794090747833252
                },
                {
                    "id": 297462,
                    "contents": "Functional integration\nFunctional integration was developed by Percy John Daniell in an article of 1919 and Norbert Wiener in a series of studies culminating in his articles of 1921 on Brownian motion. They developed a rigorous method (now known as the Wiener measure) for assigning a probability to a particle's random path. Richard Feynman developed another functional integral, the path integral, useful for computing the quantum properties of systems. In Feynman's path integral, the classical notion of a unique trajectory for a particle is replaced by an infinite sum of classical paths, each weighted differently according to its classical properties. Functional integration is central to quantization techniques in theoretical physics. The algebraic properties of functional integrals are used to develop series used to calculate properties in quantum electrodynamics and the standard model of particle physics. Functional Integration",
                    "score": 0.8790582418441772
                },
                {
                    "id": 6992429,
                    "contents": "C. T. C. Wall\nHis notable students include Michael Boardman, Bill Bruce, Andrew Casson, Francis E. A. Johnson, David Mond, Andrew du Plessis, and David Trotman. Awards 1965 – Berwick Prize 1966 – Invited address at the 1966 ICM in Moscow 1969 – Elected Fellow of the Royal Society 1970 – Invited address at the 1970 ICM in Nice 1976 – Senior Whitehead Prize 1988 – Pólya Prize 1988 – Sylvester Medal 1990 – Elected a Foreign Member of the Royal Danish Academy of Sciences and Letters 2000 – Elected Honorary Member of the Irish Mathematical Society 2012 – Fellow of the American Mathematical Society",
                    "score": 0.8782984018325806
                },
                {
                    "id": 29423415,
                    "contents": "Do We Have a Problem?\n\"Do We Have a Problem?\" is a song by Trinidadian-born rapper Nicki Minaj and American rapper Lil Baby. It was released on February 4, 2022. It is Minaj's first song as a lead artist in two years. The music video runs nine minutes long and is inspired by the 2010 movie Salt. In the United States, the song debuted at number two on the Billboard Hot 100, and topped the Hot R&B/Hip-Hop Songs chart. Internationally, it reached number three in South Africa, number seven in Hungary, and number 14 in Canada.",
                    "score": 0.8781086802482605
                },
                {
                    "id": 4463015,
                    "contents": "Richard Lowenstein\nModels – \"Barbados\" (1985) Pete Townshend – \"Face the Face\" (1985), \"Secondhand Love\" (1985), \"Give Blood\" (1985) Big Pig – \"Hungry Town\" (1986) \"Boy Wonder\" (1988) Crowded House – \"Mean to Me\" (1986), \"Into Temptation\" (1988) Michael Hutchence – \"Rooms for the Memory\" (1987) U2 – \"Desire\" (1988), \"Angel of Harlem\" (1988) Max Q – \"Way of the World\" (1989), \"Sometimes\" (1990) Jenny Morris – \"Saved Me\" (1989)",
                    "score": 0.877973735332489
                },
                {
                    "id": 12058265,
                    "contents": "The Thirteen Problems\nThe Tuesday Night Club – first published in Volume 101, Number 5 on 2 June under the title The Solving Six. The Idol House of Astarte – first published in Volume 101, Number 6 on 9 June under the title The Solving Six and the Evil Hour. Ingots of Gold – first published in Volume 102, Number 1 on 16 June under the title The Solving Six and the Golden Grave. The Blood-Stained Pavement – first published in Volume 102, Number 2 on 23 June under the title Drip! Drip! Motive versus Opportunity – first published in Volume 102, Number 3 on 30 June under the title Where's the Catch? The Thumb Mark of St. Peter – first published in Volume 102, Number 4 on 7 July under its original title.",
                    "score": 0.8776658177375793
                },
                {
                    "id": 4986920,
                    "contents": "Optimization problem\nSee also − the optimum need not be found, just a \"good enough\" solution. References External links Computational problems",
                    "score": 0.8771229982376099
                },
                {
                    "id": 9649935,
                    "contents": "History of variational principles in physics\nA variational principle in physics is an alternative method for determining the state or dynamics of a physical system, by identifying it as an extremum (minimum, maximum or saddle point) of a function or functional. This article describes the historical development of such principles. Before modern times Variational principles are found among earlier ideas in surveying and optics. The rope stretchers of ancient Egypt stretched corded ropes between two points to measure the path which minimized the distance of separation, and Claudius Ptolemy, in his Geographia (Bk 1, Ch 2), emphasized that one must correct for \"deviations from a straight course\"; in ancient Greece Euclid states in his Catoptrica that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection; and Hero of Alexandria later showed that this path was the shortest length and least time.",
                    "score": 0.8767006993293762
                },
                {
                    "id": 20126425,
                    "contents": "Lemoine's problem\nLudwig Kiepert's solution Kiepert establishes the validity of his construction by proving a few lemmas. Problem Let A1, B1, C1 be the vertices of the equilateral triangles placed on the sides of a triangle ABC. Given A1, B1, C1 construct A, B, C. Lemma 1 If on the three sides of an arbitrary triangle ABC, one describes equilateral triangles ABC1, ACB1, BCA1, then the line segments AA1, BB1, CC1 are equal, they concur in a point P, and the angles they form one another are equal to 60°. Lemma 2 If on A1B1C1 one makes the same construction as that on ABC, there will have three equilateral triangles A1B1C2, A1C1B2, B1C1A2, three equal line segments A1A2, B1B2, C1C2, which will also concur at the point P. Lemma 3 A, B, C are respectively the midpoints of A1A2, B1B2, C1C2.",
                    "score": 0.8764293193817139
                },
                {
                    "id": 3331376,
                    "contents": "D'Alembert's principle\nNewton's dot notation is used to represent the derivative with respect to time. This above equation is often called d'Alembert's principle, but it was first written in this variational form by Joseph Louis Lagrange. D'Alembert's contribution was to demonstrate that in the totality of a dynamic system the forces of constraint vanish. That is to say that the generalized forces need not include constraint forces. It is equivalent to the somewhat more cumbersome Gauss's principle of least constraint. Derivations General case with variable mass The general statement of D'Alembert's principle mentions \"the time derivatives of the momenta of the system.\" By Newton's second law, the first time derivative of momentum is the force. The momentum of the -th mass is the product of its mass and velocity: and its time derivative is . In many applications, the masses are constant and this equation reduces to .",
                    "score": 0.8762650489807129
                },
                {
                    "id": 165154,
                    "contents": "Johann Bernoulli\nThe Bernoulli brothers often worked on the same problems, but not without friction. Their most bitter dispute concerned the brachistochrone curve problem, or the equation for the path followed by a particle from one point to another in the shortest amount of time, if the particle is acted upon by gravity alone. Johann presented the problem in 1696, offering a reward for its solution. Entering the challenge, Johann proposed the cycloid, the path of a point on a moving wheel, also pointing out the relation this curve bears to the path taken by a ray of light passing through layers of varied density. Jacob proposed the same solution, but Johann's derivation of the solution was incorrect, and he presented his brother Jacob's derivation as his own.",
                    "score": 0.8760547041893005
                },
                {
                    "id": 14381301,
                    "contents": "Equilibrium (Erik Mongrain album)\n5. \"The Silent Fool\" For all the moments in my life that I spoke too quickly, uncontrollably or stupidly. Living with regrets is harsh... Sometimes I just wish that I was mute! 6. \"Pandora's Box\" When you receive a new guitar, there is always the magic of discovering how it sounds on every move you can think of. Although you've tried it first or figured out how it should sound, you truly never know exactly what you're going to get! Just like the Pandora's box, everything could pop out of it! 7. \"Eon's Illusion\" A minute can be an eternity. This is a period of my life when I had to wait for a guitar in construction and my apartment in renovation. It seems to me it took forever when I was plunged into it, but now that I look back, it wasn't as bad as I thought it was! The illusion of time is tricky...",
                    "score": 0.8758493661880493
                },
                {
                    "id": 1448957,
                    "contents": "Action (physics)\nDare A. Wells, Lagrangian Dynamics, Schaum's Outline Series (McGraw-Hill, 1967) , A 350-page comprehensive \"outline\" of the subject. Robert Weinstock, Calculus of Variations, with Applications to Physics and Engineering (Dover Publications, 1974). . An oldie but goodie, with the formalism carefully defined before use in physics and engineering. Wolfgang Yourgrau and Stanley Mandelstam, Variational Principles in Dynamics and Quantum Theory (Dover Publications, 1979). A nice treatment that does not avoid the philosophical implications of the theory and lauds the Feynman treatment of quantum mechanics that reduces to the principle of least action in the limit of large mass. Edwin F. Taylor's page",
                    "score": 0.8758124709129333
                },
                {
                    "id": 6283173,
                    "contents": "Ritz method\nThe Ritz method can be used to achieve this goal. In the language of mathematics, it is exactly the finite element method used to compute the eigenvectors and eigenvalues of a Hamiltonian system. Discussion As with other variational methods, a trial wave function, , is tested on the system. This trial function is selected to meet boundary conditions (and any other physical constraints). The exact function is not known; the trial function contains one or more adjustable parameters, which are varied to find a lowest energy configuration. It can be shown that the ground state energy, , satisfies an inequality: That is, the ground-state energy is less than this value. The trial wave-function will always give an expectation value larger than or equal to the ground-energy. If the trial wave function is known to be orthogonal to the ground state, then it will provide a boundary for the energy of some excited state.",
                    "score": 0.875653862953186
                },
                {
                    "id": 714695,
                    "contents": "Norbert Wiener\n100 V1's as they entered Britain from the English channel, on their way to London. What emerged was a mathematical theory of great generality—a theory for predicting the future as best one can on the basis of incomplete information about the past. It was a statistical theory that included applications that did not, strictly speaking, predict the future, but only tried to remove noise. It made use of Wiener's earlier work on integral equations and Fourier transforms.",
                    "score": 0.8754984736442566
                },
                {
                    "id": 25136396,
                    "contents": "A Quiet End\nto solution without ever taking the time to truly analyze himself. Max responds that he felt that everyone, particularly Jason, was more perfect than he was. Jason begs for Max to take him back, but Max refuses, telling Jason that his future is outside the apartment. The two kiss passionately, and Jason leaves.",
                    "score": 0.8751626014709473
                },
                {
                    "id": 11718890,
                    "contents": "Steiner's calculus problem\nSteiner's problem, asked and answered by , is the problem of finding the maximum of the function It is named after Jakob Steiner. The maximum is at , where e denotes the base of the natural logarithm. One can determine that by solving the equivalent problem of maximizing Applying the first derivative test, the derivative of is so is positive for and negative for , which implies that – and therefore – is increasing for and decreasing for Thus, is the unique global maximum of References Functions and mappings Mathematical optimization",
                    "score": 0.8750984072685242
                },
                {
                    "id": 23599153,
                    "contents": "How Do You Solve a Problem Like Camilla?\nSee also Whatever Love Means (2005 television movie) References External links 2009 television specials Channel 4 documentaries Camilla, Duchess of Cornwall 2000s British documentary television series 2000s British drama television series Films about Elizabeth II Cultural depictions of Elizabeth II",
                    "score": 0.8749755620956421
                },
                {
                    "id": 9470717,
                    "contents": "Julius Thomas Fraser\natemporal - blank sheet of paper, objects travelling at speed of light, black hole/Big Bang, causation has no meaning prototemporal - fragmented shaft of an arrow, particle-waves travelling at less than speed of light, instants may be specified only statistically, probabilistic causation joins prototemporal events eotemporal - shaft of an arrow, countable and orderable without a preferred direction, nowless time, physical matter, time orientable but not time oriented, deterministic causation joins eotemporal events biotemporal - short arrow, future, past, present, limited temporal horizons, organic present, simultaneities of necessity, organic intentionality directed toward concrete goals and serving the continuity of the organism's life, multiple and final causation, rigid programming gives way to dynamic programming",
                    "score": 0.8749135136604309
                },
                {
                    "id": 3916854,
                    "contents": "Hamilton–Jacobi equation\nThe Hamilton–Jacobi equation is also the only formulation of mechanics in which the motion of a particle can be represented as a wave. In this sense, it fulfilled a long-held goal of theoretical physics (dating at least to Johann Bernoulli in the eighteenth century) of finding an analogy between the propagation of light and the motion of a particle. The wave equation followed by mechanical systems is similar to, but not identical with, Schrödinger's equation, as described below; for this reason, the Hamilton–Jacobi equation is considered the \"closest approach\" of classical mechanics to quantum mechanics. In mathematics, the Hamilton–Jacobi equation is a necessary condition describing extremal geometry in generalizations of problems from the calculus of variations. It can be understood as a special case of the Hamilton–Jacobi–Bellman equation from dynamic programming. Notation Boldface variables such as represent a list of generalized coordinates,",
                    "score": 0.874821662902832
                },
                {
                    "id": 17419536,
                    "contents": "Max D. Barnes\n\"Chiseled in Stone\" – 1989 Country Award \"Don't Take It Away\" – 1980 Country Award \"Don't Tell Me What to Do\" – 1992 Country Award/Million-Air (Two million) \"Drinkin' and Dreamin'\" – 1986 Country Award \"I Can't Love You Enough\" – 1978 Country Award \"I've Got It Made\" – 1995 Country Award/Million-Air \"I Won't Need You Anymore (Always and Forever)\" – 1988 Country Award/Million-Air \"If I Didn't Have You\" – 1993 Country Award/Million-Air (Two million) \"Joe Knows How To Live\" – 1989 Country Award/Million-Air \"Let Go of The Stone\" – 1993 Country Award/Million-Air \"Look at Us\" – 1992 Country Award/Million-Air (Two million) \"Ten Feet Away\" – 1987 Country Award \"Red Neckin' Love Makin' Night\" – 1982 Pop Award/1982 Country Award \"Thank God for the Radio\" – 1985 Country Award \"That Just About Does It\" – 1990 Country Award \"Way Down Deep\" – 1984 Country Award \"Who's Gonna Fill Their Shoes\" – 1987 Country Award \"Do You Believe Me Now?\" – Million-Air References",
                    "score": 0.8748029470443726
                },
                {
                    "id": 21754564,
                    "contents": "El señor del cero\nthat can recite mathematical problems. He also meets Emma, the daughter of a man that fled an Arab land. In the end José marries Emma and they live happily in Navarre. The two also send a letter to José's father saying that they would like to go to Toledo and raise a family.",
                    "score": 0.8747498393058777
                },
                {
                    "id": 2233906,
                    "contents": "1944 in science\nChemistry February – Lars Onsager publishes the exact solution to the two-dimensional Ising model. Americium discovered by Glenn T. Seaborg, et al. Computer science August 7 – IBM dedicates the first program-controlled calculator, the Automatic Sequence Controlled Calculator, best known as the Harvard Mark I. Geology March 18 – Last eruption of Mount Vesuvius. History of science November 4 – The Whipple Museum of the History of Science is established when Robert Whipple presents his collection of scientific instruments to the University of Cambridge, England. C. Doris Hellman publishes her Columbia University thesis The Comet of 1577: Its Place in the History of Astronomy. Mathematics John von Neumann and Oskar Morgenstern's book Theory of Games and Economic Behavior is published by Princeton University Press.",
                    "score": 0.874668538570404
                },
                {
                    "id": 9649942,
                    "contents": "History of variational principles in physics\nFurther developments Euler continued to write on the topic; in his Reflexions sur quelques loix generales de la nature (1748), he called the quantity \"effort\". His expression corresponds to what we would now call potential energy, so that his statement of least action in statics is equivalent to the principle that a system of bodies at rest will adopt a configuration that minimizes total potential energy. The full importance of the principle to mechanics was stated by Joseph Louis Lagrange in 1760, although the variational principle was not used to derive the equations of motion until almost 75 years later, when William Rowan Hamilton in 1834 and 1835 applied the variational principle to the function to obtain what are now called the Lagrangian equations of motion.",
                    "score": 0.8745534420013428
                },
                {
                    "id": 8254994,
                    "contents": "Maupertuis's principle\nIn classical mechanics, Maupertuis's principle (named after Pierre Louis Maupertuis) states that the path followed by a physical system is the one of least length (with a suitable interpretation of path and length). It is a special case of the more generally stated principle of least action. Using the calculus of variations, it results in an integral equation formulation of the equations of motion for the system. Mathematical formulation Maupertuis's principle states that the true path of a system described by generalized coordinates between two specified states and is a stationary point (i.e., an extremum (minimum or maximum) or a saddle point) of the abbreviated action functional where are the conjugate momenta of the generalized coordinates, defined by the equation",
                    "score": 0.8745126724243164
                },
                {
                    "id": 1200376,
                    "contents": "Richard Feynman\nWhile papers by others initially cited Schwinger, papers citing Feynman and employing Feynman diagrams appeared in 1950, and soon became prevalent. Students learned and used the powerful new tool that Feynman had created. Computer programs were later written to compute Feynman diagrams, providing a tool of unprecedented power. It is possible to write such programs because the Feynman diagrams constitute a formal language with a formal grammar. Marc Kac provided the formal proofs of the summation under history, showing that the parabolic partial differential equation can be re-expressed as a sum under different histories (that is, an expectation operator), what is now known as the Feynman–Kac formula, the use of which extends beyond physics to many applications of stochastic processes. To Schwinger, however, the Feynman diagram was \"pedagogy, not physics\".",
                    "score": 0.8742257356643677
                },
                {
                    "id": 26411757,
                    "contents": "John M. Blatt\nwith Stuart Thomas Butler: A modern introduction to physics, Sydney, Horwitz-Grahame 1960, 1965 volume 1 Mechanics of particles, volume 2 Kinetic theory of matter and mechanics of solids",
                    "score": 0.8738653063774109
                },
                {
                    "id": 25823702,
                    "contents": "Simon problems\nThe 2000 list was a refinement of a similar set of problems that Simon had posed in 1984. Context Background definitions for the \"Coulomb energies\" problems: is the space of functions on which are antisymmetric in spin and space. The Hamiltonian is . We have . We define to be the smallest value of for all positive integers ; it is known that such a number always exists and is always between and , inclusive. The 1984 list Simon listed the following problems in 1984: In 2000, Simon claimed that five of the problems he listed had been solved. The 2000 list The Simon problems as listed in 2000 (with original categorizations) are: See also Almost Mathieu operator Lieb–Thirring inequality External links References unsolved problems in mathematics Unsolved problems in physics Mathematical physics",
                    "score": 0.8737179040908813
                },
                {
                    "id": 14381302,
                    "contents": "Equilibrium (Erik Mongrain album)\n8. \"Raindigger\" There are times in life when we try to reach for something or someone that we know will remain out of our grasp regardless of how fast we try to run for it. Try to picture yourself running with abandon into the night, your whole body slashed by rain and wind, reaching out as far as you can to try and get a glimpse of your goal... 9. \"Maelström\" Ever been confused by love and all the choices it can lay upon you ? Confusion, like a maelstrom, can and will most likely swallow everything in its path... including part of your sanity!",
                    "score": 0.873645544052124
                },
                {
                    "id": 1200417,
                    "contents": "Richard Feynman\nIncludes Feynman's Tips on Physics (with Michael Gottlieb and Ralph Leighton), which includes four previously unreleased lectures on problem solving, exercises by Robert Leighton and Rochus Vogt, and a historical essay by Matthew Sands. Three volumes; originally published as separate volumes in 1964 and 1966.",
                    "score": 0.8735589385032654
                },
                {
                    "id": 3112065,
                    "contents": "Path integral formulation\nThe path integral and the partition function The path integral is just the generalization of the integral above to all quantum mechanical problems— is the action of the classical problem in which one investigates the path starting at time and ending at time , and denotes integration over all paths. In the classical limit, , the path of minimum action dominates the integral, because the phase of any path away from this fluctuates rapidly and different contributions cancel.",
                    "score": 0.8735446929931641
                },
                {
                    "id": 4748824,
                    "contents": "Saltimbanco\nThe 1993 Costa Mesa cast of 36 included; With Miguel Arias, Dimitrii Arnaoutov, Rene Bazinet, Alain Berge, Pawel Biegaj , Witek Biegaj, Martin Boisvert, Jean-Paul Boun, Jenny Clement, Andrea Conway, Vincent Cotnoir, Nicolas Dupere, Joscelyn Drainville, Alain Gauthier, Nui Guishan, Sun Hongli, Miguel Herrera, Galina Karableva, Guy Kaye, Brigitt Larochelle, Isabelle Larose, Jean-Francois Lemieux, Marco Lorador, Paulo Lorador, Daniel Olivier, Francois Dumais, Francine Poitras, Mathieu Roy, Karyne Steben, Sarah Steben, Sonia St-Martin, Zhang Shengli, Anton Tchelnokov, Nikolai Tchelnokov, Neomi Tamelio, Guennadi Tchijov, Huang Zhen.",
                    "score": 0.8734725713729858
                },
                {
                    "id": 9649940,
                    "contents": "History of variational principles in physics\nAs Euler states, is the integral of the momentum over distance traveled (note that here contrary to usual notation denotes the squared velocity) which, in modern notation, equals the reduced action . Thus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. In rather general terms he wrote that \"Since the fabric of the Universe is most perfect and is the work of a most wise Creator, nothing whatsoever takes place in the Universe in which some relation of maximum and minimum does not appear.\" However, Euler did not claim any priority, as the following episode shows.",
                    "score": 0.873464822769165
                },
                {
                    "id": 20468033,
                    "contents": "Problems (TV series)\nProblems is an Australian television comedy series starring Sam Simmons. The series premiered on 21 November 2012 on ABC. Cast Sam Simmons as Sam Lawrence Mooney as Mooney Ronny Chieng as Mr Meowgi Anthony Morgan as Morgan Claudia O'Doherty as Claudia David Quirk as Dave Gary Sweet as Mr Moth Susie Porter as Mrs Moth Reg Gorman as Ron Doug Bayne as Brian Kate McCartney Laura Hughes Episodes Episode 1: Taco Night Episode 2: Bus Crush Episode 3: First Hot Day Episode 4: The Cardboard Angel See also List of Australian television series List of Australian Broadcasting Corporation programs References External links Official ABC site 2012 Australian television series debuts Australian Broadcasting Corporation original programming Australian comedy television series",
                    "score": 0.8733526468276978
                },
                {
                    "id": 1941123,
                    "contents": "Stationary-action principle\nwhich is the integral of twice what we now call the kinetic energy T of the system. Euler Leonhard Euler gave a formulation of the action principle in 1744, in very recognizable terms, in the Additamentum 2 to his Methodus Inveniendi Lineas Curvas Maximi Minive Proprietate Gaudentes. Beginning with the second paragraph: As Euler states, ∫Mvds is the integral of the momentum over distance travelled, which, in modern notation, equals the abbreviated or reduced action Thus, Euler made an equivalent and (apparently) independent statement of the variational principle in the same year as Maupertuis, albeit slightly later. Curiously, Euler did not claim any priority, as the following episode shows. Disputed priority",
                    "score": 0.8731775283813477
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_5",
        "question": "A household lightbulb is a blackbody radiator. Many lightbulbs use tungsten filaments that are heated by an electric current. What temperature is needed so that $\\lambda_{\\max }=550 \\mathrm{~nm}$ ?",
        "golden_answers": [
            " 5300"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 22205061,
                    "contents": "Edison light bulb\nUS inventor Robert (Bob) Kyp patented light bulbs under the name Balafire, and radiometers; the company he ran since 1964, Kyp-Go, is the only US manufacturer of carbon-filament bulbs, which have been used in a commercial. References Types of lamp",
                    "score": 0.910285234451294
                },
                {
                    "id": 971380,
                    "contents": "Incandescent light bulb\nHistorian Thomas Hughes has attributed Edison's success to his development of an entire, integrated system of electric lighting. Early pre-commercial research In 1761, Ebenezer Kinnersley demonstrated heating a wire to incandescence. In 1802, Humphry Davy used what he described as \"a battery of immense size\", consisting of 2,000 cells housed in the basement of the Royal Institution of Great Britain, to create an incandescent light by passing the current through a thin strip of platinum, chosen because the metal had an extremely high melting point. It was not bright enough nor did it last long enough to be practical, but it was the precedent behind the efforts of scores of experimenters over the next 75 years. Over the first three-quarters of the 19th century, many experimenters worked with various combinations of platinum or iridium wires, carbon rods, and evacuated or semi-evacuated enclosures. Many of these devices were demonstrated and some were patented.",
                    "score": 0.8973674178123474
                },
                {
                    "id": 971387,
                    "contents": "Incandescent light bulb\nlighting with relatively high vacuum, a carbon conductor, and platinum lead-in wires. This bulb lasted about 40 hours. Swan then turned his attention to producing a better carbon filament and the means of attaching its ends. He devised a method of treating cotton to produce 'parchmentised thread' in the early 1880s and obtained British Patent 4933 that same year. From this year he began installing light bulbs in homes and landmarks in England. His house, Underhill, Low Fell, Gateshead, was the first in the world to be lit by a lightbulb. In the early 1880s he had started his company. In 1881, the Savoy Theatre in the City of Westminster, London was lit by Swan incandescent lightbulbs, which was the first theatre, and the first public building in the world, to be lit entirely by electricity. The first street in the world to be lit by an incandescent lightbulb was Mosley Street, Newcastle upon Tyne, United Kingdom. It was lit by Joseph Swan's incandescent lamp on 3 February 1879.",
                    "score": 0.8970553278923035
                },
                {
                    "id": 8872168,
                    "contents": "List of light sources\nThis is a list of sources of light. Light sources include light bulbs and stars like the Sun. Reflectors (such as the moon, cat's eyes, and mirrors) do not actually produce the light that comes from them. Incandescence Incandescence is the emission of light from a hot body as a result of its temperature. Nernst lamp (Defunct) Volcanic eruption Combustion Lamps Argand lamp (Defunct) Argon flash Carbide lamp Coleman lantern Betty lamp (error) Butter lamp Flash-lamp (error) Gas lighting Gas mantle Kerosene lamps Koniaphostic light, see Limelight Lanterns Limelights (Defunct) Oil lamps Tilley lamp Other Brazier Bunsen burner Candle Embers Explosives Fire Fire whirl Fireworks Flamethrower Muzzle flash Rubens' tube Torch Nuclear and high-energy particle Annihilation Nuclear bomb Cherenkov radiation Synchrotron radiation Free electron laser Bremsstrahlung Celestial and atmospheric",
                    "score": 0.896134078502655
                },
                {
                    "id": 10496205,
                    "contents": "Centennial Light\nHistory The Centennial Light was originally a 30-watt (or 60-watt) bulb, but is now very dim, emitting about the same light as a 4-watt nightlight. The hand-blown, carbon-filament common light bulb was invented by Adolphe Chaillet, a French engineer who filed a patent for this technology. It was manufactured in Shelby, Ohio, by the Shelby Electric Company in the late 1890s; many just like it still exist and can be found functioning. According to Zylpha Bernal Beck, the bulb was donated to the Fire Department by her father, Dennis Bernal, in 1901. Bernal owned the Livermore Power and Water Company and donated the bulb to the fire station when he sold the company. That story has been supported by firefighter volunteers of that era.",
                    "score": 0.8957180976867676
                },
                {
                    "id": 22205056,
                    "contents": "Edison light bulb\nEdison light bulbs, retroactively referred to as antique light bulbs, and vintage light bulbs, refer to carbon- or early tungsten-filament incandescent lamps, or modern bulbs reproducing their appearance. Most of these bulbs are reproductions of the wound filament bulbs made popular by Edison Electric Light Company at the turn of the 20th century. They are easily identified by the long and complicated windings of their internal filaments, and by the very warm-yellow glow of the light they produce (many of the bulbs emit light at a color temperature of 2200–2400K). History",
                    "score": 0.895568311214447
                },
                {
                    "id": 10439888,
                    "contents": "Longest-lasting light bulbs\nEdison and the Eternal Light Thomas Edison designed a bulb that was supposed to last forever, called the Eternal Light, and turned it on on October 22, 1929. The bulb is located in the Edison Memorial Tower at the Thomas Edison Center at Menlo Park, a small museum near the tower in Menlo Park, New Jersey. The tower fell down in 1937, but the bulb's power was supposedly uninterrupted, according to General Electric, and the bulb continued to burn while a second tower was constructed. However, according to museum curator Jack Stanley, the bulb is fake, consisting of a hollow bulb illuminated by a series of automobile headlights mounted in the display's base. See also Adolphe Alexandre Chaillet Eternal flame Phoebus cartel Sanctuary lamp References External links Livermore's Centennial Light Light Bulb Methuselahs Christian Science Monitor: Livermore's Centennial Light Watt a Lightbulb at Snopes.com Television Television News Archive: Eternal Light Bulb",
                    "score": 0.8935915231704712
                },
                {
                    "id": 13161686,
                    "contents": "Infrared lamp\nTypes Incandescent light bulbs use a tungsten filament heated to high temperature to produce visible light and, necessarily, even more infrared radiation. Round bulbs, often tinted red to reduce visible light, provide infrared radiant heat suitable for warming of people or animals, but the power density available is low. The development of quartz halogen linear lamps allowed much higher power density up to 200 watts/inch of lamp (8 w/mm), useful for industrial heating, drying and processing applications. By adjusting the voltage applied to incandescent lamps, the spectrum of the radiated energy can be made to reduce visible light and emphasize infrared energy production. Different wavelengths of infrared radiation are differently absorbed by different materials.",
                    "score": 0.8926243782043457
                },
                {
                    "id": 1268978,
                    "contents": "Thomas Edison\nElectric light In 1878, Edison began working on a system of electrical illumination, something he hoped could compete with gas and oil-based lighting. He began by tackling the problem of creating a long-lasting incandescent lamp, something that would be needed for indoor use. However, Thomas Edison did not invent the light bulb. In 1840, British scientist Warren de la Rue developed an efficient light bulb using a coiled platinum filament but the high cost of platinum kept the bulb from becoming a commercial success. Many other inventors had also devised incandescent lamps, including Alessandro Volta's demonstration of a glowing wire in 1800 and inventions by Henry Woodward and Mathew Evans. Others who developed early and commercially impractical incandescent electric lamps included Humphry Davy, James Bowman Lindsay, Moses G. Farmer, William E. Sawyer, Joseph Swan, and Heinrich Göbel.",
                    "score": 0.8907244205474854
                },
                {
                    "id": 23002384,
                    "contents": "Electro-Dynamic Light Company\nIn 1878, the company demonstrated an electric light that was the invention of Sawyer and Man. An exhibition was set up in New York City on October 29, 1878. The same exhibition was mentioned several weeks later in a newspaper of Princeton, Minnesota, and Bismarck, North Dakota. The lamp was described as a strip of pencil carbon graphite connected with two wires to an electric generator. The carbon strip was in a hermetically sealed glass bulb that was filled with nitrogen gas. When electricity was applied, the internal strip developed a temperature of between 5,000 and . Since there was no oxygen in the glass globe the carbon filament did not burn out and produced light instead when it got hot.",
                    "score": 0.8901302218437195
                },
                {
                    "id": 20942592,
                    "contents": "Liter of Light\nTechnology description The Solar Bottle Bulb, as it has also been called, is installed in the roof of homes with the purpose of refracting sunlight in order to light up a room. The project's innovation lies in its utilization of cheap, durable and readily available materials to produce high quality natural lighting enabling the urban poor to have access to an affordable, environmentally friendly long-term alternative to electric light for use during the day.",
                    "score": 0.8895237445831299
                },
                {
                    "id": 6570266,
                    "contents": "Joseph Swan\nHis house, Underhill, Low Fell, Gateshead, was the world's first to have working light bulbs installed. The Lit & Phil Library in Westgate Road, Newcastle, was the first public room lit by electric light during a lecture by Swan on 20 October 1880. In 1881, he founded his own company, The Swan Electric Light Company, and started commercial production.",
                    "score": 0.8888841867446899
                },
                {
                    "id": 10496204,
                    "contents": "Centennial Light\nThe Centennial Light is the world's longest-lasting light bulb, burning since 1901, and almost never switched off. It is at 4550 East Avenue, Livermore, California, and maintained by the Livermore-Pleasanton Fire Department. Due to its longevity, the bulb has been noted by The Guinness Book of World Records, Ripley's Believe It or Not!, and General Electric.",
                    "score": 0.8883799910545349
                },
                {
                    "id": 1106177,
                    "contents": "Timeline of lighting technology\n20th century 1901 Peter Cooper Hewitt creates the first commercial mercury-vapor lamp. 1904 Alexander Just and Franjo Hanaman invent the tungsten filament for incandescent lightbulbs. 1910 Georges Claude demonstrates neon lighting at the Paris Motor Show. 1912 Charles P. Steinmetz invents the metal-halide lamp. 1913 Irving Langmuir discovers that inert gas could double the luminous efficacy of incandescent lightbulbs. 1917 Burnie Lee Benbow patents the coiled coil filament. 1920 Arthur H. Compton invents the sodium-vapor lamp. 1921 Junichi Miura creates the first incandescent lightbulb to utilize a coiled coil filament. 1925 Marvin Pipkin invents the first internal frosted lightbulb. 1926 Edmund Germer patents the modern fluorescent lamp. 1927 Oleg Losev creates the first LED (light-emitting diode). 1953 Elmer Fridrich invents the halogen light bulb. 1953 André Bernanose and several colleagues observe electroluminescence in organic materials.",
                    "score": 0.8883340954780579
                },
                {
                    "id": 7029122,
                    "contents": "Nernst lamp\nAfter Nernst lamps became obsolete, \"Nernst glowers\" became the infrared-emitting source used in IR spectroscopy devices. (Recently, even this has become obsolete as Nernst glowers have been largely replaced for this purpose by silicon carbide glow bars or \"globars\", which are conductive even at room temperature and therefore need no preheating.) See also Globar, a silicon carbide rod used as thermal light source for infrared spectroscopy List of light sources References External links - video showing the lamp in operation. Walther Nernst Products introduced in 1897 Incandescent light bulbs",
                    "score": 0.8875241875648499
                },
                {
                    "id": 4339420,
                    "contents": "Livermore, California\nWorld's longest-lasting light bulb The city is noted for one world record. A 120+ year old 4-watt light bulb, called the Centennial Light, housed in the Livermore-Pleasanton Fire Department main station, is still burning. Originally installed by Augustus Donner Wilson, the bulb has been maintained through successive generations until his great-great granddaughter Alissa Wilson. It glows dimly, but still functions as a light bulb. The Guinness Book of World Records, Ripley's Believe It or Not!, and General Electric have concluded that the bulb has been burning continuously since 1901 with the exception of power failures and the three times it was disconnected for moves to new stations. The light bulb was manufactured by the Shelby Electric Company and was hand blown with a carbon filament.",
                    "score": 0.8874599933624268
                },
                {
                    "id": 22205060,
                    "contents": "Edison light bulb\nModern \"Edison light bulbs\" are designed to replicate the same light color and bulb shape to offer a more energy-efficient version of the popular vintage reproduction bulbs. (Modern tungsten coils are already more efficient.) These bulbs also maintain the same \"exposed\" look to further preserve the vintage reproduction style, and often employ the \"ST\" long-pear bulb shape for the same reason. LED bulbs, including LED retro types, are much more energy-efficient than any incandescent lighting.",
                    "score": 0.8871378302574158
                },
                {
                    "id": 971381,
                    "contents": "Incandescent light bulb\nIn 1835, James Bowman Lindsay demonstrated a constant electric light at a public meeting in Dundee, Scotland. He stated that he could \"read a book at a distance of one and a half feet\". However he did not develop the electric light any further. In 1838, Belgian lithographer Marcellin Jobard invented an incandescent light bulb with a vacuum atmosphere using a carbon filament. In 1840, British scientist Warren de la Rue enclosed a coiled platinum filament in a vacuum tube and passed an electric current through it. The design was based on the concept that the high melting point of platinum would allow it to operate at high temperatures and that the evacuated chamber would contain fewer gas molecules to react with the platinum, improving its longevity. Although a workable design, the cost of the platinum made it impractical for commercial use.",
                    "score": 0.8861982822418213
                },
                {
                    "id": 6371100,
                    "contents": "Luminous efficacy\nSources that depend on thermal emission from a solid filament, such as incandescent light bulbs, tend to have low overall efficacy because, as explained by Donald L. Klipstein, \"An ideal thermal radiator produces visible light most efficiently at temperatures around 6300 °C (6600 K or 11,500 °F). Even at this high temperature, a lot of the radiation is either infrared or ultraviolet, and the theoretical luminous [efficacy] is 95 lumens per watt. No substance is solid and usable as a light bulb filament at temperatures anywhere close to this. The surface of the sun is not quite that hot.\" At temperatures where the tungsten filament of an ordinary light bulb remains solid (below 3683 kelvin), most of its emission is in the infrared. SI photometry units See also Photometry Light pollution Wall-plug efficiency Coefficient of utilization List of light sources Notes References",
                    "score": 0.8861340284347534
                },
                {
                    "id": 18413236,
                    "contents": "John Wellington Starr\nJohn Wellington Starr (1822? – November 21, 1846) was an American inventor and pioneer in development of the incandescent light bulb. Life Starr was born in Cincinnati. In 1844, in association with John Milton Sanders (1821?–1877?), he filed a U.S. patent caveat for an incandescent lamp and generator. He appears never to have demonstrated his electric lamp while in Cincinnati.",
                    "score": 0.8858302235603333
                },
                {
                    "id": 971382,
                    "contents": "Incandescent light bulb\nIn 1841, Frederick de Moleyns of England was granted the first patent for an incandescent lamp, with a design using platinum wires contained within a vacuum bulb. He also used carbon. In 1845, American John W. Starr patented an incandescent light bulb using carbon filaments. His invention was never produced commercially. In 1851, Jean Eugène Robert-Houdin publicly demonstrated incandescent light bulbs on his estate in Blois, France. His light bulbs are on display in the museum of the Château de Blois. In 1859, Moses G. Farmer built an electric incandescent light bulb using a platinum filament. Thomas Edison later saw one of these bulbs in a shop in Boston, and asked Farmer for advice on the electric light business.",
                    "score": 0.8854162096977234
                },
                {
                    "id": 1697452,
                    "contents": "Electric light\nUses other than illumination Electric lamps can be used as heat sources, for example in incubators, as infrared lamps in fast food restaurants and toys such as the Kenner Easy-Bake Oven. Due to their nonlinear resistance characteristics, tungsten filament lamps have long been used as fast-acting thermistors in electronic circuits. Popular uses have included: Stabilization of sine wave oscillators Protection of tweeters in loudspeaker enclosures; excess current that is too high for the tweeter illuminates the light rather than destroying the tweeter. Automatic volume control in telephones A stylized depiction of a light bulb features as the logo of the Turkish AK Party. Circuit symbols In circuit diagrams lamps usually are shown as symbols. There are two main types of symbols, these are: Cultural symbolism In Western culture, a lightbulb — in particular, the appearance of an illuminated lightbulb above a person's head — signifies sudden inspiration.",
                    "score": 0.8848094940185547
                },
                {
                    "id": 21884790,
                    "contents": "LuminAID\nThe LuminAID is a solar-rechargeable light that is waterproof, lightweight, and inflates to diffuse light like a lantern. LuminAID technology was invented in 2010 by Anna Stork and Andrea Sreshta. Product The LuminAID light has a solar panel, rechargeable battery, and a multi-chip LED light. According to its makers, after a full charge, it can deliver 35 lumens for 8 hours or 20 lumens for 16 hours and the battery can be recharged over 500 times, for years of use. The product has won first place in several business competitions, including the $100K Midwest 2013 Clean Energy Challenge, the William James Business Plan Competition, and the Chicago Booth School of Business Social New Venture Challenge. The technology is currently patent-pending under the United States and the Patent Cooperation Treaty internationally. History",
                    "score": 0.884453535079956
                },
                {
                    "id": 20225608,
                    "contents": "Vittoria Light\nThe light itself is an electrical light since its first lighting. The current light is a 1000 watt halogen bulb. Visiting The site of the lighthouse is open to the public. The lighthouse itself is open Saturday and Sunday 3 pm to 7 pm, from the last Saturday of April to the second Sunday of October. Reaching the top requires climbing 285 steps. See also List of tallest lighthouses in the world List of lighthouses in Italy Notes References External links Servizio Fari Marina Militare Lighthouses completed in 1927 Lighthouses in Italy Buildings and structures in Trieste Tourist attractions in Friuli-Venezia Giulia",
                    "score": 0.8844396471977234
                },
                {
                    "id": 1697438,
                    "contents": "Electric light\nDifferent types of lights have vastly differing efficacies and color of light. *Color temperature is defined as the temperature of a black body emitting a similar spectrum; these spectra are quite different from those of black bodies. The most efficient source of electric light is the low-pressure sodium lamp. It produces, for all practical purposes, a monochromatic orange-yellow light, which gives a similarly monochromatic perception of any illuminated scene. For this reason, it is generally reserved for outdoor public lighting applications. Low-pressure sodium lights are favoured for public lighting by astronomers, since the light pollution that they generate can be easily filtered, contrary to broadband or continuous spectra. Incandescent light bulb The modern incandescent light bulb, with a coiled filament of tungsten, and commercialized in the 1920s, developed from the carbon filament lamp introduced about 1880.",
                    "score": 0.8842471837997437
                },
                {
                    "id": 7093941,
                    "contents": "Mazda (light bulb)\nIn 1909 the Mazda name was created for the tungsten filament light bulb. GE sold bulbs under this trademark starting in 1909. GE promoted the mark as identifying tungsten filament bulbs with predictable performance and life expectancy. GE also licensed the Mazda name, socket sizes, and tungsten filament technology to other manufacturers to establish a standard for lighting. Bulbs were soon sold by many manufacturers with the Mazda name licensed from GE, including British Thomson-Houston in the United Kingdom, Toshiba in Japan, and GE's chief competitor Westinghouse. Tungsten-filament bulbs of the Mazda type were initially more costly than carbon filament bulbs, but used less electricity. Often electrical utilities would trade new lamps for consumers' burned-out bulbs. In at least one case the authority regulating energy rates required the utility to use only tungsten bulbs so as not to inflate customer's energy use.",
                    "score": 0.8841114044189453
                },
                {
                    "id": 2045221,
                    "contents": "Henry Woodward (inventor)\nThe relationship of the Woodward/Evans work on the incandescent bulb to that of others, including Edison, on electric light is explained in the following passage of an article in a 1900 issue of Electrical World and Engineer as follows:",
                    "score": 0.884064257144928
                },
                {
                    "id": 10439884,
                    "contents": "Longest-lasting light bulbs\nOther long-lasting light bulbs Second The second-longest-lasting light bulb is in Fort Worth, Texas. The bulb, known as the Eternal Light, was credited as being the longest-lasting bulb in the 1970 edition of the Guinness Book of World Records, two years before the discovery of the Livermore bulb. The bulb was originally at the Byers Opera House, and was installed by a stage-hand, Barry Burke, on , above the backstage door. The theater was demolished in 1977, and the bulb was transported to a museum located in the Livestock Exchange Building. Since its installation in the museum, it has only been turned off once—by accident—before being put on its own unswitched circuit.",
                    "score": 0.8840504288673401
                },
                {
                    "id": 971436,
                    "contents": "Incandescent light bulb\nBulb shapes Bulb shape and size designations are given in national standards. Some designations are one or more letters followed by one or more numbers, e.g. A55 or PAR38, where the letters identify the shape and the numbers some characteristic size. National standards such as ANSI C79.1-2002, IS 14897:2000 and JIS C 7710:1988 cover a common terminology for bulb shapes. Common shape codes General Service Light emitted in (nearly) all directions. Available either clear or frosted. Types: General (A), Mushroom, elliptical (E), sign (S), tubular (T) 120 V sizes: A17, 19 and 21 230 V sizes: A55 and 60 High Wattage General Service Lamps greater than 200 watts. Types: Pear-shaped (PS) Decorative lamps used in chandeliers, etc. Smaller candle-sized bulbs may use a smaller socket. Types: candle (B), twisted candle, bent-tip candle (CA & BA), flame (F), globe (G), lantern chimney (H), fancy round (P) 230 V sizes: P45, G95",
                    "score": 0.8836482167243958
                },
                {
                    "id": 22982384,
                    "contents": "Hammer Historical Collection of Incandescent Electric Lamps\nCase 1 Case No. 1 contained a selection of lamps that represented the basics of the technology involved in the early stages of the electric light bulb development. The case displayed actual physical light bulbs made by Thomas Edison and others from 1878 to 1883. Edison's earliest steps in the bamboo filament and graphite filament lamps are displayed in this case. The first lamp made in a pear shape was in this display, which was the universal shape for electric light bulbs for decades. These in this display case contained numbers that ranged from 1 – 109. Case 2",
                    "score": 0.8830010890960693
                },
                {
                    "id": 1912745,
                    "contents": "Flashlight\nLED Powerful white-light-emitting diodes (LEDs) have mostly replaced incandescent bulbs in practical flashlights. LEDs existed for decades, mainly as low-power indicator lights. In 1999, Lumileds Corporation of San Jose, California, introduced the Luxeon LED, a high-power white-light emitter. This made possible LED flashlights with power and running time better than incandescent lights. The first Luxeon LED flashlight was the Arc LS, designed in 2001. White LEDs in 5 mm diameter packages produce only a few lumens each; many units may be grouped together to provide additional light. Higher-power LEDs, drawing more than 100 milliamperes each, simplify the optical design problem of producing a powerful and tightly controlled beam.",
                    "score": 0.8825230598449707
                },
                {
                    "id": 1697439,
                    "contents": "Electric light\nIncandescent light bulb The modern incandescent light bulb, with a coiled filament of tungsten, and commercialized in the 1920s, developed from the carbon filament lamp introduced about 1880. Less than 3% of the input energy is converted into usable light. Nearly all of the input energy ends up as heat that, in warm climates, must then be removed from the building by ventilation or air conditioning, often resulting in more energy consumption. In colder climates where heating and lighting is required during the cold and dark winter months, the heat byproduct has some value. Incandescent bulbs are being phased out in many countries due to their low energy efficiency. As well as bulbs for normal illumination, there is a very wide range, including low voltage, low-power types often used as components in equipment, but now largely displaced by LEDs. Halogen lamp",
                    "score": 0.8821090459823608
                },
                {
                    "id": 3491541,
                    "contents": "Electrification\nIncandescent light bulbs Various forms of incandescent light bulbs had numerous inventors; however, the most successful early bulbs were those that used a carbon filament sealed in a high vacuum. These were invented by Joseph Swan in 1878 in Britain and by Thomas Edison in 1879 in the US. Edison’s lamp was more successful than Swan’s because Edison used a thinner filament, giving it higher resistance and thus conducting much less current. Edison began commercial production of carbon filament bulbs in 1880. Swan's light began commercial production in 1881. Swan's house, in Low Fell, Gateshead, was the world's first to have working light bulbs installed. The Lit & Phil Library in Newcastle, was the first public room lit by electric light, and the Savoy Theatre was the first public building in the world lit entirely by electricity. Central power stations and isolated systems",
                    "score": 0.8819760680198669
                },
                {
                    "id": 971393,
                    "contents": "Incandescent light bulb\nIn 1896, Italian inventor Arturo Malignani (1865–1939) patented an evacuation method for mass production, which allowed obtaining economic bulbs lasting 800 hours. The patent was acquired by Edison in 1898. In 1897, German physicist and chemist Walther Nernst developed the Nernst lamp, a form of incandescent lamp that used a ceramic globar and did not require enclosure in a vacuum or inert gas. Twice as efficient as carbon filament lamps, Nernst lamps were briefly popular until overtaken by lamps using metal filaments. Metal filament, inert gas",
                    "score": 0.8818320035934448
                },
                {
                    "id": 22982391,
                    "contents": "Hammer Historical Collection of Incandescent Electric Lamps\nLamps in this case also show how some of these experimental bulbs progressed into vacuum tubes that were used in radios and TVs. Below are some of the incandescent electric lamps that were in this display case. These were numbered 530 to 871. Later history The Edison Association of Illuminating Companies gave the entire Hammer collection to Henry Ford in 1929 for his new museum in Dearborn, Michigan. As of June 2000 they were still in the museum's collection. References Sources External links Some of the lamps at the Henry Ford museum Works about Thomas Edison Incandescent light bulbs 1882 introductions The Henry Ford",
                    "score": 0.8812577128410339
                },
                {
                    "id": 28073,
                    "contents": "Incandescence\nAt higher temperatures, the substance becomes brighter and its color changes from red towards white and finally blue. Incandescence is exploited in incandescent light bulbs, in which a filament is heated to a temperature at which a fraction of the radiation falls in the visible spectrum. The majority of the radiation, however, is emitted in the infrared part of the spectrum, rendering incandescent lights relatively inefficient as a light source. If the filament could be made hotter, efficiency would increase; however, there are currently no materials able to withstand such temperatures which would be appropriate for use in lamps. More efficient light sources, such as fluorescent lamps and LEDs, do not function by incandescence. Sunlight is the incandescence of the \"white hot\" surface of the sun. See also Red heat List of light sources luminescence (light emission by substances not resulting from heat) References External links",
                    "score": 0.8809689283370972
                },
                {
                    "id": 21542657,
                    "contents": "Archer Point Light\nThe current light source is a 35,000 cd 120 Volt 1000 Watt Tungsten-halogen bulb, fed from the mains electricity, with a diesel generator for backup. Site operation and visiting The site and the light are operated by the Australian Maritime Safety Authority. The site is open to the public, accessible by a gravel road, but the tower is closed. See also List of lighthouses in Australia Notes References External links Lighthouses completed in 1975 Lighthouses in Queensland Buildings and structures in Far North Queensland 1975 establishments in Australia Tourist attractions in Far North Queensland",
                    "score": 0.8798792958259583
                },
                {
                    "id": 1697433,
                    "contents": "Electric light\nAn electric light is a device that produces visible light from electric power. It is the most common form of artificial lighting and is essential to modern society, providing interior lighting for buildings and exterior light for evening and nighttime activities. In technical usage, a replaceable component that produces light from electricity is called a lamp. Lamps are commonly called light bulbs; for example, the incandescent light bulb. Lamps usually have a base made of ceramic, metal, glass, or plastic, which secures the lamp in the socket of a light fixture. The electrical connection to the socket may be made with a screw-thread base, two metal pins, two metal caps or a bayonet cap.",
                    "score": 0.8797621726989746
                },
                {
                    "id": 1697434,
                    "contents": "Electric light\nThe three main categories of electric lights are incandescent lamps, which produce light by a filament heated white-hot by electric current, gas-discharge lamps, which produce light by means of an electric arc through a gas, such as fluorescent lamps, and LED lamps, which produce light by a flow of electrons across a band gap in a semiconductor. Before electric lighting became common in the early 20th century, people used candles, gas lights, oil lamps, and fires. English chemist Humphry Davy developed the first incandescent light in 1802, followed by the first practical electric arc light in 1806. By the 1870s, Davy's arc lamp had been successfully commercialized, and was used to light many public spaces. Efforts by Joseph Swan and Thomas Edison led to commercial incandescent light bulbs becoming widely available in the 1880s, and by the early twentieth century these had completely replaced arc lamps.",
                    "score": 0.8797422647476196
                },
                {
                    "id": 21543870,
                    "contents": "Aerolux Light Corporation\nAerolux Light Corporation was a manufacturer of artful gas-discharge light bulbs from the 1930s through the 1970s. Aerolux made these bulbs in a factory in New York City. US Patents dating back to the 1930s describe the design and construction of these bulbs. Philip J. Kayatt (1896–1975) was president of the company. Description Aerolux gas discharge light bulbs contained low pressure gas, either neon or argon, or a mixture of the two. Also within the bulb were metal sculptures coated with phosphors. These phosphors fluoresced when excited by glow discharge. Because glow discharge occurs readily at 110-120 volts AC, one could use these bulbs in standard household lamps in the United States.",
                    "score": 0.8793659210205078
                },
                {
                    "id": 17136589,
                    "contents": "Trwyn Du Lighthouse\nThe light-source initially was a 4-wick Argand lamp, set within a first-order fixed catadioptric optic manufactured by Isaac Cookson & co. It displayed a fixed red light. One of the many lighthouse keepers was Joseph Steer, born in 1831 at Bovey Tracey, Devon. Modernisation In 1922 Trwyn Du became the first Trinity House lighthouse to be automated, when it was converted to unwatched acetylene operation. The lamp was converted to solar power in 1996 and the lighthouse was modernised extensively at that time. At present the Lighthouse has a 15,000 candela light that flashes once every 5 seconds and can be seen away. Additionally, a 178-kilogram (3½ cwt) fog bell sounds once every thirty seconds. There was also a lifeboat station built in 1832, nearby, but this closed in 1915.",
                    "score": 0.8789893984794617
                },
                {
                    "id": 971445,
                    "contents": "Incandescent light bulb\nSee also 3-way lamp Flash (photography) Lampshade Light tube Lightbulb jokes List of light sources Longest-lasting light bulbs Over-illumination Photometry (optics) Resistance wire Spectrometer Explanatory notes References External links Light Source Spectra 60 W-100 W Incandescent light bulb spectra, from Cornell University Program of Computer Graphics Slow-motion video of an incandescent lightbulb filament Ribbon machine in operation at Osram-Sylvania in 2016 1878 introductions 19th-century inventions Articles containing video clips Discovery and invention controversies English inventions Thomas Edison",
                    "score": 0.8789088726043701
                },
                {
                    "id": 27343902,
                    "contents": "Twinkle bulb\nA twinkle bulb is a special type of light bulb which blinks on and off for decorative effect. They are most commonly used on Christmas lights and other string lights, but can also be used for other ornamental purposes like electric jack-o-lanterns for Halloween and replica traffic lights.",
                    "score": 0.8788999319076538
                },
                {
                    "id": 6257995,
                    "contents": "Barnegat Light, New Jersey\nto the bay, making Barnegat Inlet the primary access point thereafter. The 1834 lighthouse was painted white, stood 40 feet tall, and was powered by a reflector system patented by Winslow Lewis, who also constructed the tower.",
                    "score": 0.8782232999801636
                },
                {
                    "id": 971388,
                    "contents": "Incandescent light bulb\nThomas Edison began serious research into developing a practical incandescent lamp in 1878. Edison filed his first patent application for \"Improvement in Electric Lights\" on 14 October 1878. After many experiments, first with carbon in the early 1880s and then with platinum and other metals, in the end Edison returned to a carbon filament. The first successful test was on 22 October 1879, and lasted 13.5 hours. Edison continued to improve this design and by 4 November 1879, filed for a US patent for an electric lamp using \"a carbon filament or strip coiled and connected ... to platina contact wires.\" Although the patent described several ways of creating the carbon filament including using \"cotton and linen thread, wood splints, papers coiled in various ways,\" Edison and his team later discovered that a carbonized bamboo filament could last more than 1200 hours. In 1880, the Oregon Railroad and Navigation Company steamer, Columbia, became the first application for Edison's",
                    "score": 0.8781352639198303
                },
                {
                    "id": 24635437,
                    "contents": "Edison and Swan Electric Light Company\nThe company had offices at 155 Charing Cross Road, London, and factories in Brimsdown, Ponders End and Sunderland. In 1928, the company was acquired by Associated Electrical Industries. In 1956, a new cathode ray tube plant was opened in Sunderland. The company was renamed Siemens Ediswan following the takeover of Siemens Brothers by AEI in 1957. In 1964, AEI merged its lamp and radio valve manufacturing interests with those of Thorn Electrical Industries to form British Lighting Industries Ltd.",
                    "score": 0.8779952526092529
                },
                {
                    "id": 7093944,
                    "contents": "Mazda (light bulb)\nSee also Edison screw Maxfield Parrish, painter, produced many (promotional) works for Mazda and General Electric. References External links Maxfield Parrish, \"Contentment\" Edison Mazda Lamp Advertisement(Flickr.com) Maxfield Parrish, \"Contentment\" Edison Mazda Lamp Advertisement(Flickr.com, revised link) General Electric Lighting brands Incandescent light bulbs Products introduced in 1909",
                    "score": 0.8778731226921082
                },
                {
                    "id": 2680882,
                    "contents": "Headlamp\nThe H1 lamp was the first tungsten-halogen headlamp light source. It was introduced in 1962 by a consortium of European bulb and headlamp makers. This bulb has a single axial filament that consumes 55 watts at 12.0 volts, and produces 1550 lumens ±15% when operated at 13.2 V. H2 (55 W @ 12.0 V, 1820 lm @ 13.2 V) followed in 1964, and the transverse-filament H3 (55 W @ 12.0 V, 1450 lm ±15%) in 1966. H1 still sees wide use in low beams, high beams and auxiliary fog and driving lamps, as does H3. The H2 is no longer a current type, since it requires an intricate bulb holder interface to the lamp, has a short life and is difficult to handle. For those reasons, H2 was withdrawn from ECE Regulation 37 for use in new lamp designs (though H2 bulbs are still manufactured for replacement purposes in existing lamps), but H1 and H3 remain current and these two bulbs were legalised in the United States in 1993. More recent single-filament bulb designs include the H7 (55 W @ 12.0 V, 1500 lm ±10% @",
                    "score": 0.8778285980224609
                },
                {
                    "id": 971390,
                    "contents": "Incandescent light bulb\nAlbon Man, a New York lawyer, started Electro-Dynamic Light Company in 1878 to exploit his patents and those of William Sawyer. Weeks later the United States Electric Lighting Company was organized. This company didn't make their first commercial installation of incandescent lamps until the fall of 1880 at the Mercantile Safe Deposit Company in New York City, about six months after the Edison incandescent lamps had been installed on the Columbia. Hiram S. Maxim was the chief engineer at the United States Electric Lighting Company. After the great success in the United States, the incandescent light bulb patented by Edison also began to gain widespread popularity in Europe as well; among other places, the first Edison light bulbs in the Nordic countries were installed at the weaving hall of the Finlayson's textile factory in Tampere, Finland in March 1882.",
                    "score": 0.8778055310249329
                },
                {
                    "id": 24635440,
                    "contents": "Edison and Swan Electric Light Company\nExternal links http://www.gracesguide.co.uk/Edison_Swan_Electric_Co http://www.vintage-technology.info/pages/ephemera/vemazda.htm Electrical engineering companies of the United Kingdom Lighting brands Vacuum tubes General Electric Company Associated Electrical Industries Manufacturing companies established in 1883",
                    "score": 0.8773148655891418
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_6",
        "question": "Evaluate the series\r\n$$\r\nS=\\frac{1}{2}+\\frac{1}{4}+\\frac{1}{8}+\\frac{1}{16}+\\cdots\r\n$$\r\n",
        "golden_answers": [
            " 1"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 11678088,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nAfter Christian Wolff read Leibniz's treatment of Grandi's series in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Briefly, if one expresses a partial sum of this series as a function of the penultimate term, one obtains either or . The mean of these values is , and assuming that at infinity yields as the value of the series. Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. The arithmetic means of neighboring partial sums do not converge to any particular value, and for all finite cases one has , not . Generally, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series. Leibniz counsels Wolff to reconsider so that he \"might produce something worthy of science and himself.\" Modern methods",
                    "score": 0.9141436815261841
                },
                {
                    "id": 21053061,
                    "contents": "Humbert series\nIn mathematics, Humbert series are a set of seven hypergeometric series Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, Ξ2 of two variables that generalize Kummer's confluent hypergeometric series 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable. The first of these double series was introduced by . Definitions The Humbert series Φ1 is defined for |x| < 1 by the double series: where the Pochhammer symbol (q)n represents the rising factorial: where the second equality is true for all complex except . For other values of x the function Φ1 can be defined by analytic continuation. The Humbert series Φ1 can also be written as a one-dimensional Euler-type integral: This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration. Similarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series:",
                    "score": 0.9123508334159851
                },
                {
                    "id": 3559416,
                    "contents": "Sinc function\nThe sum of the squares also equals : When the signs of the addends alternate and begin with +, the sum equals : The alternating sums of the squares and cubes also equal : Series expansion The Taylor series of the unnormalized function can be obtained from that of the sine: The series converges for all . The normalized version follows easily: Euler famously compared this series to the expansion of the infinite product form to solve the Basel problem.",
                    "score": 0.9120402336120605
                },
                {
                    "id": 20560519,
                    "contents": "Isidore Isaac Hirschman Jr.\nSelected publications Articles } Books Hirschman, I. (1962). Infinite Series. New York: Holt, Rinehart & Winston. – A textbook for advanced undergraduate and graduate mathematics. Hirschman, Isidore Isaac; Widder, David Vernon (1955). The Convolution Transform. New York: Princeton University Press; now available from Dover Publications. References http://mathdl.maa.org/mathDL/46/?pa=content&sa=viewDocument&nodeId=3801&bodyId=4189 20th-century American mathematicians Harvard University alumni Washington University in St. Louis faculty Washington University in St. Louis mathematicians",
                    "score": 0.9101979732513428
                },
                {
                    "id": 10313842,
                    "contents": "Summation of Grandi's series\nThe Cesàro sum of 1 + 0 − 1 + 1 + 0 − 1 + · · · is 2⁄3. So the Cesàro sum of a series can be altered by inserting infinitely many 0s as well as infinitely many brackets. The series can also be summed by the more general fractional (C, a) methods. Abel sum Abel summation is similar to Euler's attempted definition of sums of divergent series, but it avoids Callet's and N. Bernoulli's objections by precisely constructing the function to use. In fact, Euler likely meant to limit his definition to power series, and in practice he used it almost exclusively in a form now known as Abel's method. Given a series a0 + a1 + a2 + · · ·, one forms a new series a0 + a1x + a2x2 + · · ·. If the latter series converges for 0 < x < 1 to a function with a limit as x tends to 1, then this limit is called the Abel sum of the original series, after Abel's theorem which guarantees that the procedure is consistent with ordinary summation. For Grandi's series one has",
                    "score": 0.9098176956176758
                },
                {
                    "id": 1216477,
                    "contents": "Real analysis\nAn example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes: In contrast, the harmonic series has been known since the Middle Ages to be a divergent series: (Here, \"\" is merely a notational convention to indicate that the partial sums of the series grow without bound.) A series is said to converge absolutely if is convergent. A convergent series for which diverges is said to converge non-absolutely. It is easily shown that absolute convergence of a series implies its convergence. On the other hand, an example of a series that converges non-absolutely is Taylor series The Taylor series of a real or complex-valued function ƒ(x) that is infinitely differentiable at a real or complex number a is the power series : which can be written in the more compact sigma notation as",
                    "score": 0.9091588258743286
                },
                {
                    "id": 11646645,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nHeuristics Srinivasa Ramanujan presented two derivations of \"\" in chapter 8 of his first notebook. The simpler, less rigorous derivation proceeds in two steps, as follows. The first key insight is that the series of positive numbers closely resembles the alternating series . The latter series is also divergent, but it is much easier to work with; there are several classical methods that assign it a value, which have been explored since the 18th century. In order to transform the series into , one can subtract 4 from the second term, 8 from the fourth term, 12 from the sixth term, and so on. The total amount to be subtracted is , which is 4 times the original series. These relationships can be expressed using algebra. Whatever the \"sum\" of the series might be, call it Then multiply this equation by 4 and subtract the second equation from the first:",
                    "score": 0.9089755415916443
                },
                {
                    "id": 11678087,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nLeibniz did not quite assert that the series had a sum, but he did infer an association with following Mercator's method. The attitude that a series could equal some finite quantity without actually adding up to it as a sum would be commonplace in the 18th century, although no distinction is made in modern mathematics.",
                    "score": 0.9089720249176025
                },
                {
                    "id": 17337406,
                    "contents": "Appell series\nIn mathematics, Appell series are a set of four hypergeometric series F1, F2, F3, F4 of two variables that were introduced by and that generalize Gauss's hypergeometric series 2F1 of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable. Definitions The Appell series F1 is defined for |x| < 1, |y| < 1 by the double series where is the Pochhammer symbol. For other values of x and y the function F1 can be defined by analytic continuation. It can be shown that Similarly, the function F2 is defined for |x| + |y| < 1 by the series and it can be shown that Also the function F3 for |x| < 1, |y| < 1 can be defined by the series and the function F4 for |x|½ + |y|½ < 1 by the series",
                    "score": 0.9081627130508423
                },
                {
                    "id": 10313712,
                    "contents": "Occurrences of Grandi's series\nHe finds that the general coefficient of sin nx in the series is For n > 1 the above series converges, while the coefficient of sin x appears as 1 − 1 + 1 − 1 + · · · and so is expected to be 1⁄2. In fact, this is correct, as can be demonstrated by directly calculating the Fourier coefficient from an integral: Dirac comb Grandi's series occurs more directly in another important series, At x = , the series reduces to −1 + 1 − 1 + 1 − · · · and so one might expect it to meaningfully equal −1⁄2. In fact, Euler held that this series obeyed the formal relation Σ cos kx = −1⁄2, while d'Alembert rejected the relation, and Lagrange wondered if it could be defended by an extension of the geometric series similar to Euler's reasoning with Grandi's numerical series. Euler's claim suggests that",
                    "score": 0.9080044031143188
                },
                {
                    "id": 12548116,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nSince the sum of an infinite series is defined as the limit of its partial sums, Notes References Page images at HTML with figures and commentary at Geometric series Proof without words",
                    "score": 0.907936692237854
                },
                {
                    "id": 17472486,
                    "contents": "List of mathematical series\nThis list of mathematical series contains formulae for finite and infinite sums. It can be used in conjunction with other tools for evaluating sums. Here, is taken to have the value denotes the fractional part of is a Bernoulli polynomial. is a Bernoulli number, and here, is an Euler number. is the Riemann zeta function. is the gamma function. is a polygamma function. is a polylogarithm. is binomial coefficient denotes exponential of Sums of powers See Faulhaber's formula. The first few values are: See zeta constants. The first few values are: (the Basel problem) Power series Low-order polylogarithms Finite sums: , (geometric series) Infinite sums, valid for (see polylogarithm): The following is a useful property to calculate low-integer-order polylogarithms recursively in closed form: Exponential function (cf. mean of Poisson distribution) (cf. second moment of Poisson distribution) where is the Touchard polynomials.",
                    "score": 0.9073177576065063
                },
                {
                    "id": 11646640,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe infinite series whose terms are the natural numbers is a divergent series. The nth partial sum of the series is the triangular number which increases without bound as n goes to infinity. Because the sequence of partial sums fails to converge to a finite limit, the series does not have a sum.",
                    "score": 0.9069457650184631
                },
                {
                    "id": 21053062,
                    "contents": "Humbert series\nSimilarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series: the function Ψ2 for all x, y by the series: the function Ξ1 for |x| < 1 by the series: and the function Ξ2 for |x| < 1 by the series: Related series There are four related series of two variables, F1, F2, F3, and F4, which generalize Gauss's hypergeometric series 2F1 of one variable in a similar manner and which were introduced by Paul Émile Appell in 1880. References (see p. 126) (see p. 225) Hypergeometric functions Mathematical series",
                    "score": 0.9068138599395752
                },
                {
                    "id": 2195776,
                    "contents": "List of real analysis topics\nConvergent series – a series whose sequence of partial sums converges Divergent series – a series whose sequence of partial sums diverges Power series – a series of the form Taylor series – a series of the form Maclaurin series – see Taylor seriesBinomial series – the Maclaurin series of the function f given by f(x) = (1 + x) α Telescoping series Alternating series Geometric series Divergent geometric series Harmonic series Fourier series Lambert series",
                    "score": 0.9056532979011536
                },
                {
                    "id": 1574967,
                    "contents": "Archimedes\nArchimedes gives the value of the square root of 3 as lying between (approximately 1.7320261) and (approximately 1.7320512) in Measurement of a Circle. The actual value is approximately 1.7320508, making this a very accurate estimate. He introduced this result without offering any explanation of how he had obtained it. This aspect of the work of Archimedes caused John Wallis to remark that he was: \"as it were of set purpose to have covered up the traces of his investigation as if he had grudged posterity the secret of his method of inquiry while he wished to extort from them assent to his results.\" It is possible that he used an iterative procedure to calculate these values. The infinite series",
                    "score": 0.905425488948822
                },
                {
                    "id": 4580092,
                    "contents": "Series expansion\nThere are several kinds of series expansions, such as: Taylor series: A power series based on a function's derivatives at a single point. Maclaurin series: A special case of a Taylor series, centred at zero. Laurent series: An extension of the Taylor series, allowing negative exponent values. Dirichlet series: Used in number theory. Fourier series: Describes periodical functions as a series of sine and cosine functions. In acoustics, e.g., the fundamental tone and the overtones together form an example of a Fourier series. Newtonian series Legendre polynomials: Used in physics to describe an arbitrary electrical field as a superposition of a dipole field, a quadrupole field, an octupole field, etc. Zernike polynomials: Used in optics to calculate aberrations of optical systems. Each term in the series describes a particular type of aberration. Stirling series: Used as an approximation for factorials. Examples The following is the Taylor series of : References",
                    "score": 0.9046981334686279
                },
                {
                    "id": 1790061,
                    "contents": "Series (mathematics)\nLaurent series Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence. Dirichlet series A Dirichlet series is one of the form where s is a complex number. For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function",
                    "score": 0.9032241106033325
                },
                {
                    "id": 2805262,
                    "contents": "Polylogarithm\nThe polylogarithm function is defined by a power series in z, which is also a Dirichlet series in s: This definition is valid for arbitrary complex order s and for all complex arguments z with |z| < 1; it can be extended to |z| ≥ 1 by the process of analytic continuation. (Here the denominator ns is understood as exp(s ln(n).) The special case s = 1 involves the ordinary natural logarithm, Li1(z) = −ln(1−z), while the special cases s = 2 and s = 3 are called the dilogarithm (also referred to as Spence's function) and trilogarithm respectively. The name of the function comes from the fact that it may also be defined as the repeated integral of itself: thus the dilogarithm is an integral of a function involving the logarithm, and so on. For nonpositive integer orders s, the polylogarithm is a rational function.",
                    "score": 0.9030600786209106
                },
                {
                    "id": 10049357,
                    "contents": "History of Grandi's series\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all x less than 1, it should hold for x equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as f(1) of some function's power series was thought to be the most natural strategy. Leibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0: 1 − 1 = 1 − 1 + 1 − 1 = 1 − 1 + 1 − 1 + 1 − 1 = 0. Taking an odd number of terms, the last term is +1 and the sum is 1: 1 = 1 − 1 + 1 = 1 − 1 + 1 − 1 + 1 = 1.",
                    "score": 0.9024957418441772
                },
                {
                    "id": 14058441,
                    "contents": "Florence Marie Mears\nMears specialized in the findings of definitions and values assigned to various infinite series of numbers. An \"infinite series\" is an endless series of numbers, each succeeding the other that is a certain amount lesser or greater than the proceeding one. An example set of an infinite series includes is 1 + ½ + ¼ etc. in which the definition of the series can be defined as the number two. As a result, Mears created several theorems about these definitions, many of which provided truth for many practicing mathematicians, engineers, chemists, physicists, and astronomers. One of her most popular theorems, called the Norlund Mean can be explained through absolute regularity, the summability of Cauchy products, and inverse properties.",
                    "score": 0.9022687077522278
                },
                {
                    "id": 19430505,
                    "contents": "Series multisection\nIn mathematics, a multisection of a power series is a new power series composed of equally spaced terms extracted unaltered from the original series. Formally, if one is given a power series then its multisection is a power series of the form where p, q are integers, with 0 ≤ p < q. Multisection of analytic functions A multisection of the series of an analytic function has a closed-form expression in terms of the function : where is a primitive q-th root of unity. This solution was first discovered by Thomas Simpson. This expression is especially useful in that it can convert an infinite sum into a finite sum. It is used, for example, in a key step of a standard proof of Gauss's digamma theorem, which gives a closed-form solution to the digamma function evaluated at rational values p/q. Examples Bisection In general, the bisections of a series are the even and odd parts of the series. Geometric series Consider the geometric series",
                    "score": 0.9021907448768616
                },
                {
                    "id": 1169612,
                    "contents": "Pi\nAs individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of . An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is: Note that (n − 1)n(n + 1) = n3 − n. The following table compares the convergence rates of these two series: After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term. Irrationality and transcendence",
                    "score": 0.9019786715507507
                },
                {
                    "id": 1280600,
                    "contents": "Taylor series\nExponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm The natural logarithm (with base ) has Maclaurin series They converge for . (In addition, the series for converges for , and the series for converges for .) Geometric series The geometric series and its derivatives have Maclaurin series All are convergent for . These are special cases of the binomial series given in the next section. Binomial series The binomial series is the power series whose coefficients are the generalized binomial coefficients (If , this product is an empty product and has value 1.) It converges for for any real or complex number . When , this is essentially the infinite geometric series mentioned in the previous section. The special cases and give the square root function and its inverse: When only the linear term is retained, this simplifies to the binomial approximation.",
                    "score": 0.9019246101379395
                },
                {
                    "id": 10049358,
                    "contents": "History of Grandi's series\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is Eli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"",
                    "score": 0.9017062187194824
                },
                {
                    "id": 6608839,
                    "contents": "Apéry's constant\nOther classical series representations include: Fast convergence Since the 19th century, a number of mathematicians have found convergence acceleration series for calculating decimal places of . Since the 1990s, this search has focused on computationally efficient series with fast convergence rates (see section \"Known digits\"). The following series representation was found by A. A. Markov in 1890, rediscovered by Hjortnaes in 1953, and rediscovered once more and widely advertised by Apéry in 1979: The following series representation gives (asymptotically) 1.43 new correct decimal places per term: The following series representation gives (asymptotically) 3.01 new correct decimal places per term: The following series representation gives (asymptotically) 5.04 new correct decimal places per term: It has been used to calculate Apéry's constant with several million correct decimal places.",
                    "score": 0.9012464880943298
                },
                {
                    "id": 13740952,
                    "contents": "Sums of powers\nThe sum of the terms in the geometric series is",
                    "score": 0.9011856317520142
                },
                {
                    "id": 24331221,
                    "contents": "Fundamental series\nthe frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and noticed that this series had a simpler formula than the others and thus called it the \"fundamental series\" and used the letter F.",
                    "score": 0.9008843302726746
                },
                {
                    "id": 1834677,
                    "contents": "Leonhard Euler\nEuler's use of power series enabled him to solve the famous Basel problem in 1735 (he provided a more elaborate argument in 1741): He introduced the constant now known as Euler's constant or the Euler–Mascheroni constant, and studied its relationship with the harmonic series, the gamma function, and values of the Riemann zeta function. Euler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms. He also defined the exponential function for complex numbers and discovered its relation to the trigonometric functions. For any real number (taken to be radians), Euler's formula states that the complex exponential function satisfies A special case of the above formula is known as Euler's identity,",
                    "score": 0.9008439779281616
                },
                {
                    "id": 10313714,
                    "contents": "Occurrences of Grandi's series\nMultiplying the terms of Grandi's series by 1/nz yields the Dirichlet series which converges only for complex numbers z with a positive real part. Grandi's series is recovered by letting z = 0. Unlike the geometric series, the Dirichlet series for η is not useful for determining what 1 − 1 + 1 − 1 + · · · \"should\" be. Even on the right half-plane, η(z) is not given by any elementary expression, and there is no immediate evidence of its limit as z approaches 0. On the other hand, if one uses stronger methods of summability, then the Dirichlet series for η defines a function on the whole complex plane — the Dirichlet eta function — and moreover, this function is analytic. For z with real part > −1 it suffices to use Cesàro summation, and so η(0) = 1⁄2 after all. The function η is related to a more famous Dirichlet series and function:",
                    "score": 0.9005895853042603
                },
                {
                    "id": 12162084,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nIn mathematics, is the infinite series whose terms are the successive powers of two. As a geometric series, it is characterized by its first term, 1, and its common ratio, 2. As a series of real numbers it diverges to infinity, so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the 2-adic metric. Summation The partial sums of are since these diverge to infinity, so does the series.",
                    "score": 0.900485098361969
                },
                {
                    "id": 10313711,
                    "contents": "Occurrences of Grandi's series\nSeveral series resulting from the introduction of zeros into Grandi's series have interesting properties; for these see Summation of Grandi's series#Dilution. Grandi's series is just one example of a divergent geometric series. The rearranged series 1 − 1 − 1 + 1 + 1 − 1 − 1 + · · · occurs in Euler's 1775 treatment of the pentagonal number theorem as the value of the Euler function at q = 1. Power series The power series most famously associated with Grandi's series is its ordinary generating function, Fourier series Hyperbolic sine In his 1822 Théorie Analytique de la Chaleur, Joseph Fourier obtains what is currently called a Fourier sine series for a scaled version of the hyperbolic sine function, He finds that the general coefficient of sin nx in the series is",
                    "score": 0.9003308415412903
                },
                {
                    "id": 11646654,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nwhere f(2k−1) is the (2k − 1)-th derivative of f and B2k is the 2k-th Bernoulli number: , , and so on. Setting , the first derivative of f is 1, and every other term vanishes, so To avoid inconsistencies, the modern theory of Ramanujan summation requires that f is \"regular\" in the sense that the higher-order derivatives of f decay quickly enough for the remainder terms in the Euler–Maclaurin formula to tend to 0. Ramanujan tacitly assumed this property. The regularity requirement prevents the use of Ramanujan summation upon spaced-out series like , because no regular function takes those values. Instead, such a series must be interpreted by zeta function regularization. For this reason, Hardy recommends \"great caution\" when applying the Ramanujan sums of known series to find the sums of related series.",
                    "score": 0.9000492691993713
                },
                {
                    "id": 11678089,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nModern methods Geometric series Any summation method possessing the properties of regularity, linearity, and stability will sum a geometric series In this case a = 1 and r = −2, so the sum is . Euler summation In his 1755 Institutiones, Leonhard Euler effectively took what is now called the Euler transform of , arriving at the convergent series . Since the latter sums to , Euler concluded that . His ideas on infinite series do not quite follow the modern approach; today one says that is Euler summable and that its Euler sum is . The Euler transform begins with the sequence of positive terms: a0 = 1, a1 = 2, a2 = 4, a3 = 8,... The sequence of forward differences is then Δa0 = a1 − a0 = 2 − 1 = 1, Δa1 = a2 − a1 = 4 − 2 = 2, Δa2 = a3 − a2 = 8 − 4 = 4, Δa3 = a4 − a3 = 16 − 8 = 8,... which is just the same sequence. Hence the iterated forward difference sequences all start with for every n. The Euler transform is the series",
                    "score": 0.8999850153923035
                },
                {
                    "id": 28929110,
                    "contents": "Mary Graustein\nof them are Seniors; they and their theses on ‘Infinite Series,’ ‘Curve Fitting’ and ‘The Number System of Algebra’ do their bit to keep me busy. I enjoy the work and the girls. The Grausteins, who had no children, enjoyed travel and made a point of going to Italy every other summer to spend time in the Dolomites until the advent of World War II made such trips inadvisable.",
                    "score": 0.8998282551765442
                },
                {
                    "id": 4580091,
                    "contents": "Series expansion\nIn mathematics, a series expansion is an expansion of a function into a series, or infinite sum. It is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division). The resulting so-called series often can be limited to a finite number of terms, thus yielding an approximation of the function. The fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions. There are several kinds of series expansions, such as:",
                    "score": 0.8997677564620972
                },
                {
                    "id": 11637867,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nThe series 1 − 2 + 3 − 4 + ... is closely related to Grandi's series . Euler treated these two as special cases of the more general sequence , where and respectively. This line of research extended his work on the Basel problem and leading towards the functional equations of what are now known as the Dirichlet eta function and the Riemann zeta function. Divergence The series' terms do not approach 0; therefore diverges by the term test. Divergence can also be shown directly from the definition: an infinite series converges if and only if the sequence of partial sums converges to limit, in which case that limit is the value of the infinite series. The partial sums of are: The sequence of partial sums shows that the series does not converge to a particular number: for any proposed limit x, there exists a point beyond which the subsequent partial sums are all outside the interval ), so diverges.",
                    "score": 0.8996527791023254
                },
                {
                    "id": 6183796,
                    "contents": "A Course of Modern Analysis\nThe book was one of the earliest to use decimal numbering for its sections, an innovation the authors attribute to Giuseppe Peano. Contents Below are the contents of the fourth edition: Part I. The Process of Analysis Part II. The Transcendental Functions Reception Reviews of the first edition George B. Mathews, in a 1903 review article published in The Mathematical Gazette opens by saying the book is \"sure of a favorable reception\" because of its \"attractive account of some of the most valuable and interesting results of recent analysis\". He notes that Part I deals mainly with infinite series, focusing on power series and Fourier expansions while including the \"elements of\" complex integration and the theory of residues. Part II, in contrast, has chapters on the gamma function, Legendre functions, the hypergeometric series, Bessel functions, elliptic functions, and mathematical physics.",
                    "score": 0.8994998931884766
                },
                {
                    "id": 11646658,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nHistory It is unclear whether Leonhard Euler summed the series to . According to Morris Kline, Euler's early work on divergent series relied on function expansions, from which he concluded . According to Raymond Ayoub, the fact that the divergent zeta series is not Abel-summable prevented Euler from using the zeta function as freely as the eta function, and he \"could not have attached a meaning\" to the series. Other authors have credited Euler with the sum, suggesting that Euler would have extended the relationship between the zeta and eta functions to negative integers. In the primary literature, the series is mentioned in Euler's 1760 publication alongside the divergent geometric series . Euler hints that series of this type have finite, negative sums, and he explains what this means for geometric series, but he does not return to discuss . In the same publication, Euler writes that the sum of is infinite.",
                    "score": 0.8994874954223633
                },
                {
                    "id": 11646646,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe second key insight is that the alternating series is the formal power series expansion of the function but with x defined as 1. Accordingly, Ramanujan writes Dividing both sides by −3, one gets c = . Generally speaking, it is incorrect to manipulate infinite series as if they were finite sums. For example, if zeroes are inserted into arbitrary positions of a divergent series, it is possible to arrive at results that are not self-consistent, let alone consistent with other methods. In particular, the step is not justified by the additive identity law alone. For an extreme example, appending a single zero to the front of the series can lead to a different result.",
                    "score": 0.8990398645401001
                },
                {
                    "id": 1779370,
                    "contents": "Isaac Newton\nNewton is generally credited with the generalised binomial theorem, valid for any exponent. He discovered Newton's identities, Newton's method, classified cubic plane curves (polynomials of degree three in two variables), made substantial contributions to the theory of finite differences, and was the first to use fractional indices and to employ coordinate geometry to derive solutions to Diophantine equations. He approximated partial sums of the harmonic series by logarithms (a precursor to Euler's summation formula) and was the first to use power series with confidence and to revert power series. Newton's work on infinite series was inspired by Simon Stevin's decimals.",
                    "score": 0.8989450335502625
                },
                {
                    "id": 7031536,
                    "contents": "Newton's identities\nOne formally differentiates both sides with respect to t, and then (for convenience) multiplies by t, to obtain where the polynomial on the right hand side was first rewritten as a rational function in order to be able to factor out a product out of the summation, then the fraction in the summand was developed as a series in t, using the formula and finally the coefficient of each t j was collected, giving a power sum. (The series in t is a formal power series, but may alternatively be thought of as a series expansion for t sufficiently close to 0, for those more comfortable with that; in fact one is not interested in the function here, but only in the coefficients of the series.) Comparing coefficients of tk on both sides one obtains which gives the k-th Newton identity.",
                    "score": 0.8985931873321533
                },
                {
                    "id": 1280599,
                    "contents": "Taylor series\nHere is the th finite difference operator with step size . The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function is analytic at , the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series. In general, for any infinite sequence , the following power series identity holds: So in particular, The series on the right is the expectation value of , where is a Poisson-distributed random variable that takes the value with probability . Hence, The law of large numbers implies that the identity holds. List of Maclaurin series of some common functions Several important Maclaurin series expansions follow. All these expansions are valid for complex arguments . Exponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm",
                    "score": 0.8985905051231384
                },
                {
                    "id": 10049374,
                    "contents": "History of Grandi's series\nFrobenius' short paper, barely two pages, begins by quoting from Leibniz's treatment of 1 − 1 + 1 − 1 + · · ·. He infers that Leibniz was actually stating a generalization of Abel's Theorem. The result, now known as Frobenius' theorem, has a simple statement in modern terms: any series that is Cesàro summable is also Abel summable to the same sum. Historian Giovanni Ferraro emphasizes that Frobenius did not actually state the theorem in such terms, and Leibniz did not state it at all. Leibniz was defending the association of the divergent series with the value 1⁄2, while Frobenius' theorem is stated in terms of convergent sequences and the epsilon-delta formulation of the limit of a function.",
                    "score": 0.8984764814376831
                },
                {
                    "id": 12548113,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nIn mathematics, the infinite series is an example of one of the first infinite series to be summed in the history of mathematics; it was used by Archimedes circa 250–200 BC. As it is a geometric series with first term and common ratio , its sum is Visual demonstrations The series lends itself to some particularly simple visual demonstrations because a square and a triangle both divide into four similar pieces, each of which contains the area of the original. In the figure on the left, if the large square is taken to have area 1, then the largest black square has area × = . Likewise, the second largest black square has area , and the third largest black square has area . The area taken up by all of the black squares together is therefore , and this is also the area taken up by the gray squares and the white squares. Since these three areas cover the unit square, the figure demonstrates that",
                    "score": 0.8982938528060913
                },
                {
                    "id": 11950793,
                    "contents": "Carl Johan Malmsten\nIn 1842, Malmsten also evaluated several important logarithmic series, among which we can find these two series and The latter series was later rediscovered in a slightly different form by Ernst Kummer, who derived a similar expression in 1847 (strictly speaking, the Kummer's result is obtained from the Malmsten's one by putting a=π(2x-1)). Moreover, this series is even known in analysis as Kummer's series for the logarithm of the Gamma function, although Malmsten derived it 5 years before Kummer. Malsmten also notably contributed into the theory of zeta-function related series and integrals. In 1842 he proved following important functional relationship for the L-function as well as for the M-function",
                    "score": 0.8981136083602905
                },
                {
                    "id": 680910,
                    "contents": "Power series\nThe power series expansion of the inverse function of an analytic function can be determined using the Lagrange inversion theorem. Behavior near the boundary The sum of a power series with a positive radius of convergence is an analytic function at every point in the interior of the disc of convergence. However, different behavior can occur at points on the boundary of that disc. For example:",
                    "score": 0.8978378176689148
                },
                {
                    "id": 1744067,
                    "contents": "Geometric series\nTypically a geometric series is thought of as a sum of numbers a + ar + ar2 + ar3 + ... but can also be thought of as a sum of functions a + ar + ar2 + ar3 + ... that converges to the function a / (1 - r) within the range |r| < 1. The adjacent image shows the contribution each of the first nine terms (i.e., functions) make to the function a / (1 - r) within the range |r| < 1 when a = 1. Changing even one of the coefficients to something other than coefficient a would (in addition to changing the geometric series to a power series) change the resulting sum of functions to some function other than a / (1 - r) within the range |r| < 1. As an aside, a particularly useful change to the coefficients is defined by the Taylor series, which describes how to change the coefficients so that the sum of functions converges to any user selected, sufficiently smooth function within a range. Common ratio r",
                    "score": 0.8976919651031494
                },
                {
                    "id": 1744092,
                    "contents": "Geometric series\n1/2 + 1/4 + 1/8 + ... , which is the geometric series with coefficient aT = S = 1 and common ratio r = 1/2 that converges to T = aT / (1-r) = S / (1-r) = a / (1-r) / (1-r) = (1/2) / (1-1/2) / (1-1/2) = 2.",
                    "score": 0.8976559638977051
                },
                {
                    "id": 17896202,
                    "contents": "Chudnovsky algorithm\nThe Chudnovsky algorithm is a fast method for calculating the digits of , based on Ramanujan’s formulae. It was published by the Chudnovsky brothers in 1988 and was used in the world record calculations of 2.7 trillion digits of in December 2009, 10 trillion digits in October 2011, 22.4 trillion digits in November 2016, 31.4 trillion digits in September 2018–January 2019, 50 trillion digits on January 29, 2020, and 62.8 trillion digits on August 14, 2021. The algorithm is based on the negated Heegner number , the j-function , and on the following rapidly convergent generalized hypergeometric series: A detailed proof of this formula can be found here: For a high performance iterative implementation, this can be simplified to There are 3 big integer terms (the multinomial term Mq, the linear term Lq, and the exponential term Xq) that make up the series and equals the constant C divided by the sum of the series, as below: , where: , , , .",
                    "score": 0.897627592086792
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.8
            }
        }
    },
    {
        "id": "test_7",
        "question": "Through what potential must a proton initially at rest fall so that its de Broglie wavelength is $1.0 \\times 10^{-10} \\mathrm{~m}$ ?",
        "golden_answers": [
            " 0.082"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 3048299,
                    "contents": "Planck length\nThe Planck length is about 10−20 times the diameter of a proton. It can be defined as the reduced Compton wavelength of a black hole for which this equals its Schwarzschild radius. The Planck length can be found also without knowledge of the Newtonian gravitational constant using a Newton force spring. History In 1899, Max Planck suggested that there existed some fundamental natural units for length, mass, time and energy. He derived these using dimensional analysis, using only the Newton gravitational constant, the speed of light and the Planck constant (though it was not yet called this). The modern convention is to use the reduced Planck constant in place of the Planck constant in the definition of the resulting units. The derived natural units became known as the \"Planck length\", the \"Planck mass\", the \"Planck time\" and the \"Planck energy\".",
                    "score": 0.8927434086799622
                },
                {
                    "id": 1164044,
                    "contents": "Proton\nHowever, in such an association with an electron, the character of the bound proton is not changed, and it remains a proton. The attraction of low-energy free protons to any electrons present in normal matter (such as the electrons in normal atoms) causes free protons to stop and to form a new chemical bond with an atom. Such a bond happens at any sufficiently \"cold\" temperature (that is, comparable to temperatures at the surface of the Sun) and with any type of atom. Thus, in interaction with any type of normal (non-plasma) matter, low-velocity free protons do not remain free but are attracted to electrons in any atom or molecule with which they come into contact, causing the proton and molecule to combine. Such molecules are then said to be \"protonated\", and chemically they are simply compounds of hydrogen, often positively charged. Often, as a result, they become so-called Brønsted acids. For example, a proton captured by a water molecule in water becomes hydronium, the aqueous",
                    "score": 0.8845936059951782
                },
                {
                    "id": 26615557,
                    "contents": "Fritz Rohrlich\nIn 1957 he was selected as a fellow of the American Physical Society. In 1974 he received a Fulbright Award to visit the University of Graz, and in 1996 he received an honorary doctorate from that university. Rohrlich died November 14, 2018 in DeWitt, New York. Selected works Research articles Thomas Fulton, Fritz Rohrlich. Classical radiation from a uniformly accelerating charge . In: Annals of Physics , Vol. 9 (1960), pp. 499–517. An elementary derivation of E = mc². In: American Journal of Physics, Volume 58, Issue 4 (1990), p. 348. The dynamics of a charged sphere and the electron. In: American Journal of Physics, Volume 65, Issue 11 (1997), p. 1051. Books (with Josef-Maria Jauch) (First edition 1955). (First edition 1965; Second edition 1990). References",
                    "score": 0.8821911811828613
                },
                {
                    "id": 1164020,
                    "contents": "Proton\nThe word proton is Greek for \"first\", and this name was given to the hydrogen nucleus by Ernest Rutherford in 1920. In previous years, Rutherford had discovered that the hydrogen nucleus (known to be the lightest nucleus) could be extracted from the nuclei of nitrogen by atomic collisions. Protons were therefore a candidate to be a fundamental or elementary particle, and hence a building block of nitrogen and all other heavier atomic nuclei.",
                    "score": 0.8793210983276367
                },
                {
                    "id": 358879,
                    "contents": "Lenz's law\ndo not, such as a proton and an electron, the interaction is different. An electron generating a magnetic field would generate an EMF that causes a proton to accelerate in the same direction as the electron. At first, this might seem to violate the law of conservation of momentum, but such an interaction is seen to conserve momentum if the momentum of electromagnetic fields is taken into account.",
                    "score": 0.8749942183494568
                },
                {
                    "id": 1164031,
                    "contents": "Proton\nStability The free proton (a proton not bound to nucleons or electrons) is a stable particle that has not been observed to break down spontaneously to other particles. Free protons are found naturally in a number of situations in which energies or temperatures are high enough to separate them from electrons, for which they have some affinity. Free protons exist in plasmas in which temperatures are too high to allow them to combine with electrons. Free protons of high energy and velocity make up 90% of cosmic rays, which propagate in vacuum for interstellar distances. Free protons are emitted directly from atomic nuclei in some rare types of radioactive decay. Protons also result (along with electrons and antineutrinos) from the radioactive decay of free neutrons, which are unstable.",
                    "score": 0.872578501701355
                },
                {
                    "id": 8808793,
                    "contents": "Tolman–Oppenheimer–Volkoff limit\nIn the case of a rigidly spinning neutron star, the mass limit is thought to increase by up to 18–20%. History The idea that there should be an absolute upper limit for the mass of a cold (as distinct from thermal pressure supported) self-gravitating body dates back to the 1932 work of Lev Landau, based on the Pauli exclusion principle. Pauli's principle shows that the fermionic particles in sufficiently compressed matter would be forced into energy states so high that their rest mass contribution would become negligible when compared with the relativistic kinetic contribution (RKC). RKC is determined just by the relevant quantum wavelength , which would be of the order of the mean interparticle separation. In terms of Planck units, with the reduced Planck constant , the speed of light , and the gravitational constant all set equal to one, there will be a corresponding pressure given roughly by",
                    "score": 0.8719782829284668
                },
                {
                    "id": 431660,
                    "contents": "Rydberg constant\nRydberg frequency Rydberg wavelength . The angular wavelength is . Occurrence in Bohr model The Bohr model explains the atomic spectrum of hydrogen (see hydrogen spectral series) as well as various other atoms and ions. It is not perfectly accurate, but is a remarkably good approximation in many cases, and historically played an important role in the development of quantum mechanics. The Bohr model posits that electrons revolve around the atomic nucleus in a manner analogous to planets revolving around the sun. In the simplest version of the Bohr model, the mass of the atomic nucleus is considered to be infinite compared to the mass of the electron, so that the center of mass of the system, the barycenter, lies at the center of the nucleus. This infinite mass approximation is what is alluded to with the subscript. The Bohr model then predicts that the wavelengths of hydrogen atomic transitions are (see Rydberg formula):",
                    "score": 0.870758593082428
                },
                {
                    "id": 2730788,
                    "contents": "Eddington number\nIn astrophysics, the Eddington number, , is the number of protons in the observable universe. Eddington originally calculated it as about ; current estimates make it approximately . The term is named for British astrophysicist Arthur Eddington, who in 1940 was the first to propose a value of and to explain why this number might be important for physical cosmology and the foundations of physics. History Eddington argued that the value of the fine-structure constant, α, could be obtained by pure deduction. He related α to the Eddington number, which was his estimate of the number of protons in the universe. This led him in 1929 to conjecture that α was exactly 1/136. He devised a \"proof\" that NEdd = 136 × 2256, or about 1.57×1079. Other physicists did not adopt this conjecture and did not accept his argument.",
                    "score": 0.8704902529716492
                },
                {
                    "id": 29385600,
                    "contents": "Timelapse of the Future\nWith protons being wholly decayed, the Black Hole Era begins. The universe now has \"zombie galaxies\" of black holes and light particles lounging around. Finally, binary black holes might come to life, releasing massive amounts of energy as gravitational waves when merging. In the year 159 novemdecillion, Hawking radiation finally makes the first black holes die. As they explode, they light back interstellar darkness. The universe then expands further by dark energy, which, if it persists at that time as it is now, will cause the universe to expand forever, making it colder, darker, and emptier.",
                    "score": 0.8704731464385986
                },
                {
                    "id": 4055543,
                    "contents": "Quasiparticle\nthe entropy production, and generally take the form of a Boltzmann-type collision term, in which figure only \"far collisions\" between virtual particles. In other words, every type of mean-field kinetic equation, and in fact every mean-field theory, involves a quasiparticle concept.",
                    "score": 0.8696916103363037
                },
                {
                    "id": 1762758,
                    "contents": "History of physics\n(producing arguments such as \"Maxwell's demon\"), and that would not be held to be definitively resolved until the behavior of atoms was firmly established in the early 20th century. In 1902, James Jeans found the length scale required for gravitational perturbations to grow in a static nearly homogeneous medium.",
                    "score": 0.8696697950363159
                },
                {
                    "id": 7312606,
                    "contents": "Critical ionization velocity\nThe critical ionization velocity of hydrogen 50.9 x 105 cm/s (50.9 km/s), and helium is 34.3 x 10 5cm/s (34.3 km/s). Background Alfvén discusses his thoughts behind critical velocity, in his NASA publications Evolution of the Solar System. After criticising the \"Inadequacy of the Homogeneous Disc Theory\", he writes:",
                    "score": 0.8696532249450684
                },
                {
                    "id": 9196922,
                    "contents": "Gravitational coupling constant\nRelated definitions Let be the dimensionless proton-to-electron mass ratio, the ratio of the rest mass of the proton to that of the electron. Other definitions of that have been proposed in the literature differ from the one above merely by a factor of or its square; If is defined using the mass of one electron, , and one proton (), then , and . defined in this manner is in Eddington (1935: 232), with Planck's constant replacing the \"reduced\" Planck constant; (4.5) in Barrow and Tipler (1986) tacitly defines as . Even though they do not name the defined in this manner, it nevertheless plays a role in their broad-ranging discussion of astrophysics, cosmology, quantum physics, and the anthropic principle; in Rees (2000) is , where the denominator is defined using a pair of protons.",
                    "score": 0.868510365486145
                },
                {
                    "id": 24167113,
                    "contents": "James Robert McConnell\nFr. McConnell was appointed a scholar in the newly founded Dublin Institute for Advanced Studies in 1942. Fr. McConnell was appointed Professor of Mathematical Physics in St. Patrick's College, Maynooth, having been awarded a D.Sc. from the National University of Ireland for his research there in 1949. He is best known for research on Rotational Brownian motion, the electric and magnetic properties of matter and the theory of the negative proton (or anti-proton). Fr. McConnell served as Dean of the Faculty of Science, of Maynooth, from 1957 to 1968, and registrar of the College from 1966 to 1968. McConnell was the 1986 recipient of the RDS Irish Times Boyle Medal for Scientific Excellence. Dr. McConnell was appointed to the Pontifical Academy of Sciences in 1990, and honoured with the title of Monsignor by Pope John Paul II in 1991. Further reading References Scientists from Dublin (city) 1915 births 1999 deaths",
                    "score": 0.8681180477142334
                },
                {
                    "id": 1157156,
                    "contents": "Oliver Heaviside\nBut this setback had the effect of turning Heaviside's attention towards electromagnetic radiation, and in two papers of 1888 and 1889, he calculated the deformations of electric and magnetic fields surrounding a moving charge, as well as the effects of it entering a denser medium. This included a prediction of what is now known as Cherenkov radiation, and inspired his friend George FitzGerald to suggest what now is known as the Lorentz–FitzGerald contraction. In 1889, Heaviside first published a correct derivation of the magnetic force on a moving charged particle, which is the magnetic component of what is now called the Lorentz force. In the late 1880s and early 1890s, Heaviside worked on the concept of electromagnetic mass. Heaviside treated this as material mass, capable of producing the same effects. Wilhelm Wien later verified Heaviside's expression (for low velocities).",
                    "score": 0.8673423528671265
                },
                {
                    "id": 7312607,
                    "contents": "Critical ionization velocity\nAlfvén discusses his thoughts behind critical velocity, in his NASA publications Evolution of the Solar System. After criticising the \"Inadequacy of the Homogeneous Disc Theory\", he writes: \".. it is more attractive to turn to the alternative that the secondary bodies derive from matter falling in from \"infinity\" (a distance large compared to. the satellite orbit). This matter (after being stopped and given sufficient angular momentum) accumulates at specific distances from the central body. Such a process may take place when atoms or molecules in free fall reach a kinetic energy equal to their ionization energy. At this stage, the gas can become ionized by the process discussed in sec. 21.4; the ionized gas can then be stopped by the magnetic field of the central body and receive angular momentum by transfer from the central body as described in sec. 16.3.\". Notes",
                    "score": 0.8655804991722107
                },
                {
                    "id": 1772929,
                    "contents": "Hydrogen atom\nIf a neutral hydrogen atom loses its electron, it becomes a cation. The resulting ion, which consists solely of a proton for the usual isotope, is written as \"H+\" and sometimes called hydron. Free protons are common in the interstellar medium, and solar wind. In the context of aqueous solutions of classical Brønsted–Lowry acids, such as hydrochloric acid, it is actually hydronium, H3O+, that is meant. Instead of a literal ionized single hydrogen atom being formed, the acid transfers the hydrogen to H2O, forming H3O+. If instead a hydrogen atom gains a second electron, it becomes an anion. The hydrogen anion is written as \"H–\" and called hydride. Theoretical analysis The hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.",
                    "score": 0.8655333518981934
                },
                {
                    "id": 11328302,
                    "contents": "Gravitational wave\nIn theory, the loss of energy through gravitational radiation could eventually drop the Earth into the Sun. However, the total energy of the Earth orbiting the Sun (kinetic energy + gravitational potential energy) is about 1.14 joules of which only 200 watts (joules per second) is lost through gravitational radiation, leading to a decay in the orbit by about 1 meters per day or roughly the diameter of a proton. At this rate, it would take the Earth approximately 3 times more than the current age of the Universe to spiral onto the Sun. This estimate overlooks the decrease in r over time, but the radius varies only slowly for most of the time and plunges at later stages, as with the initial radius and the total time needed to fully coalesce. More generally, the rate of orbital decay can be approximated by",
                    "score": 0.8652752637863159
                },
                {
                    "id": 9507314,
                    "contents": "Bernard Haisch\nHaisch has worked at the Solar & Astrophysics Laboratory at Lockheed Martin in Palo Alto, California and served as deputy director of the Center for Extreme Ultraviolet Astrophysics Laboratory at the University of California, Berkeley. He has been a visiting scientist at the Max Planck Institute for Extraterrestrial Physics in Garching, Germany and at the University of Utrecht in the Netherlands. His main research from the mid 1970s until the late 1990s was high energy astrophysics, and specifically the ultraviolet and X-ray emissions from coronae and flares on the Sun and other late-type stars. Haisch has published more than one hundred research papers on a variety of topics, many in prestigious journals such as Nature, Science, Physical Review, Astrophysical Journal, and Annalen der Physik. He also served for ten years as an editor of the Astrophysical Journal. The Quantum Vacuum and the Principle of Inertia",
                    "score": 0.8652035593986511
                },
                {
                    "id": 1772931,
                    "contents": "Hydrogen atom\nBohr–Sommerfeld Model In 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simple assumptions in order to correct the failed classical model. The assumptions included: Electrons can only be in certain, discrete circular orbits or stationary states, thereby having a discrete set of possible radii and energies. Electrons do not emit radiation while in one of these stationary states. An electron can gain or lose energy by jumping from one discrete orbit to another. Bohr supposed that the electron's angular momentum is quantized with possible values: where and is Planck constant over . He also supposed that the centripetal force which keeps the electron in its orbit is provided by the Coulomb force, and that energy is conserved. Bohr derived the energy of each orbit of the hydrogen atom to be:",
                    "score": 0.8649865388870239
                },
                {
                    "id": 11936157,
                    "contents": "Wolfgang Finkelnburg\nWolfgang Finkelnburg Ionization Potentials of Higher Atomic Ions, Phys. Rev. Volume 77, Issue 2, 304 - 304 (1950). The author is cited as being at the Engineer Research and Development Laboratories, Fort Belvoir, Virginia. Received 2 December 1949.",
                    "score": 0.8649654388427734
                },
                {
                    "id": 4041639,
                    "contents": "Oh-My-God particle\nAssuming it was a proton, this particle traveled at of the speed of light, its Lorentz factor was and its rapidity was . At this speed, if a photon were travelling with the particle, it would take over 215,000 years for the photon to gain a 1 cm lead as seen from the Earth's reference frame. Due to special relativity, the relativistic time dilation experienced by a proton traveling at this speed would be extreme. If the proton originated from a distance of 1.5 billion light years, it would take approximately 1.71 days from the reference frame of the proton to travel that distance.",
                    "score": 0.8649283647537231
                },
                {
                    "id": 680010,
                    "contents": "Schrödinger equation\nHydrogen atom The Schrödinger equation for the hydrogen atom (or a hydrogen-like atom) is where is the electron charge, is the position of the electron relative to the nucleus, is the magnitude of the relative position, the potential term is due to the Coulomb interaction, wherein is the permittivity of free space and is the 2-body reduced mass of the hydrogen nucleus (just a proton) of mass and the electron of mass . The negative sign arises in the potential term since the proton and electron are oppositely charged. The reduced mass in place of the electron mass is used since the electron and proton together orbit each other about a common centre of mass, and constitute a two-body problem to solve. The motion of the electron is of principle interest here, so the equivalent one-body problem is the motion of the electron using the reduced mass.",
                    "score": 0.8643832206726074
                },
                {
                    "id": 1590110,
                    "contents": "Plum pudding model\nIn this model, the orbits of the electrons were stable under classical mechanics because when an electron moved away from the centre of the positively charged sphere, it was subjected to a greater net positive inward force, because there was more positive charge inside its orbit (see Gauss's law). Electrons were free to rotate in rings which were further stabilized by interactions among the electrons, and spectroscopic measurements were meant to account for energy differences associated with different electron rings. Thomson attempted unsuccessfully to reshape his model to account for some of the major spectral lines experimentally known for several elements. As early as 1897, theoretical physicist, Joseph Larmor had explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons. According to a centennial celebration of the Bohr atom in Nature magazine, it was John William Nicholson in 1912 who first discovered that electrons radiate the spectral",
                    "score": 0.86433345079422
                },
                {
                    "id": 19487276,
                    "contents": "Magnetic radiation reaction force\nwhich falls off rapidly for times greater than in the future. Therefore, signals from an interval approximately into the future affect the acceleration in the present. For an electron, this time is approximately sec, which is the time it takes for a light wave to travel across the \"size\" of an electron. See also Max Abraham Hendrik Lorentz Cyclotron radiation Electromagnetic mass Radiation resistance Radiation damping Synchrotron radiation Wheeler–Feynman absorber theory References Further reading See sections 11.2.2 and 11.2.3 \\ Jose A. Heras, The Radiation Force of an Electron Reexamined, 2003, http://www.joseheras.com/jheras_papers/JAH-PAPER_16.pdf. Donald H. Menzel, Fundamental Formulas of Physics, 1960, Dover Publications Inc., , vol. 1, page 345. External links MathPages - Does A Uniformly Accelerating Charge Radiate? Feynman: The Development of the Space-Time View of Quantum Electrodynamics Heras: The Radiation Reaction Force of an Electron Reexamined",
                    "score": 0.8642879724502563
                },
                {
                    "id": 16681019,
                    "contents": "Timeline of quantum mechanics\n1946 – Theodor V. Ionescu and Vasile Mihu report the construction of the first hydrogen maser by stimulated emission of radiation in molecular hydrogen. 1947 – Willis Lamb and Robert Retherford measure a small difference in energy between the energy levels 2S1/2 and 2P1/2 of the hydrogen atom, known as the Lamb shift. 1947 – George Rochester and Clifford Charles Butler publishes two cloud chamber photographs of cosmic ray-induced events, one showing what appears to be a neutral particle decaying into two charged pions, and one that appears to be a charged particle decaying into a charged pion and something neutral. The estimated mass of the new particles is very rough, about half a proton's mass. More examples of these \"V-particles\" were slow in coming, and they are soon given the name kaons.",
                    "score": 0.863885760307312
                },
                {
                    "id": 986659,
                    "contents": "Atomic, molecular, and optical physics\n. where E0 is the magnitude of the electric field amplitude, and E is the magnitude of the electric field at position x. From this basic, Planck's law was derived. In 1911, Ernest Rutherford concluded, based on alpha particle scattering, that an atom has a central pointlike proton. He also thought that an electron would be still attracted to the proton by Coulomb's law, which he had verified still held at small scales. As a result, he believed that electrons revolved around the proton. Niels Bohr, in 1913, combined the Rutherford model of the atom with the quantisation ideas of Planck. Only specific and well-defined orbits of the electron could exist, which also do not radiate light. In jumping orbit the electron would emit or absorb light corresponding to the difference in energy of the orbits. His prediction of the energy levels was then consistent with observation.",
                    "score": 0.8637151122093201
                },
                {
                    "id": 11114523,
                    "contents": "London equations\nthese electrons should encounter a uniform force, and thus they should in fact accelerate uniformly. This is precisely what the first London equation states. To obtain the second equation, take the curl of the first London equation and apply Faraday's law, , to obtain As it currently stands, this equation permits both constant and exponentially decaying solutions. The Londons recognized from the Meissner effect that constant nonzero solutions were nonphysical, and thus postulated that not only was the time derivative of the above expression equal to zero, but also that the expression in the parentheses must be identically zero. This results in the second London equation. Canonical momentum arguments It is also possible to justify the London equations by other means. Current density is defined according to the equation",
                    "score": 0.8637145161628723
                },
                {
                    "id": 992989,
                    "contents": "Robert Andrews Millikan\nor charged particles (Compton's view). Millikan thought his cosmic ray photons were the \"birth cries\" of new atoms continually being created to counteract entropy and prevent the heat death of the universe. Compton was eventually proven right by the observation that cosmic rays are deflected by the Earth's magnetic field (hence must be charged particles).",
                    "score": 0.8634456396102905
                },
                {
                    "id": 18846007,
                    "contents": "Peter R. Holland\nHolland has published many peer-reviewed articles on the foundations of physics including the quantum potential, quantum hydrodynamics, quantum field theory, symmetries, hidden-variables theories, quantum back-reaction, quantum Hamilton-Jacobi theory, classical-like quantum systems, and the history of physics. Publications Book Peter R. Holland: The Quantum Theory of Motion: An Account of the De Broglie-Bohm Causal Interpretation of Quantum Mechanics, Cambridge University Press, Cambridge (first published June 25 1993), hardback, paperback, transferred to digital printing 2004 and available as an e-book from 2010",
                    "score": 0.8634016513824463
                },
                {
                    "id": 348731,
                    "contents": "Hans Bethe\nA major talking point at the conference was the discovery by Willis Lamb and his graduate student, Robert Retherford, shortly before the conference began that one of the two possible quantum states of hydrogen atoms had slightly more energy than that predicted by the theory of Paul Dirac; this became known as the Lamb shift. Oppenheimer and Weisskopf suggested that this was a result of quantum fluctuations of the electromagnetic field, which gave the electron more energy. According to pre-war quantum electrodynamics (QED), the energy of the electron consisted of the bare energy it had when uncoupled from an electromagnetic field, and the self-energy resulting from the electromagnetic coupling, but both were unobservable, since the electromagnetic field cannot be switched off. QED gave infinite values for the self-energies; but the Lamb shift showed that they were both real and finite. Hans Kramers proposed renormalization as a solution, but no one knew how to do the calculation.",
                    "score": 0.8631125092506409
                },
                {
                    "id": 1556906,
                    "contents": "Albert Einstein\n1905 – Annus Mirabilis papers The Annus Mirabilis papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc2 that Einstein published in the Annalen der Physik scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are: Statistical mechanics Thermodynamic fluctuations and statistical physics",
                    "score": 0.8630500435829163
                },
                {
                    "id": 1187966,
                    "contents": "Positron\nDirac wrote a follow-up paper in December 1929 that attempted to explain the unavoidable negative-energy solution for the relativistic electron. He argued that \"... an electron with negative energy moves in an external [electromagnetic] field as though it carries a positive charge.\" He further asserted that all of space could be regarded as a \"sea\" of negative energy states that were filled, so as to prevent electrons jumping between positive energy states (negative electric charge) and negative energy states (positive charge). The paper also explored the possibility of the proton being an island in this sea, and that it might actually be a negative-energy electron. Dirac acknowledged that the proton having a much greater mass than the electron was a problem, but expressed \"hope\" that a future theory would resolve the issue.",
                    "score": 0.8629099726676941
                },
                {
                    "id": 26112168,
                    "contents": "Classical Electrodynamics (book)\nOverview Advanced topics treated in the first edition include magnetohydrodynamics, plasma physics, the vector form of Kirchhoff's diffraction theory, special relativity, and radiation emitted by moving and colliding charges. Jackson's choice of these topics is aimed at students interested in theoretical physics in general and nuclear and high-energy physics in particular. The necessary mathematical methods include vector calculus, ordinary and partial differential equations, Fourier series, and some special functions (the Bessel functions and Legendre polynomials).",
                    "score": 0.8629027605056763
                },
                {
                    "id": 17038124,
                    "contents": "Proton-to-electron mass ratio\nwhere β0 = −11 + 2n/3, with n being the number of flavors of quarks. Variation of μ over time Astrophysicists have tried to find evidence that μ has changed over the history of the universe. (The same question has also been asked of the fine structure constant.) One interesting cause of such change would be change over time in the strength of the strong force. Astronomical searches for time-varying μ have typically examined the Lyman series and Werner transitions of molecular hydrogen which, given a sufficiently large redshift, occur in the optical region and so can be observed with ground-based spectrographs. If μ were to change, then the change in the wavelength λi of each rest frame wavelength can be parameterised as: where Δμ/μ is the proportional change in μ and Ki is a constant which must be calculated within a theoretical (or semi-empirical) framework.",
                    "score": 0.8626198172569275
                },
                {
                    "id": 20130764,
                    "contents": "Jeans's theorem\nIn astrophysics and statistical mechanics, Jeans's theorem, named after James Jeans, states that any steady-state solution of the collisionless Boltzmann equation depends on the phase space coordinates only through integrals of motion in the given potential, and conversely any function of the integrals is a steady-state solution.",
                    "score": 0.862439751625061
                },
                {
                    "id": 1723065,
                    "contents": "Elementary particle\nThe number of protons in the observable universe is called the Eddington number. In terms of number of particles, some estimates imply that nearly all the matter, excluding dark matter, occurs in neutrinos, which constitute the majority of the roughly elementary particles of matter that exist in the visible universe. Other estimates imply that roughly elementary particles exist in the visible universe (not including dark matter), mostly photons and other massless force carriers. Standard Model",
                    "score": 0.8619513511657715
                },
                {
                    "id": 5963002,
                    "contents": "Annus mirabilis papers\nThe equation sets forth that the energy of a body at rest () equals its mass () times the speed of light () squared, or . The mass-energy relation can be used to predict how much energy will be released or consumed by nuclear reactions; one simply measures the mass of all constituents and the mass of all the products and multiplies the difference between the two by . The result shows how much energy will be released or consumed, usually in the form of light or heat. When applied to certain nuclear reactions, the equation shows that an extraordinarily large amount of energy will be released, millions of times as much as in the combustion of chemical explosives, where the amount of mass converted to energy is negligible. This explains why nuclear weapons and nuclear reactors produce such phenomenal amounts of energy, as they release binding energy during nuclear fission and nuclear fusion, and convert a portion of subatomic mass to energy.",
                    "score": 0.861912727355957
                },
                {
                    "id": 2939968,
                    "contents": "1932 in science\nPhysics April 14 – John Cockcroft and Ernest Walton focus a proton beam on lithium and split its nucleus. May 10 – James Chadwick discovers the neutron. Werner Heisenberg explains its symmetries by introducing the concept of isospin. August 2 – The positron is observed by Carl Anderson. The Kennedy–Thorndike experiment shows that measured time as well as length are affected by motion, in accordance with the theory of special relativity. John von Neumann rigorously establishes a mathematical framework for quantum mechanics in Mathematische Grundlagen der Quantenmechanik. Zero-length springs are invented, revolutionizing seismometers and gravimeters. Awards Nobel Prizes Physics – Werner Karl Heisenberg Chemistry – Irving Langmuir Medicine – Sir Charles Sherrington, Edgar Adrian",
                    "score": 0.8616942763328552
                },
                {
                    "id": 22831470,
                    "contents": "Index of physics articles (P)\nPropulsive efficiency Prosper-René Blondlot Protogalaxy Proton Proton-to-electron mass ratio Proton Synchrotron Proton Synchrotron Booster Proton decay Proton emission Proton spin crisis Protonium Proton–gyromagnetic ratio Proton–proton chain reaction Protoplanetary nebula Protostar Proximity effect (atomic physics) Proximity effect (audio) Przybylski's Star Pseudo-Anosov map Pseudo-Euclidean space Pseudo-Goldstone boson Pseudo-Riemannian manifold Pseudo Stirling cycle Pseudogap Pseudophakic photic phenomena Pseudopotential Pseudorapidity Pseudoscalar Pseudoscalar meson Pseudotensor Pseudovector Pseudovector meson Psi meson Psychic determinism Psychoacoustics Psychrometrics Ptychography Pucadyil Ittoop John Pulsar Pulsar kicks Pulsatile flow Pulse (physics) Pulse forming network Pulse height analyzer Pulsed EPR Pulsed field gradient Pulsed laser deposition Pulsed power Pulsometer steam pump Pump Pumplinx Purcell effect Purdue University Reactor Number One Pure bending Pure gauge",
                    "score": 0.8615314364433289
                },
                {
                    "id": 6331629,
                    "contents": "Pbar\npbar may refer to: picobar, a unit of pressure antiproton, a fundamental particle, its symbol is , \"p-bar\"",
                    "score": 0.8613812327384949
                },
                {
                    "id": 1842856,
                    "contents": "Lorentz transformation\nHistory Many physicists—including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz himself—had been discussing the physics implied by these equations since 1887. Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the luminiferous aether. FitzGerald then conjectured that Heaviside's distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 aether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis. Their explanation was widely known before 1905.",
                    "score": 0.8613342642784119
                },
                {
                    "id": 4605688,
                    "contents": "Henry Stapp\nIn 1969 Stapp was invited by Werner Heisenberg to work with him at the Max Planck Institute in Munich. In 1976 Stapp was invited by J.A. Wheeler to work with him on problems in the foundations of Quantum Mechanics. Dr. Stapp has published many papers pertaining to the non-local aspects of quantum mechanics and Bell's theorem, including two books published by Springer-Verlag, and a third one in progress. Stapp has worked also in a number of conventional areas of high energy physics, including analysis of the scattering of polarized protons, parity violation, and S-matrix theory. Research Some of Stapp's work concerns the implications of quantum mechanics (QM). He has argued for the relevance of QM to consciousness and free will.",
                    "score": 0.8612626791000366
                },
                {
                    "id": 1565315,
                    "contents": "Atomic orbital\nWith de Broglie's suggestion of the existence of electron matter waves in 1924, and for a short time before the full 1926 Schrödinger equation treatment of hydrogen-like atoms, a Bohr electron \"wavelength\" could be seen to be a function of its momentum, and thus a Bohr orbiting electron was seen to orbit in a circle at a multiple of its half-wavelength. The Bohr model for a short time could be seen as a classical model with an additional constraint provided by the 'wavelength' argument. However, this period was immediately superseded by the full three-dimensional wave mechanics of 1926. In our current understanding of physics, the Bohr model is called a semi-classical model because of its quantization of angular momentum, not primarily because of its relationship with electron wavelength, which appeared in hindsight a dozen years after the Bohr model was proposed.",
                    "score": 0.861126184463501
                },
                {
                    "id": 25197486,
                    "contents": "Energy well\nIn physics, an energy well describes a 'stable' equilibrium that is not at lowest possible energy. In general, modern physics holds the view that the universe - and systems therein - spontaneously drives toward a state of lower energy, if possible. For example, a bowling ball pitched atop a smooth hump (which has potential energy in the presence of gravity), will tend to roll down to the lowest point it possibly can. Once there, this reduces the total potential energy of the system. On the other hand, if the bowling ball is resting in a valley between two humps - no matter how big the drops outside the humps - it will stay there indefinitely. Even though the system could achieve a lower energy state, it cannot do so without external energy being applied: (locally) it is at its lowest energy state, and only a force from outside the system can 'push' it over one of the humps so a lower state can be achieved.",
                    "score": 0.8609118461608887
                },
                {
                    "id": 2876032,
                    "contents": "Jean Baptiste Perrin\nResearch and achievements In 1895, Perrin showed that cathode rays were of negative electric charge in nature. He determined Avogadro's number (now known as the Avogadro constant) by several methods. He explained solar energy as due to the thermonuclear reactions of hydrogen. After Albert Einstein published (1905) his theoretical explanation of Brownian motion in terms of atoms, Perrin did the experimental work to test and verify Einstein's predictions, thereby settling the century-long dispute about John Dalton's atomic theory. Carl Benedicks argued Perrin should receive the Nobel Prize in Physics; Perrin received the award in 1926 for this and other work on the discontinuous structure of matter, which put a definite end to the long struggle regarding the question of the physical reality of molecules.",
                    "score": 0.8608936667442322
                },
                {
                    "id": 1164043,
                    "contents": "Proton\nInteraction of free protons with ordinary matter Although protons have affinity for oppositely charged electrons, this is a relatively low-energy interaction and so free protons must lose sufficient velocity (and kinetic energy) in order to become closely associated and bound to electrons. High energy protons, in traversing ordinary matter, lose energy by collisions with atomic nuclei, and by ionization of atoms (removing electrons) until they are slowed sufficiently to be captured by the electron cloud in a normal atom.",
                    "score": 0.8608081340789795
                },
                {
                    "id": 1164022,
                    "contents": "Proton\nFree protons occur occasionally on Earth: thunderstorms can produce protons with energies of up to several tens of MeV. At sufficiently low temperatures and kinetic energies, free protons will bind to electrons. However, the character of such bound protons does not change, and they remain protons. A fast proton moving through matter will slow by interactions with electrons and nuclei, until it is captured by the electron cloud of an atom. The result is a protonated atom, which is a chemical compound of hydrogen. In vacuum, when free electrons are present, a sufficiently slow proton may pick up a single free electron, becoming a neutral hydrogen atom, which is chemically a free radical. Such \"free hydrogen atoms\" tend to react chemically with many other types of atoms at sufficiently low energies. When free hydrogen atoms react with each other, they form neutral hydrogen molecules (H2), which are the most common molecular component of molecular clouds in interstellar space.",
                    "score": 0.8607277274131775
                },
                {
                    "id": 26112172,
                    "contents": "Classical Electrodynamics (book)\nChapter 14: Radiation by Moving Charges Chapter 15: Bremsstrahlung, Method of Virtual Quanta, Radiative Beta Processes Chapter 16: Radiation Damping, Classical Models of Charged Particles Appendix on Units and Dimensions Bibliography Index",
                    "score": 0.8607175350189209
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_8",
        "question": "Example 5-3 shows that a Maclaurin expansion of a Morse potential leads to\r\n$$\r\nV(x)=D \\beta^2 x^2+\\cdots\r\n$$\r\nGiven that $D=7.31 \\times 10^{-19} \\mathrm{~J} \\cdot$ molecule ${ }^{-1}$ and $\\beta=1.81 \\times 10^{10} \\mathrm{~m}^{-1}$ for $\\mathrm{HCl}$, calculate the force constant of $\\mathrm{HCl}$.",
        "golden_answers": [
            " 479"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 24403989,
                    "contents": "Morse/Long-range potential\nThis long-range form of the MLR model is guaranteed because the argument of the exponent is defined to have long-range behavior: , where is the equilibrium bond length. There are a few ways in which this long-range behavior can be achieved, the most common is to make a polynomial that is constrained to become at long-range: , , where n is an integer greater than 1, which value is defined by the model chosen for the long-range potential . It is clear to see that: . Applications The MLR potential has successfully summarized all experimental spectroscopic data (and/or virial data) for a number of diatomic molecules, including: N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH. More sophisticated versions are used for polyatomic molecules.",
                    "score": 0.9083892107009888
                },
                {
                    "id": 24403987,
                    "contents": "Morse/Long-range potential\nthe c-state of Li2: where the MLR potential was successfully able to bridge a gap of more than 5000 cm−1 in experimental data. Two years later it was found that the MLR potential was able to successfully predict the energies in the middle of this gap, correctly within about 1 cm−1. The accuracy of these predictions was much better than the most sophisticated ab initio techniques at the time. the A-state of Li2: where Le Roy et al. constructed an MLR potential which determined the C3 value for atomic lithium to a higher-precision than any previously measured atomic oscillator strength, by an order of magnitude. This lithium oscillator strength is related to the radiative lifetime of atomic lithium and is used as a benchmark for atomic clocks and measurements of fundamental constants. the a-state of KLi: where the MLR was used to build an analytic global potential successfully despite there only being a small amount of levels observed near the top of the potential.",
                    "score": 0.9018059372901917
                },
                {
                    "id": 3936515,
                    "contents": "Morse potential\nThis trend matches the anharmonicity found in real molecules. However, this equation fails above some value of where is calculated to be zero or negative. Specifically, integer part. This failure is due to the finite number of bound levels in the Morse potential, and some maximum that remains bound. For energies above , all the possible energy levels are allowed and the equation for is no longer valid. Below , is a good approximation for the true vibrational structure in non-rotating diatomic molecules. In fact, the real molecular spectra are generally fit to the form1 in which the constants and can be directly related to the parameters for the Morse potential. As is clear from dimensional analysis, for historical reasons the last equation uses spectroscopic notation in which represents a wavenumber obeying , and not an angular frequency given by . Morse/Long-range potential",
                    "score": 0.9016988277435303
                },
                {
                    "id": 3936513,
                    "contents": "Morse potential\nwhich is usually written as where is now the coordinate perpendicular to the surface. This form approaches zero at infinite and equals at its minimum, i.e. . It clearly shows that the Morse potential is the combination of a short-range repulsion term (the former) and a long-range attractive term (the latter), analogous to the Lennard-Jones potential. Vibrational states and energies Like the quantum harmonic oscillator, the energies and eigenstates of the Morse potential can be found using operator methods. One approach involves applying the factorization method to the Hamiltonian. To write the stationary states on the Morse potential, i.e. solutions and of the following Schrödinger equation: it is convenient to introduce the new variables: Then, the Schrödinger equation takes the simple form: Its eigenvalues and eigenstates can be written as: where with [x] denoting the largest integer smaller than x. where and is a generalized Laguerre polynomial:",
                    "score": 0.8963677883148193
                },
                {
                    "id": 3936514,
                    "contents": "Morse potential\nIts eigenvalues and eigenstates can be written as: where with [x] denoting the largest integer smaller than x. where and is a generalized Laguerre polynomial: There also exists the following analytical expression for matrix elements of the coordinate operator: which is valid for and . The eigenenergies in the initial variables have the form: where is the vibrational quantum number and has units of frequency. The latter is mathematically related to the particle mass, , and the Morse constants via Whereas the energy spacing between vibrational levels in the quantum harmonic oscillator is constant at , the energy between adjacent levels decreases with increasing in the Morse oscillator. Mathematically, the spacing of Morse levels is This trend matches the anharmonicity found in real molecules. However, this equation fails above some value of where is calculated to be zero or negative. Specifically, integer part.",
                    "score": 0.8935926556587219
                },
                {
                    "id": 24403988,
                    "contents": "Morse/Long-range potential\nHistorical origins The MLR potential is based on the classic Morse potential which was first introduced in 1929 by Philip M. Morse. A primitive version of the MLR potential was first introduced in 2006 by Robert J. Le Roy and colleagues for a study on N2. This primitive form was used on Ca2, KLi and MgH, before the more modern version was introduced in 2009 . A further extension of the MLR potential referred to as the MLR3 potential was introduced in a 2010 study of Cs2, and this potential has since been used on HF, HCl, HBr and HI. Function The Morse/Long-range potential energy function is of the form where for large , , so is defined according to the theoretically correct long-range behavior expected for the interatomic interaction. This long-range form of the MLR model is guaranteed because the argument of the exponent is defined to have long-range behavior: , where is the equilibrium bond length.",
                    "score": 0.8931417465209961
                },
                {
                    "id": 25311583,
                    "contents": "Trigonometric Rosen–Morse potential\nThe trigonometric Rosen–Morse potential, named after the physicists Nathan Rosen and Philip M. Morse, is among the exactly solvable quantum mechanical potentials. Definition In dimensionless units and modulo additive constants, it is defined as where is a relative distance, is an angle rescaling parameter, and is so far a matching length parameter. Another parametrization of same potential is which is the trigonometric version of a one-dimensional hyperbolic potential introduced in molecular physics by Nathan Rosen and Philip M. Morse and given by,",
                    "score": 0.8930529952049255
                },
                {
                    "id": 3936511,
                    "contents": "Morse potential\nThe Morse potential, named after physicist Philip M. Morse, is a convenient interatomic interaction model for the potential energy of a diatomic molecule. It is a better approximation for the vibrational structure of the molecule than the quantum harmonic oscillator because it explicitly includes the effects of bond breaking, such as the existence of unbound states. It also accounts for the anharmonicity of real bonds and the non-zero transition probability for overtone and combination bands. The Morse potential can also be used to model other interactions such as the interaction between an atom and a surface. Due to its simplicity (only three fitting parameters), it is not used in modern spectroscopy. However, its mathematical form inspired the MLR (Morse/Long-range) potential, which is the most popular potential energy function used for fitting spectroscopic data. Potential energy function The Morse potential energy function is of the form",
                    "score": 0.8925225138664246
                },
                {
                    "id": 25311584,
                    "contents": "Trigonometric Rosen–Morse potential\nwhich is the trigonometric version of a one-dimensional hyperbolic potential introduced in molecular physics by Nathan Rosen and Philip M. Morse and given by, a parallelism that explains the potential's name. The most prominent application concerns the parametrization, with non-negative integer, and is due to Schrödinger who intended to formulate the hydrogen atom problem on Albert Einstein's closed universe, , the direct product of a time line with a three-dimensional closed space of positive constant curvature, the hypersphere , and introduced it on this geometry in his celebrated equation as the counterpart to the Coulomb potential, a mathematical problem briefly highlighted below. The case: Four-dimensional rigid rotator in inertial quantum motion on the three dimensional hypersphere The hypersphere is a surface in a four-dimensional Euclidean space, , and is defined as,",
                    "score": 0.8894714117050171
                },
                {
                    "id": 15666021,
                    "contents": "Pöschl–Teller potential\nIn mathematical physics, a Pöschl–Teller potential, named after the physicists Herta Pöschl (credited as G. Pöschl) and Edward Teller, is a special class of potentials for which the one-dimensional Schrödinger equation can be solved in terms of special functions. Definition In its symmetric form is explicitly given by and the solutions of the time-independent Schrödinger equation with this potential can be found by virtue of the substitution , which yields . Thus the solutions are just the Legendre functions with , and , . Moreover, eigenvalues and scattering data can be explicitly computed. In the special case of integer , the potential is reflectionless and such potentials also arise as the N-soliton solutions of the Korteweg-de Vries equation. The more general form of the potential is given by Rosen–Morse potential A related potential is given by introducing an additional term: See also Morse potential Trigonometric Rosen–Morse potential References list",
                    "score": 0.885745644569397
                },
                {
                    "id": 11308585,
                    "contents": "List of quantum-mechanical potentials\nOscillators Harmonic potential (harmonic oscillator) Morse potential (morse oscillator) Morse/Long-range potential (Morse/Long-range oscillator) Kratzer potential (Kratzer oscillator) Quantum Field theory Yukawa potential Coleman–Weinberg potential Uehling potential Woods–Saxon potential Miscellaneous Quantum potential Pseudopotential Kolos–Wolniewicz potential See also List of quantum-mechanical systems with analytical solutions List of integrable models Quantum mechanics Science-related lists Quantum mechanical potentials",
                    "score": 0.8841357231140137
                },
                {
                    "id": 25311606,
                    "contents": "Trigonometric Rosen–Morse potential\nwhere is the reduced mass of the two-body system under consideration. The partition function (statistical mechanics) for this energy spectrum is defined in the standard way as, Here, the thermodynamic beta is defined as with standing for the Boltzmann constant. In evaluating it is useful to recall that with the increase of the second term on the right hand side in () becomes negligible compared to the term proportional , a behavior which becomes even more pronounced for the choices, , and . In both cases is much smaller compared to the corresponding dimensionless factor, , multiplying . For this reason the partition function under investigation might be well approximated by, Along same lines, the partition function for the parametrization corresponding to the Hydrogen atom on has been calculated in, where a more sophisticated approximation has been employed. When transcribed to the current notations and units, the partition function in presents itself as,",
                    "score": 0.8831186294555664
                },
                {
                    "id": 3936516,
                    "contents": "Morse potential\nMorse/Long-range potential An extension of the Morse potential that made the Morse form useful for modern (high-resolution) spectroscopy is the MLR (Morse/Long-range) potential. The MLR potential is used as a standard for representing spectroscopic and/or virial data of diatomic molecules by a potential energy curve. It has been used on N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH. More sophisticated versions are used for polyatomic molecules. See also Lennard-Jones potential Molecular mechanics References 1 CRC Handbook of chemistry and physics, Ed David R. Lide, 87th ed, Section 9, SPECTROSCOPIC CONSTANTS OF DIATOMIC MOLECULES pp. 9–82 I.G. Kaplan, in Handbook of Molecular Physics and Quantum Chemistry, Wiley, 2003, p207. Chemical bonding Quantum chemistry Quantum models Quantum mechanical potentials",
                    "score": 0.8798798322677612
                },
                {
                    "id": 6500849,
                    "contents": "Philip M. Morse\nPhysics Philip Morse had a distinguished career in physics. Amongst his contributions to physics are the textbooks Quantum Mechanics (with Edward Condon), Methods of Theoretical Physics (with Herman Feshbach), Vibration and Sound, Theoretical Acoustics, and Thermal Physics. Morse is also one of the founding editors of Annals of Physics. In 1929 he proposed the Morse potential function for diatomic molecules which was often used to interpret vibrational spectra, though the standard is now the more modern Morse/Long-range potential. Administration His administrative talents were applied in roles as co-founder of the MIT Acoustics Laboratory, first director of the Brookhaven National Laboratory, founder and first director of the MIT Computation Center, and board member of the RAND Corporation and the Institute for Defense Analyses. He chaired the advisory committee that supervised preparation of Handbook of Mathematical Functions, with Formulas, Graphs, and Mathematical Tables.",
                    "score": 0.879460334777832
                },
                {
                    "id": 3936512,
                    "contents": "Morse potential\nPotential energy function The Morse potential energy function is of the form Here is the distance between the atoms, is the equilibrium bond distance, is the well depth (defined relative to the dissociated atoms), and controls the 'width' of the potential (the smaller is, the larger the well). The dissociation energy of the bond can be calculated by subtracting the zero point energy from the depth of the well. The force constant (stiffness) of the bond can be found by Taylor expansion of around to the second derivative of the potential energy function, from which it can be shown that the parameter, , is where is the force constant at the minimum of the well. Since the zero of potential energy is arbitrary, the equation for the Morse potential can be rewritten any number of ways by adding or subtracting a constant value. When it is used to model the atom-surface interaction, the energy zero can be redefined so that the Morse potential becomes which is usually written as",
                    "score": 0.8794413208961487
                },
                {
                    "id": 24403986,
                    "contents": "Morse/Long-range potential\nThe Morse/Long-range potential (MLR potential) is an interatomic interaction model for the potential energy of a diatomic molecule. Due to the simplicity of the regular Morse potential (it only has three adjustable parameters), it is very limited in its applicability in modern spectroscopy. The MLR potential is a modern version of the Morse potential which has the correct theoretical long-range form of the potential naturally built into it. It has been an important tool for spectroscopists to represent experimental data, verify measurements, and make predictions. It is useful for its extrapolation capability when data for certain regions of the potential are missing, its ability to predict energies with accuracy often better than the most sophisticated ab initio techniques, and its ability to determine precise empirical values for physical parameters such as the dissociation energy, equilibrium bond length, and long-range constants. Cases of particular note include:",
                    "score": 0.8785061836242676
                },
                {
                    "id": 22434402,
                    "contents": "Maria Adelaide Sneider\nSee also Harmonic polynomial Potential theory Notes References General references , is a collection of papers, published in issues 3 and 4 of the 10th volume, and partly on the 1st issue of the 11th volume of the seventh series of the \"Rendiconti di Matematica e delle sue Applicazioni\" mathematical journal as an homage to her memory. . The brief \"Introduction\" written for the double issue of the \"Rendiconti di Matematica\" dedicated to Maria Adelaide Sneider. . An obituary on Maria Adelaide Sneider, including a detailed sketch of her major research contributions and a full list of her publications. .",
                    "score": 0.8780509233474731
                },
                {
                    "id": 21226352,
                    "contents": "Phase-space formulation\nMorse potential The Morse potential is used to approximate the vibrational structure of a diatomic molecule. Quantum tunneling Tunneling is a hallmark quantum effect where a quantum particle, not having sufficient energy to fly above, still goes through a barrier. This effect does not exist in classical mechanics. Quartic potential Schrödinger cat state References Quantum mechanics Hamiltonian mechanics Symplectic geometry Mathematical quantization Foundational quantum physics Articles containing video clips",
                    "score": 0.8746564388275146
                },
                {
                    "id": 24403990,
                    "contents": "Morse/Long-range potential\nIt has also become customary to fit ab initio points to the MLR potential, to achieve a fully analytic ab initio potential and to take advantage of the MLR's ability to incorporate the correct theoretically known short- and long-range behavior into the potential (the latter usually being of higher accuracy than the molecular ab initio points themselves because it is based on atomic ab initio calculations rather than molecular ones, and because features like spin-orbit coupling which are difficult to incorporate into molecular ab initio calculations can more easily be treated in the long-range). MLR has been used to represent ab initio points for KLi and KBe. See also Dilithium Morse potential Lennard-Jones potential References Thermodynamics Chemical bonding Intermolecular forces Computational chemistry Theoretical chemistry Quantum mechanical potentials",
                    "score": 0.8735846281051636
                },
                {
                    "id": 20458157,
                    "contents": "Wu–Sprung potential\nIn mathematical physics, the Wu–Sprung potential, named after Hua Wu and Donald Sprung, is a potential function in one dimension inside a Hamiltonian with the potential defined by solving a non-linear integral equation defined by the Bohr–Sommerfeld quantization conditions involving the spectral staircase, the energies and the potential . here a is a classical turning point so , the quantum energies of the model are the roots of the Riemann Xi function and . In general, although Wu and Sprung considered only the smooth part, the potential is defined implicitly by ; with N(x) being the eigenvalue staircase and H(x) is the Heaviside step function. For the case of the Riemann zeros Wu and Sprung and others have shown that the potential can be written implicitly in terms of the Gamma function and zeroth-order Bessel function. and that the density of states of this Hamiltonian is just the Delsarte's formula for the Riemann zeta function and defined semiclassically as",
                    "score": 0.8722439408302307
                },
                {
                    "id": 15316644,
                    "contents": "Balayage\nIn potential theory, a mathematical discipline, balayage (from French: balayage \"scanning, sweeping\") is a method devised by Henri Poincaré for reconstructing a harmonic function in a domain from its values on the boundary of the domain. In modern terms, the balayage operator maps a measure μ on a closed domain D to a measure ν on the boundary ∂ D, so that the Newtonian potentials of μ and ν coincide outside . The procedure is called balayage since the mass is \"swept out\" from D onto the boundary. For x in D, the balayage of δx yields the harmonic measure νx corresponding to x. Then the value of a harmonic function f at x is equal to References Potential theory",
                    "score": 0.8707337975502014
                },
                {
                    "id": 25311595,
                    "contents": "Trigonometric Rosen–Morse potential\nBefore closing this section, it is in order to bring the exact solutions to the equations ()-(), given by where stand for the Romanovski polynomials. Application to Coulomb fluids Coulomb fluids consist of dipolar particles and are modelled by means of direct numerical simulations. It is commonly used to choose cubic cells with periodic boundary conditions in conjunction with Ewald summation techniques. In a more efficient alternative method pursued by, one employs as a simulation cell the hyper spherical surface in (). As already mentioned above, the basic object on is the electric charge dipole, termed to as \"bi-charge\" in fluid dynamics, which can be visualized classically as a rigid \"dumbbell\" (rigid rotator) of two antipodal charges of opposite signs, and . The potential of a bi-charge is calculated by solving on the Poisson equation,",
                    "score": 0.8705497980117798
                },
                {
                    "id": 16683887,
                    "contents": "Wordsmith (TV series)\n8. Numbers III 100, cent, Dec, Quadr, Quart, 4, tri, triskaidekaphobia, trivia, square, all, 9. Walk and Run Gress, Grad, Cur, Drom, Fug, Ambul 10. Water Aqua, Mar, Flu, Sea, Und, Hydr, H2O, Hydra, diluv, antediluvian 11. Time Pre, Post, A.M., P.M.,chron, temp, daisy 12. Cutting Tom, sect, guillotine, atom, cis, ec, nostril 13. Animals I pecu, anim, can(cyn), mus, -ine, goose bumps, dandelion, zo, Kangaroo, impecunious, parroting, ferret, badger,Cancer, Taurus, Ares, Pisces, Leo, Scorpio, Capricorn, 14. Animals II bio, anim, drom, greg, skunk, gnu, hippodrome, gaggle, school, flock, herd, swarm, pack, colony, pride, leap, gam, crash, piggy bank, monkey wrench, etc. 15. Serendipity serendipity, volcano, Asian characters, English logograms, atom, water idioms, Anatomy, Avocado, Mark Twain 16. Nature earth, geo, nat, aaron, navidad, terr, eco (oiko), bio, Copernicus, Galileo 17. Leading duc, agog, pedagogy, crat, cracy, reg, pop",
                    "score": 0.8696709871292114
                },
                {
                    "id": 1973036,
                    "contents": "Thermodynamic potential\nThe set of natural variables for the above four potentials are formed from every combination of the - and - variables, excluding any pairs of conjugate variables. There is no reason to ignore the − conjugate pairs, and in fact we may define four additional potentials for each species. Using IUPAC notation in which the brackets contain the natural variables (other than the main four), we have: If there is only one species, then we are done. But, if there are, say, two species, then there will be additional potentials such as and so on. If there are dimensions to the thermodynamic space, then there are unique thermodynamic potentials. For the most simple case, a single phase ideal gas, there will be three dimensions, yielding eight thermodynamic potentials. The fundamental equations",
                    "score": 0.8680930137634277
                },
                {
                    "id": 101826,
                    "contents": "Lennard-Jones potential\nThe functional form of the attractive term, the exponent '6', has a physical justification, which does not hold as rigorously for the repulsive term with the exponent '12'. The attractive dispersive interactions between simple atoms and molecules are a result of fluctuating partial charges. It has been shown by quantum-chemical calculations that this dispersive contribution has to decay with . The term is mainly used because it can be implemented computationally very efficiently as the square of , which does not hold to the same extent for values other than '12'. Also, approximates the Pauli repulsion reasonably well. The Lennard-Jones potential can be generalized using arbitrary exponents instead of 12 and 6. The resulting potential is called Mie potential. The present article exclusively discusses the classical (12-6) Lennard-Jones potential.",
                    "score": 0.8670204877853394
                },
                {
                    "id": 25311586,
                    "contents": "Trigonometric Rosen–Morse potential\nThe solutions, , to this equation are the so-called four-dimensional hyper-spherical harmonics defined as where are the Gegenbauer polynomials. Changing in () variables as one observes that the function satisfies the one-dimensional Schrödinger equation with the potential according to The one-dimensional potential in the latter equation, in coinciding with the Rosen–Morse potential in () for and , clearly reveals that for integer values, the first term of this potential takes its origin from the centrifugal barrier on . Stated differently, the equation (), and its version () describe inertial (free) quantum motion of a rigid rotator in the four-dimensional Euclidean space, , such as the H Atom, the positronium, etc. whose \"ends\" trace the large \"circles\" (i.e. spheres) on . Now the question arises whether the second term in () could also be related in some way to the geometry. The case: Electric charge confinement on and a dipole potential shaped after",
                    "score": 0.8667339086532593
                },
                {
                    "id": 15005731,
                    "contents": "Simon–Glatzel equation\nThe Simon–Glatzel equation can be viewed as a combination of the Murnaghan equation of state and the Lindemann law, and an alternative form was proposed by J. J. Gilvarry (1956): where is general at , is pressure derivative at , is Grüneisen ratio, and is the coefficient in Morse potential. Example parameters For methanol the following parameters can be obtained: The reference temperature has been Tref = 174.61 K and the reference pressure Pref has been set to 0 kPa. Methanol is a component where the Simon–Glatzel works well in the given validity range.",
                    "score": 0.8655019998550415
                },
                {
                    "id": 19824676,
                    "contents": "Richard Palais\nArticles Richard Palais and Stephen Smale, A generalized Morse theory, Research Announcement, Bulletin of the American Mathematical Society 70 (1964), 165-172 R. Palais, Morse Theory on Hilbert Manifolds, Topology 2 (1963), 299–340. R. Palais, Linear and Nonlinear Waves and Solitons, in The Princeton Companion to Mathematics, T. Gower Ed., Princeton Univ. Press 2008, 234-239 () R. Palais, The Symmetries of Solitons, Bulletin. Amer. Math. Soc., New Series 34, No. 4, 339-403 (1997) [ISSN 0273-0979], () R. Palais, The Visualization of Mathematics: Towards a Mathematical Exploratorium, Notices Amer. Math. Soc., 46, No. 6 (June–July 1999, () R. Palais, A Simple Proof of the Banach Contraction Principle, The Journal for Fixed Point Theory and its Applications, 2 (2007) 221–223, () A nearly complete list of all papers authored or co-authored by Richard Palais is available for downloading as PDF files at http://vmm.math.uci.edu/PalaisPapers References External links",
                    "score": 0.8651614189147949
                },
                {
                    "id": 5122517,
                    "contents": "The Early Asimov\nContents \"The Callistan Menace\" (published 1940) \"Ring Around the Sun\" (1940) \"The Magnificent Possession\" (1940) \"Trends\" (1939) \"The Weapon Too Dreadful to Use\" (1939) \"Black Friar of the Flame\" (1942), novelette \"Half-Breed\" (1940), novelette, Half-Breed series #1 \"The Secret Sense\" (1941) \"Homo Sol\" (1940), Homo Sol series #1 \"Half-Breeds on Venus\" (1940), novelette, Half-Breed series #2 \"The Imaginary\" (1942), Homo Sol series #2 \"Heredity\" (1941), novelette \"History\" (1941) \"Christmas on Ganymede\" (1942) \"The Little Man on the Subway\" (1950) \"The Hazing\" (1942), Homo Sol series #3 \"Super-Neutron\" (1941) \"Not Final!\" (1941), Jovians series #1 \"Legal Rites\" (1950), novelette \"Time Pussy\" (1942), Probability Zero series \"Author! Author!\" (1964), novelette \"Death Sentence\" (1943) \"Blind Alley\" (1945), Galactic Empire series \"No Connection\" (1948) \"The Endochronic Properties of Resublimated Thiotimoline\" (1948), Thiotimoline series #1",
                    "score": 0.864635169506073
                },
                {
                    "id": 16795576,
                    "contents": "Komornik–Loreti constant\nThe special case of , , and or 1 is sometimes called a -development. gives the only 2-development. However, for almost all , there are an infinite number of different -developments. Even more surprisingly though, there exist exceptional for which there exists only a single -development. Furthermore, there is a smallest number known as the Komornik–Loreti constant for which there exists a unique -development. Value The Komornik–Loreti constant is the value such that where is the Thue–Morse sequence, i.e., is the parity of the number of 1's in the binary representation of . It has approximate value The constant is also the unique positive real root of This constant is transcendental. See also Euler-Mascheroni constant Fibonacci word Golay–Rudin–Shapiro sequence Prouhet–Thue–Morse constant References Mathematical constants Non-standard positional numeral systems",
                    "score": 0.8626169562339783
                },
                {
                    "id": 101875,
                    "contents": "Lennard-Jones potential\nSee also Molecular mechanics Embedded atom model Force field (chemistry) Comparison of force field implementations Morse potential and Morse/Long-range potential Virial expansion References External links Lennard-Jones model on SklogWiki. Thermodynamics Chemical bonding Intermolecular forces Computational chemistry Theoretical chemistry Quantum mechanical potentials",
                    "score": 0.862537682056427
                },
                {
                    "id": 17410099,
                    "contents": "Reeb graph\nDescription for Morse functions If f is a Morse function with distinct critical values, the Reeb graph can be described more explicitly. Its nodes, or vertices, correspond to the critical level sets f−1(c). The pattern in which the arcs, or edges, meet at the nodes/vertices reflects the change in topology of the level set f−1(t) as t passes through the critical value c. For example, if c is a minimum or a maximum of f, a component is created or destroyed; consequently, an arc originates or terminates at the corresponding node, which has degree 1. If c is a saddle point of index 1 and two components of f−1(t) merge at t = c as t increases, the corresponding vertex of the Reeb graph has degree 3 and looks like the letter \"Y\"; the same reasoning applies if the index of c is dim X−1 and a component of f−1(c) splits into two. References Graph families Application-specific graphs",
                    "score": 0.8621909618377686
                },
                {
                    "id": 3730690,
                    "contents": "Newtonian potential\nThe Newtonian potential is defined more broadly as the convolution when μ is a compactly supported Radon measure. It satisfies the Poisson equation in the sense of distributions. Moreover, when the measure is positive, the Newtonian potential is subharmonic on Rd. If ƒ is a compactly supported continuous function (or, more generally, a finite measure) that is rotationally invariant, then the convolution of ƒ with Γ satisfies for x outside the support of ƒ In dimension d = 3, this reduces to Newton's theorem that the potential energy of a small mass outside a much larger spherically symmetric mass distribution is the same as if all of the mass of the larger object were concentrated at its center.",
                    "score": 0.8621798157691956
                },
                {
                    "id": 3808933,
                    "contents": "Marston Morse\nHe died on June 22, 1977, at his home in Princeton, New Jersey. Marston Morse should not be confused with Anthony Morse, famous for the Morse–Sard theorem, or Samuel Morse, who morse code is named after. Selected publications Articles Books with Stewart Cairns: Film \"Pits, Peaks, and Passes: A Lecture on Critical Point Theory\", Mathematical Association of America Lecture Films, 1966 Notes Biographical references . References . (e-book: ). 1892 births 1977 deaths 20th-century American mathematicians Colby College alumni Cornell University faculty Differential geometers Harvard University alumni Harvard University faculty Institute for Advanced Study faculty People from Waterville, Maine Princeton University faculty Brown University faculty Topologists National Medal of Science laureates Presidents of the American Mathematical Society Variational analysts Mathematicians from Maine Members of the United States National Academy of Sciences",
                    "score": 0.8614468574523926
                },
                {
                    "id": 101836,
                    "contents": "Lennard-Jones potential\neventually also consists of other potential types, e.g. partial charges). Molecular models (often referred to as 'force fields') for practically all molecular and ionic particles can be constructed using this scheme for example for alkanes.",
                    "score": 0.8603666424751282
                },
                {
                    "id": 20458159,
                    "contents": "Wu–Sprung potential\nWu and Sprung also showed that the zeta-regularized functional determinant is the Riemann Xi-function The main idea inside this problem is to recover the potential from spectral data as in some inverse spectral problems in this case the spectral data is the Eigenvalue staircase, which is a quantum property of the system, the inverse of the potential then, satisfies an Abel integral equation (fractional calculus) which can be immediately solved to obtain the potential. Asymptotics For big x if we take only the smooth part of the eigenvalue staircase , then the potential as is positive and it is given by the asymptotic expression with and in the limit . This potential is approximately a Morse Potential with The asymptotic of the energies depend on the quantum number n as , where W is the Lambert W function. References",
                    "score": 0.8594440817832947
                },
                {
                    "id": 8177203,
                    "contents": "Gilbert Ames Bliss\nBliss once headed a government commission that devised rules for apportioning seats in the U.S. House of Representatives among the several states. Work Bliss's work on the calculus of variations culminated in his classic 1946 monograph, Lectures on the Calculus of Variations, which treated the subject as an end in itself and not as an adjunct of mechanics. Here Bliss achieved a substantial simplification of the transformation theories of Clebsch and Weierstrass. Bliss also strengthened the necessary conditions of Euler, Weierstrass, Legendre, and Jacobi into sufficient conditions. Bliss set out the canonical formulation and solution of the problem of Bolza with side conditions and variable end-points. Bliss's Lectures more or less constitutes the culmination of the classic calculus of variations of Weierstrass, Hilbert, and Bolza. Subsequent work on variational problems would strike out in new directions, such as Morse theory, optimal control, and dynamic programming.",
                    "score": 0.8591914176940918
                },
                {
                    "id": 25311588,
                    "contents": "Trigonometric Rosen–Morse potential\nFor this reason, the wave equation which transforms upon the variable change, , into the familiar one-dimensional Schrödinger equation with the trigonometric Rosen–Morse potential, in reality describes quantum motion of a charge dipole perturbed by the field due to another charge dipole, and not the motion of a single charge within the field produced by another charge. Stated differently, the two equations () and () do not describe strictly speaking a Hydrogen Atom on , but rather quantum motion on of a light dipole perturbed by the dipole potential of another very heavy dipole, like the H Atom, so that the reduced mass, , would be of the order of the electron mass and could be neglected in comparison with the energy.",
                    "score": 0.8591076731681824
                },
                {
                    "id": 17988481,
                    "contents": "Larry Guth\nHe was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to harmonic analysis, combinatorics and geometry, and for exposition of high level mathematics\". On February 20, 2020, the National Academy of Sciences announced that Guth is the first winner of their new $20,000 Maryam Mirzakhani Prize in Mathematics for mid-career mathematicians. The citation states that his award is \"for developing surprising, original, and deep connections between geometry, analysis, topology, and combinatorics, which have led to the solution of, or major advances on, many outstanding problems in these fields.\" In 2021, he was elected member of the U. S. National Academy of Sciences. Personal He is the son of Alan Guth, a physicist known for the theory of inflation in cosmology. Work Metaphors in systolic geometry: the video . . . . . . . References",
                    "score": 0.8589049577713013
                },
                {
                    "id": 22498935,
                    "contents": "Romanovski polynomials\nwhich derive from the Rodrigues formula () in conjunction with Pearson's ODE (). Orthogonality The two polynomials, and with , are orthogonal, if and only if, In other words, for arbitrary parameters, only a finite number of Romanovski polynomials are orthogonal. This property is referred to as finite orthogonality. However, for some special cases in which the parameters depend in a particular way on the polynomial degree infinite orthogonality can be achieved. This is the case of a version of equation () that has been independently encountered anew within the context of the exact solubility of the quantum mechanical problem of the trigonometric Rosen–Morse potential and reported in Compean & Kirchbach (2006). There, the polynomial parameters and are no longer arbitrary but are expressed in terms of the potential parameters, and , and the degree of the polynomial according to the relations, Correspondingly, emerges as , while the weight function takes the shape",
                    "score": 0.8582818508148193
                },
                {
                    "id": 25311607,
                    "contents": "Trigonometric Rosen–Morse potential\nThe infinite integral has first been treated by means of partial integration giving, Then the argument of the exponential under the sign of the integral has been cast as, thus reaching the following intermediate result, As a next step the differential has been represented as an algebraic manipulation which allows to express the partition function in () in terms of the function of complex argument according to, where is an arbitrary path on the complex plane starting in zero and ending in . For more details and physical interpretations, see. See also Romanovski polynomials Pöschl–Teller potential References Quantum mechanics Quantum mechanical potentials Mathematical physics",
                    "score": 0.8581741452217102
                },
                {
                    "id": 101835,
                    "contents": "Lennard-Jones potential\nThe Lennard-Jones potential is extensively used for molecular modeling. There are essentially two ways the Lennard-Jones potential can be used for molecular modeling: (1) A real substance atom or molecule is modeled directly by the Lennard-Jones potential, which yields very good results for noble gases and methane, i.e. dispersively interacting spherical particles. In the case of methane, the molecule is assumed to be spherically symmetric and the hydrogen atoms are fused with the carbon atom to a common unit. This simplification can in general also be applied to more complex molecules, but yields usually poor results. (2) A real substance molecule is built of multiple Lennard-Jones interactions sites, which can be connected either by rigid bonds or flexible additional potentials (and eventually also consists of other potential types, e.g. partial charges). Molecular models (often referred to as 'force fields') for practically all molecular and ionic particles can be constructed",
                    "score": 0.8576024770736694
                },
                {
                    "id": 3167730,
                    "contents": "Brilliant Light Power\n\"Unlike most schemes for free energy, the hydrino process of Randy Mills is not without ample theory. Mills has written a 1000 page tome, entitled, \"The Grand Unified Theory of Classical Quantum Mechanics\", that takes the reader all the way from hydrinos to antigravity. Fortunately, Aaron Barth [...] has taken upon himself to look through it, checking for accuracy. Barth is a post doctoral researcher at the Harvard–Smithsonian Center for Astrophysics, and holds a PhD in Astronomy, 1998, from UC Berkeley. What he found initially were mathematical blunders and unjustified assumptions. To his surprise, however, portions of the book seemed well organized. These, it now turns out, were lifted verbatim from various texts. This has been the object of a great deal of discussion from Mills' Hydrino Study Group. \"Mills seems not to understand what the fuss is all about.\" – Park",
                    "score": 0.8574753999710083
                },
                {
                    "id": 1941126,
                    "contents": "Stationary-action principle\nto obtain the Euler–Lagrange equations in their present form. Jacobi, Morse and Caratheodory In 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian. A particularly elegant derivation of the Euler-Lagrange equation was formulated by Constantin Caratheodory and published by him in 1935. Gauss and Hertz Other extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.",
                    "score": 0.8570203185081482
                },
                {
                    "id": 5182442,
                    "contents": "100,000\nIn the Irish language, () is a popular greeting meaning \"a hundred thousand welcomes\". Selected 6-digit numbers (100,001–999,999) 100,001 to 199,999 100,001 to 109,999 100,003 – smallest 6-digit prime number 100,128 – smallest triangular number with 6 digits and the 447th triangular number 100,255 – Friedman number 101,101 – smallest palindromic Carmichael number 101,723 – smallest prime number whose square is a pandigital number containing each digit from 0 to 9 102,564 – The smallest parasitic number 103,680 – highly totient number 103,769 – the number of combinatorial types of 5-dimensional parallelohedra 103,823 – 473, nice Friedman number (−1 + 0 + 3×8×2)3 104,723 – the 9,999th prime number 104,729 – the 10,000th prime number 104,869 – the smallest prime number containing every non-prime digit 104,976 – 184, 3-smooth number 105,664 – harmonic divisor number 109,376 - 1-automorphic number",
                    "score": 0.8569332957267761
                },
                {
                    "id": 19525207,
                    "contents": "Jacobi set\nIn Morse theory, a mathematical discipline, Jacobi sets provide a method of studying the relationship between two or more Morse functions. For two Morse functions, the Jacobi set is defined as the set of critical points of the restriction of one function to the level sets of the other function. The Jacobi set can also be defined as the set of points where the gradients of the two functions are parallel. If both the functions are generic, the Jacobi set is a smoothly embedded 1-manifold. Definition Consider two generic Morse functions defined on a smooth -manifold. Let the restriction of to the level set for a regular value, be called ; it is a Morse function. Then the Jacobi set of and is , Alternatively, the Jacobi set is the collection of points where the gradients of the functions align with each other or one of the gradients vanish (cite?), for some ,",
                    "score": 0.8566981554031372
                },
                {
                    "id": 20458022,
                    "contents": "Variational method (quantum mechanics)\nLet's assume there is some overlap between the ansatz and the ground state (otherwise, it's a bad ansatz). We still wish to normalize the ansatz, so we have the constraints and we wish to minimize This, in general, is not an easy task, since we are looking for a global minimum and finding the zeroes of the partial derivatives of ε over all αi is not sufficient. If ψ (α) is expressed as a linear combination of other functions (αi being the coefficients), as in the Ritz method, there is only one minimum and the problem is straightforward. There are other, non-linear methods, however, such as the Hartree–Fock method, that are also not characterized by a multitude of minima and are therefore comfortable in calculations.",
                    "score": 0.85648113489151
                },
                {
                    "id": 10906609,
                    "contents": "P. A. P. Moran\nSee also Moran's I Moran process Moran's theorem Publications In addition to over 170 papers, Moran wrote 4 books, The Theory of Storage (1959; translated into Russian, 1963; Czech, 1967) The Statistical Processes of Evolutionary Theory (1962; translated into Russian, 1973) (With M.G. Kendall) Geometrical Probability (1963; translated into Russian, 1972) An Introduction to Probability Theory (1967) References External links For Moran's PhD students see: Moran sent his first paper to R. A. Fisher: Correspondence of Sir R.A. Fisher: Calendar of Correspondence with P.A.P. Moran MS 018 – MORAN, Patrick Alfred Pierce, Manuscript collection, Basser Library, Australian Academy of Science The Moran Award for History of Science Research, Basser Library, Australian Academy of Science",
                    "score": 0.8561225533485413
                },
                {
                    "id": 11363243,
                    "contents": "Wick's theorem\nWick's theorem is a method of reducing high-order derivatives to a combinatorics problem. It is named after Italian physicist Gian-Carlo Wick. It is used extensively in quantum field theory to reduce arbitrary products of creation and annihilation operators to sums of products of pairs of these operators. This allows for the use of Green's function methods, and consequently the use of Feynman diagrams in the field under study. A more general idea in probability theory is Isserlis' theorem. In perturbative quantum field theory, Wick's theorem is used to quickly rewrite each time ordered summand in the Dyson series as a sum of normal ordered terms. In the limit of asymptotically free ingoing and outgoing states, these terms correspond to Feynman diagrams. Definition of contraction For two operators and we define their contraction to be where denotes the normal order of an operator . Alternatively, contractions can be denoted by a line joining and , like .",
                    "score": 0.8560061454772949
                },
                {
                    "id": 9870213,
                    "contents": "Gego\n2000: Force Fields. Phases of the Kinetic, Hayward Gallery, London 2000/2001: Heterotopías. Medio siglo sin lugar 1918–1968, Museo Nacional Centro de Arte Reina Sofía, Madrid 2001: Geometric Abstraction. Latin American Art in the Patricia Phelps de Cisneros Collection, Fogg Art Museum, Harvard University. 2004: Ruth Vollmer & Gego: Thinking the Line, ZKM Center for Art and Media Karlsruhe, Karlsruhe 2013: \"Zero\" Museu Oscar Niemeyer (in collaboration with D.O.P. Foundation and The Goethe Institut), Curitiba, Brazil. 2013/2014: Zero Iberê Camargo Foundation (in collaboration with D.O.P. Collection and The Goethe Institut), Porto Alegre, RS, Brazil. 2014: Zero Pinacoteca do Estado de São Paulo (in collaboration with D.O.P. Foundation, The Goethe Institut, Prohelvetia & Alliance), São Paulo, Brazil. 2016: Revolution in the Making: Abstract Sculpture by Women, 1947–2016, Hauser, Wirth & Schimmel, Los Angeles 2016: Unfinished: Thoughts Left Visible, The Met Breuer, New York",
                    "score": 0.855926513671875
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_9",
        "question": "A line in the Lyman series of hydrogen has a wavelength of $1.03 \\times 10^{-7} \\mathrm{~m}$. Find the original energy level of the electron.",
        "golden_answers": [
            " 3"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 2666431,
                    "contents": "Lyman series\nIn physics and chemistry, the Lyman series is a hydrogen spectral series of transitions and resulting ultraviolet emission lines of the hydrogen atom as an electron goes from n ≥ 2 to n = 1 (where n is the principal quantum number), the lowest energy level of the electron. The transitions are named sequentially by Greek letters: from n = 2 to n = 1 is called Lyman-alpha, 3 to 1 is Lyman-beta, 4 to 1 is Lyman-gamma, and so on. The series is named after its discoverer, Theodore Lyman. The greater the difference in the principal quantum numbers, the higher the energy of the electromagnetic emission. History",
                    "score": 0.925677478313446
                },
                {
                    "id": 7109382,
                    "contents": "Hydrogen spectral series\nThe wavelength will always be positive because is defined as the lower level and so is less than . This equation is valid for all hydrogen-like species, i.e. atoms having only a single electron, and the particular case of hydrogen spectral lines is given by Z=1. Series Lyman series ( = 1) In the Bohr model, the Lyman series includes the lines emitted by transitions of the electron from an outer orbit of quantum number n > 1 to the 1st orbit of quantum number n' = 1. The series is named after its discoverer, Theodore Lyman, who discovered the spectral lines from 1906–1914. All the wavelengths in the Lyman series are in the ultraviolet band. Balmer series ( = 2) The Balmer series includes the lines due to transitions from an outer orbit n > 2 to the orbit n' = 2.",
                    "score": 0.9067041277885437
                },
                {
                    "id": 16680978,
                    "contents": "Timeline of quantum mechanics\n1885 – Johann Jakob Balmer discovers a numerical relationship between visible spectral lines of hydrogen, the Balmer series. 1887 – Heinrich Hertz discovers the photoelectric effect, shown by Einstein in 1905 to involve quanta of light. 1888 – Hertz demonstrates experimentally that electromagnetic waves exist, as predicted by Maxwell. 1888 – Johannes Rydberg modifies the Balmer formula to include all spectral series of lines for the hydrogen atom, producing the Rydberg formula which is employed later by Niels Bohr and others to verify Bohr's first quantum model of the atom. 1895 – Wilhelm Conrad Röntgen discovers X-rays in experiments with electron beams in plasma.",
                    "score": 0.9041818380355835
                },
                {
                    "id": 2666420,
                    "contents": "Balmer series\nAfter Balmer's discovery, five other hydrogen spectral series were discovered, corresponding to electrons transitioning to values of n other than two . Overview The Balmer series is characterized by the electron transitioning from n ≥ 3 to n = 2, where n refers to the radial quantum number or principal quantum number of the electron. The transitions are named sequentially by Greek letter: n = 3 to n = 2 is called H-α, 4 to 2 is H-β, 5 to 2 is H-γ, and 6 to 2 is H-δ. As the first spectral lines associated with this series are located in the visible part of the electromagnetic spectrum, these lines are historically referred to as \"H-alpha\", \"H-beta\", \"H-gamma\", and so on, where H is the element hydrogen.",
                    "score": 0.9021106362342834
                },
                {
                    "id": 2666394,
                    "contents": "H-alpha\nFor the Lyman series the naming convention is: n = 2 to n = 1 is called Lyman-alpha, n = 3 to n = 1 is called Lyman-beta, etc. H-alpha has a wavelength of 656.281 nm, is visible in the red part of the electromagnetic spectrum, and is the easiest way for astronomers to trace the ionized hydrogen content of gas clouds. Since it takes nearly as much energy to excite the hydrogen atom's electron from n = 1 to n = 3 (12.1 eV, via the Rydberg formula) as it does to ionize the hydrogen atom (13.6 eV), ionization is far more probable than excitation to the n = 3 level. After ionization, the electron and proton recombine to form a new hydrogen atom. In the new atom, the electron may begin in any energy level, and subsequently cascades to the ground state (n = 1), emitting photons with each transition. Approximately half the time, this cascade will include the n = 3 to n = 2 transition and the atom will emit H-alpha light. Therefore, the H-alpha line occurs where hydrogen is being ionized.",
                    "score": 0.8986287117004395
                },
                {
                    "id": 431658,
                    "contents": "Rydberg constant\nThe constant is expressed for either hydrogen as , or at the limit of infinite nuclear mass as . In either case, the constant is used to express the limiting value of the highest wavenumber (inverse wavelength) of any photon that can be emitted from an atom, or, alternatively, the wavenumber of the lowest-energy photon capable of ionizing an atom from its ground state. The hydrogen spectral series can be expressed simply in terms of the Rydberg constant for hydrogen and the Rydberg formula. In atomic physics, Rydberg unit of energy, symbol Ry, corresponds to the energy of the photon whose wavenumber is the Rydberg constant, i.e. the ionization energy of the hydrogen atom in a simplified Bohr model. Value Rydberg constant The CODATA value is , where is the rest mass of the electron, is the elementary charge, is the permittivity of free space, is the Planck constant, and is the speed of light in vacuum.",
                    "score": 0.896684467792511
                },
                {
                    "id": 7109387,
                    "contents": "Hydrogen spectral series\nSee also Astronomical spectroscopy The hydrogen line (21 cm) Lamb shift Moseley's law Quantum optics Theoretical and experimental justification for the Schrödinger equation References External links Spectral series of hydrogen animation Hydrogen physics Emission spectroscopy Hydrogen",
                    "score": 0.8959004878997803
                },
                {
                    "id": 154981,
                    "contents": "Rydberg formula\nBy setting to 1 and letting run from 2 to infinity, the spectral lines known as the Lyman series converging to 91 nm are obtained, in the same manner: For any hydrogen-like element The formula above can be extended for use with any hydrogen-like chemical elements with where is the wavelength (in vacuum) of the light emitted, is the Rydberg constant for this element, is the atomic number, i.e. the number of protons in the atomic nucleus of this element, is the principal quantum number of the lower energy level, and is the principal quantum number of the higher energy level for the atomic electron transition. This formula can be directly applied only to hydrogen-like, also called hydrogenic atoms of chemical elements, i.e. atoms with only one electron being affected by an effective nuclear charge (which is easily estimated). Examples would include He+, Li2+, Be3+ etc., where no other electrons exist in the atom.",
                    "score": 0.8958593010902405
                },
                {
                    "id": 431660,
                    "contents": "Rydberg constant\nRydberg frequency Rydberg wavelength . The angular wavelength is . Occurrence in Bohr model The Bohr model explains the atomic spectrum of hydrogen (see hydrogen spectral series) as well as various other atoms and ions. It is not perfectly accurate, but is a remarkably good approximation in many cases, and historically played an important role in the development of quantum mechanics. The Bohr model posits that electrons revolve around the atomic nucleus in a manner analogous to planets revolving around the sun. In the simplest version of the Bohr model, the mass of the atomic nucleus is considered to be infinite compared to the mass of the electron, so that the center of mass of the system, the barycenter, lies at the center of the nucleus. This infinite mass approximation is what is alluded to with the subscript. The Bohr model then predicts that the wavelengths of hydrogen atomic transitions are (see Rydberg formula):",
                    "score": 0.8945393562316895
                },
                {
                    "id": 1772933,
                    "contents": "Hydrogen atom\nFor , the value is called the Rydberg unit of energy. It is related to the Rydberg constant of atomic physics by The exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) which have finite mass, the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. This includes the kinetic energy of the nucleus in the problem, because the total (electron plus nuclear) kinetic energy is equivalent to the kinetic energy of the reduced mass moving with a velocity equal to the electron velocity relative to the nucleus. However, since the nucleus is much heavier than the electron, the electron mass and reduced mass are nearly the same. The Rydberg constant RM for a hydrogen atom (one electron), R is given by",
                    "score": 0.8931356072425842
                },
                {
                    "id": 7715670,
                    "contents": "Thomas Ralph Merton\nBy an ingenious technique Merton measured the discontinuities in the lines due to their partial breaking up into components under the influence of the magnetic field between adjacent atoms. The two men applied the same technique to the measurement of the spectra of hydrogen and helium, reproducing the distribution of intensity of some stellar lines in the laboratory for the first time.",
                    "score": 0.8901708126068115
                },
                {
                    "id": 2666424,
                    "contents": "Balmer series\nLater, it was discovered that when the Balmer series lines of the hydrogen spectrum were examined at very high resolution, they were closely spaced doublets. This splitting is called fine structure. It was also found that excited electrons from shells with n greater than 6 could jump to the n = 2 shell, emitting shades of ultraviolet when doing so.",
                    "score": 0.8897750973701477
                },
                {
                    "id": 154975,
                    "contents": "Rydberg formula\nIn atomic physics, the Rydberg formula calculates the wavelengths of a spectral line in many chemical elements. The formula was primarily presented as a generalization of the Balmer series for all atomic electron transitions of hydrogen. It was first empirically stated in 1888 by the Swedish physicist Johannes Rydberg, then theoretically by Niels Bohr in 1913, who used a primitive form of quantum mechanics. The formula directly generalizes the equations used to calculate the wavelengths of the hydrogen spectral series.",
                    "score": 0.8897687196731567
                },
                {
                    "id": 1772932,
                    "contents": "Hydrogen atom\nwhere is the electron mass, is the electron charge, is the vacuum permittivity, and is the quantum number (now known as the principal quantum number). Bohr's predictions matched experiments measuring the hydrogen spectral series to the first order, giving more confidence to a theory that used quantized values. For , the value is called the Rydberg unit of energy. It is related to the Rydberg constant of atomic physics by",
                    "score": 0.8886488080024719
                },
                {
                    "id": 20938780,
                    "contents": "History of spectroscopy\nJohann Balmer discovered in 1885 that the four visible lines of hydrogen were part of a series that could be expressed in terms of integers. This was followed a few years later by the Rydberg formula, which described additional series of lines. Meanwhile, the substantial summary of past experiments performed by Maxwell (1873), resulted in his equations of electromagnetic waves. In 1895, the German physicist Wilhelm Conrad Röntgen discovered and extensively studied X-rays, which were later used in X-ray spectroscopy. One year later, in 1896, French physicist Antoine Henri Becquerel discovered radioactivity, and Dutch physicist Pieter Zeeman observed spectral lines being split by a magnetic field. In 1897, theoretical physicist, Joseph Larmor explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons.",
                    "score": 0.8875131011009216
                },
                {
                    "id": 7109380,
                    "contents": "Hydrogen spectral series\nThere are emission lines from hydrogen that fall outside of these series, such as the 21 cm line. These emission lines correspond to much rarer atomic events such as hyperfine transitions. The fine structure also results in single spectral lines appearing as two or more closely grouped thinner lines, due to relativistic corrections. In quantum mechanical theory, the discrete spectrum of atomic emission was based on the Schrödinger equation, which is mainly devoted to the study of energy spectra of hydrogenlike atoms, whereas the time-dependent equivalent Heisenberg equation is convenient when studying an atom driven by an external electromagnetic wave.",
                    "score": 0.8873866200447083
                },
                {
                    "id": 7109385,
                    "contents": "Hydrogen spectral series\nPfund series ( = 5) Experimentally discovered in 1924 by August Herman Pfund. Humphreys series ( = 6) Discovered in 1953 by American physicist Curtis J. Humphreys. Further series ( > 6) Further series are unnamed, but follow the same pattern and equation as dictated by the Rydberg equation. Series are increasingly spread out and occur at increasing wavelengths. The lines are also increasingly faint, corresponding to increasingly rare atomic events. The seventh series of atomic hydrogen was first demonstrated experimentally at infrared wavelengths in 1972 by Peter Hansen and John Strong at the University of Massachusetts Amherst.",
                    "score": 0.8868564367294312
                },
                {
                    "id": 7109381,
                    "contents": "Hydrogen spectral series\nIn the processes of absorption or emission of photons by an atom, the conservation laws hold for the whole isolated system, such as an atom plus a photon. Therefore the motion of the electron in the process of photon absorption or emission is always accompanied by motion of the nucleus, and, because the mass of the nucleus is always finite, the energy spectra of hydrogen-like atoms must depend on the nuclear mass. Rydberg formula The energy differences between levels in the Bohr model, and hence the wavelengths of emitted or absorbed photons, is given by the Rydberg formula: where is the atomic number, (often written ) is the principal quantum number of the lower energy level, (or ) is the principal quantum number of the upper energy level, and is the Rydberg constant. ( for hydrogen and for heavy metals).",
                    "score": 0.8852454423904419
                },
                {
                    "id": 2666419,
                    "contents": "Balmer series\nThe Balmer series, or Balmer lines in atomic physics, is one of a set of six named series describing the spectral line emissions of the hydrogen atom. The Balmer series is calculated using the Balmer formula, an empirical equation discovered by Johann Balmer in 1885. The visible spectrum of light from hydrogen displays four wavelengths, 410 nm, 434 nm, 486 nm, and 656 nm, that correspond to emissions of photons by electrons in excited states transitioning to the quantum level described by the principal quantum number n equals 2. There are several prominent ultraviolet Balmer lines with wavelengths shorter than 400 nm. The number of these lines is an infinite continuum as it approaches a limit of 364.5 nm in the ultraviolet. After Balmer's discovery, five other hydrogen spectral series were discovered, corresponding to electrons transitioning to values of n other than two . Overview",
                    "score": 0.8848260641098022
                },
                {
                    "id": 7109377,
                    "contents": "Hydrogen spectral series\nThe emission spectrum of atomic hydrogen has been divided into a number of spectral series, with wavelengths given by the Rydberg formula. These observed spectral lines are due to the electron making transitions between two energy levels in an atom. The classification of the series by the Rydberg formula was important in the development of quantum mechanics. The spectral series are important in astronomical spectroscopy for detecting the presence of hydrogen and calculating red shifts. Physics",
                    "score": 0.8839712738990784
                },
                {
                    "id": 1129312,
                    "contents": "Niels Bohr\nIn 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atom: where λ is the wavelength of the absorbed or emitted light and RH is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model: where me is the electron's mass, e is its charge, h is Planck's constant and Z is the atom's atomic number (1 for hydrogen).",
                    "score": 0.883021354675293
                },
                {
                    "id": 7109378,
                    "contents": "Hydrogen spectral series\nPhysics A hydrogen atom consists of an electron orbiting its nucleus. The electromagnetic force between the electron and the nuclear proton leads to a set of quantum states for the electron, each with its own energy. These states were visualized by the Bohr model of the hydrogen atom as being distinct orbits around the nucleus. Each energy level, or electron shell , or orbit, is designated by an integer, as shown in the figure. The Bohr model was later replaced by quantum mechanics in which the electron occupies an atomic orbital rather than an orbit, but the allowed energy levels of the hydrogen atom remained the same as in the earlier theory.",
                    "score": 0.8805164098739624
                },
                {
                    "id": 1753305,
                    "contents": "Hydrogen\nHydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. At standard conditions hydrogen is a gas of diatomic molecules having the formula H2. It is colorless, odorless, tasteless, non-toxic, and highly combustible. Hydrogen is the most abundant chemical substance in the universe, constituting roughly 75% of all normal matter. Stars such as the Sun are mainly composed of hydrogen in the plasma state. Most of the hydrogen on Earth exists in molecular forms such as water and organic compounds. For the most common isotope of hydrogen (symbol 1H) each atom has one proton, one electron, and no neutrons. In the early universe, the formation of protons, the nuclei of hydrogen, occurred during the first second after the Big Bang. The emergence of neutral hydrogen atoms throughout the universe occurred about 370,000 years later during the recombination epoch, when the plasma had cooled enough for electrons to remain bound to protons.",
                    "score": 0.8794533014297485
                },
                {
                    "id": 2666434,
                    "contents": "Lyman series\nThe Lyman series The version of the Rydberg formula that generated the Lyman series was: Where n is a natural number greater than or equal to 2 (i.e., ). Therefore, the lines seen in the image above are the wavelengths corresponding to n = 2 on the right, to n = ∞ on the left. There are infinitely many spectral lines, but they become very dense as they approach n = ∞ (the Lyman limit), so only some of the first lines and the last one appear. The wavelengths in the Lyman series are all ultraviolet: Explanation and derivation In 1914, when Niels Bohr produced his Bohr model theory, the reason why hydrogen spectral lines fit Rydberg's formula was explained. Bohr found that the electron bound to the hydrogen atom must have quantized energy levels described by the following formula, According to Bohr's third assumption, whenever an electron falls from an initial energy level Ei to a final energy level Ef, the atom must emit radiation with a wavelength of",
                    "score": 0.8786903619766235
                },
                {
                    "id": 7748003,
                    "contents": "Introduction to quantum mechanics\nwhere R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m. Rydberg's formula accounts for the four visible wavelengths of hydrogen by setting and . It also predicts additional wavelengths in the emission spectrum: for and for , the emission spectrum should contain certain ultraviolet wavelengths, and for and , it should also contain certain infrared wavelengths. Experimental observation of these wavelengths came two decades later: in 1908 Louis Paschen found some of the predicted infrared wavelengths, and in 1914 Theodore Lyman found some of the predicted ultraviolet wavelengths. Both Balmer and Rydberg's formulas involve integers: in modern terms, they imply that some property of the atom is quantized. Understanding exactly what this property was, and why it was quantized, was a major part of the development of quantum mechanics, as shown in the rest of this article.",
                    "score": 0.8786865472793579
                },
                {
                    "id": 25792027,
                    "contents": "George Series\nSelected works Spectrum of Atomic Hydrogen, 1957 Laser Spectroscopy and other topics, 1985 Spectrum of Atomic Hydrogen: advances, 1988 References British physicists 1920 births 1995 deaths People from Bushey People educated at Queen Mary's School for Boys, Basingstoke People educated at Reading School Alumni of St John's College, Oxford Fellows of St Edmund Hall, Oxford Fellows of the Royal Society Academics of the University of Reading",
                    "score": 0.8779709339141846
                },
                {
                    "id": 154983,
                    "contents": "Rydberg formula\nFinally, with certain modifications (replacement of Z by Z − 1, and use of the integers 1 and 2 for the ns to give a numerical value of for the difference of their inverse squares), the Rydberg formula provides correct values in the special case of K-alpha lines, since the transition in question is the K-alpha transition of the electron from the 1s orbital to the 2p orbital. This is analogous to the Lyman-alpha line transition for hydrogen, and has the same frequency factor. Because the 2p electron is not screened by any other electrons in the atom from the nucleus, the nuclear charge is diminished only by the single remaining 1s electron, causing the system to be effectively a hydrogenic atom, but with a diminished nuclear charge Z − 1. Its frequency is thus the Lyman-alpha hydrogen frequency, increased by a factor of (Z − 1)2. This formula of f = c/λ = (Lyman-alpha frequency)⋅(Z − 1)2 is historically known as Moseley's law (having added a factor c to convert wavelength to",
                    "score": 0.8779408931732178
                },
                {
                    "id": 7109384,
                    "contents": "Hydrogen spectral series\nPaschen series (Bohr series, = 3) Named after the German physicist Friedrich Paschen who first observed them in 1908. The Paschen lines all lie in the infrared band. This series overlaps with the next (Brackett) series, i.e. the shortest line in the Brackett series has a wavelength that falls among the Paschen series. All subsequent series overlap. Brackett series ( = 4) Named after the American physicist Frederick Sumner Brackett who first observed the spectral lines in 1922.The spectral lines of Brackett series lie in far infrared band. Pfund series ( = 5) Experimentally discovered in 1924 by August Herman Pfund. Humphreys series ( = 6) Discovered in 1953 by American physicist Curtis J. Humphreys.",
                    "score": 0.8768246173858643
                },
                {
                    "id": 3565881,
                    "contents": "Hydrogen line\nThe hydrogen line, 21-centimeter line, or H I line is the electromagnetic radiation spectral line that is created by a change in the energy state of neutral hydrogen atoms. This electromagnetic radiation has a precise frequency of , which is equivalent to the vacuum wavelength of in free space. This wavelength falls below the microwave region of the electromagnetic spectrum, which begins at 3.0 GHz (10cm wavelength), and it is observed frequently in radio astronomy because those radio waves can penetrate the large clouds of interstellar cosmic dust that are opaque to visible light. This line is also the theoretical basis of the hydrogen maser.",
                    "score": 0.8763370513916016
                },
                {
                    "id": 7748008,
                    "contents": "Introduction to quantum mechanics\nEach photon from glowing atomic hydrogen is due to an electron moving from a higher orbit, with radius , to a lower orbit, . The energy of this photon is the difference in the energies and of the electron: Since Planck's equation shows that the photon's energy is related to its wavelength by , the wavelengths of light that can be emitted are given by This equation has the same form as the Rydberg formula, and predicts that the constant should be given by Therefore, the Bohr model of the atom can predict the emission spectrum of hydrogen in terms of fundamental constants. However, it was not able to make accurate predictions for multi-electron atoms, or to explain why some spectral lines are brighter than others. Wave–particle duality Just as light has both wave-like and particle-like properties, matter also has wave-like properties.",
                    "score": 0.8762773275375366
                },
                {
                    "id": 15528279,
                    "contents": "John William Nicholson\nThe constitution of atoms and molecules. Nature, Lond. 93, 268-269. (1914) Sur les poids atomiques des elements des nebuleuses. C.R. Acad. Sci. Paris, 158, 1322-1323. (1914) The high frequency spectra of the elements and the structure of the atom. Phil. Mag. 27, 541-564. Atomic structure and the spectrum of helium. Phil. Mag. 28, 90-103. (With T. R. Merton.) On the distribution of intensity in broadened spectral lines Phil. Trans. A, 216, 459-488. (With T. R. Merton.) On intensity relations in the spectrum of helium. Phil. Trans. A, 220, 137-173.",
                    "score": 0.8760976791381836
                },
                {
                    "id": 16471308,
                    "contents": "Triatomic hydrogen\nThe molecule can only exist in an excited state. The different excited electronic states are represented by symbols for the outer electron nLΓ with n the principal quantum number, L is the electronic angular momentum, and Γ is the electronic symmetry selected from the D3h group. Extra bracketed symbols can be attached showing vibration in the core: {s,dl} with s representing symmetrical stretch, d degenerate mode, and l vibrational angular momentum. Yet another term can be inserted to indicate molecular rotation: (N,G) with N angular momentum apart from electrons as projected on the molecular axis, and G the Hougen's convenient quantum number determined by G=l+λ-K. This is often (1,0), as the rotational states are restricted by the constituent particles all being fermions. Examples of these states are: 2sA1' 3sA1' 2pA2\" 3dE' 3DE\" 3dA1' 3pE' 3pA2\". The 2p2A2\" state has a lifetime of 700 ns. If the molecule attempts to lose energy and go to the repulsive ground state, it spontaneously",
                    "score": 0.8760402202606201
                },
                {
                    "id": 652353,
                    "contents": "Spectral line\nMore detailed designations usually include the line wavelength and may include a multiplet number (for atomic lines) or band designation (for molecular lines). Many spectral lines of atomic hydrogen also have designations within their respective series, such as the Lyman series or Balmer series. Originally all spectral lines were classified into series: the Principal series, Sharp series, and Diffuse series. These series exist across atoms of all elements, and the patterns for all atoms are well-predicted by the Rydberg-Ritz formula. These series were later associated with suborbitals.",
                    "score": 0.8757383227348328
                },
                {
                    "id": 2666425,
                    "contents": "Balmer series\nBalmer's formula Balmer noticed that a single wavelength had a relation to every line in the hydrogen spectrum that was in the visible light region. That wavelength was . When any integer higher than 2 was squared and then divided by itself squared minus 4, then that number multiplied by (see equation below) gave the wavelength of another line in the hydrogen spectrum. By this formula, he was able to show that some measurements of lines made in his time by spectroscopy were slightly inaccurate and his formula predicted lines that were later found although had not yet been observed. His number also proved to be the limit of the series. The Balmer equation could be used to find the wavelength of the absorption/emission lines and was originally presented as follows (save for a notation change to give Balmer's constant as B): Where λ is the wavelength. B is a constant with the value of or . m is equal to 2 n is an integer such that n > m.",
                    "score": 0.8755508661270142
                },
                {
                    "id": 2666432,
                    "contents": "Lyman series\nHistory The first line in the spectrum of the Lyman series was discovered in 1906 by Harvard physicist Theodore Lyman, who was studying the ultraviolet spectrum of electrically excited hydrogen gas. The rest of the lines of the spectrum (all in the ultraviolet) were discovered by Lyman from 1906-1914. The spectrum of radiation emitted by hydrogen is non-continuous or discrete. Here is an illustration of the first series of hydrogen emission lines:",
                    "score": 0.8755478858947754
                },
                {
                    "id": 986656,
                    "contents": "Atomic, molecular, and optical physics\nLater, the connection between atomic physics and optical physics became apparent, by the discovery of spectral lines and attempts to describe the phenomenon - notably by Joseph von Fraunhofer, Fresnel, and others in the 19th century. From that time to the 1920s, physicists were seeking to explain atomic spectra and blackbody radiation. One attempt to explain hydrogen spectral lines was the Bohr atom model. Experiments including electromagnetic radiation and matter - such as the photoelectric effect, Compton effect, and spectra of sunlight the due to the unknown element of Helium, the limitation of the Bohr model to Hydrogen, and numerous other reasons, lead to an entirely new mathematical model of matter and light: quantum mechanics.",
                    "score": 0.8743056654930115
                },
                {
                    "id": 552407,
                    "contents": "Wavenumber\nFor example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula: where R is the Rydberg constant, and ni and nf are the principal quantum numbers of the initial and final levels respectively (ni is greater than nf for emission). A spectroscopic wavenumber can be converted into energy per photon E by Planck's relation: It can also be converted into wavelength of light: where n is the refractive index of the medium. Note that the wavelength of light changes as it passes through different media, however, the spectroscopic wavenumber (i.e., frequency) remains constant. Conventionally, inverse centimeter (cm−1) units are used for , so often that such spatial frequencies are stated by some authors \"in wavenumbers\", incorrectly transferring the name of the quantity to the CGS unit cm−1 itself. See also Spatial frequency Refractive index Zonal wavenumber References Wave mechanics Physical quantities Units of frequency",
                    "score": 0.8739336729049683
                },
                {
                    "id": 2475376,
                    "contents": "Fraunhofer lines\nNaming The major Fraunhofer lines, and the elements they are associated with, are shown in the following table: The Fraunhofer C, F, G', and h lines correspond to the alpha, beta, gamma and delta lines of the Balmer series of emission lines of the hydrogen atom. The Fraunhofer letters are now rarely used for those lines. The D1 and D2 lines form the well-known \"sodium doublet\", the centre wavelength of which (589.29 nm) is given the designation letter \"D\". This historical designation for this line has stuck and is given to all the transitions between the ground state and the first excited state of the other alkali atoms as well. The D1 and D2 lines correspond to the fine-structure splitting of the excited states. This may be confusing because the excited state for this transition is the P-state of the alkali and should not be confused with the higher D-states.",
                    "score": 0.8736922740936279
                },
                {
                    "id": 7748021,
                    "contents": "Introduction to quantum mechanics\nIn the same year, building on de Broglie's hypothesis, Erwin Schrödinger developed the equation that describes the behavior of a quantum-mechanical wave. The mathematical model, called the Schrödinger equation after its creator, is central to quantum mechanics, defines the permitted stationary states of a quantum system, and describes how the quantum state of a physical system changes in time. The wave itself is described by a mathematical function known as a \"wave function\". Schrödinger said that the wave function provides the \"means for predicting the probability of measurement results\". Schrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's electron as a classical wave, moving in a well of the electrical potential created by the proton. This calculation accurately reproduced the energy levels of the Bohr model.",
                    "score": 0.873114287853241
                },
                {
                    "id": 1753336,
                    "contents": "Hydrogen\nStates Throughout the universe, hydrogen is mostly found in the atomic and plasma states, with properties quite distinct from those of molecular hydrogen. As a plasma, hydrogen's electron and proton are not bound together, resulting in very high electrical conductivity and high emissivity (producing the light from the Sun and other stars). The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind they interact with the Earth's magnetosphere giving rise to Birkeland currents and the aurora. Hydrogen is found in the neutral atomic state in the interstellar medium because the atoms seldom collide and combine. They are the source of the 21-cm hydrogen line at 1420 MHz that is detected in order to probe primordial hydrogen. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the universe up to a redshift of z = 4.",
                    "score": 0.8730514049530029
                },
                {
                    "id": 7748002,
                    "contents": "Introduction to quantum mechanics\nIn 1885 the Swiss mathematician Johann Balmer discovered that each wavelength (lambda) in the visible spectrum of hydrogen is related to some integer by the equation where is a constant Balmer determined is equal to 364.56 nm. In 1888 Johannes Rydberg generalized and greatly increased the explanatory utility of Balmer's formula. He predicted that is related to two integers and according to what is now known as the Rydberg formula: where R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m.",
                    "score": 0.872008204460144
                },
                {
                    "id": 16931574,
                    "contents": "Roswell Clifton Gibbs\nPhysicist As a physicist, Gibbs's primary area of interest was spectroscopy. At the time, this was the fairly new and exciting field of research in physics. Physicists investigated the emission and absorption of radiation, creating an understanding of atomic structure. The new theory of quantum mechanics attempted to explain phenomena at the atomic and subatomic level. Gibbs was the author or co-author of over forty research papers, on subjects such as the ultraviolet spectra of isoelectronic sequences, and the hyperfine structure of spectra. He determined the charge-to-mass ratio of the electron by studying the intervals between the H-alpha lines of hydrogen and deuterium, and investigated the absorption spectra of organic compounds in solution.",
                    "score": 0.871461033821106
                },
                {
                    "id": 2567623,
                    "contents": "1885 in science\nPhysics Johann Balmer publishes an empirical mathematical formula for the visible spectral lines of the hydrogen atom. Psychology Hermann Ebbinghaus publishes Über das Gedächtnis (\"On Memory\", later translated as Memory: a Contribution to Experimental Psychology).",
                    "score": 0.8714312314987183
                },
                {
                    "id": 1772942,
                    "contents": "Hydrogen atom\nwhich, for the bound states, results in where denotes a Gegenbauer polynomial and is in units of . The solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines and fully reproduced the Bohr model and went beyond it. It also yields two other quantum numbers and the shape of the electron's wave function (\"orbital\") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds. The Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.",
                    "score": 0.8711816072463989
                },
                {
                    "id": 3953003,
                    "contents": "Johann Jakob Balmer\nfor n = 2 and m = 3, 4, 5, 6, and so forth; h = 3.6456×10−7 m. In his 1885 notice, he referred to h (now known as the Balmer constant) as the \"fundamental number of hydrogen.\" Balmer then used this formula to predict the wavelength for m = 7 and Hagenbach informed him that Ångström had observed a line with wavelength 397 nm. This portion of the Hydrogen emission spectrum, from energy levels n<2 to n=2, became known as the series. The Balmer lines refer to the four emission spectrum lines that occur within the visible region of the Hydrogen emission spectrum at 410nm, 434nm, 486nm, and 656nm. These lines are caused by electrons in an excited state returning to ground state energy levels. Two of his colleagues, Hermann Wilhelm Vogel and William Huggins, were able to confirm the existence of other lines of the Balmer series in the spectrum of hydrogen in white stars. Balmer's formula was later found to be a special case of the Rydberg formula, devised by Johannes Rydberg in 1888.",
                    "score": 0.8710131049156189
                },
                {
                    "id": 20938781,
                    "contents": "History of spectroscopy\nIn 1897, theoretical physicist, Joseph Larmor explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons. Physicist, Joseph Larmor, created the first solar system model of the atom in 1897. He also postulated the proton, calling it a “positive electron.” He said the destruction of this type of atom making up matter “is an occurrence of infinitely small probability.” Early 20th century (1900–1950) The first decade of the 20th century brought the basics of quantum theory (Planck, Einstein) and interpretation of spectral series of hydrogen by Lyman in VUV and by Paschen in infrared. Ritz formulated the combination principle.",
                    "score": 0.8703476190567017
                },
                {
                    "id": 552406,
                    "contents": "Wavenumber\nwhich remains essentially the same in air, and so the spectroscopic wavenumber is directly related to the angles of light scattered from diffraction gratings and the distance between fringes in interferometers, when those instruments are operated in air or vacuum. Such wavenumbers were first used in the calculations of Johannes Rydberg in the 1880s. The Rydberg–Ritz combination principle of 1908 was also formulated in terms of wavenumbers. A few years later spectral lines could be understood in quantum theory as differences between energy levels, energy being proportional to wavenumber, or frequency. However, spectroscopic data kept being tabulated in terms of spectroscopic wavenumber rather than frequency or energy. For example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula:",
                    "score": 0.8698112368583679
                },
                {
                    "id": 24331222,
                    "contents": "Fundamental series\nThe formula that more resembled the hydrogen spectrum calculations was because of a smaller quantum defect. There is no physical basis to call this fundamental. The fundamental series was described as badly-named. It is the last spectroscopic series to have a special designation. The next series involving transitions between F and G subshells is known as the FG series. Frequencies of the lines in the series are given by this formula: R is the Rydberg correction, is the series limit, represented by 3D, and is represented by mF. A shortened formula is then given by with values of m being integers from 4 upwards. The two numbers separated by the \"−\" are called terms, that represent the energy level of an atom. The limit of the fundamental series is the same as the 3D level. The terms can have different designations, mF for single line systems, mΦ for doublets and mf for triplets.",
                    "score": 0.8696839213371277
                },
                {
                    "id": 1753334,
                    "contents": "Hydrogen\nOne of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.",
                    "score": 0.8694026470184326
                },
                {
                    "id": 7109386,
                    "contents": "Hydrogen spectral series\nExtension to other systems The concepts of the Rydberg formula can be applied to any system with a single particle orbiting a nucleus, for example a He+ ion or a muonium exotic atom. The equation must be modified based on the system's Bohr radius; emissions will be of a similar character but at a different range of energies. The Pickering–Fowler series was originally attributed to an unknown form of hydrogen with half-integer transition levels by both Pickering and Fowler, but Bohr correctly recognised them as spectral lines arising from the He+ nucleus. All other atoms have at least two electrons in their neutral form and the interactions between these electrons makes analysis of the spectrum by such simple methods as described here impractical. The deduction of the Rydberg formula was a major step in physics, but it was long before an extension to the spectra of other elements could be accomplished.",
                    "score": 0.8691412210464478
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.6
            }
        }
    },
    {
        "id": "test_10",
        "question": "A helium-neon laser (used in supermarket scanners) emits light at $632.8 \\mathrm{~nm}$. Calculate the frequency of this light.",
        "golden_answers": [
            " 4.738"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 2504888,
                    "contents": "Helium–neon laser\nPrior to the invention of cheap, abundant diode lasers, red He-Ne lasers were widely used in barcode scanners at supermarket checkout counters. Laser gyroscopes have employed He-Ne lasers operating at 633 nm in a ring laser configuration. He-Ne lasers are generally present in educational and research optical laboratories. Applications Red He-Ne lasers have an enormous number of industrial and scientific uses. They are widely used in laboratory demonstrations in the field of optics because of their relatively low cost and ease of operation compared to other visible lasers producing beams of similar quality in terms of spatial coherence (a single-mode Gaussian beam) and long coherence length (however, since about 1990 semiconductor lasers have offered a lower-cost alternative for many such applications).",
                    "score": 0.9085719585418701
                },
                {
                    "id": 16918755,
                    "contents": "Kenneth M. Baird\nThe development of the Iodine-stabilized helium neon laser, and one of the first demonstrations of saturated absorption spectroscopy and widely used realization of the primary standard of length and wavelengths. The confirmation of the most recent new values for the speed of light , 100 times more accurate than previous measurements (The 7th International Conference on Quantum Electronics, Montreal, 1972). The development of the technique of transition–difference generated frequencies in spectroscopic wavelength and frequency measurements. The first measurement of the frequency of visible light, in collaboration with the US National Bureau of Standards.",
                    "score": 0.8564777374267578
                },
                {
                    "id": 28462649,
                    "contents": "Alan David White\nHe was fond of art, in particular, sculpture. For his achievements he was awarded the 1984 IEEE David Sarnoff Award, and in 2000 he was elected to the New Jersey Inventors Hall of Fame. Scientific achievements The first gas laser, using a mixture of helium and neon, was demonstrated in 1960 and emitted radiation at a wavelength of 1.15 μm (infrared range). Two years later, White, together with Dane Rigden, showed that a helium-neon laser can emit radiation at a wavelength of 632.8 nm, i.e., in the visible range of the spectrum. In subsequent years, White, with Eugene I. Gordon and others, investigated the reasons for the limitation of the power of such lasers, established scaling laws for gas-discharge lasers, and developed frequency stabilization methods for such devices. The first continuous-wave visible laser, invented by White and Rigden, is still widely used in research and education, and is a part of various instruments.",
                    "score": 0.853683590888977
                },
                {
                    "id": 22022480,
                    "contents": "List of companies based in Kent, Washington\nG H I J K L LaserMotive - engineering firm developing technologies for efficiently transmitting power via lasers, a form of wireless energy transfer commonly called \"laser power beaming\" M N Novara - REI's brand of bicycles and cycling clothing O Oberto Sausage Company - family-owned; makes beef jerky, pepperoni and other snack sausages Omax Corporation - second largest water jet manufacturer in the US P Pacific Coast Condensed Milk Company - manufactured and marketed food products, including Carnation evaporated milk, with its famous slogan that it came from \"Contented Cows\" Pay 'n Pak - home improvement chain; operated 112 stores on the West Coast Puget Sound Electric Railway - interurban railway that ran between Tacoma and Seattle Puget Systems - custom computer business operating primarily through their website; sells a mixture of custom and preconfigured computers including laptops, desktops, and servers Q",
                    "score": 0.8532716631889343
                },
                {
                    "id": 2504889,
                    "contents": "Helium–neon laser\nStarting in 1978, HeNe tube lasers (manufactured by Toshiba and NEC) were used in Pioneer LaserDisc players. This continued until the 1984 model lineup, which contained infrared laser diodes instead. Pioneer continued to use laser diodes in all subsequent players until the format's discontinuation in 2009. See also List of laser types References Gas lasers Helium Neon",
                    "score": 0.8502362370491028
                },
                {
                    "id": 2627645,
                    "contents": "List of laser types\nThis is a list of laser types, their operational wavelengths, and their applications. Thousands of kinds of laser are known, but most of them are used only for specialized research. Overview Gas lasers Chemical lasers Used as directed-energy weapons. Dye lasers Metal-vapor lasers Solid-state lasers Semiconductor lasers Other types of lasers See also Laser construction List of laser articles Notes Further references Silfvast, William T. Laser fundamentals, Cambridge University Press, 2004. Weber, Marvin J. Handbook of laser wavelengths, CRC Press, 1999.",
                    "score": 0.8499058485031128
                },
                {
                    "id": 1753429,
                    "contents": "Helium\nHelium–neon lasers, a type of low-powered gas laser producing a red beam, had various practical applications which included barcode readers and laser pointers, before they were almost universally replaced by cheaper diode lasers. For its inertness and high thermal conductivity, neutron transparency, and because it does not form radioactive isotopes under reactor conditions, helium is used as a heat-transfer medium in some gas-cooled nuclear reactors. Helium, mixed with a heavier gas such as xenon, is useful for thermoacoustic refrigeration due to the resulting high heat capacity ratio and low Prandtl number. The inertness of helium has environmental advantages over conventional refrigeration systems which contribute to ozone depletion or global warming. Helium is also used in some hard disk drives.",
                    "score": 0.8487130403518677
                },
                {
                    "id": 12015740,
                    "contents": "List of Iron Man enemies\nL Living Laser - A laser expert who eventually evolved into a being made of pure light energy. Lucifer - Has a gifted intellect, and extensive knowledge of advanced Quistalian science and technology, and talent as an inventor using this technology.",
                    "score": 0.8486261963844299
                },
                {
                    "id": 19802310,
                    "contents": "Laserglow Technologies\nThe Aries laser series, made by Laserglow, was recommended by the USGA for the purpose of bird abatement. Guinness World Record The Hercules 500, made by Laserglow, is the current world record holder for the world's most powerful handheld laser, and appeared in the Guinness Book of World Records in 2009. The laser was tested three times in five-minute durations using two separate laser power meters, and created an output 1W peak and 940 mW (+/- 20 mW) average power. References External links Official website Photonics companies Manufacturing companies based in Toronto",
                    "score": 0.848534107208252
                },
                {
                    "id": 4844916,
                    "contents": "Laser safety\nThe eye focuses visible and near-infrared light onto the retina. A laser beam can be focused to an intensity on the retina which may be up to 200,000 times higher than at the point where the laser beam enters the eye. Most of the light is absorbed by melanin pigments in the pigment epithelium just behind the photoreceptors, and causes burns in the retina. Ultraviolet light with wavelengths shorter than 400 nm tends to be absorbed by lens and 300 nm in the cornea, where it can produce injuries at relatively low powers due to photochemical damage. Infrared light mainly causes thermal damage to the retina at near-infrared wavelengths and to more frontal parts of the eye at longer wavelengths. The table below summarizes the various medical conditions caused by lasers at different wavelengths, not including injuries due to pulsed lasers.",
                    "score": 0.8484759330749512
                },
                {
                    "id": 6347962,
                    "contents": "Blue laser\nA blue laser is a laser that emits electromagnetic radiation with a wavelength between 360 and 480 nanometers, which the human eye sees as blue or violet. Blue beams are produced by helium-cadmium gas lasers at 441.6 nm, and argon-ion lasers at 458 and 488 nm. Semiconductor lasers with blue beams are typically based on gallium(III) nitride (GaN; violet color) or indium gallium nitride (often true blue in color, but also able to produce other colors). Both blue and violet lasers can also be constructed using frequency-doubling of infrared laser wavelengths from diode lasers or diode-pumped solid-state lasers.",
                    "score": 0.8481759428977966
                },
                {
                    "id": 1741278,
                    "contents": "Green\ngenerate light at a frequency that is twice that of the incident beam (563.5 THz); in this case corresponding to the wavelength of 532nm (\"green\"). Other green wavelengths are also available using DPSS technology ranging from 501 nm to 543 nm. Green wavelengths are also available from gas lasers, including the helium–neon laser (543nm), the Argon-ion laser (514nm) and the Krypton-ion laser (521nm and 531nm), as well as liquid dye lasers. Green lasers have a wide variety of applications, including pointing, illumination, surgery, laser light shows, spectroscopy, interferometry, fluorescence, holography, machine vision, non-lethal weapons and bird control.",
                    "score": 0.8470823764801025
                },
                {
                    "id": 1151320,
                    "contents": "Optics\nThe first working laser was demonstrated on 16 May 1960 by Theodore Maiman at Hughes Research Laboratories. When first invented, they were called \"a solution looking for a problem\". Since then, lasers have become a multibillion-dollar industry, finding utility in thousands of highly varied applications. The first application of lasers visible in the daily lives of the general population was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser, but the compact disc player was the first laser-equipped device to become truly common in consumers' homes, beginning in 1982. These optical storage devices use a semiconductor laser less than a millimetre wide to scan the surface of the disc for data retrieval. Fibre-optic communication relies on lasers to transmit large amounts of information at the speed of light. Other common applications of lasers include laser printers and laser pointers.",
                    "score": 0.8442692756652832
                },
                {
                    "id": 20938796,
                    "contents": "History of spectroscopy\nLaser spectroscopy is a spectroscopic technique that uses lasers to be able determine the emitted frequencies of matter. The laser was invented because spectroscopists took the concept of its predecessor, the maser, and applied it to the visible and infrared ranges of light. The maser was invented by Charles Townes and other spectroscopists to stimulate matter to determine the radiative frequencies that specific atoms and molecules emitted. While working on the maser, Townes realized that more accurate detections were possible as the frequency of the microwave emitted increased. This led to an idea a few years later to use the visible and eventually the infrared ranges of light for spectroscopy that became a reality with the help of Arthur Schawlow. Since then, lasers have gone on to significantly advance experimental spectroscopy. The laser light allowed for much higher precision experiments specifically in the uses of studying collisional effects of light as well as being able to",
                    "score": 0.8435822129249573
                },
                {
                    "id": 2504878,
                    "contents": "Helium–neon laser\nA helium–neon laser or He-Ne laser, is a type of gas laser whose high energetic medium gain medium consists of a mixture of 10:1 ratio of helium and neon at a total pressure of about 1 torr inside of a small electrical discharge. The best-known and most widely used He-Ne laser operates at a wavelength of 632.8 nm, in the red part of the visible spectrum.",
                    "score": 0.8424125909805298
                },
                {
                    "id": 6347963,
                    "contents": "Blue laser\nDiode lasers which emit light at 445 nm are becoming popular as handheld lasers. Lasers emitting wavelengths below 445 nm appear violet (but are sometimes called blue lasers). Some of the most commercially common blue lasers are the diode lasers used in Blu-ray applications which emit 405 nm \"violet\" light, which is a short enough wavelength to cause fluorescence in some chemicals, in the same way as radiation further into the ultraviolet (\"black light\") does. Light of a shorter wavelength than 400 nm is classified as ultraviolet. Devices that employ blue laser light have applications in many areas ranging from optoelectronic data storage at high density to medical applications. History Semiconductor lasers",
                    "score": 0.8418101668357849
                },
                {
                    "id": 5146774,
                    "contents": "Laser pointer\nColors and wavelengths Early laser pointers were helium–neon (HeNe) gas lasers and generated laser radiation at 633 nanometers (nm), usually designed to produce a laser beam with an output power under 1 milliwatt (mW). The least expensive laser pointers use a deep-red laser diode near the 650 nm wavelength. Slightly more expensive ones use a red-orange 635 nm diode, more easily visible because of the greater sensitivity of the human eye at 635 nm. Other colors are possible too, with the 532 nm green laser being the most common alternative. Yellow-orange laser pointers, at 593.5 nm, later became available. In September 2005 handheld blue laser pointers at 473 nm became available. In early 2010 \"Blu-ray\" (actually violet) laser pointers at 405 nm went on sale.",
                    "score": 0.8413873314857483
                },
                {
                    "id": 1828966,
                    "contents": "Laser\nNatural lasers Like astrophysical masers, irradiated planetary or stellar gases may amplify light producing a natural laser. Mars, Venus and MWC 349 exhibit this phenomenon. Uses When lasers were invented in 1960, they were called \"a solution looking for a problem\". Since then, they have become ubiquitous, finding utility in thousands of highly varied applications in every section of modern society, including consumer electronics, information technology, science, medicine, industry, law enforcement, entertainment, and the military. Fiber-optic communication using lasers is a key technology in modern communications, allowing services such as the Internet.",
                    "score": 0.8372908234596252
                },
                {
                    "id": 282851,
                    "contents": "Intense pulsed light\nThe technology uses a high-powered, hand-held, computer-controlled flashgun to deliver an intense, visible, broad-spectrum pulse of light, generally in the visible spectral range of 400 to 1200 nm. Various cutoff filters are commonly used to selectively filter out shorter wavelengths, especially potentially damaging ultra violet light. The resulting light has a spectral range that targets specific structures and chromophores (e.g. melanin in hair, or oxyhemoglobin in blood vessels) that are heated to destruction and reabsorbed by the body. IPL shares some similarities with laser treatments, in that they both use light to heat and destroy their targets. But unlike lasers that use a single wavelength (color) of light which typically matches only one chromophore and hence only treats one condition, IPL uses a broad spectrum that when used with interchangeable filters, allowing it to be used against several conditions. This can be achieved when the IPL technician selects the",
                    "score": 0.836982011795044
                },
                {
                    "id": 1828969,
                    "contents": "Laser\nResearch: spectroscopy, laser ablation, laser annealing, laser scattering, laser interferometry, lidar, laser capture microdissection, fluorescence microscopy, metrology, laser cooling. Commercial products: laser printers, barcode scanners, thermometers, laser pointers, holograms, bubblegrams. Entertainment: optical discs, laser lighting displays, laser turntables",
                    "score": 0.8367992639541626
                },
                {
                    "id": 1828896,
                    "contents": "Laser\nA laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The word \"laser\" is an acronym for \"light amplification by stimulated emission of radiation\". The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow ().",
                    "score": 0.8366601467132568
                },
                {
                    "id": 1834077,
                    "contents": "Laser science\nHistory Laser science predates the invention of the laser itself. Albert Einstein created the foundations for the laser and maser in 1917, via a paper in which he re-derived Max Planck’s law of radiation using a formalism based on probability coefficients (Einstein coefficients) for the absorption, spontaneous emission, and stimulated emission of electromagnetic radiation. The existence of stimulated emission was confirmed in 1928 by Rudolf W. Ladenburg. In 1939, Valentin A. Fabrikant made the earliest laser proposal. He specified the conditions required for light amplification using stimulated emission. In 1947, Willis E. Lamb and R. C. Retherford found apparent stimulated emission in hydrogen spectra and effected the first demonstration of stimulated emission; in 1950, Alfred Kastler (Nobel Prize for Physics 1966) proposed the method of optical pumping, experimentally confirmed, two years later, by Brossel, Kastler, and Winter.",
                    "score": 0.8364899754524231
                },
                {
                    "id": 2478937,
                    "contents": "Ford Laser\nIn March 2002, due to falling sales, Ford made one last attempt to restore the Laser's popularity to its former glory, by announcing minor upgrades to the SR2, and added three new exterior colours to the range, being \"Goldrush\", \"Red Revenge\", and \"Electric Blue\". Three engines were available, a 1.6-litre that was fitted to the LXi, a 1.8-litre that was fitted to the GLXi & SR, and a 2.0-litre that was exclusive to the SR2. Despite the Laser having a good reputation with buyers in the marketplace, and many attempts from Ford to reignite interest in the model, it was unable to sell in reasonable numbers. In September 2002, Ford decided to discontinue the Laser in Australia, replacing it with the European-sourced Focus. However, the Laser continued in New Zealand until mid-2003, when it was also replaced by the facelifted Focus.",
                    "score": 0.8360466957092285
                },
                {
                    "id": 2342033,
                    "contents": "Laser diode\nLaser diodes find wide use in telecommunication as easily modulated and easily coupled light sources for fiber optics communication. They are used in various measuring instruments, such as rangefinders. Another common use is in barcode readers. Visible lasers, typically red but later also green, are common as laser pointers. Both low and high-power diodes are used extensively in the printing industry both as light sources for scanning (input) of images and for very high-speed and high-resolution printing plate (output) manufacturing. Infrared and red laser diodes are common in CD players, CD-ROMs and DVD technology. Violet lasers are used in HD DVD and Blu-ray technology. Diode lasers have also found many applications in laser absorption spectrometry (LAS) for high-speed, low-cost assessment or monitoring of the concentration of various species in gas phase. High-power laser diodes are used in industrial applications such as heat treating, cladding, seam welding and for pumping other",
                    "score": 0.8348343968391418
                },
                {
                    "id": 1828979,
                    "contents": "Laser\nReferences Further reading Books Bertolotti, Mario (1999, trans. 2004). The History of the Laser. Institute of Physics. . Bromberg, Joan Lisa (1991). The Laser in America, 1950–1970. MIT Press. . Csele, Mark (2004). Fundamentals of Light Sources and Lasers. Wiley. . Koechner, Walter (1992). Solid-State Laser Engineering. 3rd ed. Springer-Verlag. . Siegman, Anthony E. (1986). Lasers. University Science Books. . Silfvast, William T. (1996). Laser Fundamentals. Cambridge University Press. . Svelto, Orazio (1998). Principles of Lasers. 4th ed. Trans. David Hanna. Springer. . Wilson, J. & Hawkes, J.F.B. (1987). Lasers: Principles and Applications. Prentice Hall International Series in Optoelectronics, Prentice Hall. . Yariv, Amnon (1989). Quantum Electronics. 3rd ed. Wiley. .",
                    "score": 0.8346748948097229
                },
                {
                    "id": 3435237,
                    "contents": "Laser cutting\nTypes There are three main types of lasers used in laser cutting. The laser is suited for cutting, boring, and engraving. The neodymium (Nd) and neodymium yttrium-aluminium-garnet (Nd:YAG) lasers are identical in style and differ only in application. Nd is used for boring and where high energy but low repetition are required. The Nd:YAG laser is used where very high power is needed and for boring and engraving. Both and Nd/Nd:YAG lasers can be used for welding.",
                    "score": 0.8344676494598389
                },
                {
                    "id": 20029282,
                    "contents": "Timeline of United States inventions (1946–1991)\n1960 Gas laser A gas laser is a laser in which an electric current is discharged through a gas to produce light. The first gas laser, the Helium-neon, was invented by William R. Bennett, Don Herriott, and Ali Javan in 1960. The first continuous visible gas laser, operating at 632.8 nm in the red, was invented by A. D. White and J. D. Rigden in 1962.",
                    "score": 0.8336827158927917
                },
                {
                    "id": 5146795,
                    "contents": "Laser pointer\nA high-powered green laser pointer bought over the Internet was reported in 2010 to have caused a decrease of visual acuity from 6/6 to 6/12 (20/20 to 20/40); after two months acuity recovered to 6/6, but some retinal damage remained. The US FDA issued a warning after two anecdotal reports it received of eye injury from laser pointers. Laser pointers available for purchase online can be capable of significantly higher power output than the pointers typically available in stores. Dubbed \"Burning Lasers\", these are designed to burn through light plastics and paper, and can have very similar external appearances to their low-power counterparts. Because of their high power, many online retailers have warned high-power laser pointer users not to point them at humans or animals.",
                    "score": 0.8327342867851257
                },
                {
                    "id": 1828976,
                    "contents": "Laser\nClass 4 lasers (≥ 500 mW) can burn skin, and in some cases, even scattered light from these lasers can cause eye and/or skin damage. Many industrial and scientific lasers are in this class. The indicated powers are for visible-light, continuous-wave lasers. For pulsed lasers and invisible wavelengths, other power limits apply. People working with class 3B and class 4 lasers can protect their eyes with safety goggles which are designed to absorb light of a particular wavelength.",
                    "score": 0.8324592113494873
                },
                {
                    "id": 17357609,
                    "contents": "Denis Hall\nin many industry/university collaborative research projects, has resulted in a series of new commercial laser devices and industrial laser-based systems. He has been involved in the creation of three successful start-up companies producing lasers and laser systems.",
                    "score": 0.8322070240974426
                },
                {
                    "id": 7516402,
                    "contents": "Theodor W. Hänsch\nIn 1970 he invented a new type of laser which generated light pulses with an extremely high spectral resolution (i.e. all the photons emitted from the laser had nearly the same energy, to a precision of 1 part in a million). Using this device he succeeded to measure the transition frequency of the Balmer line of atomic hydrogen with a much higher precision than before. During the late 1990s, he and his coworkers developed a new method to measure the frequency of laser light to an even higher precision, using a device called the optical frequency comb generator. This invention was then used to measure the Lyman line of atomic hydrogen to an extraordinary precision of 1 part in a hundred trillion. At such a high precision, it became possible to search for possible changes in the fundamental physical constants of the universe over time. For these achievements he became co-recipient of the Nobel Prize in Physics for 2005.",
                    "score": 0.8320409059524536
                },
                {
                    "id": 1828948,
                    "contents": "Laser\nThe molecular fluorine laser, emitting at 157 nm in the vacuum ultraviolet is sometimes referred to as an excimer laser, however this appears to be a misnomer inasmuch as F2 is a stable compound.",
                    "score": 0.8316060900688171
                },
                {
                    "id": 19802308,
                    "contents": "Laserglow Technologies\nLaserglow Technologies is an optoelectronics company headquartered in Toronto, Ontario, Canada, specializing in the sale of lasers, particularly DPSS and collimated diode lasers. The company markets laser systems and components (including laser pointers, portable lasers, and Laboratory/OEM lasers), laser alignment products, optics, and laser accessories, which are used in industry, education and scientific research. The legal entity is called Laserglow.com Ltd., whereas elsewhere the company always refers to itself as Laserglow Technologies or simply Laserglow.",
                    "score": 0.8315039277076721
                },
                {
                    "id": 1205942,
                    "contents": "Red\nLasers Lasers emitting in the red region of the spectrum have been available since the invention of the ruby laser in 1960. In 1962 the red helium–neon laser was invented, and these two types of lasers were widely used in many scientific applications including holography, and in education. Red helium–neon lasers were used commercially in LaserDisc players. The use of red laser diodes became widespread with the commercial success of modern DVD players, which use a 660 nm laser diode technology. Today, red and red-orange laser diodes are widely available to the public in the form of extremely inexpensive laser pointers. Portable, high-powered versions are also available for various applications. More recently, 671 nm diode-pumped solid state (DPSS) lasers have been introduced to the market for all-DPSS laser display systems, particle image velocimetry, Raman spectroscopy, and holography.",
                    "score": 0.8313459753990173
                },
                {
                    "id": 28773285,
                    "contents": "Marshall G. Jones\nThroughout Jones' career, he has been awarded more than 65 U.S. patents. Jones' Patent No. 4676586 is the patent that is on display at the National Inventors Hall of Fame. This patent describes a laser beam delivery system through a fiber optic which resulted in minimal optical losses. The system was unique in that it allowed unprecedented freedom in the manipulation of the laser beam such that hard-to-reach places were accessible. He made pivotal innovations in the method of making lead wires that is used for light bulbs. This innovation has wide ranging impacts, including: flat emitters for x-ray tubes, diesel engine head-liner assemblies, the production of ceramic metal halide lamps, and control rods for nuclear reactors. Jones pioneered the use of lasers for materials processing in a way that made them readily adoptable for industrial applications. Not only have Jones' work and inventions transformed many industrial products, they have also been foundational to modern industrial",
                    "score": 0.8304760456085205
                },
                {
                    "id": 2341989,
                    "contents": "Laser diode\nThe choice of the semiconductor material determines the wavelength of the emitted beam, which in today's laser diodes range from infra-red to the UV spectrum. Laser diodes are the most common type of lasers produced, with a wide range of uses that include fiber optic communications, barcode readers, laser pointers, CD/DVD/Blu-ray disc reading/recording, laser printing, laser scanning and light beam illumination. With the use of a phosphor like that found on white LEDs, Laser diodes can be used for general illumination. Theory",
                    "score": 0.8302114605903625
                },
                {
                    "id": 1828898,
                    "contents": "Laser\nLasers are used in optical disc drives, laser printers, barcode scanners, DNA sequencing instruments, fiber-optic, semiconducting chip manufacturing (photolithography), and free-space optical communication, laser surgery and skin treatments, cutting and welding materials, military and law enforcement devices for marking targets and measuring range and speed, and in laser lighting displays for entertainment. Semiconductor lasers in the blue to near-UV have also been used in place of light-emitting diodes (LEDs) to excite fluorescence as a white light source. This permits a much smaller emitting area due to the much greater radiance of a laser and avoids the droop suffered by LEDs; such devices are already used in some car headlamps. Fundamentals",
                    "score": 0.8301775455474854
                },
                {
                    "id": 19889947,
                    "contents": "IBM 3800\nLong Life Helium-Neon Laser By using low helium diffusion glass, IBM increased the life of the laser to 20,000 operation hours, an improvement of 10× over off-the-shelf products available at that time. The HeNe laser used in the IBM 3800 used more than 25 milliwatts (compared to 5 milliwatts in the later IBM 6670 or 1 milliwatt in the IBM 3666 barcode scanner). Long Life Hot Roll The hot roll that fuses the toner to the page needed to operate without silicone oil (to avoid contamination) and to have an extended life, so a new elastomer material was developed as well as a multi-zone preheat platen to warm the paper to prior to fusion. Long Life Xenon Flash Lamp A flash lamp was used as part of the form overlay system that optionally allowed you to print a fixed form onto each page. This system used a lamp that had to pulse for only 125 μs. Initial life span of the lamp was only one month, but through a variety of design changes this was extended to 60 months.",
                    "score": 0.8299655914306641
                },
                {
                    "id": 19076018,
                    "contents": "Technological and industrial history of 20th-century Canada\nLasers The use of lasers became common throughout Canada during these years. The devices are usually found as components of larger systems. Lasers are used in the field of telecommunications, where they are act as the modulated light sources for fibre optic systems. The high frequency of the pulsed beams of light they produce enables the transmission of large quantities of information and the absence of an electromagnetic field around the fiber optic cable lessens transmission loss and increases the security of the data. Lasers are also used in many mechanical manufacturing systems to start and stop processes, measure component size and monitor and maintain quality. In public they are commonly found at the retail checkout counter, where they scan bar codes. They are also used to open and close doors for people and cars and are common in public washrooms, where they control the flow of water for taps and the flushing of urinals and toilettes.",
                    "score": 0.8299620151519775
                },
                {
                    "id": 19630970,
                    "contents": "International Conference of Laser Applications\nThe International Conference on Lasers and Applications, Lasers 'XX was an annual conference organized by the former Society for Optical and Quantum Electronics. The conference, known in short by Lasers 'XX (where XX refers to the particular year), was held at various locations in The United States from 1978 to 2000. The emphasis of these conferences was laser development and in particular the development of high-power lasers. The papers delivered at these conferences were published in a series of hard-bound volumes known as Proceedings of the International Conference on Lasers 'XX () by STS Press. In total, more than 20 book proceedings were published.",
                    "score": 0.8298745155334473
                },
                {
                    "id": 188823,
                    "contents": "Theodore Maiman\nSee also Gordon Gould References External links Laser Inventor - Creator of the World's First Laser Theodore H. Maiman via IEEE Global History Network Bright Idea: The First Lasers (history) Time Photos, \"20th Century Technology: Laser\" SPIE, \"Lasers and Sources, Video: Theodore Maiman on the First Laser\" SPIE, \"Lasers and Sources, Video: Maiman's First Laser Light Shines Again\" MIT Tech TV, \"Video: The Laser at 50, symposium, October 8, 2010\"",
                    "score": 0.8296399116516113
                },
                {
                    "id": 1828967,
                    "contents": "Laser\nThe first widely noticeable use of lasers was the supermarket barcode scanner, introduced in 1974. The laserdisc player, introduced in 1978, was the first successful consumer product to include a laser but the compact disc player was the first laser-equipped device to become common, beginning in 1982 followed shortly by laser printers.",
                    "score": 0.8289501667022705
                },
                {
                    "id": 1834076,
                    "contents": "Laser science\nLaser science or laser physics is a branch of optics that describes the theory and practice of lasers. Laser science is principally concerned with quantum electronics, laser construction, optical cavity design, the physics of producing a population inversion in laser media, and the temporal evolution of the light field in the laser. It is also concerned with the physics of laser beam propagation, particularly the physics of Gaussian beams, with laser applications, and with associated fields such as nonlinear optics and quantum optics. History",
                    "score": 0.8289176821708679
                },
                {
                    "id": 9262033,
                    "contents": "Project Excalibur\nPhysics Lasers Lasers rely on two physical phenomena to work, stimulated emission and population inversion. An atom is made of a nucleus and a number of electrons orbiting in shells around it. Electrons can be found in many discrete energy states, defined by quantum mechanics. The energy levels depend on the structure of the nucleus, so they vary from element to element. Electrons can gain or lose energy by absorbing or emitting a photon with the same energy as the difference between two allowable energy states. This is why different elements have unique spectrums and gives rise to the science of spectroscopy.",
                    "score": 0.828839123249054
                },
                {
                    "id": 7142271,
                    "contents": "Chemical laser\nA chemical laser is a laser that obtains its energy from a chemical reaction. Chemical lasers can reach continuous wave output with power reaching to megawatt levels. They are used in industry for cutting and drilling.",
                    "score": 0.8284178972244263
                },
                {
                    "id": 2544245,
                    "contents": "Dye laser\nDye lasers are used in many applications including: astronomy (as laser guide stars), atomic vapor laser isotope separation manufacturing medicine spectroscopy In laser medicine these lasers are applied in several areas, including dermatology where they are used to make skin tone more even. The wide range of wavelengths possible allows very close matching to the absorption lines of certain tissues, such as melanin or hemoglobin, while the narrow bandwidth obtainable helps reduce the possibility of damage to the surrounding tissue. They are used to treat port-wine stains and other blood vessel disorders, scars and kidney stones. They can be matched to a variety of inks for tattoo removal, as well as a number of other applications.",
                    "score": 0.8282476663589478
                },
                {
                    "id": 2622397,
                    "contents": "Biophotonics\nLasers Lasers play an increasingly important role in biophotonics. Their unique intrinsic properties like precise wavelength selection, widest wavelength coverage, highest focusability and thus best spectral resolution, strong power densities and broad spectrum of excitation periods make them the most universal light tool for a wide spectrum of applications. As a consequence a variety of different laser technologies from a broad number of suppliers can be found in the market today. Gas lasers Major gas lasers used for biophotonics applications, and their most important wavelengths, are: - Argon Ion laser: 457.8 nm, 476.5 nm, 488.0 nm, 496.5 nm, 501.7 nm, 514.5 nm (multi-line operation possible) - Krypton Ion laser: 350.7 nm, 356.4 nm, 476.2 nm, 482.5 nm, 520.6 nm, 530.9 nm, 568.2 nm, 647.1 nm, 676.4 nm, 752.5 nm, 799.3 nm - Helium–neon laser: 632.8 nm (543.5 nm, 594.1 nm, 611.9 nm) - HeCd lasers: 325 nm, 442 nm",
                    "score": 0.8280981779098511
                },
                {
                    "id": 17817428,
                    "contents": "Optical sorting\nLasers While cameras capture product information based primarily on material reflectance, lasers and their sensors are able to distinguish a material's structural properties along with their color. This structural property inspection allows lasers to detect a wide range of organic and inorganic foreign material such as insects, glass, metal, sticks, rocks and plastic; even if they are the same color as the good product. Lasers can be designed to operate within specific wavelengths of light; whether on the visible spectrum or beyond. For example, lasers can detect chlorophyll by stimulating fluorescence using specific wavelengths; which is a process that is very effective for removing foreign material from green vegetables.",
                    "score": 0.8280403017997742
                },
                {
                    "id": 771608,
                    "contents": "Timeline of historic inventions\n1960s 1960: The first functioning laser is invented by Theodore Maiman. 1961: The first crewed spaceflight is achieved by Vostok 1. 1963: The first electronic cigarette is created by Herbert A. Gilbert. Hon Lik is often credited with its invention as he developed the modern electronic cigarette and was the first to commercialize it. 1964: Shinkansen, the first high-speed rail commercial passenger service. 1965: Kevlar is invented by Stephanie Kwolek at DuPont. 1969: ARPANET first deployed via UCLA, SRI, UCSB, and The University of Utah.",
                    "score": 0.8269137144088745
                },
                {
                    "id": 1828970,
                    "contents": "Laser\nIn 2004, excluding diode lasers, approximately 131,000 lasers were sold with a value of US$2.19 billion. In the same year, approximately 733 million diode lasers, valued at $3.20 billion, were sold. In medicine Lasers have many uses in medicine, including laser surgery (particularly eye surgery), laser healing, kidney stone treatment, ophthalmoscopy, and cosmetic skin treatments such as acne treatment, cellulite and striae reduction, and hair removal.",
                    "score": 0.8265565037727356
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_11",
        "question": "What is the uncertainty of the momentum of an electron if we know its position is somewhere in a $10 \\mathrm{pm}$ interval?",
        "golden_answers": [
            " 6.6"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 8557319,
                    "contents": "Angular momentum operator\nThe Robertson–Schrödinger relation gives the following uncertainty principle: where is the standard deviation in the measured values of X and denotes the expectation value of X. This inequality is also true if x, y, z are rearranged, or if L is replaced by J or S. Therefore, two orthogonal components of angular momentum (for example Lx and Ly) are complementary and cannot be simultaneously known or measured, except in special cases such as . It is, however, possible to simultaneously measure or specify L2 and any one component of L; for example, L2 and Lz. This is often useful, and the values are characterized by the azimuthal quantum number (l) and the magnetic quantum number (m). In this case the quantum state of the system is a simultaneous eigenstate of the operators L2 and Lz, but not of Lx or Ly. The eigenvalues are related to l and m, as shown in the table below. Quantization",
                    "score": 0.9106148481369019
                },
                {
                    "id": 1590090,
                    "contents": "Angular momentum\nQuantization In quantum mechanics, angular momentum is quantized – that is, it cannot vary continuously, but only in \"quantum leaps\" between certain allowed values. For any system, the following restrictions on measurement results apply, where is the reduced Planck constant and is any Euclidean vector such as x, y, or z: (There are additional restrictions as well, see angular momentum operator for details.) The reduced Planck constant is tiny by everyday standards, about 10−34 J s, and therefore this quantization does not noticeably affect the angular momentum of macroscopic objects. However, it is very important in the microscopic world. For example, the structure of electron shells and subshells in chemistry is significantly affected by the quantization of angular momentum. Quantization of angular momentum was first postulated by Niels Bohr in his Bohr model of the atom and was later predicted by Erwin Schrödinger in his Schrödinger equation.",
                    "score": 0.9021624326705933
                },
                {
                    "id": 1194764,
                    "contents": "Quantum mechanics\nwhich has Fourier transform, and therefore momentum distribution We see that as we make smaller the spread in position gets smaller, but the spread in momentum gets larger. Conversely, by making larger we make the spread in momentum smaller, but the spread in position gets larger. This illustrates the uncertainty principle. As we let the Gaussian wave packet evolve in time, we see that its center moves through space at a constant velocity (like a classical particle with no forces acting on it). However, the wave packet will also spread out as time progresses, which means that the position becomes more and more uncertain. The uncertainty in momentum, however, stays constant. Particle in a box",
                    "score": 0.9019416570663452
                },
                {
                    "id": 7748028,
                    "contents": "Introduction to quantum mechanics\nHeisenberg gave, as an illustration, the measurement of the position and momentum of an electron using a photon of light. In measuring the electron's position, the higher the frequency of the photon, the more accurate is the measurement of the position of the impact of the photon with the electron, but the greater is the disturbance of the electron. This is because from the impact with the photon, the electron absorbs a random amount of energy, rendering the measurement obtained of its momentum increasingly uncertain (momentum is velocity multiplied by mass), for one is necessarily measuring its post-impact disturbed momentum from the collision products and not its original momentum (momentum which should be simultaneously measured with position). With a photon of lower frequency, the disturbance (and hence uncertainty) in the momentum is less, but so is the accuracy of the measurement of the position of the impact.",
                    "score": 0.8991416692733765
                },
                {
                    "id": 2875336,
                    "contents": "Measurement in quantum mechanics\nand likewise for the momentum: The Kennard–Pauli–Weyl uncertainty relation is This inequality means that no preparation of a quantum particle can imply simultaneously precise predictions for a measurement of position and for a measurement of momentum. The Robertson inequality generalizes this to the case of an arbitrary pair of self-adjoint operators and . The commutator of these two operators is and this provides the lower bound on the product of standard deviations: Substituting in the canonical commutation relation , an expression first postulated by Max Born in 1925, recovers the Kennard–Pauli–Weyl statement of the uncertainty principle. From uncertainty to no-hidden-variables",
                    "score": 0.897416889667511
                },
                {
                    "id": 1194756,
                    "contents": "Quantum mechanics\nand likewise for the momentum: The uncertainty principle states that Either standard deviation can in principle be made arbitrarily small, but not both simultaneously. This inequality generalizes to arbitrary pairs of self-adjoint operators and . The commutator of these two operators is and this provides the lower bound on the product of standard deviations:",
                    "score": 0.8970637321472168
                },
                {
                    "id": 1762780,
                    "contents": "History of physics\n\"uncertainty principle\" (indicating the impossibility of precisely and simultaneously measuring position and momentum) and the \"Copenhagen interpretation\" of quantum mechanics (named after Bohr's home city) continued to deny the possibility of fundamental causality, though opponents such as Einstein would metaphorically assert that \"God does not play dice with the universe\". The new quantum mechanics became an indispensable tool in the investigation and explanation of phenomena at the atomic level. Also in the 1920s, the Indian scientist Satyendra Nath Bose's work on photons and quantum mechanics provided the foundation for Bose–Einstein statistics, the theory of the Bose–Einstein condensate.",
                    "score": 0.8965689539909363
                },
                {
                    "id": 8557335,
                    "contents": "Angular momentum operator\nNotes References Further reading Quantum Mechanics Demystified, D. McMahon, Mc Graw Hill (USA), 2006, Quantum mechanics, E. Zaarur, Y. Peleg, R. Pnini, Schaum's Easy Outlines Crash Course, Mc Graw Hill (USA), 2006, Quantum Physics of Atoms, Molecules, Solids, Nuclei, and Particles (2nd Edition), R. Eisberg, R. Resnick, John Wiley & Sons, 1985, Quantum Mechanics, E. Abers, Pearson Ed., Addison Wesley, Prentice Hall Inc, 2004, Physics of Atoms and Molecules, B.H. Bransden, C.J.Joachain, Longman, 1983, Angular Momentum. Understanding Spatial Aspects in Chemistry and Physics, R. N. Zare, Wiley-Interscience, 1991, Rotational symmetry Quantum mechanics",
                    "score": 0.8964881896972656
                },
                {
                    "id": 1305744,
                    "contents": "Uncertainty principle\nKennard in 1927 first proved the modern inequality: where , and , are the standard deviations of position and momentum. Heisenberg only proved relation () for the special case of Gaussian states. Terminology and translation Throughout the main body of his original 1927 paper, written in German, Heisenberg used the word \"Ungenauigkeit\" (\"indeterminacy\"), to describe the basic theoretical principle. Only in the endnote did he switch to the word \"Unsicherheit\" (\"uncertainty\"). When the English-language version of Heisenberg's textbook, The Physical Principles of the Quantum Theory, was published in 1930, however, the translation \"uncertainty\" was used, and it became the more commonly used term in the English language thereafter. Heisenberg's microscope",
                    "score": 0.8958538770675659
                },
                {
                    "id": 2875335,
                    "contents": "Measurement in quantum mechanics\nThe uncertainty principle dates to this period. It is frequently attributed to Heisenberg, who introduced the concept in analyzing a thought experiment where one attempts to measure an electron's position and momentum simultaneously. However, Heisenberg did not give precise mathematical definitions of what the \"uncertainty\" in these measurements meant. The precise mathematical statement of the position-momentum uncertainty principle is due to Kennard, Pauli, and Weyl, and its generalization to arbitrary pairs of noncommuting observables is due to Robertson and Schrödinger. Writing and for the self-adjoint operators representing position and momentum respectively, a standard deviation of position can be defined as and likewise for the momentum: The Kennard–Pauli–Weyl uncertainty relation is",
                    "score": 0.8954864144325256
                },
                {
                    "id": 8557318,
                    "contents": "Angular momentum operator\nMathematically, is a Casimir invariant of the Lie algebra SO(3) spanned by . As above, there is an analogous relationship in classical physics: where is a component of the classical angular momentum operator, and is the Poisson bracket. Returning to the quantum case, the same commutation relations apply to the other angular momentum operators (spin and total angular momentum), as well, Uncertainty principle In general, in quantum mechanics, when two observable operators do not commute, they are called complementary observables. Two complementary observables cannot be measured simultaneously; instead they satisfy an uncertainty principle. The more accurately one observable is known, the less accurately the other one can be known. Just as there is an uncertainty principle relating position and momentum, there are uncertainty principles for angular momentum. The Robertson–Schrödinger relation gives the following uncertainty principle:",
                    "score": 0.8938983082771301
                },
                {
                    "id": 1305746,
                    "contents": "Uncertainty principle\nHe imagines an experimenter trying to measure the position and momentum of an electron by shooting a photon at it. Problem 1 – If the photon has a short wavelength, and therefore, a large momentum, the position can be measured accurately. But the photon scatters in a random direction, transferring a large and uncertain amount of momentum to the electron. If the photon has a long wavelength and low momentum, the collision does not disturb the electron's momentum very much, but the scattering will reveal its position only vaguely. Problem 2 – If a large aperture is used for the microscope, the electron's location can be well resolved (see Rayleigh criterion); but by the principle of conservation of momentum, the transverse momentum of the incoming photon affects the electron's beamline momentum and hence, the new momentum of the electron resolves poorly. If a small aperture is used, the accuracy of both resolutions is the other way around.",
                    "score": 0.8937947750091553
                },
                {
                    "id": 1590088,
                    "contents": "Angular momentum\nHowever, in quantum physics, there is another type of angular momentum, called spin angular momentum, represented by the spin operator S. Almost all elementary particles have nonzero spin. Spin is often depicted as a particle literally spinning around an axis, but this is a misleading and inaccurate picture: spin is an intrinsic property of a particle, unrelated to any sort of motion in space and fundamentally different from orbital angular momentum. All elementary particles have a characteristic spin (possibly zero), for example electrons have \"spin 1/2\" (this actually means \"spin ħ/2\"), photons have \"spin 1\" (this actually means \"spin ħ\"), and pi-mesons have spin 0.",
                    "score": 0.8928237557411194
                },
                {
                    "id": 1168434,
                    "contents": "Photon\nAnother difficulty is finding the proper analogue for the uncertainty principle, an idea frequently attributed to Heisenberg, who introduced the concept in analyzing a thought experiment involving an electron and a high-energy photon. However, Heisenberg did not give precise mathematical definitions of what the \"uncertainty\" in these measurements meant. The precise mathematical statement of the position–momentum uncertainty principle is due to Kennard, Pauli, and Weyl. The uncertainty principle applies to situations where an experimenter has a choice of measuring either one of two \"canonically conjugate\" quantities, like the position and the momentum of a particle. According to the uncertainty principle, no matter how the particle is prepared, it is not possible to make a precise prediction for both of the two alternative measurements: if the outcome of the position measurement is made more certain, the outcome of the momentum measurement becomes less so, and vice versa. A coherent",
                    "score": 0.892428994178772
                },
                {
                    "id": 1590091,
                    "contents": "Angular momentum\nQuantization of angular momentum was first postulated by Niels Bohr in his Bohr model of the atom and was later predicted by Erwin Schrödinger in his Schrödinger equation. Uncertainty In the definition , six operators are involved: The position operators , , , and the momentum operators , , . However, the Heisenberg uncertainty principle tells us that it is not possible for all six of these quantities to be known simultaneously with arbitrary precision. Therefore, there are limits to what can be known or measured about a particle's angular momentum. It turns out that the best that one can do is to simultaneously measure both the angular momentum vector's magnitude and its component along one axis. The uncertainty is closely related to the fact that different components of an angular momentum operator do not commute, for example . (For the precise commutation relations, see angular momentum operator.)",
                    "score": 0.8916937112808228
                },
                {
                    "id": 1168435,
                    "contents": "Photon\nfor both of the two alternative measurements: if the outcome of the position measurement is made more certain, the outcome of the momentum measurement becomes less so, and vice versa. A coherent state minimizes the overall uncertainty as far as quantum mechanics allows. Quantum optics makes use of coherent states for modes of the electromagnetic field. There is a tradeoff, reminiscent of the position–momentum uncertainty relation, between measurements of an electromagnetic wave's amplitude and its phase. This is sometimes informally expressed in terms of the uncertainty in the number of photons present in the electromagnetic wave, , and the uncertainty in the phase of the wave, . However, this cannot be an uncertainty relation of the Kennard–Pauli–Weyl type, since unlike position and momentum, the phase cannot be represented by a Hermitian operator.",
                    "score": 0.8913357257843018
                },
                {
                    "id": 1590056,
                    "contents": "Angular momentum\nMany problems in physics involve matter in motion about some certain point in space, be it in actual rotation about it, or simply moving past it, where it is desired to know what effect the moving matter has on the point—can it exert energy upon it or perform work about it? Energy, the ability to do work, can be stored in matter by setting it in motion—a combination of its inertia and its displacement. Inertia is measured by its mass, and displacement by its velocity. Their product,",
                    "score": 0.8911445140838623
                },
                {
                    "id": 1590086,
                    "contents": "Angular momentum\nAngular momentum in quantum mechanics In quantum mechanics, angular momentum (like other quantities) is expressed as an operator, and its one-dimensional projections have quantized eigenvalues. Angular momentum is subject to the Heisenberg uncertainty principle, implying that at any time, only one projection (also called \"component\") can be measured with definite precision; the other two then remain uncertain. Because of this, the axis of rotation of a quantum particle is undefined. Quantum particles do possess a type of non-orbital angular momentum called \"spin\", but this angular momentum does not correspond to a spinning motion. In relativistic quantum mechanics the above relativistic definition becomes a tensorial operator. Spin, orbital, and total angular momentum",
                    "score": 0.8893905282020569
                },
                {
                    "id": 1194755,
                    "contents": "Quantum mechanics\nUncertainty principle One consequence of the basic quantum formalism is the uncertainty principle. In its most familiar form, this states that no preparation of a quantum particle can imply simultaneously precise predictions both for a measurement of its position and for a measurement of its momentum. Both position and momentum are observables, meaning that they are represented by Hermitian operators. The position operator and momentum operator do not commute, but rather satisfy the canonical commutation relation: Given a quantum state, the Born rule lets us compute expectation values for both and , and moreover for powers of them. Defining the uncertainty for an observable by a standard deviation, we have and likewise for the momentum: The uncertainty principle states that",
                    "score": 0.887009859085083
                },
                {
                    "id": 7748031,
                    "contents": "Introduction to quantum mechanics\nEigenstates and eigenvalues Because of the uncertainty principle, statements about both the position and momentum of particles can assign only a probability that the position or momentum has some numerical value. Therefore, it is necessary to formulate clearly the difference between the state of something indeterminate, such as an electron in a probability cloud, and the state of something having a definite value. When an object can definitely be \"pinned-down\" in some respect, it is said to possess an eigenstate. In the Stern–Gerlach experiment discussed above, the spin of the atom about the vertical axis has two eigenstates: up and down. Before measuring it, we can only say that any individual atom has an equal probability of being found to have spin up or spin down. The measurement process causes the wavefunction to collapse into one of the two states.",
                    "score": 0.8859400749206543
                },
                {
                    "id": 8557324,
                    "contents": "Angular momentum operator\nQuantization in macroscopic systems The quantization rules are widely thought to be true even for macroscopic systems, like the angular momentum L of a spinning tire. However they have no observable effect so this has not been tested. For example, if is roughly 100000000, it makes essentially no difference whether the precise value is an integer like 100000000 or 100000001, or a non-integer like 100000000.2—the discrete steps are currently too small to measure. Angular momentum as the generator of rotations The most general and fundamental definition of angular momentum is as the generator of rotations. More specifically, let be a rotation operator, which rotates any quantum state about axis by angle . As , the operator approaches the identity operator, because a rotation of 0° maps all states to themselves. Then the angular momentum operator about axis is defined as: where 1 is the identity operator. Also notice that R is an additive morphism : ; as a consequence",
                    "score": 0.8857883214950562
                },
                {
                    "id": 1305718,
                    "contents": "Uncertainty principle\nConstant momentum Assume a particle initially has a momentum space wave function described by a normal distribution around some constant momentum p0 according to where we have introduced a reference scale , with describing the width of the distribution—cf. nondimensionalization. If the state is allowed to evolve in free space, then the time-dependent momentum and position space wave functions are Since and , this can be interpreted as a particle moving along with constant momentum at arbitrarily high precision. On the other hand, the standard deviation of the position is such that the uncertainty product can only increase with time as Additional uncertainty relations Systematic and statistical errors",
                    "score": 0.8852264881134033
                },
                {
                    "id": 8557311,
                    "contents": "Angular momentum operator\nIn quantum mechanics, the angular momentum operator is one of several related operators analogous to classical angular momentum. The angular momentum operator plays a central role in the theory of atomic and molecular physics and other quantum problems involving rotational symmetry. Such an operator is applied to a mathematical representation of the physical state of a system and yields an angular momentum value if the state has a definite value for it. In both classical and quantum mechanical systems, angular momentum (together with linear momentum and energy) is one of the three fundamental properties of motion.",
                    "score": 0.8852131366729736
                },
                {
                    "id": 1305698,
                    "contents": "Uncertainty principle\nOne way to quantify the precision of the position and momentum is the standard deviation σ. Since is a probability density function for position, we calculate its standard deviation. The precision of the position is improved, i.e. reduced σx, by using many plane waves, thereby weakening the precision of the momentum, i.e. increased σp. Another way of stating this is that σx and σp have an inverse relationship or are at least bounded from below. This is the uncertainty principle, the exact limit of which is the Kennard bound. Click the show button below to see a semi-formal derivation of the Kennard inequality using wave mechanics. Matrix mechanics interpretation (Ref ) In matrix mechanics, observables such as position and momentum are represented by self-adjoint operators. When considering pairs of observables, an important quantity is the commutator. For a pair of operators and , one defines their commutator as",
                    "score": 0.8850764036178589
                },
                {
                    "id": 15708224,
                    "contents": "Planck constant\nBohr also introduced the quantity , now known as the reduced Planck constant, as the quantum of angular momentum. At first, Bohr thought that this was the angular momentum of each electron in an atom: this proved incorrect and, despite developments by Sommerfeld and others, an accurate description of the electron angular momentum proved beyond the Bohr model. The correct quantization rules for electrons – in which the energy reduces to the Bohr model equation in the case of the hydrogen atom – were given by Heisenberg's matrix mechanics in 1925 and the Schrödinger wave equation in 1926: the reduced Planck constant remains the fundamental quantum of angular momentum. In modern terms, if is the total angular momentum of a system with rotational invariance, and the angular momentum measured along any given direction, these quantities can only take on the values Uncertainty principle",
                    "score": 0.8848747611045837
                },
                {
                    "id": 1590057,
                    "contents": "Angular momentum\nis the matter's momentum. Referring this momentum to a central point introduces a complication: the momentum is not applied to the point directly. For instance, a particle of matter at the outer edge of a wheel is, in effect, at the end of a lever of the same length as the wheel's radius, its momentum turning the lever about the center point. This imaginary lever is known as the moment arm. It has the effect of multiplying the momentum's effort in proportion to its length, an effect known as a moment. Hence, the particle's momentum referred to a particular point,",
                    "score": 0.8832004070281982
                },
                {
                    "id": 1305700,
                    "contents": "Uncertainty principle\nOn the other hand, the above canonical commutation relation requires that This implies that no quantum state can simultaneously be both a position and a momentum eigenstate. When a state is measured, it is projected onto an eigenstate in the basis of the relevant observable. For example, if a particle's position is measured, then the state amounts to a position eigenstate. This means that the state is not a momentum eigenstate, however, but rather it can be represented as a sum of multiple momentum basis eigenstates. In other words, the momentum must be less precise. This precision may be quantified by the standard deviations, As in the wave mechanics interpretation above, one sees a tradeoff between the respective precisions of the two, quantified by the uncertainty principle.",
                    "score": 0.8829861879348755
                },
                {
                    "id": 14720791,
                    "contents": "Introduction to eigenstates\nBecause of the uncertainty principle, statements about both the position and momentum of particles can only assign a probability that the position or momentum will have some numerical value. The uncertainty principle also says that eliminating uncertainty about position maximises uncertainty about momentum, and eliminating uncertainty about momentum maximizes uncertainty about position. A probability distribution assigns probabilities to all possible values of position and momentum. Schrödinger's wave equation gives wavefunction solutions, the squares of which are probabilities of where the electron might be, just as Heisenberg's probability distribution does.",
                    "score": 0.8828091025352478
                },
                {
                    "id": 8557312,
                    "contents": "Angular momentum operator\nThere are several angular momentum operators: total angular momentum (usually denoted J), orbital angular momentum (usually denoted L), and spin angular momentum (spin for short, usually denoted S). The term angular momentum operator can (confusingly) refer to either the total or the orbital angular momentum. Total angular momentum is always conserved, see Noether's theorem. Overview In quantum mechanics, angular momentum can refer to one of three different, but related things. Orbital angular momentum The classical definition of angular momentum is . The quantum-mechanical counterparts of these objects share the same relationship: where r is the quantum position operator, p is the quantum momentum operator, × is cross product, and L is the orbital angular momentum operator. L (just like p and r) is a vector operator (a vector whose components are operators), i.e. where Lx, Ly, Lz are three different quantum-mechanical operators.",
                    "score": 0.882422685623169
                },
                {
                    "id": 8557313,
                    "contents": "Angular momentum operator\nIn the special case of a single particle with no electric charge and no spin, the orbital angular momentum operator can be written in the position basis as: where ∇ is the vector differential operator, del. Spin angular momentum There is another type of angular momentum, called spin angular momentum (more often shortened to spin), represented by the spin operator . Spin is often depicted as a particle literally spinning around an axis, but this is only a metaphor: spin is an intrinsic property of a particle, unrelated to any sort of (yet experimentally observable) motion in space. All elementary particles have a characteristic spin, which is usually nonzero. For example, electrons always have \"spin 1/2\" while photons always have \"spin 1\" (details below). Total angular momentum Finally, there is total angular momentum , which combines both the spin and orbital angular momentum of a particle or system:",
                    "score": 0.8817022442817688
                },
                {
                    "id": 6237105,
                    "contents": "Philosophy of physics\nPhysicist Roland Omnés noted that it is impossible to experimentally differentiate between Everett's view, which says that as the wave-function decoheres into distinct worlds, each of which exists equally, and the more traditional view that says that a decoherent wave-function leaves only one unique real result. Hence, the dispute between the two views represents a great \"chasm.\" \"Every characteristic of reality has reappeared in its reconstruction by our theoretical model; every feature except one: the uniqueness of facts.\" Uncertainty principle The uncertainty principle is a mathematical relation asserting an upper limit to the accuracy of the simultaneous measurement of any pair of conjugate variables, e.g. position and momentum. In the formalism of operator notation, this limit is the evaluation of the commutator of the variables' corresponding operators.",
                    "score": 0.8816066980361938
                },
                {
                    "id": 6713745,
                    "contents": "Momentum operator\nIn quantum mechanics, the momentum operator is the operator associated with the linear momentum. The momentum operator is, in the position representation, an example of a differential operator. For the case of one particle in one spatial dimension, the definition is: where is Planck's reduced constant, the imaginary unit, is the spatial coordinate, and a partial derivative (denoted by ) is used instead of a total derivative () since the wave function is also a function of time. The \"hat\" indicates an operator. The \"application\" of the operator on a differentiable wave function is as follows:",
                    "score": 0.8813444972038269
                },
                {
                    "id": 15708225,
                    "contents": "Planck constant\nUncertainty principle The Planck constant also occurs in statements of Werner Heisenberg's uncertainty principle. Given numerous particles prepared in the same state, the uncertainty in their position, , and the uncertainty in their momentum, , obey where the uncertainty is given as the standard deviation of the measured value from its expected value. There are several other such pairs of physically measurable conjugate variables which obey a similar rule. One example is time vs. energy. The inverse relationship between the uncertainty of the two conjugate variables forces a tradeoff in quantum experiments, as measuring one quantity more precisely results in the other quantity becoming imprecise. In addition to some assumptions underlying the interpretation of certain values in the quantum mechanical formulation, one of the fundamental cornerstones to the entire theory lies in the commutator relationship between the position operator and the momentum operator :",
                    "score": 0.8811622262001038
                },
                {
                    "id": 1305708,
                    "contents": "Uncertainty principle\nSince the Robertson and Schrödinger relations are for general operators, the relations can be applied to any two observables to obtain specific uncertainty relations. A few of the most common relations found in the literature are given below. For position and linear momentum, the canonical commutation relation implies the Kennard inequality from above: For two orthogonal components of the total angular momentum operator of an object: where i, j, k are distinct, and Ji denotes angular momentum along the xi axis. This relation implies that unless all three components vanish together, only a single component of a system's angular momentum can be defined with arbitrary precision, normally the component parallel to an external (magnetic or electric) field. Moreover, for , a choice , , in angular momentum multiplets, ψ = |j, m⟩, bounds the Casimir invariant (angular momentum squared, ) from below and thus yields useful constraints such as , and hence j ≥ m, among others.",
                    "score": 0.8807352781295776
                },
                {
                    "id": 1305690,
                    "contents": "Uncertainty principle\nIntroduced first in 1927 by the German physicist Werner Heisenberg, the uncertainty principle states that the more precisely the position of some particle is determined, the less precisely its momentum can be predicted from initial conditions, and vice versa. In the published 1927 paper, Heisenberg concludes that the uncertainty principle was originally pq ~ h using the full Planck constant. The formal inequality relating the standard deviation of position σx and the standard deviation of momentum σp was derived by Earle Hesse Kennard later that year and by Hermann Weyl in 1928: where is the reduced Planck constant, ).",
                    "score": 0.880352258682251
                },
                {
                    "id": 5387978,
                    "contents": "Total angular momentum quantum number\nIn quantum mechanics, the total angular momentum quantum number parametrises the total angular momentum of a given particle, by combining its orbital angular momentum and its intrinsic angular momentum (i.e., its spin). If s is the particle's spin angular momentum and ℓ its orbital angular momentum vector, the total angular momentum j is The associated quantum number is the main total angular momentum quantum number j. It can take the following range of values, jumping only in integer steps: where ℓ is the azimuthal quantum number (parameterizing the orbital angular momentum) and s is the spin quantum number (parameterizing the spin). The relation between the total angular momentum vector j and the total angular momentum quantum number j is given by the usual relation (see angular momentum quantum number) The vector's z-projection is given by",
                    "score": 0.8803020119667053
                },
                {
                    "id": 1590087,
                    "contents": "Angular momentum\nSpin, orbital, and total angular momentum The classical definition of angular momentum as can be carried over to quantum mechanics, by reinterpreting r as the quantum position operator and p as the quantum momentum operator. L is then an operator, specifically called the orbital angular momentum operator. The components of the angular momentum operator satisfy the commutation relations of the Lie algebra so(3). Indeed, these operators are precisely the infinitesimal action of the rotation group on the quantum Hilbert space. (See also the discussion below of the angular momentum operators as the generators of rotations.)",
                    "score": 0.8794763684272766
                },
                {
                    "id": 1917708,
                    "contents": "Scientific law\nHeisenberg uncertainty principle: Uncertainty in position multiplied by uncertainty in momentum is at least half of the reduced Planck constant, similarly for time and energy; The uncertainty principle can be generalized to any pair of observables - see main article. |- | Wave mechanics Schrödinger equation (original form): |- style=\"border-top: 3px solid;\" | colspan=\"2\" style=\"width:600px;\"| Pauli exclusion principle: No two identical fermions can occupy the same quantum state (bosons can). Mathematically, if two particles are interchanged, fermionic wavefunctions are anti-symmetric, while bosonic wavefunctions are symmetric: where ri is the position of particle i, and s is the spin of the particle. There is no way to keep track of particles physically, labels are only used mathematically to prevent confusion. |} Radiation laws",
                    "score": 0.8791508674621582
                },
                {
                    "id": 17554047,
                    "contents": "Observer effect (physics)\nThe measured momentum of the electron is then related to , whereas its momentum after the measurement is related to . This is a best-case scenario. Electronics In electronics, ammeters and voltmeters are usually wired in series or parallel to the circuit, and so by their very presence affect the current or the voltage they are measuring by way of presenting an additional real or complex load to the circuit, thus changing the transfer function and behavior of the circuit itself. Even a more passive device such as a current clamp, which measures the wire current without coming into physical contact with the wire, affects the current through the circuit being measured because the inductance is mutual. Thermodynamics In thermodynamics, a standard mercury-in-glass thermometer must absorb or give up some thermal energy to record a temperature, and therefore changes the temperature of the body which it is measuring. Quantum mechanics",
                    "score": 0.8790215253829956
                },
                {
                    "id": 358879,
                    "contents": "Lenz's law\ndo not, such as a proton and an electron, the interaction is different. An electron generating a magnetic field would generate an EMF that causes a proton to accelerate in the same direction as the electron. At first, this might seem to violate the law of conservation of momentum, but such an interaction is seen to conserve momentum if the momentum of electromagnetic fields is taken into account.",
                    "score": 0.8787484169006348
                },
                {
                    "id": 1696333,
                    "contents": "Electronvolt\nMomentum In high-energy physics, the electronvolt is often used as a unit of momentum. A potential difference of 1 volt causes an electron to gain an amount of energy (i.e., ). This gives rise to usage of eV (and keV, MeV, GeV or TeV) as units of momentum, for the energy supplied results in acceleration of the particle. The dimensions of momentum units are . The dimensions of energy units are . Then, dividing the units of energy (such as eV) by a fundamental constant that has units of velocity (), facilitates the required conversion of using energy units to describe momentum. In the field of high-energy particle physics, the fundamental velocity unit is the speed of light in vacuum c. By dividing energy in eV by the speed of light, one can describe the momentum of an electron in units of eV/c.",
                    "score": 0.8782287836074829
                },
                {
                    "id": 23606096,
                    "contents": "Relativistic angular momentum\nIn physics, relativistic angular momentum refers to the mathematical formalisms and physical concepts that define angular momentum in special relativity (SR) and general relativity (GR). The relativistic quantity is subtly different from the three-dimensional quantity in classical mechanics.",
                    "score": 0.8780591487884521
                },
                {
                    "id": 1694145,
                    "contents": "Electron\nThe orbital angular momentum of electrons is quantized. Because the electron is charged, it produces an orbital magnetic moment that is proportional to the angular momentum. The net magnetic moment of an atom is equal to the vector sum of orbital and spin magnetic moments of all electrons and the nucleus. The magnetic moment of the nucleus is negligible compared with that of the electrons. The magnetic moments of the electrons that occupy the same orbital (so called, paired electrons) cancel each other out.",
                    "score": 0.8778843283653259
                },
                {
                    "id": 1590076,
                    "contents": "Angular momentum\nAnd thus the angular momentum around the z-axis is conserved. This analysis can be repeated separately for each axis, giving conversation of the angular momentum vector. However, the angles around the three axes cannot be treated simultaneously as generalized coordinates, since they are not independent; in particular, two angles per point suffice to determine its position. While it is true that in the case of a rigid body, fully describing it requires, in addition to three translational degrees of freedom, also specification of three rotational degrees of freedom; however these cannot be defined as rotations around the Cartesian axes (see Euler angles). This caveat is reflected in quantum mechanics in the non-trivial commutation relations of the different components of the angular momentum operator.",
                    "score": 0.8777481913566589
                },
                {
                    "id": 12236137,
                    "contents": "Popper's experiment\nThis will reduce the state of particle 2 to . The momentum uncertainty of particle 2 can now be calculated, and is given by If we go to the extreme limit of slit A being infinitesimally narrow (), the momentum uncertainty of particle 2 is , which is exactly what the momentum spread was to begin with. In fact, one can show that the momentum spread of particle 2, conditioned on particle 1 going through slit A, is always less than or equal to (the initial spread), for any value of , and . Thus, particle 2 does not acquire any extra momentum spread than it already had. This is the prediction of standard quantum mechanics. So, the momentum spread of particle 2 will always be smaller than what was contained in the original beam. This is what was actually seen in the experiment of Kim and Shih. Popper's proposed experiment, if carried out in this way, is incapable of testing the Copenhagen interpretation of quantum mechanics.",
                    "score": 0.8775400519371033
                },
                {
                    "id": 23606099,
                    "contents": "Relativistic angular momentum\nRelativistic angular momentum is less obvious. The classical definition of angular momentum is the cross product of position x with momentum p to obtain a pseudovector , or alternatively as the exterior product to obtain a second order antisymmetric tensor . What does this combine with, if anything? There is another vector quantity not often discussed – it is the time-varying moment of mass polar-vector (not the moment of inertia) related to the boost of the centre of mass of the system, and this combines with the classical angular momentum pseudovector to form an antisymmetric tensor of second order, in exactly the same way as the electric field polar-vector combines with the magnetic field pseudovector to form the electromagnetic field antisymmetric tensor. For rotating mass–energy distributions (such as gyroscopes, planets, stars, and black holes) instead of point-like particles, the angular momentum tensor is expressed in terms of the stress–energy tensor of the rotating object.",
                    "score": 0.8765575885772705
                },
                {
                    "id": 6713750,
                    "contents": "Momentum operator\n(In certain artificial situations, such as the quantum states on the semi-infinite interval , there is no way to make the momentum operator Hermitian. This is closely related to the fact that a semi-infinite interval cannot have translational symmetry—more specifically, it does not have unitary translation operators. See below.) Canonical commutation relation One can easily show that by appropriately using the momentum basis and the position basis: The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables. Fourier transform The following discussion uses the bra–ket notation. One may write so the tilde represents the Fourier transform, in converting from coordinate space to momentum space. It then holds that that is, the momentum acting in coordinate space corresponds to spatial frequency,",
                    "score": 0.8750361204147339
                },
                {
                    "id": 14177622,
                    "contents": "Relativistic quantum mechanics\nExperiments 1897 J. J. Thomson discovers the electron and measures its mass-to-charge ratio. Discovery of the Zeeman effect: the splitting a spectral line into several components in the presence of a static magnetic field. 1908 Millikan measures the charge on the electron and finds experimental evidence of its quantization, in the oil drop experiment. 1911 Alpha particle scattering in the Geiger–Marsden experiment, led by Rutherford, showed that atoms possess an internal structure: the atomic nucleus. 1913 The Stark effect is discovered: splitting of spectral lines due to a static electric field (compare with the Zeeman effect). 1922 Stern–Gerlach experiment: experimental evidence of spin and its quantization. 1924 Stoner studies splitting of energy levels in magnetic fields. 1932 Experimental discovery of the neutron by Chadwick, and positrons by Anderson, confirming the theoretical prediction of positrons.",
                    "score": 0.8748817443847656
                },
                {
                    "id": 552405,
                    "contents": "Wavenumber\nHere p is the momentum of the particle, m is the mass of the particle, E is the kinetic energy of the particle, and ħ is the reduced Planck constant. Wavenumber is also used to define the group velocity. In spectroscopy In spectroscopy, \"wavenumber\" refers to a frequency which has been divided by the speed of light in vacuum usually in centimeters per second (cm.s−1): : The historical reason for using this spectroscopic wavenumber rather than frequency is that it is a convenient unit when studying atomic spectra by counting fringes per cm with an interferometer : the spectroscopic wavenumber is the reciprocal of the wavelength of light in vacuum:",
                    "score": 0.8745795488357544
                },
                {
                    "id": 1590054,
                    "contents": "Angular momentum\nwhere is the moment of inertia for a point mass, is the orbital angular velocity in radians/sec (units 1/sec) of the particle about the origin, is the position vector of the particle relative to the origin, , is the linear velocity of the particle relative to the origin, and is the mass of the particle. This can be expanded, reduced, and by the rules of vector algebra, rearranged: which is the cross product of the position vector and the linear momentum of the particle. By the definition of the cross product, the vector is perpendicular to both and . It is directed perpendicular to the plane of angular displacement, as indicated by the right-hand rule – so that the angular velocity is seen as counter-clockwise from the head of the vector. Conversely, the vector defines the plane in which and lie.",
                    "score": 0.8742902874946594
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_12",
        "question": "Using the Bohr theory, calculate the ionization energy (in electron volts and in $\\mathrm{kJ} \\cdot \\mathrm{mol}^{-1}$ ) of singly ionized helium.",
        "golden_answers": [
            " 54.394"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 15921199,
                    "contents": "Helium atom\nWhere again, E1 represents the ionization energy of hydrogen. By using more complicated/accurate wave functions, the ground state energy of helium has been calculated closer and closer to the experimental value −78.95 eV. The variational approach has been refined to very high accuracy for a comprehensive regime of quantum states by G.W.F. Drake and co-workers as well as J.D. Morgan III, Jonathan Baker and Robert Hill using Hylleraas or Frankowski-Pekeris basis functions. One needs to include relativistic and quantum electrodynamic corrections to get full agreement with experiment to spectroscopic accuracy. Experimental value of ionization energy Helium's first ionization energy is −24.587387936(25) eV. This value was derived by experiment. The theoretic value of Helium atom's second ionization energy is −54.41776311(2) eV. The total ground state energy of the helium atom is −79.005151042(40) eV, or −2.90338583(13) Atomic units a.u., which equals −5.80677166 (26) Ry. See also",
                    "score": 0.8929277658462524
                },
                {
                    "id": 1620509,
                    "contents": "Bohr model\nThe Bohr formula properly uses the reduced mass of electron and proton in all situations, instead of the mass of the electron, However, these numbers are very nearly the same, due to the much larger mass of the proton, about 1836.1 times the mass of the electron, so that the reduced mass in the system is the mass of the electron multiplied by the constant 1836.1/(1+1836.1) = 0.99946. This fact was historically important in convincing Rutherford of the importance of Bohr's model, for it explained the fact that the frequencies of lines in the spectra for singly ionized helium do not differ from those of hydrogen by a factor of exactly 4, but rather by 4 times the ratio of the reduced mass for the hydrogen vs. the helium systems, which was much closer to the experimental ratio than exactly 4.",
                    "score": 0.8863059878349304
                },
                {
                    "id": 15921186,
                    "contents": "Helium atom\nA helium atom is an atom of the chemical element helium. Helium is composed of two electrons bound by the electromagnetic force to a nucleus containing two protons along with either one or two neutrons, depending on the isotope, held together by the strong force. Unlike for hydrogen, a closed-form solution to the Schrödinger equation for the helium atom has not been found. However, various approximations, such as the Hartree–Fock method, can be used to estimate the ground state energy and wavefunction of the atom. Introduction The quantum mechanical description of the helium atom is of special interest, because it is the simplest multi-electron system and can be used to understand the concept of quantum entanglement. The Hamiltonian of helium, considered as a three-body system of two electrons and a nucleus and after separating out the centre-of-mass motion, can be written as",
                    "score": 0.8858829140663147
                },
                {
                    "id": 15921195,
                    "contents": "Helium atom\nWhere and use the wave functions for the hydrogen Hamiltonian. For helium, Z = 2 from where E = −4 a.u. which is approximately −108.8 eV, which corresponds to an ionization potential V = 2 a.u. (≅54.4 eV). The experimental values are E = −2.90 a.u. (≅ −79.0 eV) and V = 0.90 a.u. (≅ 24.6 eV). The energy that we obtained is too low because the repulsion term between the electrons was ignored, whose effect is to raise the energy levels. As Z gets bigger, our approach should yield better results, since the electron-electron repulsion term will get smaller. So far a very crude independent-particle approximation has been used, in which the electron-electron repulsion term is completely omitted. Splitting the Hamiltonian showed below will improve the results: where and",
                    "score": 0.8791914582252502
                },
                {
                    "id": 23955920,
                    "contents": "Helium compounds\nThe HeHN molecule is linear. The He-H bondlength is 1.72 Å. It has an infrared band, due to B-H stretching, with a base at 3158.42 cm−1. The binding energy is 378 cm−1 in the 000 vibrational state, and 431 cm−1 in the 100 vibrational state. He2HN is also known. One helium atom is linked to a hydrogen, and the other is less tightly bound. H2O+, H2OSF5+, SF5+ and SF6+ can form clusters with varying numbers of Helium atoms. Excimers The He excimer is responsible for the Hopfield continuum. Helium also forms an excimer with barium, Ba+He*. Predicted compounds Predicted solids is predicted to form a solid with orthorhombic structure Ibam.",
                    "score": 0.8787660598754883
                },
                {
                    "id": 15921193,
                    "contents": "Helium atom\nwhere the zero-order unperturbed Hamiltonian is while the perturbation term: is the electron-electron interaction. H0 is just the sum of the two hydrogenic Hamiltonians: where Eni, the energy eigenvalues and , the corresponding eigenfunctions of the hydrogenic Hamiltonian will denote the normalized energy eigenvalues and the normalized eigenfunctions. So: where Neglecting the electron-electron repulsion term, the Schrödinger equation for the spatial part of the two-electron wave function will reduce to the 'zero-order' equation This equation is separable and the eigenfunctions can be written in the form of single products of hydrogenic wave functions: The corresponding energies are (in atomic units, hereafter a.u.): Note that the wave function",
                    "score": 0.8768209218978882
                },
                {
                    "id": 15921198,
                    "contents": "Helium atom\nAfter calculating the expectation value of and Vee the expectation value of the Hamiltonian becomes: The minimum value of Z needs to be calculated, so taking a derivative with respect to Z and setting the equation to 0 will give the minimum value of Z: This shows that the other electron somewhat shields the nucleus reducing the effective charge from 2 to 1.69. So we obtain the most accurate result yet: Where again, E1 represents the ionization energy of hydrogen.",
                    "score": 0.8757829666137695
                },
                {
                    "id": 15921192,
                    "contents": "Helium atom\nNevertheless, quite good theoretical descriptions of helium can be obtained within the Hartree–Fock and Thomas–Fermi approximations (see below). The Hartree–Fock method is used for a variety of atomic systems. However it is just an approximation, and there are more accurate and efficient methods used today to solve atomic systems. The \"many-body problem\" for helium and other few electron systems can be solved quite accurately. For example, the ground state of helium is known to fifteen digits. In Hartree–Fock theory, the electrons are assumed to move in a potential created by the nucleus and the other electrons. Perturbation method The Hamiltonian for helium with two electrons can be written as a sum of the Hamiltonians for each electron: where the zero-order unperturbed Hamiltonian is while the perturbation term: is the electron-electron interaction. H0 is just the sum of the two hydrogenic Hamiltonians: where",
                    "score": 0.8744007349014282
                },
                {
                    "id": 20458026,
                    "contents": "Variational method (quantum mechanics)\nHelium atom ground state The helium atom consists of two electrons with mass m and electric charge −e, around an essentially fixed nucleus of mass M ≫ m and charge +2e. The Hamiltonian for it, neglecting the fine structure, is: where ħ is the reduced Planck constant, ε0 is the vacuum permittivity, ri (for i = 1, 2) is the distance of the i-th electron from the nucleus, and |r1 − r2| is the distance between the two electrons. If the term Vee = e2/(4πε0|r1 − r2|), representing the repulsion between the two electrons, were excluded, the Hamiltonian would become the sum of two hydrogen-like atom Hamiltonians with nuclear charge +2e. The ground state energy would then be 8E1 = −109 eV, where E1 is the Rydberg constant, and its ground state wavefunction would be the product of two wavefunctions for the ground state of hydrogen-like atoms:",
                    "score": 0.8732121586799622
                },
                {
                    "id": 23955929,
                    "contents": "Helium compounds\nHeBeO+ is predicted to have a relatively high binding energy of 25 kcal mol−1. HCHe+ HCHeHe+ For negative ions the adduct is very weakly bound. Those studied include HeCl−, HeBr−, HeF−, HeO− and HeS−. FHeS− FHeSe− C7H6He2+ C7H6HeHe2+ FHeCC− HHeOH HHeBF+ HeNC+ HeNN+ HHeNN+ H-He 0.765 Å He-N bond length 2.077 Å. Decomposition barrier of 2.3 kJ/mol. HHeNH is predicted to have a C3v symmetry and a H-He bond length of 0.768 Å and He-N 1.830. The energy barrier against decomposition to ammonium is 19.1 kJ/mol with an energy release of 563.4 kJ/mol. Decomposition to hydrohelium ion and ammonium releases 126.2 kJ/mol.",
                    "score": 0.8725463151931763
                },
                {
                    "id": 1753383,
                    "contents": "Helium\nIn the perspective of quantum mechanics, helium is the second simplest atom to model, following the hydrogen atom. Helium is composed of two electrons in atomic orbitals surrounding a nucleus containing two protons and (usually) two neutrons. As in Newtonian mechanics, no system that consists of more than two particles can be solved with an exact analytical mathematical approach (see 3-body problem) and helium is no exception. Thus, numerical mathematical methods are required, even to solve the system of one nucleus and two electrons. Such computational chemistry methods have been used to create a quantum mechanical picture of helium electron binding which is accurate to within < 2% of the correct value, in a few computational steps. Such models show that each electron in helium partly screens the nucleus from the other, so that the effective nuclear charge Z which each electron sees, is about 1.69 units, not the 2 charges of a classic \"bare\" helium nucleus.",
                    "score": 0.8725463151931763
                },
                {
                    "id": 23470616,
                    "contents": "Helium dimer\nThe negative helium dimer He2− is metastable and was discovered by Bae, Coggiola and Peterson in 1984 by passing He2+ through cesium vapor. Subsequently, H. H. Michels theoretically confirmed its existence and concluded that the 4Πg state of He2− is bound relative to the a2Σ+u state of He2. The calculated electron affinity is 0.233 eV compared to 0.077 eV for the He−[4P∘] ion. The He2− decays through the long-lived 5/2g component with τ~350 μsec and the much shorter-lived 3/2g, 1/2g components with τ~10 μsec. The 4Πg state has a 1σ2g1σu2σg2πu electronic configuration, its electron affinity E is 0.18±0.03 eV, and its lifetime is 135±15 μsec; only the v=0 vibrational state is responsible for this long-lived state.",
                    "score": 0.8686308860778809
                },
                {
                    "id": 1753372,
                    "contents": "Helium\nIn 1938, Russian physicist Pyotr Leonidovich Kapitsa discovered that helium-4 has almost no viscosity at temperatures near absolute zero, a phenomenon now called superfluidity. This phenomenon is related to Bose–Einstein condensation. In 1972, the same phenomenon was observed in helium-3, but at temperatures much closer to absolute zero, by American physicists Douglas D. Osheroff, David M. Lee, and Robert C. Richardson. The phenomenon in helium-3 is thought to be related to pairing of helium-3 fermions to make bosons, in analogy to Cooper pairs of electrons producing superconductivity. Extraction and use",
                    "score": 0.8681426048278809
                },
                {
                    "id": 23470615,
                    "contents": "Helium dimer\nThe helium dication dimer He22+ is extremely repulsive and would release much energy when it dissociated, around 835 kJ/mol. Dynamical stability of the ion was predicted by Linus Pauling. An energy barrier of 33.2 kcal/mol prevents immediate decay. This ion is isoelectronic with the hydrogen molecule. He22+ is the smallest possible molecule with a double positive charge. It is detectable using mass spectroscopy.",
                    "score": 0.8662502765655518
                },
                {
                    "id": 15921191,
                    "contents": "Helium atom\nIf the electron-electron interaction term is included, the Schrödinger equation is non separable. However, also if is neglected, all states described above (even with two identical quantum numbers, like with ) cannot be written as a product of one-electron wave functions: — the wave function is entangled. One cannot say, particle 1 is in state 1 and the other in state 2, and measurements cannot be made on one particle without affecting the other. Nevertheless, quite good theoretical descriptions of helium can be obtained within the Hartree–Fock and Thomas–Fermi approximations (see below).",
                    "score": 0.8640673756599426
                },
                {
                    "id": 1753407,
                    "contents": "Helium\nHelium has a valence of zero and is chemically unreactive under all normal conditions. It is an electrical insulator unless ionized. As with the other noble gases, helium has metastable energy levels that allow it to remain ionized in an electrical discharge with a voltage below its ionization potential. Helium can form unstable compounds, known as excimers, with tungsten, iodine, fluorine, sulfur, and phosphorus when it is subjected to a glow discharge, to electron bombardment, or reduced to plasma by other means. The molecular compounds HeNe, HgHe10, and WHe2, and the molecular ions , , , and have been created this way. HeH+ is also stable in its ground state, but is extremely reactive—it is the strongest Brønsted acid known, and therefore can exist only in isolation, as it will protonate any molecule or counteranion it contacts. This technique has also produced the neutral molecule He2, which has a large number of band systems, and HgHe, which is apparently held together only by",
                    "score": 0.8625322580337524
                },
                {
                    "id": 1620506,
                    "contents": "Bohr model\nThe smallest possible value of r in the hydrogen atom () is called the Bohr radius and is equal to: The energy of the n-th level for any atom is determined by the radius and quantum number: An electron in the lowest energy level of hydrogen () therefore has about 13.6 eV less energy than a motionless electron infinitely far from the nucleus. The next energy level () is −3.4 eV. The third (3) is −1.51 eV, and so on. For larger values of n, these are also the binding energies of a highly excited atom with one electron in a large circular orbit around the rest of the atom. The hydrogen formula also coincides with the Wallis product. The combination of natural constants in the energy formula is called the Rydberg energy (RE): This expression is clarified by interpreting it in combinations that form more natural units: is the rest mass energy of the electron (511 keV), is the fine-structure constant, .",
                    "score": 0.8622780442237854
                },
                {
                    "id": 23470619,
                    "contents": "Helium dimer\nThe first clues that dihelium exists were noticed in 1900 when W. Heuse observed a band spectrum in a helium discharge. However, no information about the nature of the spectrum was published. Independently E. Goldstein from Germany and W. E. Curtis from London published details of the spectrum in 1913. Curtis was called away to military service in World War I, and the study of the spectrum was continued by Alfred Fowler. Fowler recognised that the double headed bands fell into two sequences analogous to principal and diffuse series in line spectra.",
                    "score": 0.8615494966506958
                },
                {
                    "id": 20458027,
                    "contents": "Variational method (quantum mechanics)\nwhere a0 is the Bohr radius and Z = 2, helium's nuclear charge. The expectation value of the total Hamiltonian H (including the term Vee) in the state described by ψ0 will be an upper bound for its ground state energy. <Vee> is −5E1/2 = 34 eV, so <H> is 8E1 − 5E1/2 = −75 eV. A tighter upper bound can be found by using a better trial wavefunction with 'tunable' parameters. Each electron can be thought to see the nuclear charge partially \"shielded\" by the other electron, so we can use a trial wavefunction equal with an \"effective\" nuclear charge Z < 2: The expectation value of H in this state is: This is minimal for Z = 27/16 implying shielding reduces the effective charge to ~1.69. Substituting this value of Z into the expression for H yields 729E1/128 = −77.5 eV, within 2% of the experimental value, −78.975 eV.",
                    "score": 0.8611785173416138
                },
                {
                    "id": 21790229,
                    "contents": "William Clyde Martin Jr.\nAnother of Martin's abiding concerns was the accurate description of the spectrum of atomic helium. This spectrum is of particular importance as a calibration reference, a benchmark for theoretical calculations of atomic structure and of quantum electrodynamics effects in atoms, and for applications in astrophysics. Martin's publications on this spectrum, beginning in his early years at NBS and spanning five decades, remain the definitive sources of reference data as of 2013. Honors Department of Commerce Silver Medal (1968) Department of Commerce Gold Medal (1981) William F. Meggers Award, Optical Society of America (1983) Allen V. Astin Measurement Science Award, National Institute of Standards and Technology (1992) Office of Measurement Services Award, National Institute of Standards and Technology (1997) Fellow, American Association for the Advancement of Science Fellow, American Physical Society Fellow, Optical Society of America Obituaries References",
                    "score": 0.861175537109375
                },
                {
                    "id": 4294148,
                    "contents": "Helium-4\nIt is theorized that at 0.2 K and 50 atm, solid helium-4 may be a superglass (an amorphous solid exhibiting superfluidity). Helium-4 also exists on the Moon and—as on Earth—it is the most abundant helium isotope. The helium-4 atom The helium atom is the second simplest atom (hydrogen is the simplest), but the extra electron introduces a third \"body\", so the solution to its wave equation becomes a \"three-body problem\", which has no analytic solution. However, numerical approximations of the equations of quantum mechanics have given a good estimate of the key atomic properties of , such as its size and ionization energy. The size of the 4He nucleus has long been known to be in the order of magnitude of 1 fm. In an experiment involving the use of exotic helium atoms where an atomic electron was replaced by a muon, the nucleus size has been estimated to be 1.67824(83) fm.",
                    "score": 0.8606919050216675
                },
                {
                    "id": 431662,
                    "contents": "Rydberg constant\nSince the Bohr model is not perfectly accurate, due to fine structure, hyperfine splitting, and other such effects, the Rydberg constant cannot be directly measured at very high accuracy from the atomic transition frequencies of hydrogen alone. Instead, the Rydberg constant is inferred from measurements of atomic transition frequencies in three different atoms (hydrogen, deuterium, and antiprotonic helium). Detailed theoretical calculations in the framework of quantum electrodynamics are used to account for the effects of finite nuclear mass, fine structure, hyperfine splitting, and so on. Finally, the value of is determined from the best fit of the measurements to the theory. Alternative expressions The Rydberg constant can also be expressed as in the following equations. and in energy units",
                    "score": 0.8596663475036621
                },
                {
                    "id": 1129313,
                    "contents": "Niels Bohr\nwhere me is the electron's mass, e is its charge, h is Planck's constant and Z is the atom's atomic number (1 for hydrogen). The model's first hurdle was the Pickering series, lines which did not fit Balmer's formula. When challenged on this by Alfred Fowler, Bohr replied that they were caused by ionised helium, helium atoms with only one electron. The Bohr model was found to work for such ions. Many older physicists, like Thomson, Rayleigh and Hendrik Lorentz, did not like the trilogy, but the younger generation, including Rutherford, David Hilbert, Albert Einstein, Enrico Fermi, Max Born and Arnold Sommerfeld saw it as a breakthrough. The trilogy's acceptance was entirely due to its ability to explain phenomena which stymied other models, and to predict results that were subsequently verified by experiments. Today, the Bohr model of the atom has been superseded, but is still the best known model of the atom, as it often appears in high school physics and chemistry texts.",
                    "score": 0.8594112396240234
                },
                {
                    "id": 1565316,
                    "contents": "Atomic orbital\nThe Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the n = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the n = 1 state can hold one or two electrons, while the n = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all n = 1 states are fully occupied; the same is true for n = 1 and n = 2 in neon. In argon, the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon",
                    "score": 0.859133243560791
                },
                {
                    "id": 1115878,
                    "contents": "Ionization energy\nGroup 1: Hydrogen's ionization energy is very high (at 13.59844 eV), compared to the alkali metals. This is due to its single electron (and hence, very small electron cloud), which is close to the nucleus. Likewise, since there aren't any other electrons that may cause shielding, that single electron experiences the full net positive charge of the nucleus. Francium's ionization energy is higher than the precedent alkali metal, cesium. This is due to its (and radium's) small ionic radii owing to relativistic effects. Because of their large mass and size, this means that its electrons are traveling at extremely high speeds which results in the electrons coming closer to the nucleus than expected, and they are consequently harder to remove (higher IE).",
                    "score": 0.8589053153991699
                },
                {
                    "id": 4294152,
                    "contents": "Helium-4\nFor example, the stability and low energy of the electron cloud of helium causes helium's chemical inertness (the most extreme of all the elements), and also the lack of interaction of helium atoms with each other (producing the lowest melting and boiling points of all the elements).",
                    "score": 0.8588244915008545
                },
                {
                    "id": 23955913,
                    "contents": "Helium compounds\nKnown ions Helium has the highest ionisation energy, so a He+ ion will strip electrons off any other neutral atom or molecule. However it can also then bind to the ion produced. The He+ ion can be studied in gas, or in liquid helium. Its chemistry is not completely trivial. For example, He+ can react with SF6 to yield SF or SF and atomic fluorine. Ionised clusters He was predicted to exist by Linus Pauling in 1933. It was discovered when doing mass spectroscopy on ionised helium. The dihelium cation is formed by an ionised helium atom combining with a helium atom: He+ + He → He. The diionised dihelium He (1Σ) is in a singlet state. It breaks up He → He+ + He+ releasing 200 kcal/mol of energy. It has a barrier to decomposition of 35 kcal/mol and a bond length of 0.70 Å. The trihelium cation He is in equilibrium with He between 135 and 200K",
                    "score": 0.8586658835411072
                },
                {
                    "id": 9730907,
                    "contents": "Helium hydride ion\nHistory Discovery in ionization experiments Hydridohelium(1+), specifically , was first detected indirectly in 1925 by T. R. Hogness and E. G. Lunn. They were injecting protons of known energy into a rarefied mixture of hydrogen and helium, in order to study the formation of hydrogen ions like , and . They observed that appeared at the same beam energy (16 eV) as , and its concentration increased with pressure much more than that of the other two ions. From these data, they concluded that the ions were transferring a proton to molecules that they collided with, including helium. In 1933, K. Bainbridge used mass spectrometry to compare the masses of the ions (helium hydride ion) and (twice-deuterated trihydrogen ion) in order to obtain an accurate measurement of the atomic mass of deuterium relative to that of helium. Both ions have 3 protons, 2 neutrons, and 2 electrons. He also compared (helium deuteride ion) with (trideuterium ion), both with 3 protons and 3 neutrons.",
                    "score": 0.8585075736045837
                },
                {
                    "id": 1620507,
                    "contents": "Bohr model\nThis expression is clarified by interpreting it in combinations that form more natural units: is the rest mass energy of the electron (511 keV), is the fine-structure constant, . Since this derivation is with the assumption that the nucleus is orbited by one electron, we can generalize this result by letting the nucleus have a charge , where Z is the atomic number. This will now give us energy levels for hydrogenic (hydrogen-like) atoms, which can serve as a rough order-of-magnitude approximation of the actual energy levels. So for nuclei with Z protons, the energy levels are (to a rough approximation): The actual energy levels cannot be solved analytically for more than one electron (see n-body problem) because the electrons are not only affected by the nucleus but also interact with each other via the Coulomb Force.",
                    "score": 0.8582825064659119
                },
                {
                    "id": 22555701,
                    "contents": "Leon Pape\n1998 : \"Superfluidity in Helium Three: The Discovery Through the Eyes of a Graduate Student\" Douglas D. Osherfoff, Nobel Laureate in Physics, 1996, MacArthur Fellow, 1981-1986, Professor of Physics and Applied Physics, Stanford University 1999 : \"Holding onto Atoms and Molecules with Laser Light\" Steven Chu, Nobel Laureate in Physics, 1997, Theodore and Frances Geballe Professor of Physics and Applied Physics, Stanford University 2000 : \"Electronic Structure of Matter: Wave Functions and Density Functionals\" Walter Kohn, Nobel Laureate in Chemistry, 1998, Professor of Physics, Emeritus, and Research Professor, University of California, Santa Barbara 2001 : \"Viruses: The Essence of Life, but Sneaky Critters\" David Baltimore, Nobel Laureate in Physiology or Medicine, 1975, National Medal of Science Recipient, 1999, President, California Institute of Technology 2002 : \"Freezing Time\"",
                    "score": 0.8577964305877686
                },
                {
                    "id": 9730903,
                    "contents": "Helium hydride ion\nThe masses in each row above are not equal, though, because the binding energies in the nuclei are different. Neutral molecule Unlike the helium hydride ion, the neutral helium hydride molecule HeH is not stable in the ground state. However, it does exist in an excited state as an excimer (HeH*), and its spectrum was first observed in the mid-1980s. The neutral molecule is the first entry in the Gmelin database. Chemical properties and reactions Preparation Since HeH+ cannot be stored in any usable form, its chemistry must be studied by forming it in situ. Reactions with organic substances, for example, can be studied by creating a tritium derivative of the desired organic compound. Decay of tritium to 3He+ followed by its extraction of a hydrogen atom yields 3HeH+ which is then surrounded by the organic material and will in turn react.",
                    "score": 0.8577631711959839
                },
                {
                    "id": 1753370,
                    "contents": "Helium\nIn 1913, Niels Bohr published his \"trilogy\" on atomic structure that included a reconsideration of the Pickering–Fowler series as central evidence in support of his model of the atom. This series is named for Edward Charles Pickering, who in 1896 published observations of previously unknown lines in the spectrum of the star ζ Puppis (these are now known to occur with Wolf–Rayet and other hot stars). Pickering attributed the observation (lines at 4551, 5411, and 10123 Å) to a new form of hydrogen with half-integer transition levels. In 1912, Alfred Fowler managed to produce similar lines from a hydrogen-helium mixture, and supported Pickering's conclusion as to their origin. Bohr's model does not allow for half-integer transitions (nor does quantum mechanics) and Bohr concluded that Pickering and Fowler were wrong, and instead assigned these spectral lines to ionised helium, He+. Fowler was initially skeptical but was ultimately convinced that Bohr was correct, and by 1915",
                    "score": 0.8577353358268738
                },
                {
                    "id": 9730908,
                    "contents": "Helium hydride ion\nEarly theoretical studies The first attempt to compute the structure of the HeH+ ion (specifically, ) by quantum mechanical theory was made by J. Beach in 1936. Improved computations were sporadically published over the next decades. Tritium decay methods in chemistry H. Schwartz observed in 1955 that the decay of the tritium molecule = should generate the helium hydride ion with high probability. In 1963, F. Cacace at the Sapienza University of Rome conceived the decay technique for preparing and studying organic radicals and carbenium ions. In a variant of that technique, the exotic species like the methonium cation are produced by reacting organic compounds with the that is produced by the decay of that is mixed with the desired reagents. Much of what we know about the chemistry of came through this technique.",
                    "score": 0.8577114343643188
                },
                {
                    "id": 2155569,
                    "contents": "Nonmetal\nFor hydrogen and helium, and from boron to neon, since the 1s and 2p subshells have no inner analogues (i.e., there is no zeroth shell and no 1p subshell) and therefore experience no electron repulsion effects, they have relatively small radii, unlike the 3p, 4p and 5p subshells of heavier elements. Ionization energies and electronegativities among these elements are consequently higher than would otherwise be expected, having regard to periodic trends. The small atomic radii of carbon, nitrogen, and oxygen facilitate the formation of double or triple bonds.",
                    "score": 0.8573140501976013
                },
                {
                    "id": 1753361,
                    "contents": "Helium\nHelium (from ) is a chemical element with the symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas and the first in the noble gas group in the periodic table. Its boiling and melting point are the lowest among all the elements. It is the second lightest and second most abundant element in the observable universe (hydrogen is the lightest and most abundant). It is present at about 24% of the total elemental mass, which is more than 12 times the mass of all the heavier elements combined. Its abundance is similar to this in both the Sun and in Jupiter, due to the very high nuclear binding energy (per nucleon) of helium-4, with respect to the next three elements after helium. This helium-4 binding energy also accounts for why it is a product of both nuclear fusion and radioactive decay. Most helium in the universe is helium-4, the vast majority of which was formed during the Big Bang. Large amounts of new helium are created by nuclear",
                    "score": 0.8566453456878662
                },
                {
                    "id": 6144971,
                    "contents": "Roland Dobbs\nHe was a member of the Physics Committee of the Science Research Council and of its Nuclear Physics Board, and a member of the Physics Committee of the Science and Engineering Research Council. He was Convener, Standing Conference of Professors of Physics of Great Britain, 1985–1988. His research interests were in various areas of condensed matter physics: metals, superconductors, condensed inert gases, and latterly Helium 3 as a Fermi liquid and superfluid. He was noted for his research on the physics of helium-3, including its behavior as a superfluid, and published what was regarded as the definitive monograph 'Helium 3' in its various states in 2000 (Oxford University Press). Other publications include: Solid Helium Three (Oxford University Press): 1993, Electricity and Magnetism (Routledge & Kegan Paul):1984 and Basic Electromagnetism (Chapman & Hall): 1993.",
                    "score": 0.8562329411506653
                },
                {
                    "id": 15921187,
                    "contents": "Helium atom\nwhere is the reduced mass of an electron with respect to the nucleus, and are the electron-nucleus distance vectors and . The nuclear charge, is 2 for helium. In the approximation of an infinitely heavy nucleus, we have and the mass polarization term disappears. In atomic units the Hamiltonian simplifies to",
                    "score": 0.8559662103652954
                },
                {
                    "id": 13333115,
                    "contents": "Hydrogen-like atom\nNegative-energy solutions to Dirac's equation exist even in the absence of a Coulomb force exerted by a nucleus. Dirac hypothesized that we can consider almost all of these states to be already filled. If one of these negative-energy states is not filled, this manifests itself as though there is an electron which is repelled by a positively-charged nucleus. This prompted Dirac to hypothesize the existence of positively-charged electrons, and his prediction was confirmed with the discovery of the positron. Beyond Gordon's solution to the Dirac equation The Dirac equation with a simple Coulomb potential generated by a point-like non-magnetic nucleus was not the last word, and its predictions differ from experimental results as mentioned earlier. More accurate results include the Lamb shift (radiative corrections arising from quantum electrodynamics) and hyperfine structure. See also Rydberg atom Positronium Exotic atom Two-electron atom Hydrogen molecular ion Notes",
                    "score": 0.8555569052696228
                },
                {
                    "id": 1772933,
                    "contents": "Hydrogen atom\nFor , the value is called the Rydberg unit of energy. It is related to the Rydberg constant of atomic physics by The exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) which have finite mass, the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. This includes the kinetic energy of the nucleus in the problem, because the total (electron plus nuclear) kinetic energy is equivalent to the kinetic energy of the reduced mass moving with a velocity equal to the electron velocity relative to the nucleus. However, since the nucleus is much heavier than the electron, the electron mass and reduced mass are nearly the same. The Rydberg constant RM for a hydrogen atom (one electron), R is given by",
                    "score": 0.8547991514205933
                },
                {
                    "id": 23955927,
                    "contents": "Helium compounds\nLiHe2 is predicted to be in an Efimov state when excited. Predicted ions Many ions have been investigated theoretically to see if they could exist. Just about every diatomic cation with helium has been studied. For the diatomic dications, for stability the second ionisation level of the partner atom has to be below the first ionisation level of helium, 24.6 eV. For Li, F, and Ne the ground state is repulsive, so molecules will not form. For N and O the molecule would break up to release He+. However HeBe2+, HeB2+ and HeC2+ are predicted to be stable. Also second row elements from Na to Cl are predicted to have a stable HeX2+ ion. HeY3+ is predicted to be the lightest stable diatomic triply charged ion. Other possibly thermochemically stable ions include HeZr3+, HeHf3+, HeLa3+, HeNd3+, HeCe3+, HePr3+, HePm3+, HeSm3+, HeGa3+, HeTb3+, HeDy3+, HeHo3+, HeEr3+, HeTm3+, and HeLu3+ where the third ionisation point is below that of helium.",
                    "score": 0.8546209335327148
                },
                {
                    "id": 431658,
                    "contents": "Rydberg constant\nThe constant is expressed for either hydrogen as , or at the limit of infinite nuclear mass as . In either case, the constant is used to express the limiting value of the highest wavenumber (inverse wavelength) of any photon that can be emitted from an atom, or, alternatively, the wavenumber of the lowest-energy photon capable of ionizing an atom from its ground state. The hydrogen spectral series can be expressed simply in terms of the Rydberg constant for hydrogen and the Rydberg formula. In atomic physics, Rydberg unit of energy, symbol Ry, corresponds to the energy of the photon whose wavenumber is the Rydberg constant, i.e. the ionization energy of the hydrogen atom in a simplified Bohr model. Value Rydberg constant The CODATA value is , where is the rest mass of the electron, is the elementary charge, is the permittivity of free space, is the Planck constant, and is the speed of light in vacuum.",
                    "score": 0.854258120059967
                },
                {
                    "id": 1775536,
                    "contents": "Helium-3\nPhysical properties Because of its low atomic mass of 3.02 atomic mass units, helium-3 has some physical properties different from those of helium-4, with a mass of 4.00 atomic mass units. Because of the weak, induced dipole–dipole interaction between the helium atoms, their microscopic physical properties are mainly determined by their zero-point energy. Also, the microscopic properties of helium-3 cause it to have a higher zero-point energy than helium-4. This implies that helium-3 can overcome dipole–dipole interactions with less thermal energy than helium-4 can. The quantum mechanical effects on helium-3 and helium-4 are significantly different because with two protons, two neutrons, and two electrons, helium-4 has an overall spin of zero, making it a boson, but with one fewer neutron, helium-3 has an overall spin of one half, making it a fermion.",
                    "score": 0.8538440465927124
                },
                {
                    "id": 25588973,
                    "contents": "Arie Bijl\nAfter his PhD, he remained affiliated with Leiden University and conducted research on liquid helium, among other things. A publication by him in 1940 led to a wave function developed by him now known as the Bijl-Dingle-Jastrow wave function that is still used. Also the Bijl-Feynman spectrum and the Bijl-Jastrow factor are named after him. In the crisis years Arie Bijl was a member of a group of scientists around the future Nobel laureate Jan Tinbergen. The reason was that Tinbergen wanted to apply physical principles to economic problems. Bijl corresponded about economics with Albert Einstein and wrote the book Werkgelegenheidspolitiek: ordening in een vrije economie (Employment politics: order in a free economy) that was posthumously published by De Arbeiderspers in 1953 with a foreword by Tinbergen.",
                    "score": 0.8534949421882629
                },
                {
                    "id": 1772936,
                    "contents": "Hydrogen atom\nobserved phenomena, such as the anomalous Zeeman effect, remained unexplained. These issues were resolved with the full development of quantum mechanics and the Dirac equation. It is often alleged that the Schrödinger equation is superior to the Bohr–Sommerfeld theory in describing hydrogen atom. This is not the case, as most of the results of both approaches coincide or are very close (a remarkable exception is the problem of hydrogen atom in crossed electric and magnetic fields, which cannot be self-consistently solved in the framework of the Bohr–Sommerfeld theory), and in both theories the main shortcomings result from the absence of the electron spin. It was the complete failure of the Bohr–Sommerfeld theory to explain many-electron systems (such as helium atom or hydrogen molecule) which demonstrated its inadequacy in describing quantum phenomena.",
                    "score": 0.8532644510269165
                },
                {
                    "id": 17531952,
                    "contents": "Hooke's atom\nThese directly force and respectively, and as a consequence of the three term recession, all higher coefficients also vanish. Solving for and yields and the radial wave function Transforming back to the ground-state (with and energy ) is finally Combining, normalizing, and transforming back to the original coordinates yields the ground state wave function: The corresponding ground-state total energy is then . Remarks The exact ground state electronic density of the Hooke atom for the special case is From this we see that the radial derivative of the density vanishes at the nucleus. This is in stark contrast to the real (non-relativistic) helium atom where the density displays a cusp at the nucleus as a result of the unbounded Coulomb potential. See also List of quantum-mechanical systems with analytical solutions References Further reading Quantum chemistry Quantum mechanics Quantum models",
                    "score": 0.853094756603241
                },
                {
                    "id": 1753371,
                    "contents": "Helium\nthat Pickering and Fowler were wrong, and instead assigned these spectral lines to ionised helium, He+. Fowler was initially skeptical but was ultimately convinced that Bohr was correct, and by 1915 \"spectroscopists had transferred [the Pickering–Fowler series] definitively [from hydrogen] to helium.\" Bohr's theoretical work on the Pickering series had demonstrated the need for \"a re-examination of problems that seemed already to have been solved within classical theories\" and provided important confirmation for his atomic theory.",
                    "score": 0.8525735139846802
                },
                {
                    "id": 15921197,
                    "contents": "Helium atom\nThe variational method To obtain a more accurate energy the variational principle can be applied to the electron-electron potential Vee using the wave function : After integrating this, the result is: This is closer to the experimental value, but if a better trial wave function is used, an even more accurate answer could be obtained. An ideal wave function would be one that doesn't ignore the influence of the other electron. In other words, each electron represents a cloud of negative charge which somewhat shields the nucleus so that the other electron actually sees an effective nuclear charge Z that is less than 2. A wave function of this type is given by: Treating Z as a variational parameter to minimize H. The Hamiltonian using the wave function above is given by: After calculating the expectation value of and Vee the expectation value of the Hamiltonian becomes:",
                    "score": 0.8519269227981567
                },
                {
                    "id": 23955916,
                    "contents": "Helium compounds\nNeHe can be made by ultraviolet photoionisation. Clusters only contain one neon atom. The number of helium atoms n can vary from 1 to 23, but NeHe and NeHe are more likely to be observed. Doubly charged ions of helium with noble gas atoms also exist including ArHe2+, KrHe2+, and XeHe2+. Metals Various metal-helium ions are known. Alkali metal helide ions are known for all the alkalis. The molecule ground state for the diatomic ions is in the X1Σ+ state. The bond length gets bigger as the periodic table is descended with lengths of 1.96, 2.41, 2.90, 3.10, and 3.38 Å for Li+He, Na+He, K+He, Rb+He, and Cs+He. The dissociation energies are 1.9, 0.9, 0.5, 0.4 and 0.3 kcal/mol, showing bond energy decreases. When the molecule breaks up the positive charge is never on the helium atom.",
                    "score": 0.8519008755683899
                },
                {
                    "id": 11242072,
                    "contents": "Timeline of chemistry\n1937Pyotr Kapitsa, John Allen and Don Misener produce supercooled helium-4, the first zero-viscosity superfluid, a substance that displays quantum mechanical properties on a macroscopic scale. 1938Otto Hahn discovers the process of nuclear fission in uranium and thorium. 1939Linus Pauling publishes The Nature of the Chemical Bond, a compilation of a decades worth of work on chemical bonding. It is one of the most important modern chemical texts. It explains hybridization theory, covalent bonding and ionic bonding as explained through electronegativity, and resonance as a means to explain, among other things, the structure of benzene. 1940Edwin McMillan and Philip H. Abelson identify neptunium, the lightest and first synthesized transuranium element, found in the products of uranium fission. McMillan would found a lab at Berkeley that would be involved in the discovery of many new elements and isotopes.",
                    "score": 0.8516634106636047
                },
                {
                    "id": 21806580,
                    "contents": "History of subatomic physics\nIn 1909 Ernest Rutherford and Thomas Royds demonstrated that an alpha particle combines with two electrons and forms a helium atom. In modern terms, alpha particles are doubly ionized helium (more precisely, ) atoms. Speculation about the structure of atoms was severely constrained by Rutherford's 1907 gold foil experiment, showing that the atom is mainly empty space, with almost all its mass concentrated in a tiny atomic nucleus. Inside the atom",
                    "score": 0.8516229391098022
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_13",
        "question": "When an excited nucleus decays, it emits a $\\gamma$ ray. The lifetime of an excited state of a nucleus is of the order of $10^{-12} \\mathrm{~s}$. What is the uncertainty in the energy of the $\\gamma$ ray produced?",
        "golden_answers": [
            " 0.527"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 16561544,
                    "contents": "Gamma ray\nIn certain cases, the excited nuclear state that follows the emission of a beta particle or other type of excitation, may be more stable than average, and is termed a metastable excited state, if its decay takes (at least) 100 to 1000 times longer than the average 10−12 seconds. Such relatively long-lived excited nuclei are termed nuclear isomers, and their decays are termed isomeric transitions. Such nuclei have half-lifes that are more easily measurable, and rare nuclear isomers are able to stay in their excited state for minutes, hours, days, or occasionally far longer, before emitting a gamma ray. The process of isomeric transition is therefore similar to any gamma emission, but differs in that it involves the intermediate metastable excited state(s) of the nuclei. Metastable states are often characterized by high nuclear spin, requiring a change in spin of several units or more with gamma decay, instead of a single unit transition that occurs in only 10−12 seconds. The rate of",
                    "score": 0.9105168581008911
                },
                {
                    "id": 16561543,
                    "contents": "Gamma ray\nThe emission of a gamma ray from an excited nucleus typically requires only 10−12 seconds. Gamma decay may also follow nuclear reactions such as neutron capture, nuclear fission, or nuclear fusion. Gamma decay is also a mode of relaxation of many excited states of atomic nuclei following other types of radioactive decay, such as beta decay, so long as these states possess the necessary component of nuclear spin. When high-energy gamma rays, electrons, or protons bombard materials, the excited atoms emit characteristic \"secondary\" gamma rays, which are products of the creation of excited nuclear states in the bombarded atoms. Such transitions, a form of nuclear gamma fluorescence, form a topic in nuclear physics called gamma spectroscopy. Formation of fluorescent gamma rays are a rapid subtype of radioactive gamma decay.",
                    "score": 0.9062277674674988
                },
                {
                    "id": 2725589,
                    "contents": "Nuclear reaction\nThe energy released in a nuclear reaction can appear mainly in one of three ways: kinetic energy of the product particles (fraction of the kinetic energy of the charged nuclear reaction products can be directly converted into electrostatic energy); emission of very high energy photons, called gamma rays; some energy may remain in the nucleus, as a metastable energy level. When the product nucleus is metastable, this is indicated by placing an asterisk (\"*\") next to its atomic number. This energy is eventually released through nuclear decay. A small amount of energy may also emerge in the form of X-rays. Generally, the product nucleus has a different atomic number, and thus the configuration of its electron shells is wrong. As the electrons rearrange themselves and drop to lower energy levels, internal transition X-rays (X-rays with precisely defined emission lines) may be emitted.",
                    "score": 0.8991860747337341
                },
                {
                    "id": 16561546,
                    "contents": "Gamma ray\nAn emitted gamma ray from any type of excited state may transfer its energy directly to any electrons, but most probably to one of the K shell electrons of the atom, causing it to be ejected from that atom, in a process generally termed the photoelectric effect (external gamma rays and ultraviolet rays may also cause this effect). The photoelectric effect should not be confused with the internal conversion process, in which a gamma ray photon is not produced as an intermediate particle (rather, a \"virtual gamma ray\" may be thought to mediate the process). Decay schemes",
                    "score": 0.8918846845626831
                },
                {
                    "id": 9432402,
                    "contents": "Decay scheme\nWhile excited nuclear states are usually very short lived, decaying almost immediately after a beta decay (see above), the excited state of the technetium isotope shown here to the right is comparatively long lived. It is therefore called \"metastable\" (hence the \"m\" in 99mTc ). It decays to the ground state via gamma decay with a half-life of 6 hours. Here, to the left, we now have an alpha decay. It is the decay of the element Polonium discovered by Marie Curie, with mass number 210. The isotope 210Po is the penultimate member of the uranium-radium-decay series; it decays into a stable lead-isotope with a half-life of 138 days. In almost all cases, the decay is via the emission of an alpha particle of 5.305 MeV. Only in one case of 100000, an alpha particle of lower energy appears; in this case, the decay leads to an excited level of 206Pb, which then decays to the ground state via gamma radiation.",
                    "score": 0.8872271776199341
                },
                {
                    "id": 1131760,
                    "contents": "Nuclear physics\nIn gamma decay, a nucleus decays from an excited state into a lower energy state, by emitting a gamma ray. The element is not changed to another element in the process (no nuclear transmutation is involved). Other more exotic decays are possible (see the first main article). For example, in internal conversion decay, the energy from an excited nucleus may eject one of the inner orbital electrons from the atom, in a process which produces high speed electrons but is not beta decay and (unlike beta decay) does not transmute one element to another.",
                    "score": 0.886448860168457
                },
                {
                    "id": 1560686,
                    "contents": "Atom\nGamma decay: this process results from a change in the energy level of the nucleus to a lower state, resulting in the emission of electromagnetic radiation. The excited state of a nucleus which results in gamma emission usually occurs following the emission of an alpha or a beta particle. Thus, gamma decay usually follows alpha or beta decay.",
                    "score": 0.8856234550476074
                },
                {
                    "id": 9432401,
                    "contents": "Decay scheme\nSince energy is conserved and since the particles emitted carry away energy, arrows can only go downward (vertically or at an angle) in a decay scheme. A somewhat more complicated scheme is shown here: the decay of the nuclide 198Au which can be produced by irradiating natural gold in a nuclear reactor. 198Au decays via beta decay to one of two excited states or to the ground state of the mercury isotope 198Hg. In the figure, mercury is to the right of gold, since the atomic number of gold is 79, that of mercury is 80. The excited states decay after very short times (2.5 and 23 ps, resp.; 1 picosecond is a millionth of a millionth of a second) to the ground state.",
                    "score": 0.8854701519012451
                },
                {
                    "id": 793217,
                    "contents": "Decay energy\nThe decay energy is the energy change of a nucleus having undergone a radioactive decay. Radioactive decay is the process in which an unstable atomic nucleus loses energy by emitting ionizing particles and radiation. This decay, or loss of energy, results in an atom of one type (called the parent nuclide) transforming to an atom of a different type (called the daughter nuclide). Decay calculation The energy difference of the reactants is often written as Q: Decay energy is usually quoted in terms of the energy units MeV (million electronvolts) or keV (thousand electronvolts): Types of radioactive decay include gamma ray beta decay (decay energy is divided between the emitted electron and the neutrino which is emitted at the same time) alpha decay",
                    "score": 0.8853065967559814
                },
                {
                    "id": 1066874,
                    "contents": "Mössbauer effect\nThe discovery was rewarded with the Nobel Prize in Physics in 1961 together with Robert Hofstadter's research of electron scattering in atomic nuclei. Description In general, gamma rays are produced by nuclear transitions from an unstable high-energy state to a stable low-energy state. The energy of the emitted gamma ray corresponds to the energy of the nuclear transition, minus an amount of energy that is lost as recoil to the emitting atom. If the lost recoil energy is small compared with the energy linewidth of the nuclear transition, then the gamma ray energy still corresponds to the energy of the nuclear transition, and the gamma ray can be absorbed by a second atom of the same type as the first. This emission and subsequent absorption is called resonant fluorescence. Additional recoil energy is also lost during absorption, so in order for resonance to occur the recoil energy must actually be less than half the linewidth for the corresponding nuclear transition.",
                    "score": 0.8847007751464844
                },
                {
                    "id": 8215371,
                    "contents": "Nuclear binding energy\nNuclear energy is released by three exoenergetic (or exothermic) processes: Radioactive decay, where a neutron or proton in the radioactive nucleus decays spontaneously by emitting either particles, electromagnetic radiation (gamma rays), or both. Note that for radioactive decay, it is not strictly necessary for the binding energy to increase. What is strictly necessary is that the mass decrease. If a neutron turns into a proton and the energy of the decay is less than 0.782343 MeV, the difference between the masses of the neutron and proton multiplied by the speed of light squared, (such as rubidium-87 decaying to strontium-87), the average binding energy per nucleon will actually decrease. Fusion, two atomic nuclei fuse together to form a heavier nucleus Fission, the breaking of a heavy nucleus into two (or more rarely three) lighter nuclei",
                    "score": 0.8835344314575195
                },
                {
                    "id": 6260803,
                    "contents": "Binding energy\nAfter a nuclear reaction occurs that results in an excited nucleus, the energy that must be radiated or otherwise removed as binding energy in order to decay to the unexcited state may be in one of several forms. This may be electromagnetic waves, such as gamma radiation; the kinetic energy of an ejected particle, such as an electron, in internal conversion decay; or partly as the rest mass of one or more emitted particles, such as the particles of beta decay. No mass deficit can appear, in theory, until this radiation or this energy has been emitted and is no longer part of the system.",
                    "score": 0.88275545835495
                },
                {
                    "id": 1617243,
                    "contents": "Beta decay\nIn nuclear physics, beta decay (β-decay) is a type of radioactive decay in which a beta particle (fast energetic electron or positron) is emitted from an atomic nucleus, transforming the original nuclide to an isobar of that nuclide. For example, beta decay of a neutron transforms it into a proton by the emission of an electron accompanied by an antineutrino; or, conversely a proton is converted into a neutron by the emission of a positron with a neutrino in so-called positron emission. Neither the beta particle nor its associated (anti-)neutrino exist within the nucleus prior to beta decay, but are created in the decay process. By this process, unstable atoms obtain a more stable ratio of protons to neutrons. The probability of a nuclide decaying due to beta and other forms of decay is determined by its nuclear binding energy. The binding energies of all existing nuclides form what is called the nuclear band or valley of stability. For either electron or positron emission to be",
                    "score": 0.8824043869972229
                },
                {
                    "id": 3384605,
                    "contents": "Internal conversion\nThe energy of the emitted gamma ray is a precise measure of the difference in energy between the excited states of the decaying nucleus. In the case of conversion electrons, the binding energy must also be taken into account: The energy of a conversion electron is given as , where and are the energies of the nucleus in its initial and final states, respectively, while is the binding energy of the electron. Similar processes Nuclei with zero-spin and high excitation energies (more than about 1.022 MeV) are also unable to rid themselves of energy by (single) gamma emission due to the constraint imposed by the conservation of momentum, but they do have sufficient decay energy to decay by pair production. In this type of decay, an electron and positron are both emitted from the atom at the same time, and conservation of angular momentum is solved by having these two product particles spin in opposite directions.",
                    "score": 0.8815916776657104
                },
                {
                    "id": 3384394,
                    "contents": "Excited state\nAfter excitation the atom may return to the ground state or a lower excited state, by emitting a photon with a characteristic energy. Emission of photons from atoms in various excited states leads to an electromagnetic spectrum showing a series of characteristic emission lines (including, in the case of the hydrogen atom, the Lyman, Balmer, Paschen and Brackett series.) An atom in a high excited state is termed a Rydberg atom. A system of highly excited atoms can form a long-lived condensed excited state e.g. a condensed phase made completely of excited atoms: Rydberg matter. Hydrogen can also be excited by heat or electricity.",
                    "score": 0.8805233240127563
                },
                {
                    "id": 1174979,
                    "contents": "Particle radiation\nParticle radiation can be emitted by an unstable atomic nucleus (via radioactive decay), or it can be produced from some other kind of nuclear reaction. Many types of particles may be emitted: protons and other hydrogen nuclei stripped of their electrons positively charged alpha particles (α), equivalent to a helium-4 nucleus helium ions at high energy levels HZE ions, which are nuclei heavier than helium positively or negatively charged beta particles (high-energy positrons β+ or electrons β−; the latter being more common) high-speed electrons that are not from the beta decay process, but others such as internal conversion and Auger effect neutrons, subatomic particles which have no charge; neutron radiation neutrinos mesons muons",
                    "score": 0.8772950172424316
                },
                {
                    "id": 424159,
                    "contents": "Radioactive decay\nBy contrast, there are radioactive decay processes that do not result in a nuclear transmutation. The energy of an excited nucleus may be emitted as a gamma ray in a process called gamma decay, or that energy may be lost when the nucleus interacts with an orbital electron causing its ejection from the atom, in a process called internal conversion. Another type of radioactive decay results in products that vary, appearing as two or more \"fragments\" of the original nucleus with a range of possible masses. This decay, called spontaneous fission, happens when a large unstable nucleus spontaneously splits into two (or occasionally three) smaller daughter nuclei, and generally leads to the emission of gamma rays, neutrons, or other particles from those products.",
                    "score": 0.8769758343696594
                },
                {
                    "id": 8215365,
                    "contents": "Nuclear binding energy\nThe two methods for this conversion are mediated by the weak force, and involve types of beta decay. In the simplest beta decay, neutrons are converted to protons by emitting a negative electron and an antineutrino. This is always possible outside a nucleus because neutrons are more massive than protons by an equivalent of about 2.5 electrons. In the opposite process, which only happens within a nucleus, and not to free particles, a proton may become a neutron by ejecting a positron and an electron neutrino. This is permitted if enough energy is available between parent and daughter nuclides to do this (the required energy difference is equal to 1.022 MeV, which is the mass of 2 electrons). If the mass difference between parent and daughter is less than this, a proton-rich nucleus may still convert protons to neutrons by the process of electron capture, in which a proton simply electron captures one of the atom's K orbital electrons, emits a neutrino, and becomes a neutron.",
                    "score": 0.8768733739852905
                },
                {
                    "id": 16561549,
                    "contents": "Gamma ray\nGamma rays are produced in many processes of particle physics. Typically, gamma rays are the products of neutral systems which decay through electromagnetic interactions (rather than a weak or strong interaction). For example, in an electron–positron annihilation, the usual products are two gamma ray photons. If the annihilating electron and positron are at rest, each of the resulting gamma rays has an energy of ~ 511 keV and frequency of ~ . Similarly, a neutral pion most often decays into two photons. Many other hadrons and massive bosons also decay electromagnetically. High energy physics experiments, such as the Large Hadron Collider, accordingly employ substantial radiation shielding. Because subatomic particles mostly have far shorter wavelengths than atomic nuclei, particle physics gamma rays are generally several orders of magnitude more energetic than nuclear decay gamma rays. Since gamma rays are at the top of the electromagnetic spectrum in terms of energy, all extremely",
                    "score": 0.875939130783081
                },
                {
                    "id": 332513,
                    "contents": "Nuclear isomer\nThe emission of a gamma ray from an excited nuclear state allows the nucleus to lose energy and reach a lower-energy state, sometimes its ground state. In certain cases, the excited nuclear state following a nuclear reaction or other type of radioactive decay can become a metastable nuclear excited state. Some nuclei are able to stay in this metastable excited state for minutes, hours, days, or occasionally far longer. The process of isomeric transition is similar to gamma emission from any excited nuclear state, but differs by involving excited metastable states of nuclei with longer half-lives. As with other excited states, the nucleus can be left in an isomeric state following the emission of an alpha particle, beta particle, or some other type of particle.",
                    "score": 0.8753998875617981
                },
                {
                    "id": 7424469,
                    "contents": "Forbidden mechanism\nAlthough gamma decays with nuclear angular momentum changes of 2, 3, 4, etc., are forbidden, they are only relatively forbidden, and do proceed, but with a slower rate than the normal allowed change of 1 unit. However, gamma emission is absolutely forbidden when the nucleus begins in a zero-spin state, as such an emission would not conserve angular momentum. These transitions cannot occur by gamma decay, but must proceed by another route, such as beta decay in some cases, or internal conversion where beta decay is not favored.",
                    "score": 0.8752078413963318
                },
                {
                    "id": 3384604,
                    "contents": "Internal conversion\nThe competition between internal conversion and gamma decay is quantified in the form of the internal conversion coefficient which is defined as where is the rate of conversion electrons and is the rate of gamma-ray emission observed from a decaying nucleus. For example, in the decay of the excited state at 35 keV of 125Te (which is produced by the decay of 125I), 7% of the decays emit energy as a gamma ray, while 93% release energy as conversion electrons. Therefore, this excited state of has an internal conversion coefficient of . For increasing atomic number (Z) and decreasing gamma-ray energy, internal conversion coefficients are observed to increase. As an example, calculated IC coefficients for electric dipole (E1) transitions, for Z = 40, 60, and 80, are shown in the figure.",
                    "score": 0.8751575350761414
                },
                {
                    "id": 1131754,
                    "contents": "Nuclear physics\nWith Yukawa's papers, the modern model of the atom was complete. The center of the atom contains a tight ball of neutrons and protons, which is held together by the strong nuclear force, unless it is too large. Unstable nuclei may undergo alpha decay, in which they emit an energetic helium nucleus, or beta decay, in which they eject an electron (or positron). After one of these decays the resultant nucleus may be left in an excited state, and in this case it decays to its ground state by emitting high-energy photons (gamma decay). The study of the strong and weak nuclear forces (the latter explained by Enrico Fermi via Fermi's interaction in 1934) led physicists to collide nuclei and electrons at ever higher energies. This research became the science of particle physics, the crown jewel of which is the standard model of particle physics, which describes the strong, weak, and electromagnetic forces. Modern nuclear physics",
                    "score": 0.8747677206993103
                },
                {
                    "id": 15591611,
                    "contents": "Atomic recoil\nFor the total energy of the particle we have: So the kinetic energy imparted to the particle is: Similarly, the kinetic energy imparted to the daughter nucleus is: When the emitted particle is a proton, neutron, or alpha particle the fraction of the decay energy going to the particle is approximately and the fraction going to the daughter nucleus For neutrinos and gamma rays, the departing particle gets almost all the energy, the fraction going to the daughter nucleus being only The speed of the emitted particle is given by divided by the total energy: Similarly, the speed of the recoiling nucleus is: If we take for neutrinos and gamma rays, this simplifies to: For similar decay energies, the recoil from emitting an alpha ray will be much greater than the recoil from emitting a neutrino (upon electron capture) or a gamma ray.",
                    "score": 0.8746438026428223
                },
                {
                    "id": 1560687,
                    "contents": "Atom\nOther more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission. Each radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth. Magnetic moment",
                    "score": 0.8737505078315735
                },
                {
                    "id": 9432399,
                    "contents": "Decay scheme\nThe decay scheme of a radioactive substance is a graphical presentation of all the transitions occurring in a decay, and of their relationships. Examples are shown below. It is useful to think of the decay scheme as placed in a coordinate system, where the vertical axis is energy, increasing from bottom to top, and the horizontal axis is the proton number, increasing from left to right. The arrows indicate the emitted particles. For the gamma rays (vertical arrows), the gamma energies are given; for the beta decay (oblique arrow), the maximum beta energy. Examples These relations can be quite complicated; a simple case is shown here: the decay scheme of the radioactive cobalt isotope cobalt-60. 60Co decays by emitting an electron (beta decay) with a half-life of 5.272 years into an excited state of 60Ni, which then decays very fast to the ground state of 60Ni, via two gamma decays. All known decay schemes can be found in the Table of Isotopes.,",
                    "score": 0.8730557560920715
                },
                {
                    "id": 16561547,
                    "contents": "Gamma ray\nDecay schemes One example of gamma ray production due to radionuclide decay is the decay scheme for cobalt-60, as illustrated in the accompanying diagram. First, decays to excited by beta decay emission of an electron of . Then the excited decays to the ground state (see nuclear shell model) by emitting gamma rays in succession of 1.17 MeV followed by . This path is followed 99.88% of the time: :{| border=\"0\" |- style=\"height:2em;\" | ||→ || ||+ || ||+ || ||+ || ||+ || |- style=\"height:2em;\" | ||→ || || || || || ||+ || ||+ || |} Another example is the alpha decay of to form ; which is followed by gamma emission. In some cases, the gamma emission spectrum of the daughter nucleus is quite simple, (e.g. /) while in other cases, such as with (/ and /), the gamma emission spectrum is complex, revealing that a series of nuclear energy levels exist.",
                    "score": 0.8728060722351074
                },
                {
                    "id": 424179,
                    "contents": "Radioactive decay\nAlthough alpha, beta, and gamma radiations were most commonly found, other types of emission were eventually discovered. Shortly after the discovery of the positron in cosmic ray products, it was realized that the same process that operates in classical beta decay can also produce positrons (positron emission), along with neutrinos (classical beta decay produces antineutrinos). In a more common analogous process, called electron capture, some proton-rich nuclides were found to capture their own atomic electrons instead of emitting positrons, and subsequently, these nuclides emit only a neutrino and a gamma ray from the excited nucleus (and often also Auger electrons and characteristic X-rays, as a result of the re-ordering of electrons to fill the place of the missing captured electron). These types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy",
                    "score": 0.8725874423980713
                },
                {
                    "id": 1560683,
                    "contents": "Atom\nRadioactive decay Every element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.",
                    "score": 0.8724384307861328
                },
                {
                    "id": 22217834,
                    "contents": "Discovery of the neutron\nIn 1927, Charles Ellis and W. Wooster at the Cavendish Laboratory measured the energies of β-decay electrons. They found that the distribution of energies from any particular radioactive nuclei was broad and continuous, a result that contrasted notably with the distinct energy values observed in alpha and gamma decay. Further, the continuous energy distribution seemed to indicate that energy was not conserved by this \"nuclear electrons\" process. Indeed, in 1929 Bohr proposed to modify the law of energy conservation to account for the continuous energy distribution. The proposal earned the support of Werner Heisenberg. Such considerations were apparently reasonable, inasmuch as the laws of quantum mechanics had so recently overturned the laws of classical mechanics.",
                    "score": 0.8721110224723816
                },
                {
                    "id": 332496,
                    "contents": "Nuclear isomer\nWhen excited atomic states decay, energy is released by fluorescence. In electronic transitions, this process usually involves emission of light near the visible range. The amount of energy released is related to bond-dissociation energy or ionization energy and is usually in the range of a few to few tens of eV per bond. However, a much stronger type of binding energy, the nuclear binding energy, is involved in nuclear processes. Due to this, most nuclear excited states decay by gamma ray emission. For example, a well-known nuclear isomer used in various medical procedures is , which decays with a half-life of about 6 hours by emitting a gamma ray of 140 keV of energy; this is close to the energy of medical diagnostic X-rays.",
                    "score": 0.8718954920768738
                },
                {
                    "id": 2725581,
                    "contents": "Nuclear reaction\nIn nuclear physics and nuclear chemistry, a nuclear reaction is a process in which two nuclei, or a nucleus and an external subatomic particle, collide to produce one or more new nuclides. Thus, a nuclear reaction must cause a transformation of at least one nuclide to another. If a nucleus interacts with another nucleus or particle and they then separate without changing the nature of any nuclide, the process is simply referred to as a type of nuclear scattering, rather than a nuclear reaction. In principle, a reaction can involve more than two particles colliding, but because the probability of three or more nuclei to meet at the same time at the same place is much less than for two nuclei, such an event is exceptionally rare (see triple alpha process for an example very close to a three-body nuclear reaction). The term \"nuclear reaction\" may refer either to a change in a nuclide induced by collision with another particle or to a spontaneous change of a nuclide without collision.",
                    "score": 0.871637761592865
                },
                {
                    "id": 16561587,
                    "contents": "Gamma ray\nAnother example is gamma-ray bursts, now known to be produced from processes too powerful to involve simple collections of atoms undergoing radioactive decay. This is part and parcel of the general realization that many gamma rays produced in astronomical processes result not from radioactive decay or particle annihilation, but rather in non-radioactive processes similar to X-rays. Although the gamma rays of astronomy often come from non-radioactive events, a few gamma rays in astronomy are specifically known to originate from gamma decay of nuclei (as demonstrated by their spectra and emission half life). A classic example is that of supernova SN 1987A, which emits an \"afterglow\" of gamma-ray photons from the decay of newly made radioactive nickel-56 and cobalt-56. Most gamma rays in astronomy, however, arise by other mechanisms. See also Annihilation Galactic Center GeV excess Gaseous ionization detectors Very-high-energy gamma ray Ultra-high-energy gamma ray Notes",
                    "score": 0.8713030815124512
                },
                {
                    "id": 424180,
                    "contents": "Radioactive decay\nThese types of decay involve the nuclear capture of electrons or emission of electrons or positrons, and thus acts to move a nucleus toward the ratio of neutrons to protons that has the least energy for a given total number of nucleons. This consequently produces a more stable (lower energy) nucleus.",
                    "score": 0.8709231615066528
                },
                {
                    "id": 16561538,
                    "contents": "Gamma ray\nThe first gamma ray source to be discovered was the radioactive decay process called gamma decay. In this type of decay, an excited nucleus emits a gamma ray almost immediately upon formation. Paul Villard, a French chemist and physicist, discovered gamma radiation in 1900, while studying radiation emitted from radium. Villard knew that his described radiation was more powerful than previously described types of rays from radium, which included beta rays, first noted as \"radioactivity\" by Henri Becquerel in 1896, and alpha rays, discovered as a less penetrating form of radiation by Rutherford, in 1899. However, Villard did not consider naming them as a different fundamental type. Later, in 1903, Villard's radiation was recognized as being of a type fundamentally different from previously named rays by Ernest Rutherford, who named Villard's rays \"gamma rays\" by analogy with the beta and alpha rays that Rutherford had differentiated in 1899. The \"rays\" emitted by radioactive elements",
                    "score": 0.870690107345581
                },
                {
                    "id": 1179003,
                    "contents": "Proton decay\nIn particle physics, proton decay is a hypothetical form of particle decay in which the proton decays into lighter subatomic particles, such as a neutral pion and a positron. The proton decay hypothesis was first formulated by Andrei Sakharov in 1967. Despite significant experimental effort, proton decay has never been observed. If it does decay via a positron, the proton's half-life is constrained to be at least years. According to the Standard Model, the proton, a type of baryon, is stable because baryon number (quark number) is conserved (under normal circumstances; see chiral anomaly for exception). Therefore, protons will not decay into other particles on their own, because they are the lightest (and therefore least energetic) baryon. Positron emission – a form of radioactive decay which sees a proton become a neutron – is not proton decay, since the proton interacts with other particles within the atom.",
                    "score": 0.8704792261123657
                },
                {
                    "id": 1131747,
                    "contents": "Nuclear physics\nPublished in 1909, with the eventual classical analysis by Rutherford published May 1911, the key preemptive experiment was performed during 1909, at the University of Manchester. Ernest Rutherford's assistant, Professor Johannes \"Hans\" Geiger, and an undergraduate, Marsden, performed an experiment in which Geiger and Marsden under Rutherford's supervision fired alpha particles (helium 4 nuclei) at a thin film of gold foil. The plum pudding model had predicted that the alpha particles should come out of the foil with their trajectories being at most slightly bent. But Rutherford instructed his team to look for something that shocked him to observe: a few particles were scattered through large angles, even completely backwards in some cases. He likened it to firing a bullet at tissue paper and having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing",
                    "score": 0.8702763319015503
                },
                {
                    "id": 1566375,
                    "contents": "Alpha decay\nwhere is the initial mass of the nucleus, is the mass of the nucleus after particle emission, and is the mass of the emitted particle, one finds that in certain cases it is positive and so alpha particle emission is possible, whereas other decay modes would require energy to be added. For example, performing the calculation for uranium-232 shows that alpha particle emission gives 5.4 MeV of energy, while a single proton emission would require 6.1 MeV. Most of the disintegration energy becomes the kinetic energy of the alpha particle itself, although to maintain conservation of momentum part of the energy goes to the recoil of the nucleus itself (see Atomic recoil). However, since the mass numbers of most alpha-emitting radioisotopes exceed 210, far greater than the mass number of the alpha particle (4) the fraction of the energy going to the recoil of the nucleus is generally quite small, less than 2%, however the recoil energy (on the scale of keV) is still much larger than the",
                    "score": 0.8699995279312134
                },
                {
                    "id": 1375811,
                    "contents": "Radionuclide\nRadionuclides occur naturally or are artificially produced in nuclear reactors, cyclotrons, particle accelerators or radionuclide generators. There are about 730 radionuclides with half-lives longer than 60 minutes (see list of nuclides). Thirty-two of those are primordial radionuclides that were created before the earth was formed. At least another 60 radionuclides are detectable in nature, either as daughters of primordial radionuclides or as radionuclides produced through natural production on Earth by cosmic radiation. More than 2400 radionuclides have half-lives less than 60 minutes. Most of those are only produced artificially, and have very short half-lives. For comparison, there are about 252 stable nuclides. (In theory, only 146 of them are stable, and the other 106 are believed to decay via alpha decay, beta decay, double beta decay, electron capture, or double electron capture.)",
                    "score": 0.8697754740715027
                },
                {
                    "id": 8215367,
                    "contents": "Nuclear binding energy\nThe curve of binding energy is a graph that plots the binding energy per nucleon against atomic mass. This curve has its main peak at iron and nickel and then slowly decreases again, and also a narrow isolated peak at helium, which as noted is very stable. The heaviest nuclei in nature, uranium 238U, are unstable, but having a half-life of 4.5 billion years, close to the age of the Earth, they are still relatively abundant; they (and other nuclei heavier than helium) have formed in stellar evolution events like supernova explosions preceding the formation of the solar system. The most common isotope of thorium, 232Th, also undergoes alpha particle emission, and its half-life (time over which half a number of atoms decays) is even longer, by several times. In each of these, radioactive decay produces daughter isotopes that are also unstable, starting a chain of decays that ends in some stable isotope of lead.",
                    "score": 0.8695491552352905
                },
                {
                    "id": 1131744,
                    "contents": "Nuclear physics\nIn the years that followed, radioactivity was extensively investigated, notably by Marie Curie, Pierre Curie, Ernest Rutherford and others. By the turn of the century, physicists had also discovered three types of radiation emanating from atoms, which they named alpha, beta, and gamma radiation. Experiments by Otto Hahn in 1911 and by James Chadwick in 1914 discovered that the beta decay spectrum was continuous rather than discrete. That is, electrons were ejected from the atom with a continuous range of energies, rather than the discrete amounts of energy that were observed in gamma and alpha decays. This was a problem for nuclear physics at the time, because it seemed to indicate that energy was not conserved in these decays.",
                    "score": 0.8695214986801147
                },
                {
                    "id": 26023199,
                    "contents": "Decay technique\nPractical considerations A major difficulty in using this method in practice is that the energetic electron released by the decay of one atom of tritium can break apart, modify, ionize, or excite hundreds of other molecules in its path. These fragments and ions can further react with the surrounding molecules producing more products. Without special precautions, it would be impossible to distinguish these \"radiolytic\" products and reactions from the \"nucleogenic\" ones due to mutation and reactions of the cation .",
                    "score": 0.8690780997276306
                },
                {
                    "id": 2725587,
                    "contents": "Nuclear reaction\nIn a nuclear reaction, the total (relativistic) energy is conserved. The \"missing\" rest mass must therefore reappear as kinetic energy released in the reaction; its source is the nuclear binding energy. Using Einstein's mass-energy equivalence formula E = mc2, the amount of energy released can be determined. We first need the energy equivalent of one atomic mass unit: 1 u c2 = (1.66054 × 10−27 kg) × (2.99792 × 108 m/s)2 = 1.49242 × 10−10 kg (m/s)2 = 1.49242 × 10−10 J (joule) × (1 MeV / 1.60218 × 10−13 J) = 931.49 MeV, so 1 u c2 = 931.49 MeV. Hence, the energy released is 0.0238 × 931 MeV = 22.2 MeV. Expressed differently: the mass is reduced by 0.3%, corresponding to 0.3% of 90 PJ/kg is 270 TJ/kg.",
                    "score": 0.8681678771972656
                },
                {
                    "id": 451644,
                    "contents": "Ionizing radiation\nPhoton radiation Even though photons are electrically neutral, they can ionize atoms indirectly through the photoelectric effect and the Compton effect. Either of those interactions will cause the ejection of an electron from an atom at relativistic speeds, turning that electron into a beta particle (secondary beta particle) that will ionize other atoms. Since most of the ionized atoms are due to the secondary beta particles, photons are indirectly ionizing radiation. Radiated photons are called gamma rays if they are produced by a nuclear reaction, subatomic particle decay, or radioactive decay within the nucleus. They are called x-rays if produced outside the nucleus. The generic term \"photon\" is used to describe both.",
                    "score": 0.8673062324523926
                },
                {
                    "id": 1560637,
                    "contents": "Atom\nThe electrons of an atom are attracted to the protons in an atomic nucleus by the electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by the nuclear force. This force is usually stronger than the electromagnetic force that repels the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force. In this case, the nucleus splits and leaves behind different elements. This is a form of nuclear decay.",
                    "score": 0.8669639825820923
                },
                {
                    "id": 1617251,
                    "contents": "Beta decay\nThe study of beta decay provided the first physical evidence for the existence of the neutrino. In both alpha and gamma decay, the resulting alpha or gamma particle has a narrow energy distribution, since the particle carries the energy from the difference between the initial and final nuclear states. However, the kinetic energy distribution, or spectrum, of beta particles measured by Lise Meitner and Otto Hahn in 1911 and by Jean Danysz in 1913 showed multiple lines on a diffuse background. These measurements offered the first hint that beta particles have a continuous spectrum. In 1914, James Chadwick used a magnetic spectrometer with one of Hans Geiger's new counters to make more accurate measurements which showed that the spectrum was continuous. The distribution of beta particle energies was in apparent contradiction to the law of conservation of energy. If beta decay were simply electron emission as assumed at the time, then the energy of the emitted electron should have a",
                    "score": 0.8665079474449158
                },
                {
                    "id": 15591612,
                    "contents": "Atomic recoil\nFor similar decay energies, the recoil from emitting an alpha ray will be much greater than the recoil from emitting a neutrino (upon electron capture) or a gamma ray. For decays that produce two particles as well as the daughter nuclide, the above formulas can be used to find the maximum energy, momentum, or speed of any of the three, by assuming that the lighter of the other two ends up with a speed of zero. For example, the maximum energy of the neutrino, if we assume its rest mass to be zero, is found by using the formula as though only the daughter and the neutrino are involved: Note that here is not the mass of the neutral daughter isotope, but that minus the electron masss:",
                    "score": 0.8664184212684631
                },
                {
                    "id": 1762765,
                    "contents": "History of physics\nexperimental evidence as showing that the atom consists of a dense, positively charged nucleus surrounded by negatively charged electrons. Classical theory, however, predicted that this structure should be unstable. Classical theory had also failed to explain successfully two other experimental results that appeared in the late 19th century. One of these was the demonstration by Albert A. Michelson and Edward W. Morley—known as the Michelson–Morley experiment—which showed there did not seem to be a preferred frame of reference, at rest with respect to the hypothetical luminiferous ether, for describing electromagnetic phenomena. Studies of radiation and radioactive decay continued to be a preeminent focus for physical and chemical research through the 1930s, when the discovery of nuclear fission by Lise Meitner and Otto Frisch opened the way to the practical exploitation of what came to be called \"atomic\" energy.",
                    "score": 0.866398811340332
                },
                {
                    "id": 332504,
                    "contents": "Nuclear isomer\nIt was first reported in 1988 by C. B. Collins that can be forced to release its energy by weaker X-rays. This way of de-excitation had never been observed; however, the de-excitation of by resonant photo-excitation of intermediate high levels of this nucleus (E ~ 1 MeV) was found in 1999 by Belic and co-workers in the Stuttgart nuclear physics group.",
                    "score": 0.8661441206932068
                },
                {
                    "id": 1131370,
                    "contents": "Neutron\nA small fraction (about one in 1000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray: This gamma ray may be thought of as an \"internal bremsstrahlung\" that arises from the electromagnetic interaction of the emitted beta particle with the proton. Internal bremsstrahlung gamma ray production is also a minor feature of beta decays of bound neutrons (as discussed below).",
                    "score": 0.865915834903717
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_14",
        "question": "Calculate the wavelength and the energy of a photon associated with the series limit of the Lyman series.",
        "golden_answers": [
            " 2.18"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 2666434,
                    "contents": "Lyman series\nThe Lyman series The version of the Rydberg formula that generated the Lyman series was: Where n is a natural number greater than or equal to 2 (i.e., ). Therefore, the lines seen in the image above are the wavelengths corresponding to n = 2 on the right, to n = ∞ on the left. There are infinitely many spectral lines, but they become very dense as they approach n = ∞ (the Lyman limit), so only some of the first lines and the last one appear. The wavelengths in the Lyman series are all ultraviolet: Explanation and derivation In 1914, when Niels Bohr produced his Bohr model theory, the reason why hydrogen spectral lines fit Rydberg's formula was explained. Bohr found that the electron bound to the hydrogen atom must have quantized energy levels described by the following formula, According to Bohr's third assumption, whenever an electron falls from an initial energy level Ei to a final energy level Ef, the atom must emit radiation with a wavelength of",
                    "score": 0.9297158122062683
                },
                {
                    "id": 2666435,
                    "contents": "Lyman series\nAccording to Bohr's third assumption, whenever an electron falls from an initial energy level Ei to a final energy level Ef, the atom must emit radiation with a wavelength of There is also a more comfortable notation when dealing with energy in units of electronvolts and wavelengths in units of angstroms, Å. Replacing the energy in the above formula with the expression for the energy in the hydrogen atom where the initial energy corresponds to energy level n and the final energy corresponds to energy level m, Where RH is the same Rydberg constant for hydrogen from Rydberg's long known formula. This also means that the inverse of the Rydberg constant is equal to the Lyman limit. For the connection between Bohr, Rydberg, and Lyman, one must replace m with 1 to obtain which is Rydberg's formula for the Lyman series. Therefore, each wavelength of the emission lines corresponds to an electron dropping from a certain energy level (greater than 1) to the first energy level.",
                    "score": 0.916081428527832
                },
                {
                    "id": 6598363,
                    "contents": "Einstein coefficients\nUnder conditions of thermodynamic equilibrium, the number densities and , the Einstein coefficients, and the spectral energy density provide sufficient information to determine the absorption and emission rates. Equilibrium conditions",
                    "score": 0.9067944288253784
                },
                {
                    "id": 6598362,
                    "contents": "Einstein coefficients\nwhere and are the Einstein coefficients for photon absorption and induced emission respectively. Like the coefficient , these are also fixed by the intrinsic properties of the relevant atom for the two relevant energy levels. For thermodynamics and for the application of Kirchhoff's law, it is necessary that the total absorption be expressed as the algebraic sum of two components, described respectively by and , which may be regarded as positive and negative absorption, which are, respectively, the direct photon absorption, and what is commonly called stimulated or induced emission. The above equations have ignored the influence of the spectroscopic line shape. To be accurate, the above equations need to be multiplied by the (normalized) spectral line shape, in which case the units will change to include a 1/Hz term.",
                    "score": 0.8997341990470886
                },
                {
                    "id": 6598361,
                    "contents": "Einstein coefficients\nwhere is the Einstein coefficient for spontaneous emission, which is fixed by the intrinsic properties of the relevant atom for the two relevant energy levels. The absorption of atomic line radiation may be described by an absorption coefficient with units of 1/length. The expression κ' dx gives the fraction of intensity absorbed for a light beam at frequency while traveling distance dx. The absorption coefficient is given by",
                    "score": 0.898574948310852
                },
                {
                    "id": 8409716,
                    "contents": "Paul Sophus Epstein\nAs well as quantum theory, Epstein also published papers in other fields. Examples include \"\"Zur Theorie des Radiometers\" (1929), \"Reflection of Waves in an Inhomogeneous Absorbing Medium\" (1930) and \"On the Air Resistance of Projectiles\" (1931). Other subjects that he worked on were the settling of gases in the atmosphere, the theory of vibrations of shells and plates and the absorption of sound in fogs and suspensions. He retired as Emeritus Professor at Caltech in 1953 and continued to serve as a consultant for a number of industrial companies. One of the significant reports he wrote during this period was \"Theory of Wave Propagation in a Gyromagnetic Medium\" (1956).",
                    "score": 0.8974383473396301
                },
                {
                    "id": 24331221,
                    "contents": "Fundamental series\nthe frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and noticed that this series had a simpler formula than the others and thus called it the \"fundamental series\" and used the letter F.",
                    "score": 0.8935108184814453
                },
                {
                    "id": 20497697,
                    "contents": "Born series\nThe Born series is the expansion of different scattering quantities in quantum scattering theory in the powers of the interaction potential (more precisely in powers of where is the free particle Green's operator). It is closely related to Born approximation, which is the first order term of the Born series. The series can formally be understood as power series introducing the coupling constant by substitution . The speed of convergence and radius of convergence of the Born series are related to eigenvalues of the operator . In general the first few terms of the Born series are good approximation to the expanded quantity for \"weak\" interaction and large collision energy. Born series for scattering states The Born series for the scattering states reads It can be derived by iterating the Lippmann–Schwinger equation",
                    "score": 0.8906920552253723
                },
                {
                    "id": 20374123,
                    "contents": "Ramanujan's master theorem\nIn mathematics, Ramanujan's master theorem (named after Srinivasa Ramanujan) is a technique that provides an analytic expression for the Mellin transform of an analytic function. The result is stated as follows: If a complex-valued function has an expansion of the form then the Mellin transform of is given by where is the gamma function. It was widely used by Ramanujan to calculate definite integrals and infinite series. Higher-dimensional versions of this theorem also appear in quantum physics (through Feynman diagrams). A similar result was also obtained by Glaisher. Alternative formalism An alternative formulation of Ramanujan's master theorem is as follows: which gets converted to the above form after substituting and using the functional equation for the gamma function. The integral above is convergent for subject to growth conditions on .",
                    "score": 0.8905885815620422
                },
                {
                    "id": 7747997,
                    "contents": "Introduction to quantum mechanics\nEinstein's description of light as being composed of particles extended Planck's notion of quantized energy, which is that a single photon of a given frequency, , delivers an invariant amount of energy, . In other words, individual photons can deliver more or less energy, but only depending on their frequencies. In nature, single photons are rarely encountered. The Sun and emission sources available in the 19th century emit vast numbers of photons every second, and so the importance of the energy carried by each photon was not obvious. Einstein's idea that the energy contained in individual units of light depends on their frequency made it possible to explain experimental results that had seemed counterintuitive. However, although the photon is a particle, it was still being described as having the wave-like property of frequency. Effectively, the account of light as a particle is insufficient, and its wave-like nature is still required.",
                    "score": 0.8897619247436523
                },
                {
                    "id": 3004965,
                    "contents": "Geometrical optics\nSince the underlying principle of geometrical optics lies in the limit , the following asymptotic series is assumed, For large but finite value of , the series diverges, and one has to be careful in keeping only appropriate first few terms. For each value of , one can find an optimum number of terms to be kept and adding more terms than the optimum number might result in a poorer approximation. Substituting the series into the equation and collecting terms of different orders, one finds in general, The first equation is known as the eikonal equation, which determines the eikonal is a Hamilton–Jacobi equation, written for example in Cartesian coordinates becomes The remaining equations determine the functions . Luneburg method",
                    "score": 0.8896245360374451
                },
                {
                    "id": 1168441,
                    "contents": "Photon\nThe and are collectively known as the Einstein coefficients. Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients , and once physicists had obtained \"mechanics and electrodynamics modified to accommodate the quantum hypothesis\". Not long thereafter, in 1926, Paul Dirac derived the rate constants by using a semiclassical approach, and, in 1927, succeeded in deriving all the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called second quantization or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.",
                    "score": 0.889470636844635
                },
                {
                    "id": 15095835,
                    "contents": "Cherenkov radiation\nThere is a cut-off frequency above which the equation can no longer be satisfied. The refractive index varies with frequency (and hence with wavelength) in such a way that the intensity cannot continue to increase at ever shorter wavelengths, even for very relativistic particles (where v/c is close to 1). At X-ray frequencies, the refractive index becomes less than 1 (note that in media, the phase velocity may exceed c without violating relativity) and hence no X-ray emission (or shorter wavelength emissions such as gamma rays) would be observed. However, X-rays can be generated at special frequencies just below the frequencies corresponding to core electronic transitions in a material, as the index of refraction is often greater than 1 just below a resonant frequency (see Kramers-Kronig relation and anomalous dispersion).",
                    "score": 0.8888653516769409
                },
                {
                    "id": 6598358,
                    "contents": "Einstein coefficients\nEinstein coefficients are mathematical quantities which are a measure of the probability of absorption or emission of light by an atom or molecule. The Einstein A coefficients are related to the rate of spontaneous emission of light, and the Einstein B coefficients are related to the absorption and stimulated emission of light. Spectral lines In physics, one thinks of a spectral line from two viewpoints. An emission line is formed when an atom or molecule makes a transition from a particular discrete energy level of an atom, to a lower energy level , emitting a photon of a particular energy and wavelength. A spectrum of many such photons will show an emission spike at the wavelength associated with these photons.",
                    "score": 0.8887693881988525
                },
                {
                    "id": 10767597,
                    "contents": "Frank–Tamm formula\nThe integral over at one instant of time is equal to the integral at one point over all time. Using : Converting this to the frequency domain: To go into the domain of Cherenkov radiation, we now consider perpendicular distance much greater than atomic distances in a medium, that is, . With this assumption we can expand the Bessel functions into their asymptotic form: and Thus: If has a positive real part (usually true), the exponential will cause the expression to vanish rapidly at large distances, meaning all the energy is deposited near the path. However, this isn't true when is purely imaginary – this instead causes the exponential to become 1 and then is independent of , meaning some of the energy escapes to infinity as radiation – this is Cherenkov radiation.",
                    "score": 0.8882771730422974
                },
                {
                    "id": 7747999,
                    "contents": "Introduction to quantum mechanics\nAll photons of the same frequency have identical energy, and all photons of different frequencies have proportionally (order 1, ) different energies. However, although the energy imparted by photons is invariant at any given frequency, the initial energy state of the electrons in a photoelectric device before absorption of light is not necessarily uniform. Anomalous results may occur in the case of individual electrons. For instance, an electron that was already excited above the equilibrium level of the photoelectric device might be ejected when it absorbed uncharacteristically low-frequency illumination. Statistically, however, the characteristic behavior of a photoelectric device reflects the behavior of the vast majority of its electrons, which are at their equilibrium level. This point helps clarify the distinction between the study of small individual particles in quantum dynamics and the study of massive individual particles in classical physics.",
                    "score": 0.8882570862770081
                },
                {
                    "id": 7747991,
                    "contents": "Introduction to quantum mechanics\nEventually, however, the photon model became favored. One of the most significant pieces of evidence in its favor was its ability to explain several puzzling properties of the photoelectric effect, described in the following section. Nonetheless, the wave analogy remained indispensable for helping to understand other characteristics of light: diffraction, refraction, and interference.",
                    "score": 0.887099027633667
                },
                {
                    "id": 15708226,
                    "contents": "Planck constant\nwhere is the Kronecker delta. Photon energy The Planck–Einstein relation connects the particular photon energy with its associated wave frequency : This energy is extremely small in terms of ordinarily perceived everyday objects. Since the frequency , wavelength , and speed of light are related by , the relation can also be expressed as The de Broglie wavelength of the particle is given by where denotes the linear momentum of a particle, such as a photon, or any other elementary particle. In applications where it is natural to use the angular frequency (i.e. where the frequency is expressed in terms of radians per second instead of cycles per second or hertz) it is often useful to absorb a factor of into the Planck constant. The resulting constant is called the reduced Planck constant. It is equal to the Planck constant divided by , and is denoted (pronounced \"h-bar\"): The energy of a photon with angular frequency is given by while its linear momentum relates to",
                    "score": 0.8870582580566406
                },
                {
                    "id": 1200395,
                    "contents": "Richard Feynman\nthey provide a deep understanding of physics. Many of his lectures and miscellaneous talks were turned into other books, including The Character of Physical Law, QED: The Strange Theory of Light and Matter, Statistical Mechanics, Lectures on Gravitation, and the Feynman Lectures on Computation.",
                    "score": 0.886431872844696
                },
                {
                    "id": 3953002,
                    "contents": "Johann Jakob Balmer\nDespite being a mathematician, Balmer is best remembered for his work on spectral series. His major contribution (made at the age of sixty, in 1885) was an empirical formula for the visible spectral lines of the hydrogen atom, the study of which he took up at the suggestion of Eduard Hagenbach also of Basel. Using Ångström's measurements of the hydrogen lines, he arrived at a formula for computing the wavelength as follows: for n = 2 and m = 3, 4, 5, 6, and so forth; h = 3.6456×10−7 m.",
                    "score": 0.8863095045089722
                },
                {
                    "id": 5102459,
                    "contents": "Lambert series\nIn mathematics, a Lambert series, named for Johann Heinrich Lambert, is a series taking the form It can be resumed formally by expanding the denominator: where the coefficients of the new series are given by the Dirichlet convolution of an with the constant function 1(n) = 1: This series may be inverted by means of the Möbius inversion formula, and is an example of a Möbius transform. Examples Since this last sum is a typical number-theoretic sum, almost any natural multiplicative function will be exactly summable when used in a Lambert series. Thus, for example, one has where is the number of positive divisors of the number n. For the higher order sum-of-divisor functions, one has where is any complex number and is the divisor function. In particular, for , the Lambert series one gets is which is (up to the factor of ) the logarithmic derivative of the usual generating function for partition numbers",
                    "score": 0.8857117295265198
                },
                {
                    "id": 6598373,
                    "contents": "Einstein coefficients\nwhere where is the speed of light and is Planck's constant. Substituting these expressions into the equation of detailed balancing and remembering that yields separating to The above equation must hold at any temperature, so and Therefore, the three Einstein coefficients are interrelated by and When this relation is inserted into the original equation, one can also find a relation between and , involving Planck's law. Oscillator strengths The oscillator strength is defined by the following relation to the cross section for absorption: where is the electron charge, is the electron mass, and and are normalized distribution functions in frequency and angular frequency respectively. This allows all three Einstein coefficients to be expressed in terms of the single oscillator strength associated with the particular atomic spectral line: See also",
                    "score": 0.8845727443695068
                },
                {
                    "id": 7993359,
                    "contents": "Rencontres numbers\nThe numbers are generated by the power series ; accordingly, an explicit formula for Dn, m can be derived as follows: This immediately implies that for n large, m fixed. Probability distribution The sum of the entries in each row for the table in \"Numerical Values\" is the total number of permutations of { 1, ..., n }, and is therefore n!. If one divides all the entries in the nth row by n!, one gets the probability distribution of the number of fixed points of a uniformly distributed random permutation of { 1, ..., n }. The probability that the number of fixed points is k is For n ≥ 1, the expected number of fixed points is 1 (a fact that follows from linearity of expectation).",
                    "score": 0.8842358589172363
                },
                {
                    "id": 10767594,
                    "contents": "Frank–Tamm formula\nCherenkov radiation does not have characteristic spectral peaks, as typical for fluorescence or emission spectra. The relative intensity of one frequency is approximately proportional to the frequency. That is, higher frequencies (shorter wavelengths) are more intense in Cherenkov radiation. This is why visible Cherenkov radiation is observed to be brilliant blue. In fact, most Cherenkov radiation is in the ultraviolet spectrum; the sensitivity of the human eye peaks at green, and is very low in the violet portion of the spectrum. The total amount of energy radiated per unit length is: This integral is done over the frequencies for which the particle's speed is greater than speed of light of the media . The integral is convergent (finite) because at high frequencies the refractive index becomes less than unity and for extremely high frequencies it becomes unity.",
                    "score": 0.8841530084609985
                },
                {
                    "id": 6183796,
                    "contents": "A Course of Modern Analysis\nThe book was one of the earliest to use decimal numbering for its sections, an innovation the authors attribute to Giuseppe Peano. Contents Below are the contents of the fourth edition: Part I. The Process of Analysis Part II. The Transcendental Functions Reception Reviews of the first edition George B. Mathews, in a 1903 review article published in The Mathematical Gazette opens by saying the book is \"sure of a favorable reception\" because of its \"attractive account of some of the most valuable and interesting results of recent analysis\". He notes that Part I deals mainly with infinite series, focusing on power series and Fourier expansions while including the \"elements of\" complex integration and the theory of residues. Part II, in contrast, has chapters on the gamma function, Legendre functions, the hypergeometric series, Bessel functions, elliptic functions, and mathematical physics.",
                    "score": 0.8838960528373718
                },
                {
                    "id": 6598370,
                    "contents": "Einstein coefficients\nwhere denotes the radiance in a 1 Hz bandwidth of the isotropic radiation field at the frequency of the transition (see Planck's law). Stimulated emission is one of the fundamental processes that led to the development of the laser. Laser radiation is, however, very far from the present case of isotropic radiation. Photon absorption Absorption is the process by which a photon is absorbed by the atom, causing an electron to jump from a lower energy level to a higher one. The process is described by the Einstein coefficient (m3 J−1 s−2), which gives the probability per unit time per unit spectral radiance of the radiation field that an electron in state 1 with energy will absorb a photon with an energy and jump to state 2 with energy . The change in the number density of atoms in state 1 per unit time due to absorption will be",
                    "score": 0.8838381767272949
                },
                {
                    "id": 29811348,
                    "contents": "Wolfgang Sellmeier\nWolfgang Sellmeier was a German theoretical physicist who made major contributions to the understanding of the interactions between light and matter. In 1872 he published his seminal work Ueber die durch die Aetherschwingungen erregten Mitschwingungen der Körpertheilchen und deren Rückwirkung auf die ersteren, besonders zur Erklärung der Dispersion und ihrer Anomalien. Before this publication, physicists tried to understand light as a periodic perturbation of an invisible substance that spanned the entire universe: the ether.",
                    "score": 0.8832215070724487
                },
                {
                    "id": 15708227,
                    "contents": "Planck constant\nThe energy of a photon with angular frequency is given by while its linear momentum relates to where is an angular wavenumber. In 1923, Louis de Broglie generalized the Planck–Einstein relation by postulating that the Planck constant represents the proportionality between the momentum and the quantum wavelength of not just the photon, but the quantum wavelength of any particle. This was confirmed by experiments soon afterward. This holds throughout the quantum theory, including electrodynamics. These two relations are the temporal and spatial parts of the special relativistic expression using 4-vectors.",
                    "score": 0.8831321001052856
                },
                {
                    "id": 154978,
                    "contents": "Rydberg formula\nAs stressed by Niels Bohr, expressing results in terms of wavenumber, not wavelength, was the key to Rydberg's discovery. The fundamental role of wavenumbers was also emphasized by the Rydberg-Ritz combination principle of 1908. The fundamental reason for this lies in quantum mechanics. Light's wavenumber is proportional to frequency , and therefore also proportional to light's quantum energy E. Thus, (in this formula the h represents Planck's constant). Modern understanding is that Rydberg's findings were a reflection of the underlying simplicity of the behavior of spectral lines, in terms of fixed (quantized) energy differences between electron orbitals in atoms. Rydberg's 1888 classical expression for the form of the spectral series was not accompanied by a physical explanation. Walther Ritz's pre-quantum 1908 explanation for the mechanism underlying the spectral series was that atomic electrons behaved like magnets and that the magnets could vibrate with respect to the atomic",
                    "score": 0.8831185102462769
                },
                {
                    "id": 2788641,
                    "contents": "List of theoretical physicists\n20th century",
                    "score": 0.8823321461677551
                },
                {
                    "id": 10767598,
                    "contents": "Frank–Tamm formula\nis purely imaginary if is real and . That is, when is real, Cherenkov radiation has the condition that . This is the statement that the speed of the particle must be larger than the phase velocity of electromagnetic fields in the medium at frequency in order to have Cherenkov radiation. With this purely imaginary condition, and the integral can be simplified to: This is the Frank–Tamm equation in Gaussian units. Notes References External links Cherenkov radiation (Tagged ‘Frank-Tamm formula’) Particle physics Experimental particle physics",
                    "score": 0.8821913599967957
                },
                {
                    "id": 11171110,
                    "contents": "Contributions of Leonhard Euler to mathematics\nNotably, Euler discovered the power series expansions for e and the inverse tangent function His use of power series enabled him to solve the famous Basel problem in 1735: In addition, Euler elaborated the theory of higher transcendental functions by introducing the gamma function and introduced a new method for solving quartic equations. He also found a way to calculate integrals with complex limits, foreshadowing the development of complex analysis. Euler invented the calculus of variations including its most well-known result, the Euler–Lagrange equation.",
                    "score": 0.882129967212677
                },
                {
                    "id": 458657,
                    "contents": "Liouville–Neumann series\nIn mathematics, the Liouville–Neumann series is an infinite series that corresponds to the resolvent formalism technique of solving the Fredholm integral equations in Fredholm theory. Definition The Liouville–Neumann (iterative) series is defined as which, provided that is small enough so that the series converges, is the unique continuous solution of the Fredholm integral equation of the second kind, If the nth iterated kernel is defined as n−1 nested integrals of n operators , then with so K0 may be taken to be . The resolvent (or solving kernel for the integral operator) is then given by a schematic analog \"geometric series\", where K0 has been taken to be . The solution of the integral equation thus becomes simply Similar methods may be used to solve the Volterra equations. See also Neumann series References Mathews, Jon; Walker, Robert L. (1970), Mathematical methods of physics (2nd ed.), New York: W. A. Benjamin,",
                    "score": 0.8820600509643555
                },
                {
                    "id": 1834677,
                    "contents": "Leonhard Euler\nEuler's use of power series enabled him to solve the famous Basel problem in 1735 (he provided a more elaborate argument in 1741): He introduced the constant now known as Euler's constant or the Euler–Mascheroni constant, and studied its relationship with the harmonic series, the gamma function, and values of the Riemann zeta function. Euler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms. He also defined the exponential function for complex numbers and discovered its relation to the trigonometric functions. For any real number (taken to be radians), Euler's formula states that the complex exponential function satisfies A special case of the above formula is known as Euler's identity,",
                    "score": 0.8818925619125366
                },
                {
                    "id": 1200375,
                    "contents": "Richard Feynman\nwhat is now called the Feynman propagator. Finally, in papers on the \"Mathematical Formulation of the Quantum Theory of Electromagnetic Interaction\" in 1950 and \"An Operator Calculus Having Applications in Quantum Electrodynamics\" in 1951, he developed the mathematical basis of his ideas, derived familiar formulae and advanced new ones.",
                    "score": 0.8817602396011353
                },
                {
                    "id": 6598364,
                    "contents": "Einstein coefficients\nThe number densities and are set by the physical state of the gas in which the spectral line occurs, including the local spectral radiance (or, in some presentations, the local spectral radiant energy density). When that state is either one of strict thermodynamic equilibrium, or one of so-called \"local thermodynamic equilibrium\", then the distribution of atomic states of excitation (which includes and ) determines the rates of atomic emissions and absorptions to be such that Kirchhoff's law of equality of radiative absorptivity and emissivity holds. In strict thermodynamic equilibrium, the radiation field is said to be black-body radiation and is described by Planck's law. For local thermodynamic equilibrium, the radiation field does not have to be a black-body field, but the rate of interatomic collisions must vastly exceed the rates of absorption and emission of quanta of light, so that the interatomic collisions entirely dominate the distribution of states of atomic excitation.",
                    "score": 0.881578266620636
                },
                {
                    "id": 10414411,
                    "contents": "Grandi's series\nTreating Grandi's series as a divergent geometric series and using the same algebraic methods that evaluate convergent geometric series to obtain a third value: S = 1 − 1 + 1 − 1 + ..., so 1 − S = 1 − (1 − 1 + 1 − 1 + ...) = 1 − 1 + 1 − 1 + ... = S 1 − S = S 1 = 2S, resulting in S = . The same conclusion results from calculating −S, subtracting the result from S, and solving 2S = 1.",
                    "score": 0.8811953663825989
                },
                {
                    "id": 25073781,
                    "contents": "Planck relation\nThe de Broglie's relation is also often encountered in vector form where is the momentum vector, and is the angular wave vector. Bohr's frequency condition Bohr's frequency condition states that the frequency of a photon absorbed or emitted during an electronic transition is related to the energy difference () between the two energy levels involved in the transition: This is a direct consequence of the Planck–Einstein relation. See also Compton wavelength References",
                    "score": 0.8811619281768799
                },
                {
                    "id": 4415416,
                    "contents": "The Physicists\nReception The critic for the Sydney Morning Herald wrote that the production:",
                    "score": 0.8811168074607849
                },
                {
                    "id": 4669538,
                    "contents": "Old quantum theory\nEinstein's theoretical argument was based on thermodynamics, on counting the number of states, and so was not completely convincing. Nevertheless, he concluded that light had attributes of both waves and particles, more precisely that an electromagnetic standing wave with frequency with the quantized energy: should be thought of as consisting of n photons each with an energy . Einstein could not describe how the photons were related to the wave. The photons have momentum as well as energy, and the momentum had to be where is the wavenumber of the electromagnetic wave. This is required by relativity, because the momentum and energy form a four-vector, as do the frequency and wave-number. In 1924, as a PhD candidate, Louis de Broglie proposed a new interpretation of the quantum condition. He suggested that all matter, electrons as well as photons, are described by waves obeying the relations. or, expressed in terms of wavelength instead,",
                    "score": 0.8808144330978394
                },
                {
                    "id": 1014675,
                    "contents": "Liouville number\n(since, even if n=1, all subsequent terms are smaller). In order to manipulate the indices so that k starts at 0, we select a partial sum from within (also less than the total value since it's a partial sum from a series whose terms are all positive). We will choose the partial sum formed by starting at k = (n+1)! which follows from our motivation to write a new series with k=0, namely by noticing that .",
                    "score": 0.8806913495063782
                },
                {
                    "id": 680026,
                    "contents": "Schrödinger equation\nHe found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself with a mistress in a mountain cabin in December 1925. While at the cabin, Schrödinger decided that his earlier nonrelativistic calculations were novel enough to publish and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl) Schrödinger showed that his nonrelativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926. Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave , moving in a potential well , created by the proton. This computation accurately reproduced the energy levels of the Bohr model.",
                    "score": 0.880652904510498
                },
                {
                    "id": 1744091,
                    "contents": "Geometric series\nAmong his insights into infinite series, in addition to his elegantly simple proof of the divergence of the harmonic series, Nicole Oresme proved that the series 1/2 + 2/4 + 3/8 + 4/16 + 5/32 + 6/64 + 7/128 + ... converges to 2. His diagram for his geometric proof, similar to the adjacent diagram, shows a two dimensional geometric series. The first dimension is horizontal, in the bottom row showing the geometric series S = 1/2 + 1/4 + 1/8 + 1/16 + ... , which is the geometric series with coefficient a = 1/2 and common ratio r = 1/2 that converges to S = a / (1-r) = (1/2) / (1-1/2) = 1. The second dimension is vertical, where the bottom row is a new coefficient aT equal to S and each subsequent row above it is scaled by the same common ratio r = 1/2, making another geometric series T = 1 + 1/2 + 1/4 + 1/8 + ... , which is the geometric series with coefficient aT = S = 1 and common ratio r = 1/2 that converges to T = aT / (1-r) = S / (1-r) = a / (1-r) / (1-r) = (1/2) / (1-1/2) / (1-1/2)",
                    "score": 0.88045334815979
                },
                {
                    "id": 10767593,
                    "contents": "Frank–Tamm formula\nThe Frank–Tamm formula yields the amount of Cherenkov radiation emitted on a given frequency as a charged particle moves through a medium at superluminal velocity. It is named for Russian physicists Ilya Frank and Igor Tamm who developed the theory of the Cherenkov effect in 1937, for which they were awarded a Nobel Prize in Physics in 1958. When a charged particle moves faster than the phase speed of light in a medium, electrons interacting with the particle can emit coherent photons while conserving energy and momentum. This process can be viewed as a decay. See Cherenkov radiation and nonradiation condition for an explanation of this effect. Equation The energy emitted per unit length travelled by the particle per unit of frequency is: provided that . Here and are the frequency-dependent permeability and index of refraction of the medium respectively, is the electric charge of the particle, is the speed of the particle, and is the speed of light in vacuum.",
                    "score": 0.8803360462188721
                },
                {
                    "id": 21053061,
                    "contents": "Humbert series\nIn mathematics, Humbert series are a set of seven hypergeometric series Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, Ξ2 of two variables that generalize Kummer's confluent hypergeometric series 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable. The first of these double series was introduced by . Definitions The Humbert series Φ1 is defined for |x| < 1 by the double series: where the Pochhammer symbol (q)n represents the rising factorial: where the second equality is true for all complex except . For other values of x the function Φ1 can be defined by analytic continuation. The Humbert series Φ1 can also be written as a one-dimensional Euler-type integral: This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration. Similarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series:",
                    "score": 0.8803070187568665
                },
                {
                    "id": 889116,
                    "contents": "William Rowan Hamilton\nIn 1827, Hamilton presented a theory of a single function, now known as Hamilton's principal function, that brings together mechanics, optics, and mathematics, and which helped to establish the wave theory of light. He proposed it when he first predicted its existence in the third supplement to his Systems of Rays, read in 1832.",
                    "score": 0.8799463510513306
                },
                {
                    "id": 1663364,
                    "contents": "Casimir effect\nIn the end, the limit is to be taken. Here s is just a complex number, not to be confused with the shape discussed previously. This integral/sum is finite for s real and larger than 3. The sum has a pole at s=3, but may be analytically continued to s=0, where the expression is finite. The above expression simplifies to: where polar coordinates were introduced to turn the double integral into a single integral. The in front is the Jacobian, and the comes from the angular integration. The integral converges if Re[s] > 3, resulting in The sum diverges at s in the neighborhood of zero, but if the damping of large-frequency excitations corresponding to analytic continuation of the Riemann zeta function to s=0 is assumed to make sense physically in some way, then one has But and so one obtains",
                    "score": 0.8798755407333374
                },
                {
                    "id": 13149088,
                    "contents": "Darboux's formula\nIn mathematical analysis, Darboux's formula is a formula introduced by for summing infinite series by using integrals or evaluating integrals using infinite series. It is a generalization to the complex plane of the Euler–Maclaurin summation formula, which is used for similar purposes and derived in a similar manner (by repeated integration by parts of a particular choice of integrand). Darboux's formula can also be used to derive the Taylor series from calculus. Statement If φ(t) is a polynomial of degree n and f an analytic function then The formula can be proved by repeated integration by parts. Special cases Taking φ to be a Bernoulli polynomial in Darboux's formula gives the Euler–Maclaurin summation formula. Taking φ to be (t − 1)n gives the formula for a Taylor series. References Whittaker, E. T. and Watson, G. N. \"A Formula Due to Darboux.\" §7.1 in A Course in Modern Analysis, 4th ed. Cambridge, England: Cambridge University Press, p. 125, 1990.",
                    "score": 0.8796557784080505
                },
                {
                    "id": 3007155,
                    "contents": "Lorentz factor\nSeries expansion (velocity) The Lorentz factor has the Maclaurin series: which is a special case of a binomial series. The approximation ≈ 1 + 2 may be used to calculate relativistic effects at low speeds. It holds to within 1% error for v < 0.4 c (v < 120,000 km/s), and to within 0.1% error for v < 0.22 c (v < 66,000 km/s). The truncated versions of this series also allow physicists to prove that special relativity reduces to Newtonian mechanics at low speeds. For example, in special relativity, the following two equations hold: For ≈ 1 and ≈ 1 + 2, respectively, these reduce to their Newtonian equivalents: The Lorentz factor equation can also be inverted to yield This has an asymptotic form . The first two terms are occasionally used to quickly calculate velocities from large values. The approximation ≈ 1 − −2 holds to within 1% tolerance for > 2, and to within 0.1% tolerance for > 3.5.",
                    "score": 0.87955242395401
                },
                {
                    "id": 2805260,
                    "contents": "Polylogarithm\nIn mathematics, the polylogarithm (also known as Jonquière's function, for Alfred Jonquière) is a special function Lis(z) of order s and argument z. Only for special values of s does the polylogarithm reduce to an elementary function such as the natural logarithm or a rational function. In quantum statistics, the polylogarithm function appears as the closed form of integrals of the Fermi–Dirac distribution and the Bose–Einstein distribution, and is also known as the Fermi–Dirac integral or the Bose–Einstein integral. In quantum electrodynamics, polylogarithms of positive integer order arise in the calculation of processes represented by higher-order Feynman diagrams.",
                    "score": 0.879534900188446
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_15",
        "question": "Another application of the relationship given in Problem $1-48$ has to do with the excitedstate energies and lifetimes of atoms and molecules. If we know that the lifetime of an excited state is $10^{-9} \\mathrm{~s}$, then what is the uncertainty in the energy of this state?\r\n",
        "golden_answers": [
            " 7"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1305710,
                    "contents": "Uncertainty principle\ninformal, heuristic meaning of the principle is the following: A state that only exists for a short time cannot have a definite energy. To have a definite energy, the frequency of the state must be defined accurately, and this requires the state to hang around for many cycles, the reciprocal of the required accuracy. For example, in spectroscopy, excited states have a finite lifetime. By the time–energy uncertainty principle, they do not have a definite energy, and, each time they decay, the energy they release is slightly different. The average energy of the outgoing photon has a peak at the theoretical energy of the state, but the distribution has a finite width called the natural linewidth. Fast-decaying states have a broad linewidth, while slow-decaying states have a narrow linewidth. The same linewidth effect also makes it difficult to specify the rest mass of unstable, fast-decaying particles in particle physics. The faster the particle decays (the shorter its lifetime), the",
                    "score": 0.8894293308258057
                },
                {
                    "id": 3384392,
                    "contents": "Excited state\nIn quantum mechanics, an excited state of a system (such as an atom, molecule or nucleus) is any quantum state of the system that has a higher energy than the ground state (that is, more energy than the absolute minimum). Excitation refers to an increase in energy level above a chosen starting point, usually the ground state but sometimes an already-excited state. The temperature of a group of particles is indicative of the level of excitation (with the notable exception of systems that exhibit negative temperature). The lifetime of a system in an excited state is usually short: spontaneous or induced emission of a quantum of energy (such as a photon or a phonon) usually occurs shortly after the system is promoted to the excited state, returning the system to a state with lower energy (a less excited state or the ground state). This return to a lower energy level is often loosely described as decay and is the inverse of excitation.",
                    "score": 0.8805148005485535
                },
                {
                    "id": 20458023,
                    "contents": "Variational method (quantum mechanics)\nThere is an additional complication in the calculations described. As ε tends toward E0 in minimization calculations, there is no guarantee that the corresponding trial wavefunctions will tend to the actual wavefunction. This has been demonstrated by calculations using a modified harmonic oscillator as a model system, in which an exactly solvable system is approached using the variational method. A wavefunction different from the exact one is obtained by use of the method described above. Although usually limited to calculations of the ground state energy, this method can be applied in certain cases to calculations of excited states as well. If the ground state wavefunction is known, either by the method of variation or by direct calculation, a subset of the Hilbert space can be chosen which is orthogonal to the ground state wavefunction.",
                    "score": 0.8686962127685547
                },
                {
                    "id": 9025031,
                    "contents": "Stationary state\nA stationary state is a quantum state with all observables independent of time. It is an eigenvector of the energy operator (instead of a quantum superposition of different energies). It is also called energy eigenvector, energy eigenstate, energy eigenfunction, or energy eigenket. It is very similar to the concept of atomic orbital and molecular orbital in chemistry, with some slight differences explained below. Introduction",
                    "score": 0.8648115992546082
                },
                {
                    "id": 884650,
                    "contents": "Zero-point energy\nA more thorough treatment, showing that the energy of the ground state actually saturates this bound and is exactly , requires solving for the ground state of the system. Atomic physics The idea of a quantum harmonic oscillator and its associated energy can apply to either an atom or subatomic particle. In ordinary atomic physics, the zero-point energy is the energy associated with the ground state of the system. The professional physics literature tends to measure frequency, as denoted by above, using angular frequency, denoted with and defined by . This leads to a convention of writing Planck's constant with a bar through its top () to denote the quantity . In these terms, the most famous such example of zero-point energy is the above associated with the ground state of the quantum harmonic oscillator. In quantum mechanical terms, the zero-point energy is the expectation value of the Hamiltonian of the system in the ground state.",
                    "score": 0.8644566535949707
                },
                {
                    "id": 9649935,
                    "contents": "History of variational principles in physics\nA variational principle in physics is an alternative method for determining the state or dynamics of a physical system, by identifying it as an extremum (minimum, maximum or saddle point) of a function or functional. This article describes the historical development of such principles. Before modern times Variational principles are found among earlier ideas in surveying and optics. The rope stretchers of ancient Egypt stretched corded ropes between two points to measure the path which minimized the distance of separation, and Claudius Ptolemy, in his Geographia (Bk 1, Ch 2), emphasized that one must correct for \"deviations from a straight course\"; in ancient Greece Euclid states in his Catoptrica that, for the path of light reflecting from a mirror, the angle of incidence equals the angle of reflection; and Hero of Alexandria later showed that this path was the shortest length and least time.",
                    "score": 0.8621337413787842
                },
                {
                    "id": 9025034,
                    "contents": "Stationary state\nStationary state properties As shown above, a stationary state is not mathematically constant: However, all observable properties of the state are in fact constant in time. For example, if represents a simple one-dimensional single-particle wavefunction , the probability that the particle is at location is: which is independent of the time . The Heisenberg picture is an alternative mathematical formulation of quantum mechanics where stationary states are truly mathematically constant in time. As mentioned above, these equations assume that the Hamiltonian is time-independent. This means simply that stationary states are only stationary when the rest of the system is fixed and stationary as well. For example, a 1s electron in a hydrogen atom is in a stationary state, but if the hydrogen atom reacts with another atom, then the electron will of course be disturbed. Spontaneous decay",
                    "score": 0.8614630103111267
                },
                {
                    "id": 15813301,
                    "contents": "Principle of least motion\nIn organic chemistry, the principle of least motion is the hypothesis that when multiple species with different nuclear structures could theoretically form as products of a given chemical reaction, the more likely to form tends to be the one requiring the least amount of change in nuclear structure or the smallest change in nuclear positions. References Organic chemistry",
                    "score": 0.8607727289199829
                },
                {
                    "id": 16086239,
                    "contents": "Linear search problem\nand could start with an infinite number of small 'oscillations'.) This problem is usually called the linear search problem and a search plan is called a trajectory. It has attracted much research, some of it quite recent.",
                    "score": 0.8598812818527222
                },
                {
                    "id": 3384393,
                    "contents": "Excited state\nLong-lived excited states are often called metastable. Long-lived nuclear isomers and singlet oxygen are two examples of this. Atomic excitation A simple example of this concept comes by considering the hydrogen atom. The ground state of the hydrogen atom corresponds to having the atom's single electron in the lowest possible orbital (that is, the spherically symmetric \"1s\" wave function, which, so far, has demonstrated to have the lowest possible quantum numbers). By giving the atom additional energy (for example, by the absorption of a photon of an appropriate energy), the electron is able to move into an excited state (one with one or more quantum numbers greater than the minimum possible). If the photon has too much energy, the electron will cease to be bound to the atom, and the atom will become ionized.",
                    "score": 0.8595929145812988
                },
                {
                    "id": 20458021,
                    "contents": "Variational method (quantum mechanics)\nObviously, if we were to vary over all possible states with norm 1 trying to minimize the expectation value of H, the lowest value would be E0 and the corresponding state would be an eigenstate of E0. Varying over the entire Hilbert space is usually too complicated for physical calculations, and a subspace of the entire Hilbert space is chosen, parametrized by some (real) differentiable parameters αi (i = 1, 2, ..., N). The choice of the subspace is called the ansatz. Some choices of ansatzes lead to better approximations than others, therefore the choice of ansatz is important. Let's assume there is some overlap between the ansatz and the ground state (otherwise, it's a bad ansatz). We still wish to normalize the ansatz, so we have the constraints and we wish to minimize",
                    "score": 0.8575809001922607
                },
                {
                    "id": 3216796,
                    "contents": "Hilbert's sixth problem\nHilbert gave the further explanation of this problem and its possible specific forms: \"As to the axioms of the theory of probabilities, it seems to me desirable that their logical investigation should be accompanied by a rigorous and satisfactory development of the method of mean values in mathematical physics, and in particular in the kinetic theory of gases. ... Boltzmann's work on the principles of mechanics suggests the problem of developing mathematically the limiting processes, there merely indicated, which lead from the atomistic view to the laws of motion of continua.\" History David Hilbert himself devoted much of his research to the sixth problem; in particular, he worked in those fields of physics that arose after he stated the problem. In the 1910s, celestial mechanics evolved into general relativity. Hilbert and Emmy Noether corresponded extensively with Albert Einstein on the formulation of the theory.",
                    "score": 0.8557974100112915
                },
                {
                    "id": 625220,
                    "contents": "Mathematical physics\nbased on a probabilistic interpretation of states, and evolution and measurements in terms of self-adjoint operators on an infinite-dimensional vector space. That is called Hilbert space (introduced by mathematicians David Hilbert (1862–1943), Erhard Schmidt(1876-1959) and Frigyes Riesz (1880-1956) in search of generalization of Euclidean space and study of integral equations), and rigorously defined within the axiomatic modern version by John von Neumann in his celebrated book Mathematical Foundations of Quantum Mechanics, where he built up a relevant part of modern functional analysis on Hilbert spaces, the spectral theory (introduced by David Hilbert who investigated quadratic forms with infinitely many variables. Many years later, it had been revealed that his spectral theory is associated with the spectrum of the hydrogen atom. He was surprised by this application.) in particular. Paul Dirac used algebraic constructions to produce a relativistic model for the electron, predicting",
                    "score": 0.855617880821228
                },
                {
                    "id": 7293292,
                    "contents": "Elliott H. Lieb\nPublication list (partial) Lieb, Elliott H.; Seiringer, Robert. The stability of matter in quantum mechanics. Cambridge University Press, 2010 Lieb, Elliott H.; Loss, Michael. Analysis. Graduate Studies in Mathematics, 14. American Mathematical Society, Providence, RI, 1997. xviii+278 pp. Lieb, Elliott H. The Stability of Matter: From Atoms to Stars. Selecta of Elliott H. Lieb. Edited by W. Thirring, with a preface by F. Dyson. Springer-Verlag, Berlin, 2005. xv+932 pp. Lieb, Elliott H. Inequalities. Selecta of Elliott H. Lieb. Edited, with a preface and commentaries, by M. Loss and M. B. Ruskai. Springer-Verlag, Berlin, 2002. xi+711 pp. Lieb, Elliott H. Statistical mechanics. Selecta of Elliott H. Lieb. Edited, with a preface and commentaries, by B. Nachtergaele, J. P. Solovej and J. Yngvason. Springer-Verlag, Berlin, 2004. x+505 pp.",
                    "score": 0.8555089235305786
                },
                {
                    "id": 1702197,
                    "contents": "Erwin Schrödinger\nIn January 1926, Schrödinger published in Annalen der Physik the paper \"\" (Quantization as an Eigenvalue Problem) on wave mechanics and presented what is now known as the Schrödinger equation. In this paper, he gave a \"derivation\" of the wave equation for time-independent systems and showed that it gave the correct energy eigenvalues for a hydrogen-like atom. This paper has been universally celebrated as one of the most important achievements of the twentieth century and created a revolution in most areas of quantum mechanics and indeed of all physics and chemistry. A second paper was submitted just four weeks later that solved the quantum harmonic oscillator, rigid rotor, and diatomic molecule problems and gave a new derivation of the Schrödinger equation. A third paper, published in May, showed the equivalence of his approach to that of Heisenberg and gave the treatment of the Stark effect. A fourth paper in this series showed how to treat problems in which the system changes with",
                    "score": 0.8542494773864746
                },
                {
                    "id": 90610,
                    "contents": "Third law of thermodynamics\nwith non-minimal energy or because the minimum energy state is non-unique. The constant value is called the residual entropy of the system. The entropy is essentially a state-function meaning the inherent value of different atoms, molecules, and other configurations of particles including subatomic or atomic material is defined by entropy, which can be discovered near 0 K.",
                    "score": 0.85408616065979
                },
                {
                    "id": 7748001,
                    "contents": "Introduction to quantum mechanics\nA second, related puzzle was the emission spectrum of atoms. When a gas is heated, it gives off light only at discrete frequencies. For example, the visible light given off by hydrogen consists of four different colors, as shown in the picture below. The intensity of the light at different frequencies is also different. By contrast, white light consists of a continuous emission across the whole range of visible frequencies. By the end of the nineteenth century, a simple rule known as Balmer's formula showed how the frequencies of the different lines related to each other, though without explaining why this was, or making any prediction about the intensities. The formula also predicted some additional spectral lines in ultraviolet and infrared light that had not been observed at the time. These lines were later observed experimentally, raising confidence in the value of the formula.",
                    "score": 0.8539063930511475
                },
                {
                    "id": 7748021,
                    "contents": "Introduction to quantum mechanics\nIn the same year, building on de Broglie's hypothesis, Erwin Schrödinger developed the equation that describes the behavior of a quantum-mechanical wave. The mathematical model, called the Schrödinger equation after its creator, is central to quantum mechanics, defines the permitted stationary states of a quantum system, and describes how the quantum state of a physical system changes in time. The wave itself is described by a mathematical function known as a \"wave function\". Schrödinger said that the wave function provides the \"means for predicting the probability of measurement results\". Schrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's electron as a classical wave, moving in a well of the electrical potential created by the proton. This calculation accurately reproduced the energy levels of the Bohr model.",
                    "score": 0.8533238768577576
                },
                {
                    "id": 8181839,
                    "contents": "Maximum entropy thermodynamics\n3. As just indicated, the MaxEnt inference runs equally well in reverse. So given a particular final state, we can ask, what can we \"retrodict\" to improve our knowledge about earlier states? However the Second Law argument above also runs in reverse: given macroscopic information at time t2, we should expect it too to become less useful. The two procedures are time-symmetric. But now the information will become less and less useful at earlier and earlier times. (Compare with Loschmidt's paradox.) The MaxEnt inference would predict that the most probable origin of a currently low-entropy state would be as a spontaneous fluctuation from an earlier high entropy state. But this conflicts with what we know to have happened, namely that entropy has been increasing steadily, even back in the past.",
                    "score": 0.8533024787902832
                },
                {
                    "id": 3353416,
                    "contents": "Spectral theory\nMathematical background The name spectral theory was introduced by David Hilbert in his original formulation of Hilbert space theory, which was cast in terms of quadratic forms in infinitely many variables. The original spectral theorem was therefore conceived as a version of the theorem on principal axes of an ellipsoid, in an infinite-dimensional setting. The later discovery in quantum mechanics that spectral theory could explain features of atomic spectra was therefore fortuitous. Hilbert himself was surprised by the unexpected application of this theory, noting that \"I developed my theory of infinitely many variables from purely mathematical interests, and even called it 'spectral analysis' without any presentiment that it would later find application to the actual spectrum of physics.\"",
                    "score": 0.8532153367996216
                },
                {
                    "id": 4186402,
                    "contents": "Fermi's golden rule\nIn quantum physics, Fermi's golden rule is a formula that describes the transition rate (the probability of a transition per unit time) from one energy eigenstate of a quantum system to a group of energy eigenstates in a continuum, as a result of a weak perturbation. This transition rate is effectively independent of time (so long as the strength of the perturbation is independent of time) and is proportional to the strength of the coupling between the initial and final states of the system (described by the square of the matrix element of the perturbation) as well as the density of states. It is also applicable when the final state is discrete, i.e. it is not part of a continuum, if there is some decoherence in the process, like relaxation or collision of the atoms, or like noise in the perturbation, in which case the density of states is replaced by the reciprocal of the decoherence bandwidth.",
                    "score": 0.852290153503418
                },
                {
                    "id": 7748005,
                    "contents": "Introduction to quantum mechanics\nStarting from only one simple assumption about the rule that the orbits must obey, the Bohr model was able to relate the observed spectral lines in the emission spectrum of hydrogen to previously known constants. In Bohr's model, the electron was not allowed to emit energy continuously and crash into the nucleus: once it was in the closest permitted orbit, it was stable forever. Bohr's model didn't explain why the orbits should be quantized in that way, nor was it able to make accurate predictions for atoms with more than one electron, or to explain why some spectral lines are brighter than others.",
                    "score": 0.8522882461547852
                },
                {
                    "id": 20458024,
                    "contents": "Variational method (quantum mechanics)\nThe resulting minimum is usually not as accurate as for the ground state, as any difference between the true ground state and results in a lower excited energy. This defect is worsened with each higher excited state. In another formulation: This holds for any trial φ since, by definition, the ground state wavefunction has the lowest energy, and any trial wavefunction will have energy greater than or equal to it. Proof: φ can be expanded as a linear combination of the actual eigenfunctions of the Hamiltonian (which we assume to be normalized and orthogonal): Then, to find the expectation value of the Hamiltonian: Now, the ground state energy is the lowest energy possible, i.e. . Therefore, if the guessed wave function φ is normalized: In general For a hamiltonian H that describes the studied system and any normalizable function Ψ with arguments appropriate for the unknown wave function of the system, we define the functional",
                    "score": 0.8517545461654663
                },
                {
                    "id": 21898897,
                    "contents": "Eigenstate thermalization hypothesis\nThis argument cannot be straightforwardly extended to quantum systems, even ones that are analogous to chaotic classical systems, because time evolution of a quantum system does not uniformly sample all vectors in Hilbert space with a given energy. Given the state at time zero in a basis of energy eigenstates the expectation value of any observable is Even if the are incommensurate, so that this expectation value is given for long times by the expectation value permanently retains knowledge of the initial state in the form of the coefficients .",
                    "score": 0.8503744006156921
                },
                {
                    "id": 17825089,
                    "contents": "Variational transition-state theory\nVariational transition-state theory is a refinement of transition-state theory. When using transition-state theory to estimate a chemical reaction rate, the dividing surface is taken to be a surface that intersects a first-order saddle point and is also perpendicular to the reaction coordinate in all other dimensions. When using variational transition-state theory, the position of the dividing surface between reactant and product regions is variationally optimized to minimize the reaction rate. This minimizes the effects of recrossing, and gives a much more accurate result. References Chemical kinetics",
                    "score": 0.8500821590423584
                },
                {
                    "id": 10526932,
                    "contents": "Eckart conditions\nThe Eckart conditions, named after Carl Eckart, simplify the nuclear motion (rovibrational) Hamiltonian that arises in the second step of the Born–Oppenheimer approximation. They make it possible to approximately separate rotation from vibration. Although the rotational and vibrational motions of the nuclei in a molecule cannot be fully separated, the Eckart conditions minimize the coupling close to a reference (usually equilibrium) configuration. The Eckart condition are explained by Louck and Galbraith and in Section 10.2 of the textbook by Bunker and Jensen, where a numerical example is given. Definition of Eckart conditions The Eckart conditions can only be formulated for a semi-rigid molecule, which is a molecule with a potential energy surface V(R1, R2,..RN) that has a well-defined minimum for RA0 (). These equilibrium coordinates of the nuclei—with masses MA—are expressed with respect to a fixed orthonormal principal axes frame and hence satisfy the relations",
                    "score": 0.8490869998931885
                },
                {
                    "id": 7221123,
                    "contents": "Thermodynamic state\nMarsland, R. , Brown, H.R., Valente, G. (2015). Time and irreversibility in axiomatic thermodynamics, Am. J. Phys., 83(7): 628–634. Planck, M., (1923/1927). Treatise on Thermodynamics, translated by A. Ogg, third English edition, Longmans, Green and Co., London. Prigogine, I., Defay, R. (1950/1954). Chemical Thermodynamics, Longmans, Green & Co, London. Tisza, L. (1966). Generalized Thermodynamics, M.I.T. Press, Cambridge MA. Zemanksy, M.W., Dittman, R.H. (1937/1981). Heat and Thermodynamics. An Intermediate Textbook, sixth edition, McGraw-Hill Book Company, New York, ISNM 0-07-072808-9. See also Excited state Ground state Stationary state Thermodynamics",
                    "score": 0.8485302329063416
                },
                {
                    "id": 17803749,
                    "contents": "Runge–Gross theorem\nIn quantum mechanics, specifically time-dependent density functional theory, the Runge–Gross theorem (RG theorem) shows that for a many-body system evolving from a given initial wavefunction, there exists a one-to-one mapping between the potential (or potentials) in which the system evolves and the density (or densities) of the system. The potentials under which the theorem holds are defined up to an additive purely time-dependent function: such functions only change the phase of the wavefunction and leave the density invariant. Most often the RG theorem is applied to molecular systems where the electronic density, ρ(r,t) changes in response to an external scalar potential, v(r,t), such as a time-varying electric field.",
                    "score": 0.8484371900558472
                },
                {
                    "id": 275195,
                    "contents": "Perturbation theory (quantum mechanics)\nThe second quantity looks at the time-dependent probability of occupation for each eigenstate. This is particularly useful in laser physics, where one is interested in the populations of different atomic states in a gas when a time-dependent electric field is applied. These probabilities are also useful for calculating the \"quantum broadening\" of spectral lines (see line broadening) and particle decay in particle physics and nuclear physics. We will briefly examine the method behind Dirac's formulation of time-dependent perturbation theory. Choose an energy basis for the unperturbed system. (We drop the (0) superscripts for the eigenstates, because it is not useful to speak of energy levels and eigenstates for the perturbed system.) If the unperturbed system is an eigenstate (of the Hamiltonian) at time = 0, its state at subsequent times varies only by a phase (in the Schrödinger picture, where state vectors evolve in time and operators are constant),",
                    "score": 0.8482999205589294
                },
                {
                    "id": 1701680,
                    "contents": "Equation of state\nIn physics, chemistry, and thermodynamics, an equation of state is a thermodynamic equation relating state variables, which describe the state of matter under a given set of physical conditions, such as pressure, volume, temperature, or internal energy. Most modern equations of state are formulated in the Helmholtz free energy. Equations of state are useful in describing the properties of pure substances and mixtures in liquids, gases, and solid states as well as the state of matter in the interior of stars.",
                    "score": 0.8482944369316101
                },
                {
                    "id": 15219742,
                    "contents": "Energy functional\nThe energy functional is the total energy of a certain system, as a functional of the system's state. In the energy methods of simulating the dynamics of complex structures, a state of the system is often described as an element of an appropriate function space. To be in this state, the system pays a certain cost in terms of energy required by the state. This energy is a scalar quantity, a function of the state, hence the term functional. The system tends to develop from the state with higher energy (higher cost) to the state with lower energy, thus local minima of this functional are usually related to the stable stationary states. Studying such states is part of the optimization problems, where the terms energy functional or cost functional are often used to describe the objective function. Examples In Hamiltonian systems, the energy functional is given by the Hamiltonian.",
                    "score": 0.8471354246139526
                },
                {
                    "id": 2777317,
                    "contents": "Pseudo-spectral method\nSuch polynomials occur naturally in several standard problems. For example, the quantum harmonic oscillator is ideally expanded in Hermite polynomials, and Jacobi-polynomials can be used to define the associated Legendre functions typically appearing in rotational problems. References Jie Shen, Tao Tang and Li-Lian Wang (2011) \"Spectral Methods: Algorithms, Analysis and Applications\" (Springer Series in Computational Mathematics, V. 41, Springer), . Numerical analysis",
                    "score": 0.8471166491508484
                },
                {
                    "id": 14720793,
                    "contents": "Introduction to eigenstates\nTherefore, it became necessary to formulate clearly the difference between the state of something that is uncertain in the way just described, such as an electron in a probability cloud, and the state of something having a definite value. When an object can definitely be \"pinned down\" in some respect, it is said to possess an eigenstate. As stated above, when the wavefunction collapses because the position of an electron has been determined, the electron's state becomes an \"eigenstate of position\", meaning that its position has a known value, an eigenvalue of the eigenstate of position.",
                    "score": 0.8471034169197083
                },
                {
                    "id": 8927838,
                    "contents": "Principle of minimum energy\nwe may then replace the and variables with a single variable and minimize with respect to this unconstrained variable. There may be any number of unconstrained variables depending on the number of atoms in the mixture. For systems with multiple sub-volumes, there may be additional volume constraints as well. The minimization is with respect to the unconstrained variables. In the case of chemical reactions this is usually the number of particles or mole fractions, subject to the conservation of elements. At equilibrium, these will take on their equilibrium values, and the internal energy will be a function only of the chosen value of entropy . By the definition of the Legendre transform, the Helmholtz free energy will be: The Helmholtz free energy at equilibrium will be: where is the (unknown) temperature at equilibrium. Substituting the expression for : By exchanging the order of the extrema: showing that the Helmholtz free energy is minimized at equilibrium.",
                    "score": 0.8470801115036011
                },
                {
                    "id": 12185441,
                    "contents": "Joan Vaccaro\nJoan Vaccaro is a physicist at Griffith University and a former student of David Pegg. Her work in quantum physics includes quantum phase, nonclassical states of light, coherent laser excitation of atomic gases, cold atomic gases, stochastic Schrödinger equations, quantum information theory, quantum references, wave–particle duality, quantum thermodynamics, and the physical nature of time.",
                    "score": 0.8470174074172974
                },
                {
                    "id": 6283173,
                    "contents": "Ritz method\nThe Ritz method can be used to achieve this goal. In the language of mathematics, it is exactly the finite element method used to compute the eigenvectors and eigenvalues of a Hamiltonian system. Discussion As with other variational methods, a trial wave function, , is tested on the system. This trial function is selected to meet boundary conditions (and any other physical constraints). The exact function is not known; the trial function contains one or more adjustable parameters, which are varied to find a lowest energy configuration. It can be shown that the ground state energy, , satisfies an inequality: That is, the ground-state energy is less than this value. The trial wave-function will always give an expectation value larger than or equal to the ground-energy. If the trial wave function is known to be orthogonal to the ground state, then it will provide a boundary for the energy of some excited state.",
                    "score": 0.8466932773590088
                },
                {
                    "id": 1113691,
                    "contents": "Energy level\nExplanation Quantized energy levels result from the wave behavior of particles, which gives a relationship between a particle's energy and its wavelength. For a confined particle such as an electron in an atom, the wave functions that have well defined energies have the form of a standing wave. States having well-defined energies are called stationary states because they are the states that do not change in time. Informally, these states correspond to a whole number of wavelengths of the wavefunction along a closed path (a path that ends where it started), such as a circular orbit around an atom, where the number of wavelengths gives the type of atomic orbital (0 for s-orbitals, 1 for p-orbitals and so on). Elementary examples that show mathematically how energy levels come about are the particle in a box and the quantum harmonic oscillator.",
                    "score": 0.8465883135795593
                },
                {
                    "id": 5492741,
                    "contents": "No free lunch in search and optimization\nOrigin Wolpert and Macready give two principal NFL theorems, the first regarding objective functions that do not change while search is in progress, and the second regarding objective functions that may change. Theorem 1: For any pair of algorithms a1 and a2 where denotes the ordered set of size of the cost values associated to input values , is the function being optimized and is the conditional probability of obtaining a given sequence of cost values from algorithm run times on function . In essence, this says that when all functions f are equally likely, the probability of observing an arbitrary sequence of m values in the course of search does not depend upon the search algorithm. Theorem 1 establishes a \"more subtle\" NFL result for time-varying objective functions. Interpretations of results",
                    "score": 0.8462669849395752
                },
                {
                    "id": 572492,
                    "contents": "Edwin Thompson Jaynes\nEdwin Thompson Jaynes (July 5, 1922 – April 30, 1998) was the Wayman Crow Distinguished Professor of Physics at Washington University in St. Louis. He wrote extensively on statistical mechanics and on foundations of probability and statistical inference, initiating in 1957 the maximum entropy interpretation of thermodynamics as being a particular application of more general Bayesian/information theory techniques (although he argued this was already implicit in the works of Josiah Willard Gibbs). Jaynes strongly promoted the interpretation of probability theory as an extension of logic. In 1963, together with Fred Cummings, he modeled the evolution of a two-level atom in an electromagnetic field, in a fully quantized way. This model is known as the Jaynes–Cummings model.",
                    "score": 0.8460347652435303
                },
                {
                    "id": 12721847,
                    "contents": "Landau–Zener formula\nThe Landau–Zener formula is an analytic solution to the equations of motion governing the transition dynamics of a two-state quantum system, with a time-dependent Hamiltonian varying such that the energy separation of the two states is a linear function of time. The formula, giving the probability of a diabatic (not adiabatic) transition between the two energy states, was published separately by Lev Landau, Clarence Zener, Ernst Stueckelberg, and Ettore Majorana, in 1932.",
                    "score": 0.8460167646408081
                },
                {
                    "id": 1011817,
                    "contents": "Quantum harmonic oscillator\nwhose solution is easily found to be the Gaussian Conceptually, it is important that there is only one solution of this equation; if there were, say, two linearly independent ground states, we would get two independent chains of eigenvectors for the harmonic oscillator. Once the ground state is computed, one can show inductively that the excited states are Hermite polynomials times the Gaussian ground state, using the explicit form of the raising operator in the position representation. One can also prove that, as expected from the uniqueness of the ground state, the Hermite functions energy eigenstates constructed by the ladder method form a complete orthonormal set of functions. Explicitly connecting with the previous section, the ground state |0⟩ in the position representation is determined by , hence so that , and so on.",
                    "score": 0.8457431793212891
                },
                {
                    "id": 21898901,
                    "contents": "Eigenstate thermalization hypothesis\nWe now imagine that we prepare our system in an initial state for which the expectation value of is far from its value predicted in a microcanonical ensemble appropriate to the energy scale in question (we assume that our initial state is some superposition of energy eigenstates which are all sufficiently \"close\" in energy). The eigenstate thermalization hypothesis says that for an arbitrary initial state, the expectation value of will ultimately evolve in time to its value predicted by a microcanonical ensemble, and thereafter will exhibit only small fluctuations around that value, provided that the following two conditions are met: The diagonal matrix elements vary smoothly as a function of energy, with the difference between neighboring values, , becoming exponentially small in the system size. The off-diagonal matrix elements , with , are much smaller than the diagonal matrix elements, and in particular are themselves exponentially small in the system size.",
                    "score": 0.8455225825309753
                },
                {
                    "id": 8927839,
                    "contents": "Principle of minimum energy\nwhere is the (unknown) temperature at equilibrium. Substituting the expression for : By exchanging the order of the extrema: showing that the Helmholtz free energy is minimized at equilibrium. The Enthalpy and Gibbs free energy, are similarly derived. References Thermodynamics",
                    "score": 0.8453561663627625
                },
                {
                    "id": 18951679,
                    "contents": "Quantum mechanics of time travel\nLloyd's prescription An alternative proposal was later presented by Seth Lloyd based upon post-selection and path integrals. In particular, the path integral is over single-valued fields, leading to self-consistent histories. He assumed it is ill-defined to speak of the actual density state of the CTC itself, and we should only focus upon the density state outside the CTC. His proposal for the time evolution of the external density state is , where . If , no solution exists due to destructive interference in the path integral. For instance, the grandfather paradox has no solution, and leads to an inconsistent state. If a solution exists, it is clearly unique. Now, quantum mechanics computers using time machines can only solve PP-complete problems. Entropy and computation",
                    "score": 0.8453434705734253
                },
                {
                    "id": 8181820,
                    "contents": "Maximum entropy thermodynamics\nIn physics, maximum entropy thermodynamics (colloquially, MaxEnt thermodynamics) views equilibrium thermodynamics and statistical mechanics as inference processes. More specifically, MaxEnt applies inference techniques rooted in Shannon information theory, Bayesian probability, and the principle of maximum entropy. These techniques are relevant to any situation requiring prediction from incomplete or insufficient data (e.g., image reconstruction, signal processing, spectral analysis, and inverse problems). MaxEnt thermodynamics began with two papers by Edwin T. Jaynes published in the 1957 Physical Review.",
                    "score": 0.8451002836227417
                },
                {
                    "id": 11147353,
                    "contents": "Joel Lebowitz\nLebowitz has been awarded several honors, such as the Boltzmann Medal (1992), the Nicholson Medal (1994) awarded by the American Physical Society, the Delmer S. Fahrney Medal (1995), the Henri Poincaré Prize (2000), the Volterra Award (2001), the Heineman Prize for Mathematical Physics (2021) and many others. Among other recognitions, Lebowitz was awarded the Max Planck Medal in 2007 \"for his important contributions to the statistical physics of equilibrium and non-equilibrium systems, in particular his contributions to the theory of phase transitions, the dynamics of infinite systems, and the stationary non-equilibrium states\" and \"for his promoting of new directions of this field at its farthest front, and for enthusiastically introducing several generations of scientists to the field.\" In 2014 he received the Grande Médaille of the French Academy of Sciences. His Heineman Prize citation reads: \"For seminal contributions to nonequilibrium and equilibrium statistical mechanics, in",
                    "score": 0.8449496626853943
                },
                {
                    "id": 5311714,
                    "contents": "Franck–Condon principle\nOverview The Franck–Condon principle has a well-established semiclassical interpretation based on the original contributions of James Franck. Electronic transitions are relatively instantaneous compared with the time scale of nuclear motions, therefore if the molecule is to move to a new vibrational level during the electronic transition, this new vibrational level must be instantaneously compatible with the nuclear positions and momenta of the vibrational level of the molecule in the originating electronic state. In the semiclassical picture of vibrations (oscillations) of a simple harmonic oscillator, the necessary conditions can occur at the turning points, where the momentum is zero.",
                    "score": 0.8449100852012634
                },
                {
                    "id": 2838152,
                    "contents": "Variational principle\nJohn Venables, \"The Variational Principle and some applications\". Dept of Physics and Astronomy, Arizona State University, Tempe, Arizona (Graduate Course: Quantum Physics) Andrew James Williamson, \"The Variational Principle -- Quantum monte carlo calculations of electronic excitations\". Robinson College, Cambridge, Theory of Condensed Matter Group, Cavendish Laboratory. September 1996. (dissertation of Doctor of Philosophy) Kiyohisa Tokunaga, \"Variational Principle for Electromagnetic Field\". Total Integral for Electromagnetic Canonical Action, Part Two, Relativistic Canonical Theory of Electromagnetics, Chapter VI Komkov, Vadim (1986) Variational principles of continuum mechanics with engineering applications. Vol. 1. Critical points theory. Mathematics and its Applications, 24. D. Reidel Publishing Co., Dordrecht. Cassel, Kevin W.: Variational Methods with Applications in Science and Engineering, Cambridge University Press, 2013.",
                    "score": 0.8444478511810303
                },
                {
                    "id": 9025033,
                    "contents": "Stationary state\nStationary states are quantum states that are solutions to the time-independent Schrödinger equation: where is a quantum state, which is a stationary state if it satisfies this equation; is the Hamiltonian operator; is a real number, and corresponds to the energy eigenvalue of the state . This is an eigenvalue equation: is a linear operator on a vector space, is an eigenvector of , and is its eigenvalue. If a stationary state is plugged into the time-dependent Schrödinger Equation, the result is: Assuming that is time-independent (unchanging in time), this equation holds for any time . Therefore, this is a differential equation describing how varies in time. Its solution is: Therefore, a stationary state is a standing wave that oscillates with an overall complex phase factor, and its oscillation angular frequency is equal to its energy divided by . Stationary state properties As shown above, a stationary state is not mathematically constant:",
                    "score": 0.8443301916122437
                },
                {
                    "id": 21659181,
                    "contents": "Free energy principle\nThe free energy principle is a formal statement that explains how living and non-living systems remain in non-equilibrium steady-states by restricting themselves to a limited number of states. It establishes that systems minimise a free energy function of their internal states (not to be confused with thermodynamic free energy), which entail beliefs about hidden states in their environment. The implicit minimisation of free energy is formally related to variational Bayesian methods and was originally introduced by Karl Friston as an explanation for embodied perception in neuroscience, where it is also known as active inference.",
                    "score": 0.8443080186843872
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_16",
        "question": "One of the most powerful modern techniques for studying structure is neutron diffraction. This technique involves generating a collimated beam of neutrons at a particular temperature from a high-energy neutron source and is accomplished at several accelerator facilities around the world. If the speed of a neutron is given by $v_{\\mathrm{n}}=\\left(3 k_{\\mathrm{B}} T / m\\right)^{1 / 2}$, where $m$ is the mass of a neutron, then what temperature is needed so that the neutrons have a de Broglie wavelength of $50 \\mathrm{pm}$ ?",
        "golden_answers": [
            " 2500"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1451010,
                    "contents": "Neutron diffraction\nNeutron diffraction or elastic neutron scattering is the application of neutron scattering to the determination of the atomic and/or magnetic structure of a material. A sample to be examined is placed in a beam of thermal or cold neutrons to obtain a diffraction pattern that provides information of the structure of the material. The technique is similar to X-ray diffraction but due to their different scattering properties, neutrons and X-rays provide complementary information: X-Rays are suited for superficial analysis, strong x-rays from synchrotron radiation are suited for shallow depths or thin specimens, while neutrons having high penetration depth are suited for bulk samples.",
                    "score": 0.9471225738525391
                },
                {
                    "id": 20965474,
                    "contents": "Neutron spectroscopy\nNeutron scattering is a spectroscopic method of measuring the atomic and magnetic motions of atoms. Inelastic neutron scattering observes the change in the energy of the neutron as it scatters from a sample and can be used to probe a wide variety of different physical phenomena such as the motions of atoms (diffusional or hopping), the rotational modes of molecules, sound modes and molecular vibrations, recoil in quantum fluids, magnetic and quantum excitations or even electronic transitions. Since its discovery, neutron spectroscopy has also become useful in medicine as it has been applied to radiation protection and radiation therapy. Although neutron spectroscopy is capable of operating on many orders of magnitude of electron volts, current and recent research has focused on expanding neutron scattering to higher energy levels. See also Neutron diffraction Raman scattering Nested Neutron Spectrometer References",
                    "score": 0.9403207302093506
                },
                {
                    "id": 1451017,
                    "contents": "Neutron diffraction\nNeutron diffraction is closely related to X-ray powder diffraction. In fact, the single crystal version of the technique is less commonly used because currently available neutron sources require relatively large samples and large single crystals are hard or impossible to come by for most materials. Future developments, however, may well change this picture. Because the data is typically a 1D powder diffractogram they are usually processed using Rietveld refinement. In fact the latter found its origin in neutron diffraction (at Petten in the Netherlands) and was later extended for use in X-ray diffraction.",
                    "score": 0.939448356628418
                },
                {
                    "id": 4452391,
                    "contents": "Neutron scattering\nInelastic neutron scattering Inelastic neutron scattering is an experimental technique commonly used in condensed matter research to study atomic and molecular motion as well as magnetic and crystal field excitations. It distinguishes itself from other neutron scattering techniques by resolving the change in kinetic energy that occurs when the collision between neutrons and the sample is an inelastic one. Results are generally communicated as the dynamic structure factor (also called inelastic scattering law) , sometimes also as the dynamic susceptibility where the scattering vector is the difference between incoming and outgoing wave vector, and is the energy change experienced by the sample (negative that of the scattered neutron). When results are plotted as function of , they can often be interpreted in the same way as spectra obtained by conventional spectroscopic techniques; insofar as inelastic neutron scattering can be seen as a special spectroscopy.",
                    "score": 0.9375190734863281
                },
                {
                    "id": 4452384,
                    "contents": "Neutron scattering\nNeutron scattering, the irregular dispersal of free neutrons by matter, can refer to either the naturally occurring physical process itself or to the man-made experimental techniques that use the natural process for investigating materials. The natural/physical phenomenon is of elemental importance in nuclear engineering and the nuclear sciences. Regarding the experimental technique, understanding and manipulating neutron scattering is fundamental to the applications used in crystallography, physics, physical chemistry, biophysics, and materials research. Neutron scattering is practiced at research reactors and spallation neutron sources that provide neutron radiation of varying intensities. Neutron diffraction (elastic scattering) techniques are used for analyzing structures; where inelastic neutron scattering is used in studying atomic vibrations and other excitations. Scattering of fast neutrons",
                    "score": 0.9368913769721985
                },
                {
                    "id": 1451013,
                    "contents": "Neutron diffraction\nNuclear scattering Like all quantum particles, neutrons can exhibit wave phenomena typically associated with light or sound. Diffraction is one of these phenomena; it occurs when waves encounter obstacles whose size is comparable with the wavelength. If the wavelength of a quantum particle is short enough, atoms or their nuclei can serve as diffraction obstacles. When a beam of neutrons emanating from a reactor is slowed and selected properly by their speed, their wavelength lies near one angstrom (0.1 nanometer), the typical separation between atoms in a solid material. Such a beam can then be used to perform a diffraction experiment. Impinging on a crystalline sample, it will scatter under a limited number of well-defined angles, according to the same Bragg's law that describes X-ray diffraction.",
                    "score": 0.9317829608917236
                },
                {
                    "id": 1451025,
                    "contents": "Neutron diffraction\nwork (1946) and the Nobel Prize awarded to Brockhouse and Shull (1994) brings them close to the delay between the invention by Ernst Ruska of the electron microscope (1933) - also in the field of particle optics - and his own Nobel prize (1986). This in turn is near to the record of 55 years between the discoveries of Peyton Rous and his award of the Nobel Prize in 1966.",
                    "score": 0.9298515319824219
                },
                {
                    "id": 1451015,
                    "contents": "Neutron diffraction\nThe nuclei of atoms, from which neutrons scatter, are tiny. Furthermore, there is no need for an atomic form factor to describe the shape of the electron cloud of the atom and the scattering power of an atom does not fall off with the scattering angle as it does for X-rays. Diffractograms therefore can show strong, well-defined diffraction peaks even at high angles, particularly if the experiment is done at low temperatures. Many neutron sources are equipped with liquid helium cooling systems that allow data collection at temperatures down to 4.2 K. The superb high angle (i.e. high resolution) information means that the atomic positions in the structure can be determined with high precision. On the other hand, Fourier maps (and to a lesser extent difference Fourier maps) derived from neutron data suffer from series termination errors, sometimes so much that the results are meaningless.",
                    "score": 0.9285053014755249
                },
                {
                    "id": 22217854,
                    "contents": "Discovery of the neutron\nThe discovery of the neutron immediately gave scientists a new tool for probing the properties of atomic nuclei. Alpha particles had been used over the previous decades in scattering experiments, but such particles, which are helium nuclei, have +2 charge. This charge makes it difficult for alpha particles to overcome the Coulomb repulsive force and interact directly with the nuclei of atoms. Since neutrons have no electric charge, they do not have to overcome this force to interact with nuclei. Almost coincident with their discovery, neutrons were used by Norman Feather, Chadwick's colleague and protege, in scattering experiments with nitrogen. Feather was able to show that neutrons interacting with nitrogen nuclei scattered to protons or induced nitrogen to disintegrate to form boron with the emission of an alpha particle. Feather was therefore the first to show that neutrons produce nuclear disintegrations.",
                    "score": 0.9284644722938538
                },
                {
                    "id": 3402464,
                    "contents": "Clifford Shull\nClifford Glenwood Shull (September 23, 1915 in Pittsburgh, Pennsylvania – March 31, 2001) was a Nobel Prize-winning American physicist. Biography He attended Schenley High School in Pittsburgh, received BS from Carnegie Institute of Technology and PhD from New York University. He worked for The Texas Company at Beacon, New York during the wartime, followed by a position in the Clinton Laboratory (Oak Ridge National Laboratory), and finally joined MIT in 1955, and retired in 1986. Research Clifford G. Shull was awarded the 1994 Nobel Prize in Physics with Canadian Bertram Brockhouse. The two won the prize for the development of the neutron scattering technique. He also conducted research on condensed matter. Professor Shull's prize was awarded for his pioneering work in neutron scattering, a technique that reveals where atoms are within a material like ricocheting bullets reveal where obstacles are in the dark.",
                    "score": 0.9255398511886597
                },
                {
                    "id": 14106158,
                    "contents": "George Bacon (physicist)\nCareer Bacon built the first neutron diffractometer in the UK at the Atomic Energy Research Establishment, Harwell, and then made the first experiments outside the United States in neutron diffraction, a means of studying the basic structures and dynamics of materials. He wrote scientific papers, textbooks and review articles on the use of neutron scattering. His work included a study of the neutron intensities diffracted by crystals, which led to studies of the basic structure of molecules and of magnetism. His book Neutron Diffraction was a standard text, beginning with the first edition in 1955. 1939–1946: Scientific Officer, Telecommunications Research Establishment of the Air Ministry 1946–1963: Deputy Chief Scientific Officer, Atomic Energy Research Establishment, Harwell 1963–1981: Professor of Physics at the University of Sheffield 1969–1971: Dean of the Faculty of Pure Science, University of Sheffield 1981 to date: Emeritus professor of the University of Sheffield",
                    "score": 0.9247774481773376
                },
                {
                    "id": 1451024,
                    "contents": "Neutron diffraction\nThe first neutron diffraction experiments were carried out in 1945 by Ernest O. Wollan using the Graphite Reactor at Oak Ridge. He was joined shortly thereafter (June 1946) by Clifford Shull, and together they established the basic principles of the technique, and applied it successfully to many different materials, addressing problems like the structure of ice and the microscopic arrangements of magnetic moments in materials. For this achievement, Shull was awarded one half of the 1994 Nobel Prize in Physics. (Wollan died in 1984). (The other half of the 1994 Nobel Prize for Physics went to Bert Brockhouse for development of the inelastic scattering technique at the Chalk River facility of AECL. This also involved the invention of the triple axis spectrometer). The delay between the achieved work (1946) and the Nobel Prize awarded to Brockhouse and Shull (1994) brings them close to the delay between the invention by Ernst Ruska of the electron microscope (1933) - also in the field of",
                    "score": 0.9230542182922363
                },
                {
                    "id": 4452387,
                    "contents": "Neutron scattering\nNeutron scattering can be incoherent or coherent, also depending on isotope. Among all isotopes, hydrogen has the highest scattering cross section. Important elements like carbon and oxygen are quite visible in neutron scattering—this is in marked contrast to X-ray scattering where cross sections systematically increase with atomic number. Thus neutrons can be used to analyze materials with low atomic numbers, including proteins and surfactants. This can be done at synchrotron sources but very high intensities are needed, which may cause the structures to change. The nucleus provides a very short range, as isotropic potential varies randomly from isotope to isotope, which makes it possible to tune the (scattering) contrast to suit the experiment.",
                    "score": 0.9215773344039917
                },
                {
                    "id": 8298167,
                    "contents": "ISIS Neutron and Muon Source\nIris is a neutron spectrometer, designed for quasi-elastic and low-energy high resolution inelastic spectroscopy. LOQ is a small angle neutron scattering instrument used to investigate the shape and size of large molecules, small particles or porous materials with dimensions typically in the range of 1 - 100 nm. Maps is a neutron spectrometer, primarily designed to tackle magnetic and structural excitations in single crystals. MARI is a neutron spectrometer, ideal for the study of phonon densities of states in crystalline and disordered systems, and crystal field excitations in magnetic materials. Merlin is a neutron spectrometer with a high count rate, medium energy resolution, direct geometry chopper spectrometer. Osiris can be used as a neutron spectrometer or diffractometer. It is optimised for very low energy studies and long wavelength diffraction Pearl is a neutron diffractometer dedicated to high-pressure powder diffraction.",
                    "score": 0.9188133478164673
                },
                {
                    "id": 1131378,
                    "contents": "Neutron\nThe energy of the gamma ray can be measured to high precision by X-ray diffraction techniques, as was first done by Bell and Elliot in 1948. The best modern (1986) values for neutron mass by this technique are provided by Greene, et al. These give a neutron mass of: mneutron = The value for the neutron mass in MeV is less accurately known, due to less accuracy in the known conversion of Da to MeV/c2: mneutron = . Another method to determine the mass of a neutron starts from the beta decay of the neutron, when the momenta of the resulting proton and electron are measured. Electric charge The total electric charge of the neutron is . This zero value has been tested experimentally, and the present experimental limit for the charge of the neutron is , or . This value is consistent with zero, given the experimental uncertainties (indicated in parentheses). By comparison, the charge of the proton is . Magnetic moment",
                    "score": 0.9186046123504639
                },
                {
                    "id": 4452393,
                    "contents": "Neutron scattering\nHistory The first neutron diffraction experiments were performed in the 1930s. However it was not until around 1945, with the advent of nuclear reactors, that high neutron fluxes became possible, leading to the possibility of in-depth structure investigations. The first neutron-scattering instruments were installed in beam tubes at multi-purpose research reactors. In the 1960s, high-flux reactors were built that were optimized for beam-tube experiments. The development culminated in the high-flux reactor of the Institut Laue-Langevin (in operation since 1972) that achieved the highest neutron flux to this date. Besides a few high-flux sources, there were some twenty medium-flux reactor sources at universities and other research institutes. Starting in the 1980s, many of these medium-flux sources were shut down, and research concentrated at a few world-leading high-flux sources. Facilities",
                    "score": 0.9180665016174316
                },
                {
                    "id": 22217851,
                    "contents": "Discovery of the neutron\nNeutron physics in the 1930s",
                    "score": 0.9175031185150146
                },
                {
                    "id": 1337596,
                    "contents": "X-ray crystallography\nNeutron diffraction is an excellent method for structure determination, although it has been difficult to obtain intense, monochromatic beams of neutrons in sufficient quantities. Traditionally, nuclear reactors have been used, although sources producing neutrons by spallation are becoming increasingly available. Being uncharged, neutrons scatter much more readily from the atomic nuclei rather than from the electrons. Therefore, neutron scattering is very useful for observing the positions of light atoms with few electrons, especially hydrogen, which is essentially invisible in the X-ray diffraction. Neutron scattering also has the remarkable property that the solvent can be made invisible by adjusting the ratio of normal water, H2O, and heavy water, D2O. Methods Overview of single-crystal X-ray diffraction",
                    "score": 0.9160153865814209
                },
                {
                    "id": 3402467,
                    "contents": "Clifford Shull\nIn Professor Shull's opinion the most important problem he worked on at the time dealt with determining the positions of hydrogen atoms in materials. \"Hydrogen atoms are ubiquitous in all biological materials and in many other inorganic materials,\" he once said, \"but you couldn't see them with other techniques. With neutrons it turned out that that was completely different, and we were very pleased and happy to find that we could learn things about hydrogen-containing structures.\" As he refined the scattering technique, Professor Shull studied the fundamental properties of the neutron itself. He also initiated the first neutron diffraction investigations of magnetic materials. ... \"If there is a ... 'Father of Neutron Scattering' in the United States, it is Professor Shull,\" wrote Anthony Nunes ..., professor of physics at the University of Rhode Island. ...",
                    "score": 0.9151961803436279
                },
                {
                    "id": 1451012,
                    "contents": "Neutron diffraction\nThe technique also requires a device that can detect the neutrons after they have been scattered. Summarizing, the main disadvantage to neutron diffraction is the requirement for a nuclear reactor. For single crystal work, the technique requires relatively large crystals, which are usually challenging to grow. The advantages to the technique are many - sensitivity to light atoms, ability to distinguish isotopes, absence of radiation damage, as well as a penetration depth of several cm",
                    "score": 0.914543867111206
                },
                {
                    "id": 6013926,
                    "contents": "Spallation Neutron Source\nNeutron scattering research Neutron scattering allows scientists to count scattered neutrons, measure their energies and the angles at which they scatter, and map their final positions. This information can reveal the molecular and magnetic structure and behavior of materials, such as high-temperature superconductors, polymers, metals, and biological samples. In addition to studies focused on fundamental physics, neutron scattering research has applications in structural biology and biotechnology, magnetism and superconductivity, chemical and engineering materials, nanotechnology, complex fluids, and others. How SNS works",
                    "score": 0.9141324758529663
                },
                {
                    "id": 2127783,
                    "contents": "Small-angle neutron scattering\nSmall-angle neutron scattering (SANS) is an experimental technique that uses elastic neutron scattering at small scattering angles to investigate the structure of various substances at a mesoscopic scale of about 1–100 nm. Small angle neutron scattering is in many respects very similar to small-angle X-ray scattering (SAXS); both techniques are jointly referred to as small-angle scattering (SAS). Advantages of SANS over SAXS are its sensitivity to light elements, the possibility of isotope labelling, and the strong scattering by magnetic moments.",
                    "score": 0.9139496684074402
                },
                {
                    "id": 6724873,
                    "contents": "Powder diffraction\nParticularly for neutron diffraction, which requires larger samples than X-ray diffraction due to a relatively weak scattering cross section, the ability to use large samples can be critical, although newer and more brilliant neutron sources are being built that may change this picture. Since all possible crystal orientations are measured simultaneously, collection times can be quite short even for small and weakly scattering samples. This is not merely convenient, but can be essential for samples which are unstable either inherently or under X-ray or neutron bombardment, or for time-resolved studies. For the latter it is desirable to have a strong radiation source. The advent of synchrotron radiation and modern neutron sources has therefore done much to revitalize the powder diffraction field because it is now possible to study temperature dependent changes, reaction kinetics and so forth by means of time-resolved powder diffraction. See also",
                    "score": 0.9134812951087952
                },
                {
                    "id": 8298172,
                    "contents": "ISIS Neutron and Muon Source\nNIMROD is a neutron diffractometer designed to access length scales ranging from the interatomic (< 1 Å) through to the mesoscopic (>300 Å). Offspec is a neutron reflectometer that gives access to nanometre length scales parallel and perpendicular to interfaces. Polref is a neutron reflectometer designed for the study of the magnetic ordering in and between the layers and surfaces of thin film materials. Sans2d is a small angle neutron scattering instruments that can be used to examine size, shape, internal structure and spatial arrangement in nanomaterials, ‘soft matter’, and colloidal systems, including those of biological origin, on length scales of between* 0.25-300 nm. Wish is a neutron diffractometer designed for powder diffraction at long d-spacing in magnetic and large unit cell systems, with the option of enabling single-crystal and polarised beam experiments. Zoom is a flexible, high count rate small-angle scattering instrument.",
                    "score": 0.9126413464546204
                },
                {
                    "id": 7383582,
                    "contents": "Neutron detection\nParticle physics: Neutron detection has been proposed as a method of enhancing neutrino detectors. Materials science: Elastic and inelastic neutron scattering enables experimentalists to characterize the morphology of materials from scales ranging from ångströms to about one micrometer. Radiation safety: Neutron radiation is a hazard associated with neutron sources, space travel, accelerators and nuclear reactors. Neutron detectors used for radiation safety must take into account the relative biological effectiveness (i.e., the way damage caused by neutrons varies with energy). Cosmic ray detection: Secondary neutrons are one component of particle showers produced in Earth's atmosphere by cosmic rays. Dedicated ground-level neutron detectors, namely neutron monitors, are employed to monitor variations in cosmic ray flux.",
                    "score": 0.912010669708252
                },
                {
                    "id": 22512642,
                    "contents": "Ernest O. Wollan\nWorking at the Oak Ridge National Laboratory (ORNL) after World War II, he continued study into neutron scattering, using the neutrons emitted from the X-10 Graphite Reactor and a modified X-ray diffractometer. In collaboration with Clifford G. Shull, who joined him at ORNL in 1946, he developed neutron diffraction methodology used for determining atomic resolution structure of substances. In 1994, Shull was awarded a share of the Nobel Prize in Physics for his work on neutron diffraction. The unusual delay in the Nobel award, which came more than four decades after the work that it recognized, may have resulted from negative views of the work's relationship to nuclear power. Wollan was not eligible for the Nobel because he had died 10 years earlier. In his Nobel lecture Shull spoke of Wollan's contributions and expressed regret that his colleague had not lived long enough to share in the prize.",
                    "score": 0.9116218090057373
                },
                {
                    "id": 1451020,
                    "contents": "Neutron diffraction\nHydrogen, null-scattering and contrast variation Neutron diffraction can be used to establish the structure of low atomic number materials like proteins and surfactants much more easily with lower flux than at a synchrotron radiation source. This is because some low atomic number materials have a higher cross section for neutron interaction than higher atomic weight materials. One major advantage of neutron diffraction over X-ray diffraction is that the latter is rather insensitive to the presence of hydrogen (H) in a structure, whereas the nuclei 1H and 2H (i.e. Deuterium, D) are strong scatterers for neutrons. The greater scattering power of protons and deuterons means that the position of hydrogen in a crystal and its thermal motions can be determined with greater precision by neutron diffraction. The structures of metal hydride complexes, e.g., Mg2FeH6 have been assessed by neutron diffraction.",
                    "score": 0.9114739298820496
                },
                {
                    "id": 8472027,
                    "contents": "Neutron temperature\nThe neutron detection temperature, also called the neutron energy, indicates a free neutron's kinetic energy, usually given in electron volts. The term temperature is used, since hot, thermal and cold neutrons are moderated in a medium with a certain temperature. The neutron energy distribution is then adapted to the Maxwellian distribution known for thermal motion. Qualitatively, the higher the temperature, the higher the kinetic energy of the free neutrons. The momentum and wavelength of the neutron are related through the de Broglie relation. The large wavelength of slow neutrons allows for the large cross section. Neutron energy distribution ranges But different ranges with different names are observed in other sources. The following is a detailed classification: Thermal",
                    "score": 0.9099857807159424
                },
                {
                    "id": 1451016,
                    "contents": "Neutron diffraction\nMagnetic scattering Although neutrons are uncharged, they carry a magnetic moment, and therefore interact with magnetic moments, including those arising from the electron cloud around an atom. Neutron diffraction can therefore reveal the microscopic magnetic structure of a material. Magnetic scattering does require an atomic form factor as it is caused by the much larger electron cloud around the tiny nucleus. The intensity of the magnetic contribution to the diffraction peaks will therefore decrease towards higher angles. Uses Neutron diffraction can be used to determine the static structure factor of gases, liquids or amorphous solids. Most experiments, however, aim at the structure of crystalline solids, making neutron diffraction an important tool of crystallography.",
                    "score": 0.9095787405967712
                },
                {
                    "id": 2022638,
                    "contents": "Institut Laue–Langevin\nThe Institut Laue–Langevin (ILL) is an internationally financed scientific facility, situated on the Polygone Scientifique in Grenoble, France. It is one of the world centres for research using neutrons. Founded in 1967 and honouring the physicists Max von Laue and Paul Langevin, the ILL provides one of the most intense neutron sources in the world and the most intense continuous neutron flux in the world in the moderator region: 1.5×1015 neutrons per second per cm2, with a thermal power of typically 58.3 MW. The ILL neutron scattering facilities allow the analysis of the structure of conducting and magnetic materials for future electronic devices, the measurement of stresses in mechanical materials. It also allows investigations into macromolecular assemblies, particularly protein dynamics and biomolecular structure. It is a world-renowned centre for nanoscale science. History",
                    "score": 0.9095290899276733
                },
                {
                    "id": 22217804,
                    "contents": "Discovery of the neutron\nThe discovery of the neutron and its properties was central to the extraordinary developments in atomic physics in the first half of the 20th century. Early in the century, Ernest Rutherford developed a crude model of the atom, based on the gold foil experiment of Hans Geiger and Ernest Marsden. In this model, atoms had their mass and positive electric charge concentrated in a very small nucleus. By 1920 chemical isotopes had been discovered, the atomic masses had been determined to be (approximately) integer multiples of the mass of the hydrogen atom, and the atomic number had been identified as the charge on the nucleus. Throughout the 1920s, the nucleus was viewed as composed of combinations of protons and electrons, the two elementary particles known at the time, but that model presented several experimental and theoretical contradictions.",
                    "score": 0.9094854593276978
                },
                {
                    "id": 28125608,
                    "contents": "Ben Nijboer\nWork in Princeton A Fulbright scholarship enabled him in 1949 to work in Princeton, New Jersey (USA) at the Institute for Advanced Study for a year. He stayed there from September 1, 1949 to June 1, 1950 and met physicists like Albert Einstein, Robert Oppenheimer, John von Neumann, Abraham Pais, Rudolf Peierls and Freeman Dyson. He worked with George Placzek and Léon Van Hove on the theory of neutron diffraction. The nuclear reactors, due to their large-scale production of neutrons, offered a new possibility for the study of solids and liquids, from which more information could be derived than from the X-ray scattering. An X-ray is a snapshot of the position of the atoms, while neutron scattering gives information about their movements. The interpretation of this information required a theory, which was developed by Placzek, Van Hove and Nijboer. Since then, this is the basis of this entire field of research. Professor in Utrecht",
                    "score": 0.9084360599517822
                },
                {
                    "id": 19316782,
                    "contents": "Quasielastic neutron scattering\nQuasielastic neutron scattering (QENS) designates a limiting case of inelastic neutron scattering, characterized by energy transfers being small compared to the incident energy of the scattered particles. In a more strict meaning, it denotes scattering processes where dynamics in the sample (such as diffusive dynamics) lead to a broadening of the incident neutron spectrum, in contrast to, e.g., the scattering from a diffusionless crystal, where the scattered neutron energy spectrum consists of an elastic line (corresponding to no energy transfer with the sample) and a number of well-separated inelastic lines due to the creation or annihilation of phonons with specific energies. The term quasielastic scattering was originally coined in nuclear physics. It was applied to thermal neutron scattering since the early 1960s, notably in an article by Leon van Hove and in a highly cited one by Pierre Gilles de Gennes.",
                    "score": 0.9078429937362671
                },
                {
                    "id": 22217819,
                    "contents": "Discovery of the neutron\nWithin a year it was noted that the equation for the relation, now called Moseley's law, could be explained in terms of the 1913 Bohr model, with reasonable extra assumptions about atomic structure in other elements. Moseley's result, by Bohr's later account, not only established atomic number as a measurable experimental quantity, but gave it a physical meaning as the positive charge on the atomic nucleus. The elements could be ordered in the periodic system in order of atomic number, rather than atomic weight. The result tied together the organization of the periodic table, the Bohr model for the atom, and Rutherford's model for alpha scattering from nuclei. It was cited by Rutherford, Bohr, and others as a critical advance in understanding the nature of the atomic nucleus.",
                    "score": 0.9071298837661743
                },
                {
                    "id": 1451021,
                    "contents": "Neutron diffraction\nThe neutron scattering lengths bH = −3.7406(11) fm and bD = 6.671(4) fm, for H and D respectively, have opposite sign, which allows the technique to distinguish them. In fact there is a particular isotope ratio for which the contribution of the element would cancel, this is called null-scattering.",
                    "score": 0.9063053131103516
                },
                {
                    "id": 25985700,
                    "contents": "Local structure\nThe local structure is a term in nuclear spectroscopy that refers to the structure of the nearest neighbours around an atom in crystals and molecules. E.g. in crystals the atoms order in a regular fashion on wide ranges to form even gigantic highly ordered crystals (Naica Mine). However, in reality, crystals are never perfect and have impurities or defects, which means that a foreign atom resides on a lattice site or in between lattice sites (interstitials). These small defects and impurities cannot be seen by methods such as X-ray diffraction or neutron diffraction, because these methods average in their nature of measurement over a large number of atoms and thus are insensitive to effects in local structure. Methods in nuclear spectroscopy use specific nuclei as probe. The nucleus of an atom is about 10,000 to 150,000 times smaller than the atom itself. It experiences the electric fields created by the atom's electrons that surround the nucleus. In addition, the electric fields",
                    "score": 0.9061288833618164
                },
                {
                    "id": 4452390,
                    "contents": "Neutron scattering\nMagnetic scattering The neutron has a net electric charge of zero, but has a significant magnetic moment, although only about 0.1% of that of the electron. Nevertheless, it is large enough to scatter from local magnetic fields inside condensed matter, providing a weakly interacting and hence penetrating probe of ordered magnetic structures and electron spin fluctuations. Inelastic neutron scattering",
                    "score": 0.9058786630630493
                },
                {
                    "id": 22217818,
                    "contents": "Discovery of the neutron\nAt the University of Manchester in 1913 Henry Moseley discussed the new Bohr model of the atom with the visiting Bohr. The model accounted for the electromagnetic emission spectrum from the hydrogen atom, and Moseley and Bohr wondered if the electromagnetic emission spectra of heavier elements such as cobalt and nickel would follow their ordering by weight, or by their position in the periodic table. In 1913-1914 Moseley tested the question experimentally by using X-ray diffraction techniques. He found that the most intense short-wavelength line in the X-ray spectrum of a particular element, known as the K-alpha line, was related to the element's position in the periodic table, that is, its atomic number, Z. Indeed, Moseley introduced this nomenclature. Moseley found that the frequencies of the radiation were related in a simple way to the atomic number of the elements for a large number of elements.",
                    "score": 0.9055262207984924
                },
                {
                    "id": 4452388,
                    "contents": "Neutron scattering\nScattering almost always presents both elastic and inelastic components. The fraction of elastic scattering is determined by the Debye-Waller factor or the Mössbauer-Lamb factor. Depending on the research question, most measurements concentrate on either elastic or inelastic scattering.",
                    "score": 0.9052190780639648
                },
                {
                    "id": 3870571,
                    "contents": "Neutron transport\nNeutron transport (also known as neutronics) is the study of the motions and interactions of neutrons with materials. Nuclear scientists and engineers often need to know where neutrons are in an apparatus, what direction they are going, and how quickly they are moving. It is commonly used to determine the behavior of nuclear reactor cores and experimental or industrial neutron beams. Neutron transport is a type of radiative transport. Background",
                    "score": 0.9047588109970093
                },
                {
                    "id": 2641322,
                    "contents": "Neutron radiation\nNeutron radiation is a form of ionizing radiation that presents as free neutrons. Typical phenomena are nuclear fission or nuclear fusion causing the release of free neutrons, which then react with nuclei of other atoms to form new isotopes—which, in turn, may trigger further neutron radiation. Free neutrons are unstable, decaying into a proton, an electron, plus an electron antineutrino with a mean lifetime of 887 seconds (14 minutes, 47 seconds). Sources Neutrons may be emitted from nuclear fusion or nuclear fission, or from other nuclear reactions such as radioactive decay or particle interactions with cosmic rays or within particle accelerators. Large neutron sources are rare, and usually limited to large-sized devices such as nuclear reactors or particle accelerators, including the Spallation Neutron Source.",
                    "score": 0.9045483469963074
                },
                {
                    "id": 22217805,
                    "contents": "Discovery of the neutron\nThe essential nature of the atomic nucleus was established with the discovery of the neutron by James Chadwick in 1932 and the determination that it was a new elementary particle, distinct from the proton. The uncharged neutron was immediately exploited as a new means to probe nuclear structure, leading to such discoveries as the creation of new radioactive elements by neutron irradiation (1934) and the fission of uranium atoms by neutrons (1938). The discovery of fission led to the creation of both nuclear power and nuclear weapons by the end of World War II. Both the proton and the neutron were presumed to be elementary particles until the 1960s, when they were determined to be composite particles built from quarks.",
                    "score": 0.9040982723236084
                },
                {
                    "id": 6724859,
                    "contents": "Powder diffraction\nA further complication in the case of neutron scattering from hydrogenous materials is the strong incoherent scattering of hydrogen (80.27(6) barn). This leads to a very high background in neutron diffraction experiments, and may make structural investigations impossible. A common solution is deuteration, i.e., replacing the 1-H atoms in the sample with deuterium (2-H). The incoherent scattering length of deuterium is much smaller (2.05(3) barn) making structural investigations significantly easier. However, in some systems, replacing hydrogen with deuterium may alter the structural and dynamic properties of interest. As neutrons also have a magnetic moment, they are additionally scattered by any magnetic moments in a sample. In the case of long range magnetic order, this leads to the appearance of new Bragg reflections. In most simple cases, powder diffraction may be used to determine the size of the moments and their spatial orientation. Aperiodically arranged clusters",
                    "score": 0.903581976890564
                },
                {
                    "id": 6724870,
                    "contents": "Powder diffraction\nThe tunability of the wavelength also makes it possible to observe anomalous scattering effects when the wavelength is chosen close to the absorption edge of one of the elements of the sample. Neutron diffraction has never been an in house technique because it requires the availability of an intense neutron beam only available at a nuclear reactor or spallation source. Typically the available neutron flux, and the weak interaction between neutrons and matter, require relative large samples. Advantages and disadvantages",
                    "score": 0.9028931856155396
                },
                {
                    "id": 6724839,
                    "contents": "Powder diffraction\nPowder diffraction is a scientific technique using X-ray, neutron, or electron diffraction on powder or microcrystalline samples for structural characterization of materials. An instrument dedicated to performing such powder measurements is called a powder diffractometer. Powder diffraction stands in contrast to single crystal diffraction techniques, which work best with a single, well-ordered crystal. Explanation",
                    "score": 0.9024122953414917
                },
                {
                    "id": 10690942,
                    "contents": "Neutron magnetic moment\nSince neutrons are neutral particles, they do not have to overcome Coulomb repulsion as they approach charged targets, as experienced by protons or alpha particles. Neutrons can deeply penetrate matter. The magnetic moment of the neutron has therefore been exploited to probe the properties of matter using scattering or diffraction techniques. These methods provide information that is complementary to X-ray spectroscopy. In particular, the magnetic moment of the neutron is used to determine magnetic properties of materials at length scales of 1–100 Å using cold or thermal neutrons. Bertram Brockhouse and Clifford Shull won the Nobel Prize in physics in 1994 for developing these scattering techniques.",
                    "score": 0.9023518562316895
                },
                {
                    "id": 1628656,
                    "contents": "Condensed matter physics\nNeutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy.",
                    "score": 0.9022586941719055
                },
                {
                    "id": 22512641,
                    "contents": "Ernest O. Wollan\nWith his background in X-ray diffraction, Wollan was one of the first to recognize the potential value of neutrons for investigating the structure of materials. In May 1944 he asked the director of Clinton Laboratories (now Oak Ridge National Laboratory) for permission to use the neutron output of the X-10 reactor to study the diffraction of neutrons in single crystals. His request was granted, and a neutron crystal spectrometer that Wollan brought from Chicago was installed in the reactor that same month to make observations on a crystal of gypsum. Wollan and his group were transferred from the Metallurgical Laboratory to the Clinton Laboratories in August of that year. After some early setbacks, in December 1944 Wollan and chemist Lyle Benjamin Borst successfully used neutron diffraction to produce \"rocking curves\" for crystals of gypsum and sodium chloride (salt).",
                    "score": 0.9019792675971985
                },
                {
                    "id": 10690929,
                    "contents": "Neutron magnetic moment\nThe neutron magnetic moment is the intrinsic magnetic dipole moment of the neutron, symbol μn. Protons and neutrons, both nucleons, comprise the nucleus of atoms, and both nucleons behave as small magnets whose strengths are measured by their magnetic moments. The neutron interacts with normal matter through either the nuclear force or its magnetic moment. The neutron's magnetic moment is exploited to probe the atomic structure of materials using scattering methods and to manipulate the properties of neutron beams in particle accelerators. The neutron was determined to have a magnetic moment by indirect methods in the mid 1930s. Luis Alvarez and Felix Bloch made the first accurate, direct measurement of the neutron's magnetic moment in 1940. The existence of the neutron's magnetic moment indicates the neutron is not an elementary particle, because for an elementary particle to have an intrinsic magnetic moment, it must have both spin and electric charge. The neutron has spin",
                    "score": 0.9019097089767456
                },
                {
                    "id": 3922763,
                    "contents": "Maurice de Broglie\nCareer De Broglie made advances in the study of X-ray diffraction and spectroscopy. During the First World War, he worked on radio communications for the navy. After the war, he resumed his research at a large laboratory in his home. He occasionally collaborated with his younger brother Louis, who followed his professional lead and was training as a physicist, and they coauthored a paper in 1921. After Louis de Broglie's rise to prominence in the 1920s, building on some of their shared research, the elder de Broglie physicist continued his own research. While Louis was primarily a theoretician, Maurice's focus was mainly experimental.",
                    "score": 0.9017834067344666
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_17",
        "question": "The temperature of the fireball in a thermonuclear explosion can reach temperatures of approximately $10^7 \\mathrm{~K}$. What value of $\\lambda_{\\max }$ does this correspond to? ",
        "golden_answers": [
            " 3"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 410667,
                    "contents": "X-ray burster\nlayer. After mere hours of accumulation and gravitational compression, nuclear fusion starts in this matter. This begins as a stable process, the hot CNO cycle, however, continued accretion causes a degenerate shell of matter, in which the temperature rises (greater than 1 × 109 kelvin) but this does not alleviate thermodynamic conditions. This causes the triple-α cycle to quickly become favored, resulting in a He flash. The additional energy provided by this flash allows the CNO burning to breakout into thermonuclear runaway. In the early phase of the burst is the alpha-p process, which quickly yields to the rp-process. Nucleosynthesis can proceed as high as A=100, but was shown to end definitively with Te107. Within seconds most of the accreted material is burned, powering a bright X-ray flash that is observable with X-ray (or Gamma ray) telescopes. Theory suggests that there are several burning regimes which cause variations in the burst, such as ignition condition, energy",
                    "score": 0.8858159780502319
                },
                {
                    "id": 1367070,
                    "contents": "Thermonuclear fusion\nOne force capable of confining the fuel well enough to satisfy the Lawson criterion is gravity. The mass needed, however, is so great that gravitational confinement is only found in stars—the least massive stars capable of sustained fusion are red dwarfs, while brown dwarfs are able to fuse deuterium and lithium if they are of sufficient mass. In stars heavy enough, after the supply of hydrogen is exhausted in their cores, their cores (or a shell around the core) start fusing helium to carbon. In the most massive stars (at least 8–11 solar masses), the process is continued until some of their energy is produced by fusing lighter elements to iron. As iron has one of the highest binding energies, reactions producing heavier elements are generally endothermic. Therefore significant amounts of heavier elements are not formed during stable periods of massive star evolution, but are formed in supernova explosions. Some lighter stars also form these elements in the outer parts of the stars",
                    "score": 0.8854784965515137
                },
                {
                    "id": 1917424,
                    "contents": "Effects of nuclear explosions\nentire atmosphere was not possible: the cooling of the fireball due to an inverse Compton effect all but guaranteed that such a scenario would not become a reality. Richard Hamming, a mathematician, was asked to make a similar calculation just before the first nuclear test, with the same result. Nevertheless, the notion has persisted as a rumor for many years and was the source of apocalyptic gallows humor at the Trinity test.",
                    "score": 0.8817541599273682
                },
                {
                    "id": 8976493,
                    "contents": "Rp-process\nto rise until it leads to a runaway thermonuclear explosion of the hydrogen and helium. During the flash, the temperature quickly rises, becoming high enough for the rp-process to occur. While the initial flash of hydrogen and helium lasts only a second, the rp-process typically takes up to 100 seconds. Therefore, the rp-process is observed as the tail of the resulting X-ray burst.",
                    "score": 0.8815922737121582
                },
                {
                    "id": 1917384,
                    "contents": "Effects of nuclear explosions\nThe high temperatures and radiation cause gas to move outward radially in a thin, dense shell called \"the hydrodynamic front\". The front acts like a piston that pushes against and compresses the surrounding medium to make a spherically expanding shock wave. At first, this shock wave is inside the surface of the developing fireball, which is created in a volume of air heated by the explosion's \"soft\" X-rays. Within a fraction of a second, the dense shock front obscures the fireball and continues to move past it, now expanding outwards, free from the fireball, causing a reduction of light emanating from a nuclear detonation. Eventually, the shock wave dissipates to the point where the light becomes visible again giving rise to the characteristic double flash due to the shock wave–fireball interaction. It is this unique feature of nuclear explosions that is exploited when verifying that an atmospheric nuclear explosion has occurred and not simply a large conventional explosion, with",
                    "score": 0.8791816234588623
                },
                {
                    "id": 1917417,
                    "contents": "Effects of nuclear explosions\ncollisions, part of the kinetic energy of the fission fragments is converted into internal and radiation energy. Some of the electrons are removed entirely from the atoms, thus causing ionization, others are raised to higher energy (or excited) states while still remaining attached to the nuclei. Within an extremely short time, perhaps a hundredth of a microsecond or so, the weapon residues consist essentially of completely and partially stripped (ionized) atoms, many of the latter being in excited states, together with the corresponding free electrons. The system then immediately emits electromagnetic (thermal) radiation, the nature of which is determined by the temperature. Since this is of the order of 107 degrees, most of the energy emitted within a microsecond or so is in the soft X-ray region. To understand this one must remember that temperature depends on the average internal energy/heat of the particles in a certain volume, and internal energy or heat is due to kinetic",
                    "score": 0.8770982027053833
                },
                {
                    "id": 25552017,
                    "contents": "Nuclear blackout\ntons/unit area where is the diameter of the disk for a given explosion. Blackout lifetime When the explosion takes place within the atmosphere, the fireball rapidly forms and initially gives off considerable energy in the form of visible and UV light. This rapidly cools the fireball to about 5000 °C, at which point the cooling process slows considerably. From then on the primary cooling effect is through thermal transfer with the surrounding air mass. This process takes as long as several minutes, and as there is less air at higher altitudes, the fireball remains ionized for longer periods. At higher altitudes, from , the density of air is not enough to be a significant effect, and the fireball continues to cool radiatively. Generally the process is described by a radiative recombination constant, , which is about 10−12 cubic centimeters per second. If the initial electron density is 1012, a density of 109 electrons/cm2 will not occur until 1,000 seconds, about 17 minutes.",
                    "score": 0.8760173916816711
                },
                {
                    "id": 1917423,
                    "contents": "Effects of nuclear explosions\nIn 1942, there was some initial speculation among the scientists developing the first nuclear weapons in the Manhattan Project that a large enough nuclear explosion might ignite the Earth's atmosphere. This notion concerned the nuclear reaction of two atmospheric nitrogen atoms forming carbon and an oxygen atom, with an associated release of energy. The scientists hypothesized that this energy would heat up the remaining atmospheric nitrogen enough to keep the reaction going until all nitrogen atoms were consumed, thereby burning all of the Earth's atmosphere (which is composed of nearly 80% diatomic nitrogen) in one single massive combustion event. Hans Bethe was assigned the task of studying this hypothesis from the project's earliest days, and eventually concluded that combustion of the entire atmosphere was not possible: the cooling of the fireball due to an inverse Compton effect all but guaranteed that such a scenario would not become a reality. Richard Hamming, a mathematician,",
                    "score": 0.8752937316894531
                },
                {
                    "id": 13979989,
                    "contents": "Internal heating\nand for sufficiently massive stars even large quantities of heavier elements. Fusion to produce elements heavier than iron and nickel no longer produces energy, and since stellar cores massive enough to attain the temperatures required to produce these elements are too massive to form stable white dwarf stars, a core collapse supernova results, producing a neutron star or a black hole, depending upon the mass. Heat generated by the collapse is trapped within a neutron star and only escapes slowly, due to the small surface area; heat cannot be conducted out of a black hole at all (however, see Hawking radiation).",
                    "score": 0.8742367029190063
                },
                {
                    "id": 1367067,
                    "contents": "Thermonuclear fusion\nThermonuclear fusion is the process of atoms combining or “fusing” together with huge amounts of heat. There are two forms of thermonuclear fusion: uncontrolled, in which the resulting energy is released in an uncontrolled manner, as it is in thermonuclear weapons (\"hydrogen bombs\") and in most stars; and controlled, where the fusion reactions take place in an environment allowing some or all of the energy released to be harnessed for constructive purposes. Temperature requirements Temperature is a measure of the average kinetic energy of particles, so by heating the material it will gain energy. After reaching sufficient temperature, given by the Lawson criterion, the energy of accidental collisions within the plasma is high enough to overcome the Coulomb barrier and the particles may fuse together.",
                    "score": 0.8735749125480652
                },
                {
                    "id": 1917416,
                    "contents": "Effects of nuclear explosions\nGamma rays from the nuclear processes preceding the true explosion may be partially responsible for the following fireball, as they may superheat nearby air and/or other material. The vast majority of the energy that goes on to form the fireball is in the soft X-ray region of the electromagnetic spectrum, with these X-rays being produced by the inelastic collisions of the high-speed fission and fusion products. It is these reaction products and not the gamma rays which contain most of the energy of the nuclear reactions in the form of kinetic energy. This kinetic energy of the fission and fusion fragments is converted into internal and then radiation energy by approximately following the process of blackbody radiation emitting in the soft X-ray region. As a result of numerous inelastic collisions, part of the kinetic energy of the fission fragments is converted into internal and radiation energy. Some of the electrons are removed entirely from the atoms, thus causing ionization,",
                    "score": 0.8721475601196289
                },
                {
                    "id": 2698153,
                    "contents": "Mass–energy equivalence\ncome out of a nuclear reaction is less than the mass of the atoms that go in, and the difference in mass shows up as heat and light with the same equivalent energy as the difference. In analyzing these explosions, Einstein's formula can be used with as the energy released and removed, and as the change in mass.",
                    "score": 0.87211012840271
                },
                {
                    "id": 1267804,
                    "contents": "Stellarator\n100 keV corresponds to a temperature of about a billion kelvins. Due to the Maxwell–Boltzmann statistics, a bulk gas at a much lower temperature will still contain some particles at these much higher energies. Because the fusion reactions release so much energy, even a small number of these reactions can release enough energy to keep the gas at the required temperature. In 1944, Enrico Fermi demonstrated that this would occur at a bulk temperature of about 50 million Celsius, still very hot but within the range of existing experimental systems. The key problem was confining such a plasma; no material container could withstand those temperatures. But because plasmas are electrically conductive, they are subject to electric and magnetic fields which provide a number of solutions.",
                    "score": 0.8720095753669739
                },
                {
                    "id": 11021222,
                    "contents": "Migma\nWhen the fuel is heated to high energies the electrons disassociate from the nuclei, which are left as ions in a gas-like plasma. Any particles in a gas are distributed across a wide range of energies in a spectrum known as the Maxwell–Boltzmann distribution. At any given temperature the majority of the particles are at lower energies, with a \"long tail\" containing smaller numbers of particles at much higher energies. So while 100 KeV represents a temperature of over one billion degrees, in order to produce fusion events the fuel does not have to be heated to this temperature as a whole. Even at a much lower temperature, the rate of fusion among the long tail members may be high enough to provide useful power output as long as it is confined for some period of time. Increased density also increases the rate, as the energy from the reactions will heat the surrounding fuel and potentially incite fusion in it as well. The combination of temperature, density and confinement time is known",
                    "score": 0.8713552355766296
                },
                {
                    "id": 25076649,
                    "contents": "X-ray flash (astronomy)\nA competing \"dirty fireball\" theory suggests that an X-ray flash comes from a hypernova that uses much of the available energy in expelling an unusually large amount of baryonic matter, thus limiting the energy available for electromagnetic radiation, and emitting a much \"cooler\" spectrum rich in X-rays and very poor in gamma rays.",
                    "score": 0.8712621331214905
                },
                {
                    "id": 3007156,
                    "contents": "Lorentz factor\nApplications in astronomy The standard model of long-duration gamma-ray bursts (GRBs) holds that these explosions are ultra-relativistic (initial greater than approximately 100), which is invoked to explain the so-called \"compactness\" problem: absent this ultra-relativistic expansion, the ejecta would be optically thick to pair production at typical peak spectral energies of a few 100 keV, whereas the prompt emission is observed to be non-thermal.",
                    "score": 0.8695303201675415
                },
                {
                    "id": 2025767,
                    "contents": "PSR B1620−26 b\nThe infalling matter produced complex and spectacular effects. The infalling matter 'spun up' the neutron star, due to the transfer of angular momentum, and for a few hundred million years, the stars formed a low-mass X-ray binary, as the infalling matter was heated to temperatures high enough to glow in X-rays.",
                    "score": 0.8691468834877014
                },
                {
                    "id": 2012927,
                    "contents": "Mushroom cloud\nAt the moment of the explosion, the fireball is formed. The ascending, roughly spherical mass of hot, incandescent gases changes shape due to atmospheric friction and cools its surface by energy radiation, turning from a sphere to a violently rotating spheroidal vortex. A Rayleigh–Taylor instability is formed as the underneath cool air initially pushes the bottom fireball gases into an inverted cup shape. This causes turbulence and a vortex that sucks more air into its center, creating external afterwinds and cooling itself. The speed of its rotating slows down as it cools, and may stop entirely during later phases. The vaporized parts of the weapon and ionized air cool into visible gases, forming the early cloud; the white-hot vortex core becomes yellow, then dark red, then loses visible incandescence. With further cooling, the bulk of the cloud fills in as atmospheric moisture condenses. As the cloud ascends and cools, its buoyancy lessens, and its ascent slows.",
                    "score": 0.8685977458953857
                },
                {
                    "id": 18922201,
                    "contents": "Nanoflares\nEpisodic heating often observed in active regions, including major events such as flares and coronal mass ejections could be provoked by cascade effects, similar to those described by the mathematical theories of catastrophes. In the hypothesis that the solar corona is in a state of self-organized criticality, the stressing of the magnetic field should be enhanced until a small perturbation switches on many small instabilities, happening together as it occurs in avalanches. One of the experimental results often cited in supporting the nanoflare theory is the fact that the distribution of the number of flares observed in the hard X-rays is a function of their energy, following a power law with negative spectral index. A sufficiently large power-law index would allow the smallest events to dominate the total energy. In the energy range of normal flares, the index has a value of approximately -1.8",
                    "score": 0.8680713176727295
                },
                {
                    "id": 1367072,
                    "contents": "Thermonuclear fusion\nAll of the elements heavier than iron have some potential energy to release, in theory. At the extremely heavy end of element production, these heavier elements can produce energy in the process of being split again back toward the size of iron, in the process of nuclear fission. Nuclear fission thus releases energy which has been stored, sometimes billions of years before, during stellar nucleosynthesis. Magnetic confinement Electrically charged particles (such as fuel ions) will follow magnetic field lines (see Guiding centre). The fusion fuel can therefore be trapped using a strong magnetic field. A variety of magnetic configurations exist, including the toroidal geometries of tokamaks and stellarators and open-ended mirror confinement systems. Inertial confinement",
                    "score": 0.8678596019744873
                },
                {
                    "id": 3746869,
                    "contents": "Castle Bravo\nThe thermonuclear burn would produce (like the fission fuel in the primary) pulsations (generations) of high-energy neutrons with an average temperature of 14 MeV through Jetter's cycle. Jetter's cycle The Jetter cycle is a combination of reactions involving lithium, deuterium, and tritium. It consumes Lithium-6 and deuterium, and in two reactions (with energies of 17.6 MeV and 4.8 MeV, mediated by a neutron and tritium) it produces two alpha particles. The reaction would produce high-energy neutrons with 14 MeV, and its neutronicity was estimated at ≈0.885 (for a Lawson criterion of ≈1.5).",
                    "score": 0.8673940300941467
                },
                {
                    "id": 7228658,
                    "contents": "Thermal runaway\nRunaway thermonuclear reactions can occur in stars when nuclear fusion is ignited in conditions under which the gravitational pressure exerted by overlying layers of the star greatly exceeds thermal pressure, a situation that makes possible rapid increases in temperature through gravitational compression. Such a scenario may arise in stars containing degenerate matter, in which electron degeneracy pressure rather than normal thermal pressure does most of the work of supporting the star against gravity, and in stars undergoing implosion. In all cases, the imbalance arises prior to fusion ignition; otherwise, the fusion reactions would be naturally regulated to counteract temperature changes and stabilize the star. When thermal pressure is in equilibrium with overlying pressure, a star will respond to the increase in temperature and thermal pressure due to initiation of a new exothermic reaction by expanding and cooling. A runaway reaction is only possible when this response is",
                    "score": 0.8669554591178894
                },
                {
                    "id": 905498,
                    "contents": "Thermodynamic temperature\nHowever, a bullet accelerates faster than a rifle given an equal force. Since kinetic energy increases as the square of velocity, nearly all the kinetic energy goes into the bullet, not the rifle, even though both experience the same force from the expanding propellant gases. In the same manner, because they are much less massive, thermal energy is readily borne by mobile conduction electrons. Additionally, because they're delocalized and very fast, kinetic thermal energy conducts extremely quickly through metals with abundant conduction electrons. The diffusion of thermal energy: Black-body radiation",
                    "score": 0.8663555383682251
                },
                {
                    "id": 6779664,
                    "contents": "Supernova nucleosynthesis\nSupernova nucleosynthesis is the nucleosynthesis of chemical elements in supernova explosions. In sufficiently massive stars, the nucleosynthesis by fusion of lighter elements into heavier ones occurs during sequential hydrostatic burning processes called helium burning, carbon burning, oxygen burning, and silicon burning, in which the byproducts of one nuclear fuel become, after compressional heating, the fuel for the subsequent burning stage. In this context, the word \"burning\" refers to nuclear fusion and not a chemical reaction.",
                    "score": 0.8655341863632202
                },
                {
                    "id": 6689618,
                    "contents": "Charles J. Joachain\nHe has co-edited four books : 1) \"Atomic and Molecular Physics of Controlled Thermonuclear Fusion\"(with D.E. Post), Plenum, New York (1983). 2) \"Photon and Electron Collisions with Atoms and Molecules\"(with P.G. Burke), Plenum, New York (1997). 3) \"Atoms, Solids and Plasmas in Super-Intense Laser Fields\" (with D. Batani, S. Martellucci and A.N. Chester), Kluwer Academic-Plenum, New-York (2001). 4) \"Atoms and Plasmas in Super-Intense Laser Fields\" (with D. Batani and S. Martellucci), Conference Proceedings, Volume 88, Italian Physical Society, Bologna (2004). He is also the author of hundred and forty-seven research articles and forty-five review articles in theoretical physics, devoted mainly to quantum collision theory with applications to atomic, nuclear and high-energy processes and to the theory of high-intensity laser-atom interactions.",
                    "score": 0.8654161691665649
                },
                {
                    "id": 18803948,
                    "contents": "Fusion ignition\nIn nature, stars reach ignition at temperatures similar to that of the Sun, around 15 million Kelvin (27 million degrees F). Stars are so large that the fusion products will almost always interact with the plasma before their energy can be lost to the environment at the outside of the star. In comparison, man-made reactors are far less dense and much smaller, allowing the fusion products to easily escape the fuel. To offset this, much higher rates of fusion are required, and thus much higher temperatures; most man-made fusion reactors are designed to work at temperatures around 100 million degrees, or higher. , no man-made reactor has reached breakeven. Ignition has been achieved in the cores of detonating thermonuclear weapons.",
                    "score": 0.865408182144165
                },
                {
                    "id": 380044,
                    "contents": "Timeline of nuclear fusion\n1920s 1920 Based on F.W. Aston's measurements of the masses of low-mass elements and Einstein's discovery that E=mc2, Arthur Eddington proposes that large amounts of energy released by fusing small nuclei together provides the energy source that powers the stars. Henry Norris Russell notes that the relationship in the Hertzsprung–Russell diagram suggests a hot core rather than burning throughout the star. Eddington uses this to calculate that the core would have to be about 40 million Kelvin. This was a matter of some debate at the time, because the value is much higher than what observations suggest, which is about one-third to one-half that value. 1928 George Gamow introduces the mathematical basis for quantum tunnelling. 1929",
                    "score": 0.8653966188430786
                },
                {
                    "id": 7228662,
                    "contents": "Thermal runaway\nX-ray bursts Analogous to the process leading to novae, degenerate matter can also accumulate on the surface of a neutron star that is accreting gas from a close companion. If a sufficiently thick layer of hydrogen accumulates, ignition of runaway hydrogen fusion can then lead to an X-ray burst. As with novae, such bursts tend to repeat and may also be triggered by helium or even carbon fusion. It has been proposed that in the case of \"superbursts\", runaway breakup of accumulated heavy nuclei into iron group nuclei via photodissociation rather than nuclear fusion could contribute the majority of the energy of the burst.",
                    "score": 0.8647961020469666
                },
                {
                    "id": 1236838,
                    "contents": "Supernova\nWithin a few seconds, a substantial fraction of the matter in the white dwarf undergoes nuclear fusion, releasing enough energy (1–) to unbind the star in a supernova. An outwardly expanding shock wave is generated, with matter reaching velocities on the order of 5,000–20,000 km/s, or roughly 3% of the speed of light. There is also a significant increase in luminosity, reaching an absolute magnitude of −19.3 (or 5 billion times brighter than the Sun), with little variation.",
                    "score": 0.8646660447120667
                },
                {
                    "id": 1583192,
                    "contents": "Arthur Eddington\nSir Arthur Stanley Eddington (28 December 1882 – 22 November 1944) was an English astronomer, physicist, and mathematician. He was also a philosopher of science and a populariser of science. The Eddington limit, the natural limit to the luminosity of stars, or the radiation generated by accretion onto a compact object, is named in his honour. Around 1920, he foreshadowed the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington was the first to correctly speculate that the source was fusion of hydrogen into helium.",
                    "score": 0.8645235300064087
                },
                {
                    "id": 17917223,
                    "contents": "Hans A. Bethe Prize\nPrize recipients 2022 - : \"For fundamental contributions to the physics of hot and dense matter, and their implications for heavy ion collisions and multi-messenger observations of neutron star structure and evolution.\" 2021 - : \"For distinguished contributions across the breadth of nuclear astrophysics, Galactic chemical evolution and cosmochronology.\" 2020 - Fiona A. Harrison: \"For pioneering work in conceiving and executing the first focusing telescope in the high energy X-ray regime, NASA’s Nuclear Spectroscopic Telescope Array (NuSTAR) satellite. NuSTAR has enabled major advances in understanding phenomena in the most extreme environments in the universe.\" 2019 - : \"For lasting contributions to our understanding of the nuclear astrophysics of the universe, including stellar evolution, the synthesis of new elements, the theory of core-collapse and thermonuclear supernovae, and gamma-ray bursts.\"",
                    "score": 0.8636632561683655
                },
                {
                    "id": 1296369,
                    "contents": "Tokamak\nTo maintain fusion and produce net energy output, the bulk of the fuel must be raised to high temperatures so its atoms are constantly colliding at high speed; this gives rise to the name thermonuclear due to the high temperatures needed to bring it about. In 1944, Enrico Fermi calculated the reaction would be self-sustaining at about 50,000,000 K; at that temperature, the rate that energy is given off by the reactions is high enough that they heat the surrounding fuel rapidly enough to maintain the temperature against losses to the environment, continuing the reaction.",
                    "score": 0.863235354423523
                },
                {
                    "id": 1917418,
                    "contents": "Effects of nuclear explosions\nX-ray region. To understand this one must remember that temperature depends on the average internal energy/heat of the particles in a certain volume, and internal energy or heat is due to kinetic energy.",
                    "score": 0.8630072474479675
                },
                {
                    "id": 744709,
                    "contents": "Island of stability\nThe process of slow neutron capture used to produce nuclides as heavy as 257Fm is blocked by short-lived isotopes of fermium that undergo spontaneous fission (for example, 258Fm has a half-life of 370 µs); this is known as the \"fermium gap\" and prevents the synthesis of heavier elements in such a reaction. It might be possible to bypass this gap, as well as another predicted region of instability around A = 275 and Z = 104–108, in a series of controlled nuclear explosions with a higher neutron flux (about a thousand times greater than fluxes in existing reactors) that mimics the astrophysical r-process. First proposed in 1972 by Meldner, such a reaction might enable the production of macroscopic quantities of superheavy elements within the island of stability; the role of fission in intermediate superheavy nuclides is highly uncertain, and may strongly influence the yield of such a reaction.",
                    "score": 0.8629510402679443
                },
                {
                    "id": 410664,
                    "contents": "X-ray burster\nThermonuclear burst astrophysics",
                    "score": 0.8629438281059265
                },
                {
                    "id": 9799720,
                    "contents": "Nuclear astrophysics\nThe Sun's primary energy source is hydrogen fusion to helium at about 15 million degrees. The proton–proton chain reactions dominate, they occur at much lower energies although much more slowly than catalytic hydrogen fusion through CNO cycle reactions. Nuclear astrophysics gives a picture of the Sun's energy source producing a lifetime consistent with the age of the Solar System derived from meteoritic abundances of lead and uranium isotopes —an age of about 4.5 billion years. The core hydrogen burning of stars, as it now occurs in the Sun, defines the main sequence of stars, illustrated in the Hertzsprung-Russell diagram that classifies stages of stellar evolution. The Sun's lifetime of H burning via pp-chains is about 9 billion years. This primarily is determined by extremely slow production of deuterium, which is governed by the weak interaction.",
                    "score": 0.8626211285591125
                },
                {
                    "id": 755613,
                    "contents": "Unbinilium\nnuclei cool to the ground state, they require emission of only one or two neutrons. However, hot fusion reactions tend to produce more neutron-rich products because the actinides have the highest neutron-to-proton ratios of any elements that can presently be made in macroscopic quantities, and is currently the only method to produce the superheavy elements from flerovium (element 114) onward.",
                    "score": 0.8625178933143616
                },
                {
                    "id": 2455556,
                    "contents": "Thomas Gold\nIn 1959, Gold expanded on his previous prediction of a collisionless shock wave, arguing that solar flares would eject material into magnetic clouds to produce a shock front that would result in geomagnetic storms. He also coined the term \"magnetosphere\" in his paper \"Motions in the Magnetosphere of the Earth\" to describe \"the region above the ionosphere in which the magnetic field of the Earth has a dominant control over the motions of gas and fast charged particles ... [which was] known to extend out to a distance of the order of 10 Earth radii\". In 1960, Gold collaborated again with Fred Hoyle to show that magnetic energy fueled solar flares and that flares were triggered when opposite magnetic loops interact and release their stored energy.",
                    "score": 0.862177848815918
                },
                {
                    "id": 17917228,
                    "contents": "Hans A. Bethe Prize\n2003 – : \"For his contributions to the experimental foundation of nuclear astrophysics, especially the delineation of the processes involved in explosive hydrogen burning in novae and x-ray bursters; and for providing an intellectual bridge between experimental nuclear astrophysicists and their theoretical col-leagues.\" 2002 – Gordon Baym: \"For his superb synthesis of fundamental concepts which have provided an understanding of matter at extreme conditions, ranging from crusts and interiors of neutron stars to matter at ultrahigh temperature.\" 2001 – Gerald E. Brown: \"For his insightful analyses of the effects of various nuclear constituents on nucleon interactions and nucleon structure, and his contributions to new viewpoints on supernovae, neutron stars, and black hole formation.\" 2000 – Igal Talmi: \"For pioneering work on the shell model of the nucleus that laid the foundation of much of what we know about nuclear structure.\"",
                    "score": 0.8620367050170898
                },
                {
                    "id": 1917382,
                    "contents": "Effects of nuclear explosions\nlethal radiation effects as explosive yield increases. This bubble is faster than the speed of sound. The physical damage mechanisms of a nuclear weapon (blast and thermal radiation) are identical to those of conventional explosives, but the energy produced by a nuclear explosion is usually millions of times more powerful per unit mass and temperatures may briefly reach the tens of millions of degrees.",
                    "score": 0.8617552518844604
                },
                {
                    "id": 13979988,
                    "contents": "Internal heating\nThe internal heating within stars is so great that (after an initial phase of gravitational contraction) they ignite and sustain thermonuclear reaction of hydrogen (with itself) to form helium, and can make heavier elements (see Stellar nucleosynthesis). The Sun for example has a core temperature of 13,600,000 K. The more massive and older the stars are, the more internal heating they have. During the end of its lifecycle, the internal heating of a star increases dramatically, caused by the change of composition of the core as successive fuels for fusion are consumed, and the resulting contraction (accompanied by faster consumption of the remaining fuel). Depending upon the mass of the star, the core may become hot enough to fuse helium (forming carbon and oxygen and traces of heavier elements), and for sufficiently massive stars even large quantities of heavier elements. Fusion to produce elements heavier than iron and nickel no longer produces energy, and since stellar cores massive",
                    "score": 0.8616005182266235
                },
                {
                    "id": 19710287,
                    "contents": "Nuclear drip line\nWhile the rp-process in X-ray bursts may have difficulty bypassing the 64Ge waiting point, certainly in X-ray pulsars where the rp-process is stable, instability toward alpha decay places an upper limit near A = 100 on the mass that can be reached through continuous burning. The exact limit is a matter presently under investigation; 104–109Te are known to undergo alpha decay whereas 103Sb is proton-unbound. Even before the limit near A = 100 is reached, the proton flux is thought to considerably decrease and thus slow down the rp-process, before low capture rate and a cycle of transmutations between isotopes of tin, antimony, and tellurium upon further proton capture terminate it altogether. However, it has been shown that if there are episodes of cooling or mixing of previous ashes into the burning zone, material as heavy as 126Xe can be created.",
                    "score": 0.8610442280769348
                },
                {
                    "id": 744686,
                    "contents": "Island of stability\nDuring the 1970s, many searches for long-lived superheavy nuclei were conducted. Experiments aimed at synthesizing elements ranging in atomic number from 110 to 127 were conducted at laboratories around the world. These elements were sought in fusion-evaporation reactions, in which a heavy target made of one nuclide is irradiated by accelerated ions of another in a cyclotron, and new nuclides are produced after these nuclei fuse and the resulting excited system releases energy by evaporating several particles (usually protons, neutrons, or alpha particles). These reactions are divided into \"cold\" and \"hot\" fusion, which respectively create systems with lower and higher excitation energies; this affects the yield of the reaction. For example, the reaction between 248Cm and 40Ar was expected to yield isotopes of element 114, and that between 232Th and 84Kr was expected to yield isotopes of element 126. None of these attempts were successful, indicating that such experiments may have",
                    "score": 0.860742449760437
                },
                {
                    "id": 7300798,
                    "contents": "Gamma Cassiopeiae\nsufficient to power an X-ray emission of nearly 1033 erg/s or 100 YW. A neutron star could easily power this X-ray flux, but X-ray emission from neutron stars is known to be non-thermal, and thus in apparent variance with the spectral properties.",
                    "score": 0.8605899810791016
                },
                {
                    "id": 22597712,
                    "contents": "Stellification\nThermonuclear ignition It is well established that Jovian-class planets consist mostly of hydrogen and helium. It is theorised that concentrations of hydrogen and helium isotopes at certain depths of a gas-giant planet may be sufficient to support a fusion chain reaction, if sufficient energy can be delivered to ignite the reaction. If a gas giant has a layer with a large concentration of deuterium (>0.3%), ultra-high-speed ( collision of a sufficiently large asteroid (diameter > ) could ignite a thermonuclear reaction.",
                    "score": 0.8605390787124634
                },
                {
                    "id": 3356625,
                    "contents": "Helium flash\nThis runaway reaction quickly climbs to about 100 billion times the star's normal energy production (for a few seconds) until the temperature increases to the point that thermal pressure again becomes dominant, eliminating the degeneracy. The core can then expand and cool down and a stable burning of helium will continue. A star with mass greater than about 2.25 starts to burn helium without its core becoming degenerate, and so does not exhibit this type of helium flash. In a very low-mass star (less than about 0.5 ), the core is never hot enough to ignite helium. The degenerate helium core will keep on contracting, and finally becomes a helium white dwarf.",
                    "score": 0.8602968454360962
                },
                {
                    "id": 25552007,
                    "contents": "Nuclear blackout\nProceeding at a slower speed is the actual explosion, which creates a powerful shock wave moving outward. The energy released by the shock wave is enough to compression heat the air into incandescence, creating a second fireball. This second fireball continues to expand, passing the radiative one. As it expands, the amount of energy in the shock wave drops according to the inverse-square law, while additional energy is lost through direct radiation in the visible and ultraviolet spectrum. Eventually the shock wave loses so much energy that it no longer heats the air enough to cause it to glow. At that point, known as breakaway, the shock front becomes transparent, and the fireball stops growing. The diameter of the fireball for a bomb exploded clear of the ground can be estimated using the formula: kilometers",
                    "score": 0.8599761724472046
                },
                {
                    "id": 17917227,
                    "contents": "Hans A. Bethe Prize\n2007 – : \"For his work in nuclear astrophysics and numerical work on supernovae core collapse, neutrino transport, and shock propagation. His codes reenergized supernovae shocks, launched numerical relativity and magnetically driven jets.\" 2006 – Alastair G.W. Cameron: \"For his pioneering work in developing the fundamental concepts of nuclear astrophysics. These basic ideas, laid out almost 50 years ago, are still the basis of current research in this field.\" 2005 – Stan Woosley: \"For his significant and wide ranging contributions in the areas of stellar evolution, element synthesis, the theory of core collapse and type Ia supernovae, and the interpretation of gamma-ray bursts - most notably, the collapsar model of gamma-ray bursts.\" 2004 – Wick Haxton: \"\"For his noteworthy contributions and scientific leadership in the field of neutrino astrophysics, in particular for his success in merging nuclear theory with experiments and observations in nuclear physics and astrophysics.\"",
                    "score": 0.8596014976501465
                },
                {
                    "id": 905499,
                    "contents": "Thermodynamic temperature\nThermal radiation is a byproduct of the collisions arising from various vibrational motions of atoms. These collisions cause the electrons of the atoms to emit thermal photons (known as black-body radiation). Photons are emitted anytime an electric charge is accelerated (as happens when electron clouds of two atoms collide). Even individual molecules with internal temperatures greater than absolute zero also emit black-body radiation from their atoms. In any bulk quantity of a substance at equilibrium, black-body photons are emitted across a range of wavelengths in a spectrum that has a bell curve-like shape called a Planck curve (see graph in Fig. 5 at right). The top of a Planck curve (the peak emittance wavelength) is located in a particular part of the electromagnetic spectrum depending on the temperature of the black-body. Substances at extreme cryogenic temperatures emit at long radio wavelengths whereas extremely hot temperatures produce short gamma rays (see Table of common",
                    "score": 0.8593119382858276
                },
                {
                    "id": 4306493,
                    "contents": "Lawson criterion\nThis product must be greater than a value related to the minimum of T 3/2/<σv>. The same requirement is traditionally expressed in terms of mass density ρ = <nmi>: Satisfaction of this criterion at the density of solid deuterium–tritium (0.2 g/cm3) would require a laser pulse of implausibly large energy. Assuming the energy required scales with the mass of the fusion plasma (Elaser ~ ρR3 ~ ρ−2), compressing the fuel to 103 or 104 times solid density would reduce the energy required by a factor of 106 or 108, bringing it into a realistic range. With a compression by 103, the compressed density will be 200 g/cm3, and the compressed radius can be as small as 0.05 mm. The radius of the fuel before compression would be 0.5 mm. The initial pellet will be perhaps twice as large since most of the mass will be ablated during the compression.",
                    "score": 0.8591558933258057
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_18",
        "question": "Show that l'Hôpital's rule amounts to forming a Taylor expansion of both the numerator and the denominator. Evaluate the limit\r\n$$\r\n\\lim _{x \\rightarrow 0} \\frac{\\ln (1+x)-x}{x^2}\r\n$$\r\nboth ways.",
        "golden_answers": [
            " -1/2"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1844566,
                    "contents": "L'Hôpital's rule\nIn mathematics, more specifically calculus, L'Hôpital's rule or L'Hospital's rule (, , ), also known as Bernoulli's rule, is a theorem which provides a technique to evaluate limits of indeterminate forms. Application (or repeated application) of the rule often converts an indeterminate form to an expression that can be easily evaluated by substitution. The rule is named after the 17th-century French mathematician Guillaume de l'Hôpital. Although the rule is often attributed to L'Hôpital, the theorem was first introduced to him in 1694 by the Swiss mathematician Johann Bernoulli. L'Hôpital's rule states that for functions and which are differentiable on an open interval except possibly at a point contained in , if and for all in with , and exists, then The differentiation of the numerator and denominator often simplifies the quotient or converts it to a limit that can be evaluated directly. History",
                    "score": 0.9103225469589233
                },
                {
                    "id": 1624809,
                    "contents": "Calculus\nThe product rule and chain rule, the notions of higher derivatives and Taylor series, and of analytic functions were used by Isaac Newton in an idiosyncratic notation which he applied to solve problems of mathematical physics. In his works, Newton rephrased his ideas to suit the mathematical idiom of the time, replacing calculations with infinitesimals by equivalent geometrical arguments which were considered beyond reproach. He used the methods of calculus to solve the problem of planetary motion, the shape of the surface of a rotating fluid, the oblateness of the earth, the motion of a weight sliding on a cycloid, and many other problems discussed in his Principia Mathematica (1687). In other work, he developed series expansions for functions, including fractional and irrational powers, and it was clear that he understood the principles of the Taylor series. He did not publish all these discoveries, and at this time infinitesimal methods were still considered disreputable.",
                    "score": 0.9080259799957275
                },
                {
                    "id": 3041468,
                    "contents": "History of calculus\nNewton Newton completed no definitive publication formalizing his fluxional calculus; rather, many of his mathematical discoveries were transmitted through correspondence, smaller papers or as embedded aspects in his other definitive compilations, such as the Principia and Opticks. Newton would begin his mathematical training as the chosen heir of Isaac Barrow in Cambridge. His aptitude was recognized early and he quickly learned the current theories. By 1664 Newton had made his first important contribution by advancing the binomial theorem, which he had extended to include fractional and negative exponents. Newton succeeded in expanding the applicability of the binomial theorem by applying the algebra of finite quantities in an analysis of infinite series. He showed a willingness to view infinite series not only as approximate devices, but also as alternative forms of expressing a term.",
                    "score": 0.906144380569458
                },
                {
                    "id": 17824792,
                    "contents": "Dixon's identity\nIn mathematics, Dixon's identity (or Dixon's theorem or Dixon's formula) is any of several different but closely related identities proved by A. C. Dixon, some involving finite sums of products of three binomial coefficients, and some evaluating a hypergeometric sum. These identities famously follow from the MacMahon Master theorem, and can now be routinely proved by computer algorithms . Statements The original identity, from , is A generalization, also sometimes called Dixon's identity, is where a, b, and c are non-negative integers . The sum on the left can be written as the terminating well-poised hypergeometric series and the identity follows as a limiting case (as a tends to an integer) of Dixon's theorem evaluating a well-poised 3F2 generalized hypergeometric series at 1, from :",
                    "score": 0.905045747756958
                },
                {
                    "id": 17423220,
                    "contents": "Boole's rule\nIn mathematics, Boole's rule, named after George Boole, is a method of numerical integration. It approximates an integral by using the values of ƒ at five equally spaced points It is expressed thus in Abramowitz and Stegun (1972, p. 886): and the error term is for some number c between x1 and x5. (945 = 1 × 3 × 5 × 7 × 9.) It is often known as Bode's rule, due to a typographical error that propagated from Abramowitz and Stegun (1972, p. 886). The following constitutes a very simple implementation of the method in Common Lisp which ignores the error term:",
                    "score": 0.903332531452179
                },
                {
                    "id": 14171783,
                    "contents": "Timeline of calculus and mathematical analysis\n1644 - Evangelista Torricelli publishes Opera geometrica, 1644 - Fermat's methods of maxima and minima published by Pierre Hérigone, 1647 - Cavalieri computes the integral , 1647 - Grégoire de Saint-Vincent discovers that the area under a hyperbola represented a logarithm, 1650 - Pietro Mengoli proves of the divergence of the harmonic series, 1654 - Johannes Hudde discovers the power series expansion for , 1656 - John Wallis publishes Arithmetica Infinitorum, 1658 - Christopher Wren shows that the length of a cycloid is four times the diameter of its generating circle, 1659 - Second edition of Van Schooten's Latin translation of Descartes' Geometry with appendices by Hudde and Heuraet, 1665 - Isaac Newton discovers the generalized binomial theorem and develops his version of infinitesimal calculus, 1667 - James Gregory publishes Vera circuli et hyperbolae quadratura, 1668 - Nicholas Mercator publishes Logarithmotechnia,",
                    "score": 0.9032891988754272
                },
                {
                    "id": 2394280,
                    "contents": "Euler's constant\nAppearances Euler's constant appears, among other places, in the following (where '*' means that this entry contains an explicit equation): Expressions involving the exponential integral* The Laplace transform* of the natural logarithm The first term of the Laurent series expansion for the Riemann zeta function*, where it is the first of the Stieltjes constants* Calculations of the digamma function A product formula for the gamma function The asymptotic expansion of the gamma function for small arguments. An inequality for Euler's totient function The growth rate of the divisor function In dimensional regularization of Feynman diagrams in quantum field theory The calculation of the Meissel–Mertens constant The third of Mertens' theorems* Solution of the second kind to Bessel's equation In the regularization/renormalization of the harmonic series as a finite value The mean of the Gumbel distribution",
                    "score": 0.9026599526405334
                },
                {
                    "id": 14617647,
                    "contents": "Alfred Cardew Dixon\nDixon's identity is any of several closely related identities involving binomial coefficients and hypergeometric functions. References External links 1865 births 1936 deaths 20th-century mathematicians Academics of Queen's University Belfast Alumni of Trinity College, Cambridge Fellows of Trinity College, Cambridge Fellows of the Royal Society Senior Wranglers People from Northallerton",
                    "score": 0.9026229381561279
                },
                {
                    "id": 6183796,
                    "contents": "A Course of Modern Analysis\nThe book was one of the earliest to use decimal numbering for its sections, an innovation the authors attribute to Giuseppe Peano. Contents Below are the contents of the fourth edition: Part I. The Process of Analysis Part II. The Transcendental Functions Reception Reviews of the first edition George B. Mathews, in a 1903 review article published in The Mathematical Gazette opens by saying the book is \"sure of a favorable reception\" because of its \"attractive account of some of the most valuable and interesting results of recent analysis\". He notes that Part I deals mainly with infinite series, focusing on power series and Fourier expansions while including the \"elements of\" complex integration and the theory of residues. Part II, in contrast, has chapters on the gamma function, Legendre functions, the hypergeometric series, Bessel functions, elliptic functions, and mathematical physics.",
                    "score": 0.9023716449737549
                },
                {
                    "id": 2640516,
                    "contents": "Leibniz's notation\nIn each of these instances the Leibniz notation for a derivative appears to act like a fraction, even though, in its modern interpretation, it isn't one. Modern justification of infinitesimals In the 1960s, building upon earlier work by Edwin Hewitt and Jerzy Łoś, Abraham Robinson developed mathematical explanations for Leibniz's infinitesimals that were acceptable by contemporary standards of rigor, and developed nonstandard analysis based on these ideas. Robinson's methods are used by only a minority of mathematicians. Jerome Keisler wrote a first-year calculus textbook, Elementary calculus: an infinitesimal approach, based on Robinson's approach. From the point of view of modern infinitesimal theory, is an infinitesimal -increment, is the corresponding -increment, and the derivative is the standard part of the infinitesimal ratio: . Then one sets , , so that by definition, is the ratio of by . Similarly, although most mathematicians now view an integral as a limit",
                    "score": 0.902358889579773
                },
                {
                    "id": 1844567,
                    "contents": "L'Hôpital's rule\nThe differentiation of the numerator and denominator often simplifies the quotient or converts it to a limit that can be evaluated directly. History Guillaume de l'Hôpital (also written l'Hospital) published this rule in his 1696 book Analyse des Infiniment Petits pour l'Intelligence des Lignes Courbes (literal translation: Analysis of the Infinitely Small for the Understanding of Curved Lines), the first textbook on differential calculus. However, it is believed that the rule was discovered by the Swiss mathematician Johann Bernoulli.",
                    "score": 0.901749849319458
                },
                {
                    "id": 5500711,
                    "contents": "Leibniz formula for π\nHowever, the Leibniz formula can be used to calculate to high precision (hundreds of digits or more) using various convergence acceleration techniques. For example, the Shanks transformation, Euler transform or Van Wijngaarden transformation, which are general methods for alternating series, can be applied effectively to the partial sums of the Leibniz series. Further, combining terms pairwise gives the non-alternating series which can be evaluated to high precision from a small number of terms using Richardson extrapolation or the Euler–Maclaurin formula. This series can also be transformed into an integral by means of the Abel–Plana formula and evaluated using techniques for numerical integration. Unusual behaviour If the series is truncated at the right time, the decimal expansion of the approximation will agree with that of for many more digits, except for isolated digits or digit groups. For example, taking five million terms yields",
                    "score": 0.9016518592834473
                },
                {
                    "id": 1145907,
                    "contents": "Newton's method\nHistory The name \"Newton's method\" is derived from Isaac Newton's description of a special case of the method in De analysi per aequationes numero terminorum infinitas (written in 1669, published in 1711 by William Jones) and in De metodis fluxionum et serierum infinitarum (written in 1671, translated and published as Method of Fluxions in 1736 by John Colson). However, his method differs substantially from the modern method given above. Newton applied the method only to polynomials, starting with an initial root estimate and extracting a sequence of error corrections. He used each correction to rewrite the polynomial in terms of the remaining error, and then solved for a new correction by neglecting higher-degree terms. He did not explicitly connect the method with derivatives or present a general formula. Newton applied this method to both numerical and algebraic problems, producing Taylor series in the latter case.",
                    "score": 0.9011605381965637
                },
                {
                    "id": 4507512,
                    "contents": "Lévy's constant\nfor and zero otherwise. This gives Lévy's constant as . The base-10 logarithm of Lévy's constant, which is approximately 0.51532041…, is half of the reciprocal of the limit in Lochs' theorem. See also Khinchin's constant References Further reading External links Continued fractions Mathematical constants Paul Lévy (mathematician)",
                    "score": 0.9006162285804749
                },
                {
                    "id": 968614,
                    "contents": "Catalan's constant\nOther quickly converging series, due to Guillera and Pilehrood and employed by the y-cruncher software, include: All of these series have time complexity . Continued fraction can be expressed in the following form See also Particular values of Riemann zeta function Mathematical constant References Further reading External links (Provides over one hundred different identities). (Provides a graphical interpretation of the relations) (Provides the first 300,000 digits of Catalan's constant) Combinatorics Mathematical constants",
                    "score": 0.9004919528961182
                },
                {
                    "id": 11950793,
                    "contents": "Carl Johan Malmsten\nIn 1842, Malmsten also evaluated several important logarithmic series, among which we can find these two series and The latter series was later rediscovered in a slightly different form by Ernst Kummer, who derived a similar expression in 1847 (strictly speaking, the Kummer's result is obtained from the Malmsten's one by putting a=π(2x-1)). Moreover, this series is even known in analysis as Kummer's series for the logarithm of the Gamma function, although Malmsten derived it 5 years before Kummer. Malsmten also notably contributed into the theory of zeta-function related series and integrals. In 1842 he proved following important functional relationship for the L-function as well as for the M-function",
                    "score": 0.8997259736061096
                },
                {
                    "id": 1088371,
                    "contents": "Abraham de Moivre\nStirling's approximation De Moivre had been studying probability, and his investigations required him to calculate binomial coefficients, which in turn required him to calculate factorials. In 1730 de Moivre published his book Miscellanea Analytica de Seriebus et Quadraturis [Analytic Miscellany of Series and Integrals], which included tables of log (n!). For large values of n, de Moivre approximated the coefficients of the terms in a binomial expansion. Specifically, given a positive integer n, where n is even and large, then the coefficient of the middle term of (1 + 1)n is approximated by the equation: On June 19, 1729, James Stirling sent to de Moivre a letter, which illustrated how he calculated the coefficient of the middle term of a binomial expansion (a + b)n for large values of n. In 1730, Stirling published his book Methodus Differentialis [The Differential Method], in which he included his series for log (n!): , so that for large , .",
                    "score": 0.8995800018310547
                },
                {
                    "id": 2394286,
                    "contents": "Euler's constant\nAn interesting comparison by Sondow is the double integral and alternating series It shows that may be thought of as an \"alternating Euler constant\". The two constants are also related by the pair of series where and are the number of 1s and 0s, respectively, in the base 2 expansion of . We have also Catalan's 1875 integral Series expansions In general, for any . However, the rate of convergence of this expansion depends significantly on . In particular, exhibits much more rapid convergence than the conventional expansion . This is because while Even so, there exist other series expansions which converge more rapidly than this; some of these are discussed below. Euler showed that the following infinite series approaches : The series for is equivalent to a series Nielsen found in 1897: In 1910, Vacca found the closely related series where is the logarithm to base 2 and is the floor function. In 1926 he found a second series:",
                    "score": 0.8991631269454956
                },
                {
                    "id": 1844568,
                    "contents": "L'Hôpital's rule\nGeneral form The general form of L'Hôpital's rule covers many cases. Let and be extended real numbers (i.e., real numbers, positive infinity, or negative infinity). Let be an open interval containing (for a two-sided limit) or an open interval with endpoint (for a one-sided limit, or a limit at infinity if is infinite). The real valued functions and are assumed to be differentiable on except possibly at , and additionally on except possibly at . It is also assumed that Thus the rule applies to situations in which the ratio of the derivatives has a finite or infinite limit, but not to situations in which that ratio fluctuates permanently as gets closer and closer to . If either or then Although we have written throughout, the limits may also be one-sided limits ( or ), when is a finite endpoint of .",
                    "score": 0.8991096615791321
                },
                {
                    "id": 22507416,
                    "contents": "Littlewood's Tauberian theorem\nIn mathematics, Littlewood's Tauberian theorem is a strengthening of Tauber's theorem introduced by . Statement Littlewood showed the following: If an = O(1/n ), and as x ↑ 1 we have then Hardy and Littlewood later showed that the hypothesis on an could be weakened to the \"one-sided\" condition an ≥ –C/n for some constant C. However in some sense the condition is optimal: Littlewood showed that if cn is any unbounded sequence then there is a series with |an| ≤ |cn|/n which is divergent but Abel summable. History",
                    "score": 0.8991075754165649
                },
                {
                    "id": 12075476,
                    "contents": "Chebyshev function\nReferences External links Riemann's Explicit Formula, with images and movies Arithmetic functions",
                    "score": 0.8990944623947144
                },
                {
                    "id": 6591711,
                    "contents": "Explicit formulae for L-functions\nwhere the LHS is an inverse Mellin transform with and and the RHS is obtained from the residue theorem, and then converting it into the formula that Riemann himself actually sketched. This series is also conditionally convergent and the sum over zeroes should again be taken in increasing order of imaginary part: where . The error involved in truncating the sum to is always smaller than in absolute value, and when divided by the natural logarithm of , has absolute value smaller than divided by the distance from to the nearest prime power. Weil's explicit formula There are several slightly different ways to state the explicit formula. André Weil's form of the explicit formula states where ρ runs over the non-trivial zeros of the zeta function p runs over positive primes m runs over positive integers F is a smooth function all of whose derivatives are rapidly decreasing is a Fourier transform of F: , where is the digamma function Γ′/Γ.",
                    "score": 0.8990836143493652
                },
                {
                    "id": 18600498,
                    "contents": "Rankin–Selberg method\n. The integral converges absolutely if one of the two forms is cuspidal; otherwise the asymptotics must be used to get a meromorphic continuation like Riemann did. The analytic continuation and functional equation then boil down to those of the Eisenstein series. The integral was identified with the convolution L-function by a technique called \"unfolding\", in which the definition of the Eisenstein series and the range of integration are converted into a simpler expression that more readily exhibits the L-function as a Dirichlet series. The simultaneous combination of an unfolding together with global control over the analytic properties, is special and what makes the technique successful.",
                    "score": 0.8984564542770386
                },
                {
                    "id": 14171784,
                    "contents": "Timeline of calculus and mathematical analysis\n1667 - James Gregory publishes Vera circuli et hyperbolae quadratura, 1668 - Nicholas Mercator publishes Logarithmotechnia, 1668 - James Gregory computes the integral of the secant function, 1670 - Isaac Newton rediscovers the power series expansion for and (originally discovered by Madhava), 1670 - Isaac Barrow publishes Lectiones Geometricae, 1671 - James Gregory rediscovers the power series expansion for and (originally discovered by Madhava), 1672 - René-François de Sluse publishes A Method of Drawing Tangents to All Geometrical Curves, 1673 - Gottfried Leibniz also develops his version of infinitesimal calculus, 1675 - Isaac Newton invents a Newton's method for the computation of roots of a function, 1675 - Leibniz uses the modern notation for an integral for the first time, 1677 - Leibniz discovers the rules for differentiating products, quotients, and the function of a function. 1683 - Jacob Bernoulli discovers the number ,",
                    "score": 0.8980891108512878
                },
                {
                    "id": 16610125,
                    "contents": "Lochs's theorem\nThe reciprocal of this limit, , is twice the base-10 logarithm of Lévy's constant. A prominent example of a number not exhibiting this behavior is the golden ratio—sometimes known as the \"most irrational\" number—whose continued fraction terms are all ones, the smallest possible in canonical form. On average it requires approximately 2.39 continued fraction terms per decimal digit. References Continued fractions Theorems in number theory",
                    "score": 0.8979356288909912
                },
                {
                    "id": 15693330,
                    "contents": "Timeline of mathematics\n1637 – First use of the term imaginary number by René Descartes; it was meant to be derogatory. 1643 – René Descartes develops Descartes' theorem. 1654 – Blaise Pascal and Pierre de Fermat create the theory of probability. 1655 – John Wallis writes Arithmetica Infinitorum. 1658 – Christopher Wren shows that the length of a cycloid is four times the diameter of its generating circle. 1665 – Isaac Newton works on the fundamental theorem of calculus and develops his version of infinitesimal calculus. 1668 – Nicholas Mercator and William Brouncker discover an infinite series for the logarithm while attempting to calculate the area under a hyperbolic segment. 1671 – James Gregory develops a series expansion for the inverse-tangent function (originally discovered by Madhava). 1671 – James Gregory discovers Taylor's Theorem. 1673 – Gottfried Leibniz also develops his version of infinitesimal calculus.",
                    "score": 0.8978668451309204
                },
                {
                    "id": 17250499,
                    "contents": "Cavalieri's principle\nToday Cavalieri's principle is seen as an early step towards integral calculus, and while it is used in some forms, such as its generalization in Fubini's theorem, results using Cavalieri's principle can often be shown more directly via integration. In the other direction, Cavalieri's principle grew out of the ancient Greek method of exhaustion, which used limits but did not use infinitesimals. History",
                    "score": 0.8975378274917603
                },
                {
                    "id": 6591708,
                    "contents": "Explicit formulae for L-functions\nIn mathematics, the explicit formulae for L-functions are relations between sums over the complex number zeroes of an L-function and sums over prime powers, introduced by for the Riemann zeta function. Such explicit formulae have been applied also to questions on bounding the discriminant of an algebraic number field, and the conductor of a number field. Riemann's explicit formula In his 1859 paper \"On the Number of Primes Less Than a Given Magnitude\" Riemann sketched an explicit formula (it was not fully proven until 1895 by von Mangoldt, see below) for the normalized prime-counting function which is related to the prime-counting function by which takes the arithmetic mean of the limit from the left and the limit from the right at discontinuities. His formula was given in terms of the related function in which a prime power counts as of a prime. The normalized prime-counting function can be recovered from this function by",
                    "score": 0.8975082039833069
                },
                {
                    "id": 141173,
                    "contents": "Lists of integrals\n(see sinc function and the Dirichlet integral) (if is a positive integer and !! is the double factorial). (for integers with and , see also Binomial coefficient) (for real, a non-negative integer, and an odd, positive integer; since the integrand is odd) (for integers with and , see also Binomial coefficient) (for integers with and , see also Binomial coefficient) (where is the exponential function , and .) (where is the Gamma function) (for and , see Beta function) (where is the modified Bessel function of the first kind) (for , this is related to the probability density function of Student's t-distribution) If the function has bounded variation on the interval , then the method of exhaustion provides a formula for the integral: The \"sophomore's dream\": attributed to Johann Bernoulli. See also Incomplete gamma function Indefinite sum List of limits List of mathematical series Symbolic integration References Further reading",
                    "score": 0.8973924517631531
                },
                {
                    "id": 150721,
                    "contents": "Roger Cotes\nCotes's major original work was in mathematics, especially in the fields of integral calculus, logarithms, and numerical analysis. He published only one scientific paper in his lifetime, titled Logometria, in which he successfully constructs the logarithmic spiral. After his death, many of Cotes's mathematical papers were edited by his cousin Robert Smith and published in a book, Harmonia mensurarum. Cotes's additional works were later published in Thomas Simpson's The Doctrine and Application of Fluxions. Although Cotes's style was somewhat obscure, his systematic approach to integration and mathematical theory was highly regarded by his peers. Cotes discovered an important theorem on the n-th roots of unity, foresaw the method of least squares, and discovered a method for integrating rational fractions with binomial denominators. He was also praised for his efforts in numerical methods, especially in interpolation methods and his table construction techniques. He was regarded as one",
                    "score": 0.8972722291946411
                },
                {
                    "id": 1779370,
                    "contents": "Isaac Newton\nNewton is generally credited with the generalised binomial theorem, valid for any exponent. He discovered Newton's identities, Newton's method, classified cubic plane curves (polynomials of degree three in two variables), made substantial contributions to the theory of finite differences, and was the first to use fractional indices and to employ coordinate geometry to derive solutions to Diophantine equations. He approximated partial sums of the harmonic series by logarithms (a precursor to Euler's summation formula) and was the first to use power series with confidence and to revert power series. Newton's work on infinite series was inspired by Simon Stevin's decimals.",
                    "score": 0.8972443342208862
                },
                {
                    "id": 13375532,
                    "contents": "Auxiliary function\nand also, using the mean value theorem, that there is some constant depending on α, say c(α), such that Combining these results gives a property that the algebraic number must satisfy; therefore any number not satisfying this criterion must be transcendental. The auxiliary function in Liouville's work is very simple, merely a polynomial that vanishes at a given algebraic number. This kind of property is usually the one that auxiliary functions satisfy. They either vanish or become very small at particular points, which is usually combined with the assumption that they do not vanish or can't be too small to derive a result. Fourier's proof of the irrationality of e Another simple, early occurrence is in Fourier's proof of the irrationality of e, though the notation used usually disguises this fact. Fourier's proof used the power series of the exponential function:",
                    "score": 0.8968130350112915
                },
                {
                    "id": 5500710,
                    "contents": "Leibniz formula for π\nProof Proof 1 Considering only the integral in the last term, we have: Therefore, by the squeeze theorem, as we are left with the Leibniz series: Proof 2 When , converges uniformly, therefore If approaches so that it is continuous and converges uniformly, the proof is complete. From Leibniz's test, converges, also approaches from within the Stolz angle, so from Abel's theorem this is correct. Convergence Leibniz's formula converges extremely slowly: it exhibits sublinear convergence. Calculating to 10 correct decimal places using direct summation of the series requires about five billion terms because for .",
                    "score": 0.896778404712677
                },
                {
                    "id": 4115440,
                    "contents": "List of important publications in mathematics\nThe eminent historian of mathematics Carl Boyer once called Euler's Introductio in analysin infinitorum the greatest modern textbook in mathematics. Published in two volumes, this book more than any other work succeeded in establishing analysis as a major branch of mathematics, with a focus and approach distinct from that used in geometry and algebra. Notably, Euler identified functions rather than curves to be the central focus in his book. Logarithmic, exponential, trigonometric, and transcendental functions were covered, as were expansions into partial fractions, evaluations of for a positive integer between 1 and 13, infinite series and infinite product formulas, continued fractions, and partitions of integers. In this work, Euler proved that every rational number can be written as a finite continued fraction, that the continued fraction of an irrational number is infinite, and derived continued fraction expansions for and . This work also contains a statement of Euler's",
                    "score": 0.8965213298797607
                },
                {
                    "id": 2805261,
                    "contents": "Polylogarithm\nThe polylogarithm function is equivalent to the Hurwitz zeta function — either function can be expressed in terms of the other — and both functions are special cases of the Lerch transcendent. Polylogarithms should not be confused with polylogarithmic functions nor with the offset logarithmic integral which has the same notation, but with one variable. The polylogarithm function is defined by a power series in z, which is also a Dirichlet series in s:",
                    "score": 0.8963531255722046
                },
                {
                    "id": 1945840,
                    "contents": "Effective results in number theory\nIn more detail, writing for a numerical sequence f(n), an effective result about its changing sign infinitely often would be a theorem including, for every value of N, a value M > N such that f(N) and f(M) have different signs, and such that M could be computed with specified resources. In practical terms, M would be computed by taking values of n from N onwards, and the question is 'how far must you go?' A special case is to find the first sign change. The interest of the question was that the numerical evidence known showed no change of sign: Littlewood's result guaranteed that this evidence was just a small number effect, but 'small' here included values of n up to a billion.",
                    "score": 0.8962230086326599
                },
                {
                    "id": 2394284,
                    "contents": "Euler's constant\nThe error term in the last equation is a rapidly decreasing function of . As a result, the formula is well-suited for efficient computation of the constant to high precision. Other interesting limits equaling Euler's constant are the antisymmetric limit: and the following formula, established in 1898 by de la Vallée-Poussin: where are ceiling brackets. This formula indicates that when taking any positive integer n and dividing it by each positive integer k less than n, the average fraction by which the quotient n/k falls short of the next integer tends to (rather than 0.5) as n tends to infinity. Closely related to this is the rational zeta series expression. By taking separately the first few terms of the series above, one obtains an estimate for the classical series limit: where is the Hurwitz zeta function. The sum in this equation involves the harmonic numbers, . Expanding some of the terms in the Hurwitz zeta function gives: where",
                    "score": 0.8961372375488281
                },
                {
                    "id": 1844580,
                    "contents": "L'Hôpital's rule\nThe proof of a more general version of L'Hôpital's rule is given below. General proof The following proof is due to , where a unified proof for the and indeterminate forms is given. Taylor notes that different proofs may be found in and . Let f and g be functions satisfying the hypotheses in the General form section. Let be the open interval in the hypothesis with endpoint c. Considering that on this interval and g is continuous, can be chosen smaller so that g is nonzero on . For each x in the interval, define and as ranges over all values between x and c. (The symbols inf and sup denote the infimum and supremum.)",
                    "score": 0.8960981965065002
                },
                {
                    "id": 1844571,
                    "contents": "L'Hôpital's rule\nThe second and third conditions are satisfied by and . The fourth condition is also satisfied with . But, L'Hôpital's rule fails in this counterexample, since . Derivative of denominator is zero The necessity of the condition that near can be seen by the following counterexample due to Otto Stolz. Let and Then there is no limit for as However, which tends to 0 as . Further examples of this type were found by Ralph P. Boas Jr. Limit of derivatives does not exist The requirement that the limit exist is essential. Without this condition, or may exhibit undamped oscillations as approaches , in which case L'Hôpital's rule does not apply. For example, if , and , then this expression does not approach a limit as goes to , since the cosine function oscillates between and . But working with the original functions, can be shown to exist: In a case such as this, all that can be concluded is that",
                    "score": 0.8960875272750854
                },
                {
                    "id": 11646649,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nFrom this point, there are a few ways to prove that One method, along the lines of Euler's reasoning, uses the relationship between the Riemann zeta function and the Dirichlet eta function η(s). The eta function is defined by an alternating Dirichlet series, so this method parallels the earlier heuristics. Where both Dirichlet series converge, one has the identities: The identity continues to hold when both functions are extended by analytic continuation to include values of s for which the above series diverge. Substituting , one gets Now, computing η(−1) is an easier task, as the eta function is equal to the Abel sum of its defining series, which is a one-sided limit: Dividing both sides by −3, one gets Cutoff regularization",
                    "score": 0.8960544466972351
                },
                {
                    "id": 23418117,
                    "contents": "Vincent Rivasseau\nBooks From Perturbative to Constructive Renormalization Princeton University Press, Princeton NJ u. a. 1991, . References External links Homepage French physicists 1955 births Living people",
                    "score": 0.8960205316543579
                },
                {
                    "id": 18738254,
                    "contents": "Baker's theorem\nand one inserts an extra variable z0 into Φ as follows: Corollaries As mentioned above, the theorem includes numerous earlier transcendence results concerning the exponential function, such as the Hermite–Lindemann theorem and Gelfond–Schneider theorem. It is not quite as encompassing as the still unproven Schanuel's conjecture, and does not imply the six exponentials theorem nor, clearly, the still open four exponentials conjecture. The main reason Gelfond desired an extension of his result was not just for a slew of new transcendental numbers. In 1935 he used the tools he had developed to prove the Gelfond–Schneider theorem to derive a lower bound for the quantity where β1 and β2 are algebraic and λ1 and λ2 are in . Baker's proof gave lower bounds for quantities like the above but with arbitrarily many terms, and he could use these bounds to develop effective means of tackling Diophantine equations and to solve Gauss' class number problem.",
                    "score": 0.8959503769874573
                },
                {
                    "id": 10049370,
                    "contents": "History of Grandi's series\nAlgebraic reasoning In 1830, a mathematician identified only as \"M. R. S.\" wrote in the Annales de Gergonne on a technique to numerically find fixed points of functions of one variable. If one can transform a problem into the form of an equation x = A + f(x), where A can be chosen at will, then should be a solution, and truncating this infinite expression results in a sequence of approximations. Conversely, given the series , the author recovers the equation to which the solution is (1⁄2)a.",
                    "score": 0.895723819732666
                },
                {
                    "id": 1280600,
                    "contents": "Taylor series\nExponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm The natural logarithm (with base ) has Maclaurin series They converge for . (In addition, the series for converges for , and the series for converges for .) Geometric series The geometric series and its derivatives have Maclaurin series All are convergent for . These are special cases of the binomial series given in the next section. Binomial series The binomial series is the power series whose coefficients are the generalized binomial coefficients (If , this product is an empty product and has value 1.) It converges for for any real or complex number . When , this is essentially the infinite geometric series mentioned in the previous section. The special cases and give the square root function and its inverse: When only the linear term is retained, this simplifies to the binomial approximation.",
                    "score": 0.8955813050270081
                },
                {
                    "id": 6183794,
                    "contents": "A Course of Modern Analysis\nA Course of Modern Analysis: an introduction to the general theory of infinite processes and of analytic functions; with an account of the principal transcendental functions (colloquially known as Whittaker and Watson) is a landmark textbook on mathematical analysis written by Edmund T. Whittaker and George N. Watson, first published by Cambridge University Press in 1902. The first edition was Whittaker's alone, but later editions were co-authored with Watson. History Its first, second, third, and the fourth edition were published in 1902, 1915, 1920, and 1927, respectively. Since then, it has continuously been reprinted and is still in print today. A revised, expanded and digitally reset fifth edition, edited by Victor H. Moll, was published in 2021.",
                    "score": 0.8954475522041321
                },
                {
                    "id": 1169611,
                    "contents": "Pi\nA record was set by the calculating prodigy Zacharias Dase, who in 1844 employed a Machin-like formula to calculate 200 decimals of in his head at the behest of German mathematician Carl Friedrich Gauss. British mathematician William Shanks calculated to 607 digits in 1853, but made a mistake in the 528th digit, rendering all subsequent digits incorrect. Though he calculated an additional 100 digits in 1873, bring the total up to 707, his previous mistake rendered all the new digits incorrect as well. Rate of convergence Some infinite series for converge faster than others. Given the choice of two infinite series for , mathematicians will generally use the one that converges more rapidly because faster convergence reduces the amount of computation needed to calculate to any given accuracy. A simple infinite series for is the Gregory–Leibniz series:",
                    "score": 0.8954405188560486
                },
                {
                    "id": 17335584,
                    "contents": "Introductio in analysin infinitorum\nChapter 1 is on the concepts of variables and functions. Chapter 4 introduces infinite series through rational functions. According to Henk Bos, The Introduction is meant as a survey of concepts and methods in analysis and analytic geometry preliminary to the study of the differential and integral calculus. [Euler] made of this survey a masterly exercise in introducing as much as possible of analysis without using differentiation or integration. In particular, he introduced the elementary transcendental functions, the logarithm, the exponential function, the trigonometric functions and their inverses without recourse to integral calculus — which was no mean feat, as the logarithm was traditionally linked to the quadrature of the hyperbola and the trigonometric functions to the arc-length of the circle.",
                    "score": 0.8953982591629028
                },
                {
                    "id": 2649697,
                    "contents": "Bonaventura Cavalieri\nto the method of indivisibles. By applying transformations to his variables, he generalised his previous integral result, showing that for n=3 to n=9, which is now known as Cavalieri's quadrature formula.",
                    "score": 0.8952062726020813
                },
                {
                    "id": 14171787,
                    "contents": "Timeline of calculus and mathematical analysis\n1742 - Modern definion of logarithm by William Gardiner, 1742 - Colin Maclaurin publishes Treatise on Fluxions, 1748 - Euler publishes Introductio in analysin infinitorum, 1748 - Maria Gaetana Agnesi discusses analysis in Instituzioni Analitiche ad Uso della Gioventu Italiana, 1762 - Joseph Louis Lagrange discovers the divergence theorem, 1797 - Lagrange publishes Théorie des fonctions analytiques,",
                    "score": 0.8950837850570679
                },
                {
                    "id": 997658,
                    "contents": "Henri Lebesgue\nHis lectures from 1902 to 1903 were collected into a \"Borel tract\" Leçons sur l'intégration et la recherche des fonctions primitives. The problem of integration regarded as the search for a primitive function is the keynote of the book. Lebesgue presents the problem of integration in its historical context, addressing Augustin-Louis Cauchy, Peter Gustav Lejeune Dirichlet, and Bernhard Riemann. Lebesgue presents six conditions which it is desirable that the integral should satisfy, the last of which is \"If the sequence fn(x) increases to the limit f(x), the integral of fn(x) tends to the integral of f(x).\" Lebesgue shows that his conditions lead to the theory of measure and measurable functions and the analytical and geometrical definitions of the integral.",
                    "score": 0.8949993848800659
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_19",
        "question": "Evaluate the series\r\n$$\r\nS=\\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{2^n}\r\n$$",
        "golden_answers": [
            " 1/3"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 12548116,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nSince the sum of an infinite series is defined as the limit of its partial sums, Notes References Page images at HTML with figures and commentary at Geometric series Proof without words",
                    "score": 0.9241862297058105
                },
                {
                    "id": 11678088,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nAfter Christian Wolff read Leibniz's treatment of Grandi's series in mid-1712, Wolff was so pleased with the solution that he sought to extend the arithmetic mean method to more divergent series such as . Briefly, if one expresses a partial sum of this series as a function of the penultimate term, one obtains either or . The mean of these values is , and assuming that at infinity yields as the value of the series. Leibniz's intuition prevented him from straining his solution this far, and he wrote back that Wolff's idea was interesting but invalid for several reasons. The arithmetic means of neighboring partial sums do not converge to any particular value, and for all finite cases one has , not . Generally, the terms of a summable series should decrease to zero; even could be expressed as a limit of such series. Leibniz counsels Wolff to reconsider so that he \"might produce something worthy of science and himself.\" Modern methods",
                    "score": 0.9235342144966125
                },
                {
                    "id": 5898948,
                    "contents": "Convergent series\nIn mathematics, a series is the sum of the terms of an infinite sequence of numbers. More precisely, an infinite sequence defines a series that is denoted The th partial sum is the sum of the first terms of the sequence; that is, A series is convergent (or converges) if the sequence of its partial sums tends to a limit; that means that, when adding one after the other in the order given by the indices, one gets partial sums that become closer and closer to a given number. More precisely, a series converges, if there exists a number such that for every arbitrarily small positive number , there is a (sufficiently large) integer such that for all , If the series is convergent, the (necessarily unique) number is called the sum of the series. The same notation",
                    "score": 0.9210975170135498
                },
                {
                    "id": 14058441,
                    "contents": "Florence Marie Mears\nMears specialized in the findings of definitions and values assigned to various infinite series of numbers. An \"infinite series\" is an endless series of numbers, each succeeding the other that is a certain amount lesser or greater than the proceeding one. An example set of an infinite series includes is 1 + ½ + ¼ etc. in which the definition of the series can be defined as the number two. As a result, Mears created several theorems about these definitions, many of which provided truth for many practicing mathematicians, engineers, chemists, physicists, and astronomers. One of her most popular theorems, called the Norlund Mean can be explained through absolute regularity, the summability of Cauchy products, and inverse properties.",
                    "score": 0.9207783937454224
                },
                {
                    "id": 17472486,
                    "contents": "List of mathematical series\nThis list of mathematical series contains formulae for finite and infinite sums. It can be used in conjunction with other tools for evaluating sums. Here, is taken to have the value denotes the fractional part of is a Bernoulli polynomial. is a Bernoulli number, and here, is an Euler number. is the Riemann zeta function. is the gamma function. is a polygamma function. is a polylogarithm. is binomial coefficient denotes exponential of Sums of powers See Faulhaber's formula. The first few values are: See zeta constants. The first few values are: (the Basel problem) Power series Low-order polylogarithms Finite sums: , (geometric series) Infinite sums, valid for (see polylogarithm): The following is a useful property to calculate low-integer-order polylogarithms recursively in closed form: Exponential function (cf. mean of Poisson distribution) (cf. second moment of Poisson distribution) where is the Touchard polynomials.",
                    "score": 0.920464277267456
                },
                {
                    "id": 11678087,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nLeibniz did not quite assert that the series had a sum, but he did infer an association with following Mercator's method. The attitude that a series could equal some finite quantity without actually adding up to it as a sum would be commonplace in the 18th century, although no distinction is made in modern mathematics.",
                    "score": 0.9201837778091431
                },
                {
                    "id": 13740952,
                    "contents": "Sums of powers\nThe sum of the terms in the geometric series is",
                    "score": 0.9177367091178894
                },
                {
                    "id": 1790041,
                    "contents": "Series (mathematics)\nBasic properties An infinite series or simply a series is an infinite sum, represented by an infinite expression of the form where is any ordered sequence of terms, such as numbers, functions, or anything else that can be added (an abelian group). This is an expression that is obtained from the list of terms by laying them side by side, and conjoining them with the symbol \"+\". A series may also be represented by using summation notation, such as If an abelian group of terms has a concept of limit (e.g., if it is a metric space), then some series, the convergent series, can be interpreted as having a value in , called the sum of the series. This includes the common cases from calculus, in which the group is the field of real numbers or the field of complex numbers. Given a series , its th partial sum is By definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes",
                    "score": 0.9167125225067139
                },
                {
                    "id": 20560519,
                    "contents": "Isidore Isaac Hirschman Jr.\nSelected publications Articles } Books Hirschman, I. (1962). Infinite Series. New York: Holt, Rinehart & Winston. – A textbook for advanced undergraduate and graduate mathematics. Hirschman, Isidore Isaac; Widder, David Vernon (1955). The Convolution Transform. New York: Princeton University Press; now available from Dover Publications. References http://mathdl.maa.org/mathDL/46/?pa=content&sa=viewDocument&nodeId=3801&bodyId=4189 20th-century American mathematicians Harvard University alumni Washington University in St. Louis faculty Washington University in St. Louis mathematicians",
                    "score": 0.9166954755783081
                },
                {
                    "id": 10313842,
                    "contents": "Summation of Grandi's series\nThe Cesàro sum of 1 + 0 − 1 + 1 + 0 − 1 + · · · is 2⁄3. So the Cesàro sum of a series can be altered by inserting infinitely many 0s as well as infinitely many brackets. The series can also be summed by the more general fractional (C, a) methods. Abel sum Abel summation is similar to Euler's attempted definition of sums of divergent series, but it avoids Callet's and N. Bernoulli's objections by precisely constructing the function to use. In fact, Euler likely meant to limit his definition to power series, and in practice he used it almost exclusively in a form now known as Abel's method. Given a series a0 + a1 + a2 + · · ·, one forms a new series a0 + a1x + a2x2 + · · ·. If the latter series converges for 0 < x < 1 to a function with a limit as x tends to 1, then this limit is called the Abel sum of the original series, after Abel's theorem which guarantees that the procedure is consistent with ordinary summation. For Grandi's series one has",
                    "score": 0.9164338111877441
                },
                {
                    "id": 1169612,
                    "contents": "Pi\nAs individual terms of this infinite series are added to the sum, the total gradually gets closer to , and – with a sufficient number of terms – can get as close to as desired. It converges quite slowly, though – after 500,000 terms, it produces only five correct decimal digits of . An infinite series for (published by Nilakantha in the 15th century) that converges more rapidly than the Gregory–Leibniz series is: Note that (n − 1)n(n + 1) = n3 − n. The following table compares the convergence rates of these two series: After five terms, the sum of the Gregory–Leibniz series is within 0.2 of the correct value of , whereas the sum of Nilakantha's series is within 0.002 of the correct value of . Nilakantha's series converges faster and is more useful for computing digits of . Series that converge even faster include Machin's series and Chudnovsky's series, the latter producing 14 correct decimal digits per term. Irrationality and transcendence",
                    "score": 0.9163182973861694
                },
                {
                    "id": 11646640,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe infinite series whose terms are the natural numbers is a divergent series. The nth partial sum of the series is the triangular number which increases without bound as n goes to infinity. Because the sequence of partial sums fails to converge to a finite limit, the series does not have a sum.",
                    "score": 0.9154461026191711
                },
                {
                    "id": 3559416,
                    "contents": "Sinc function\nThe sum of the squares also equals : When the signs of the addends alternate and begin with +, the sum equals : The alternating sums of the squares and cubes also equal : Series expansion The Taylor series of the unnormalized function can be obtained from that of the sine: The series converges for all . The normalized version follows easily: Euler famously compared this series to the expansion of the infinite product form to solve the Basel problem.",
                    "score": 0.9150130748748779
                },
                {
                    "id": 11646645,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nHeuristics Srinivasa Ramanujan presented two derivations of \"\" in chapter 8 of his first notebook. The simpler, less rigorous derivation proceeds in two steps, as follows. The first key insight is that the series of positive numbers closely resembles the alternating series . The latter series is also divergent, but it is much easier to work with; there are several classical methods that assign it a value, which have been explored since the 18th century. In order to transform the series into , one can subtract 4 from the second term, 8 from the fourth term, 12 from the sixth term, and so on. The total amount to be subtracted is , which is 4 times the original series. These relationships can be expressed using algebra. Whatever the \"sum\" of the series might be, call it Then multiply this equation by 4 and subtract the second equation from the first:",
                    "score": 0.9145689010620117
                },
                {
                    "id": 11678089,
                    "contents": "1 − 2 + 4 − 8 + ⋯\nModern methods Geometric series Any summation method possessing the properties of regularity, linearity, and stability will sum a geometric series In this case a = 1 and r = −2, so the sum is . Euler summation In his 1755 Institutiones, Leonhard Euler effectively took what is now called the Euler transform of , arriving at the convergent series . Since the latter sums to , Euler concluded that . His ideas on infinite series do not quite follow the modern approach; today one says that is Euler summable and that its Euler sum is . The Euler transform begins with the sequence of positive terms: a0 = 1, a1 = 2, a2 = 4, a3 = 8,... The sequence of forward differences is then Δa0 = a1 − a0 = 2 − 1 = 1, Δa1 = a2 − a1 = 4 − 2 = 2, Δa2 = a3 − a2 = 8 − 4 = 4, Δa3 = a4 − a3 = 16 − 8 = 8,... which is just the same sequence. Hence the iterated forward difference sequences all start with for every n. The Euler transform is the series",
                    "score": 0.9142256379127502
                },
                {
                    "id": 10049357,
                    "contents": "History of Grandi's series\nLeibniz thought that the argument from was valid; he took it as an example of his law of continuity. Since the relation holds for all x less than 1, it should hold for x equal to 1 as well. Still, Leibniz thought that one should be able to find the sum of the series directly, without needing to refer back to the expression from which it came. This approach may seem obvious by modern standards, but it is a significant step from the point of view of the history of summing divergent series. In the 18th century, the study of series was dominated by power series, and summing a numerical series by expressing it as f(1) of some function's power series was thought to be the most natural strategy. Leibniz begins by observing that taking an even number of terms from the series, the last term is −1 and the sum is 0: 1 − 1 = 1 − 1 + 1 − 1 = 1 − 1 + 1 − 1 + 1 − 1 = 0. Taking an odd number of terms, the last term is +1 and the sum is 1: 1 = 1 − 1 + 1 = 1 − 1 + 1 − 1 + 1 = 1.",
                    "score": 0.9135644435882568
                },
                {
                    "id": 24331221,
                    "contents": "Fundamental series\nthe frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and noticed that this series had a simpler formula than the others and thus called it the \"fundamental series\" and used the letter F.",
                    "score": 0.9132291078567505
                },
                {
                    "id": 3863129,
                    "contents": "Divergent series\nSum of a series Cauchy's classical definition of the sum of a series defines the sum to be the limit of the sequence of partial sums . This is the default definition of convergence of a sequence. Nørlund means Suppose pn is a sequence of positive terms, starting from p0. Suppose also that If now we transform a sequence s by using p to give weighted means, setting then the limit of tn as n goes to infinity is an average called the Nørlund mean Np(s). The Nørlund mean is regular, linear, and stable. Moreover, any two Nørlund means are consistent. Cesàro summation The most significant of the Nørlund means are the Cesàro sums. Here, if we define the sequence pk by then the Cesàro sum Ck is defined by Cesàro sums are Nørlund means if , and hence are regular, linear, stable, and consistent. C0 is ordinary summation, and C1 is ordinary Cesàro summation. Cesàro sums have the property that if then Ch is stronger than Ck.",
                    "score": 0.9129878282546997
                },
                {
                    "id": 1216477,
                    "contents": "Real analysis\nAn example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes: In contrast, the harmonic series has been known since the Middle Ages to be a divergent series: (Here, \"\" is merely a notational convention to indicate that the partial sums of the series grow without bound.) A series is said to converge absolutely if is convergent. A convergent series for which diverges is said to converge non-absolutely. It is easily shown that absolute convergence of a series implies its convergence. On the other hand, an example of a series that converges non-absolutely is Taylor series The Taylor series of a real or complex-valued function ƒ(x) that is infinitely differentiable at a real or complex number a is the power series : which can be written in the more compact sigma notation as",
                    "score": 0.9129157066345215
                },
                {
                    "id": 12548083,
                    "contents": "1/2 + 1/4 + 1/8 + 1/16 + ⋯\nIn mathematics, the infinite series is an elementary example of a geometric series that converges absolutely. The sum of the series is 1. In summation notation, this may be expressed as The series is related to philosophical questions considered in antiquity, particularly to Zeno's paradoxes. Proof As with any infinite series, the sum is defined to mean the limit of the partial sum of the first terms as approaches infinity. By various arguments, one can show that this finite sum is equal to As approaches infinity, the term approaches 0 and so tends to 1. History",
                    "score": 0.9123423099517822
                },
                {
                    "id": 4580091,
                    "contents": "Series expansion\nIn mathematics, a series expansion is an expansion of a function into a series, or infinite sum. It is a method for calculating a function that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division). The resulting so-called series often can be limited to a finite number of terms, thus yielding an approximation of the function. The fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the partial sum of the omitted terms) can be described by an equation involving Big O notation (see also asymptotic expansion). The series expansion on an open interval will also be an approximation for non-analytic functions. There are several kinds of series expansions, such as:",
                    "score": 0.9123281836509705
                },
                {
                    "id": 680910,
                    "contents": "Power series\nThe power series expansion of the inverse function of an analytic function can be determined using the Lagrange inversion theorem. Behavior near the boundary The sum of a power series with a positive radius of convergence is an analytic function at every point in the interior of the disc of convergence. However, different behavior can occur at points on the boundary of that disc. For example:",
                    "score": 0.9122111201286316
                },
                {
                    "id": 10049358,
                    "contents": "History of Grandi's series\nNow, the infinite series 1 − 1 + 1 − 1 + · · · has neither an even nor an odd number of terms, so it produces neither 0 nor 1; by taking the series out to infinity, it becomes something between those two options. There is no more reason why the series should take one value than the other, so the theory of \"probability\" and the \"law of justice\" dictate that one should take the arithmetic mean of 0 and 1, which is Eli Maor says of this solution, \"Such a brazen, careless reasoning indeed seems incredible to us today…\" Kline portrays Leibniz as more self-conscious: \"Leibniz conceded that his argument was more metaphysical than mathematical, but said that there is more metaphysical truth in mathematics than is generally recognized.\"",
                    "score": 0.9116055965423584
                },
                {
                    "id": 5898954,
                    "contents": "Convergent series\nIf the series converges, then the series is absolutely convergent. The Maclaurin series of the exponential function is absolutely convergent for every complex value of the variable. If the series converges but the series diverges, then the series is conditionally convergent. The Maclaurin series of the logarithm function is conditionally convergent for . The Riemann series theorem states that if a series converges conditionally, it is possible to rearrange the terms of the series in such a way that the series converges to any value, or even diverges. Uniform convergence Let be a sequence of functions. The series is said to converge uniformly to f if the sequence of partial sums defined by converges uniformly to f. There is an analogue of the comparison test for infinite series of functions called the Weierstrass M-test. Cauchy convergence criterion The Cauchy convergence criterion states that a series",
                    "score": 0.9106811285018921
                },
                {
                    "id": 1169606,
                    "contents": "Pi\nThe calculation of was revolutionized by the development of infinite series techniques in the 16th and 17th centuries. An infinite series is the sum of the terms of an infinite sequence. Infinite series allowed mathematicians to compute with much greater precision than Archimedes and others who used geometrical techniques. Although infinite series were exploited for most notably by European mathematicians such as James Gregory and Gottfried Wilhelm Leibniz, the approach was first discovered in India sometime between 1400 and 1500 AD. The first written description of an infinite series that could be used to compute was laid out in Sanskrit verse by Indian astronomer Nilakantha Somayaji in his Tantrasamgraha, around 1500 AD. The series are presented without proof, but proofs are presented in a later Indian work, Yuktibhāṣā, from around 1530 AD. Nilakantha attributes the series to an earlier Indian mathematician, Madhava of Sangamagrama, who lived c. 1350 – c. 1425. Several infinite",
                    "score": 0.9103835225105286
                },
                {
                    "id": 12162084,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nIn mathematics, is the infinite series whose terms are the successive powers of two. As a geometric series, it is characterized by its first term, 1, and its common ratio, 2. As a series of real numbers it diverges to infinity, so in the usual sense it has no sum. In a much broader sense, the series is associated with another value besides ∞, namely −1, which is the limit of the series using the 2-adic metric. Summation The partial sums of are since these diverge to infinity, so does the series.",
                    "score": 0.9101392030715942
                },
                {
                    "id": 1790042,
                    "contents": "Series (mathematics)\nBy definition, the series converges to the limit (or simply sums to ), if the sequence of its partial sums has a limit . In this case, one usually writes A series is said to be convergent if it converges to some limit, or divergent when it does not. The value of this limit, if it exists, is then the value of the series. Convergent series A series is said to converge or to be convergent when the sequence of partial sums has a finite limit. If the limit of is infinite or does not exist, the series is said to diverge. When the limit of partial sums exists, it is called the value (or sum) of the series An easy way that an infinite series can converge is if all the are zero for sufficiently large. Such a series can be identified with a finite sum, so it is only infinite in a trivial sense. Working out the properties of the series that converge, even if infinitely many terms are nonzero, is the essence of the study of series. Consider the example",
                    "score": 0.9099109768867493
                },
                {
                    "id": 1216476,
                    "contents": "Real analysis\nGiven an (infinite) sequence , we can define an associated series as the formal mathematical object sometimes simply written as . The partial sums of a series are the numbers . A series is said to be convergent if the sequence consisting of its partial sums, , is convergent; otherwise it is divergent. The sum of a convergent series is defined as the number The word \"sum\" is used here in a metaphorical sense as a shorthand for taking the limit of a sequence of partial sums and should not be interpreted as simply \"adding\" an infinite number of terms. For instance, in contrast to the behavior of finite sums, rearranging the terms of an infinite series may result in convergence to a different number (see the article on the Riemann rearrangement theorem for further discussion). An example of a convergent series is a geometric series which forms the basis of one of Zeno's famous paradoxes:",
                    "score": 0.90971440076828
                },
                {
                    "id": 2195776,
                    "contents": "List of real analysis topics\nConvergent series – a series whose sequence of partial sums converges Divergent series – a series whose sequence of partial sums diverges Power series – a series of the form Taylor series – a series of the form Maclaurin series – see Taylor seriesBinomial series – the Maclaurin series of the function f given by f(x) = (1 + x) α Telescoping series Alternating series Geometric series Divergent geometric series Harmonic series Fourier series Lambert series",
                    "score": 0.9093841910362244
                },
                {
                    "id": 1790056,
                    "contents": "Series (mathematics)\ncan be made minimal independently of x by choosing a sufficiently large N. Uniform convergence is desirable for a series because many properties of the terms of the series are then retained by the limit. For example, if a series of continuous functions converges uniformly, then the limit function is also continuous. Similarly, if the ƒn are integrable on a closed and bounded interval I and converge uniformly, then the series is also integrable on I and can be integrated term-by-term. Tests for uniform convergence include the Weierstrass' M-test, Abel's uniform convergence test, Dini's test, and the Cauchy criterion.",
                    "score": 0.9091151356697083
                },
                {
                    "id": 12548113,
                    "contents": "1/4 + 1/16 + 1/64 + 1/256 + ⋯\nIn mathematics, the infinite series is an example of one of the first infinite series to be summed in the history of mathematics; it was used by Archimedes circa 250–200 BC. As it is a geometric series with first term and common ratio , its sum is Visual demonstrations The series lends itself to some particularly simple visual demonstrations because a square and a triangle both divide into four similar pieces, each of which contains the area of the original. In the figure on the left, if the large square is taken to have area 1, then the largest black square has area × = . Likewise, the second largest black square has area , and the third largest black square has area . The area taken up by all of the black squares together is therefore , and this is also the area taken up by the gray squares and the white squares. Since these three areas cover the unit square, the figure demonstrates that",
                    "score": 0.9087969064712524
                },
                {
                    "id": 23465699,
                    "contents": "List of sums of reciprocals\nInfinitely many terms Convergent series",
                    "score": 0.9086902141571045
                },
                {
                    "id": 11637875,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nEuler and Borel Euler applied another technique to the series: the Euler transform, one of his own inventions. To compute the Euler transform, one begins with the sequence of positive terms that makes up the alternating series—in this case The first element of this sequence is labeled a0. Next one needs the sequence of forward differences among ; this is just The first element of this sequence is labeled Δa0. The Euler transform also depends on differences of differences, and higher iterations, but all the forward differences among are 0. The Euler transform of is then defined as In modern terminology, one says that is Euler summable to . The Euler summability also implies Borel summability, with the same summation value, as it does in general.",
                    "score": 0.9084164500236511
                },
                {
                    "id": 3863128,
                    "contents": "Divergent series\nClassical summation methods The two classical summation methods for series, ordinary convergence and absolute convergence, define the sum as a limit of certain partial sums. These are included only for completeness; strictly speaking they are not true summation methods for divergent series since, by definition, a series is divergent only if these methods do not work. Most but not all summation methods for divergent series extend these methods to a larger class of sequences. Absolute convergence Absolute convergence defines the sum of a sequence (or set) of numbers to be the limit of the net of all partial sums , if it exists. It does not depend on the order of the elements of the sequence, and a classical theorem says that a sequence is absolutely convergent if and only if the sequence of absolute values is convergent in the standard sense. Sum of a series",
                    "score": 0.9083706736564636
                },
                {
                    "id": 21053061,
                    "contents": "Humbert series\nIn mathematics, Humbert series are a set of seven hypergeometric series Φ1, Φ2, Φ3, Ψ1, Ψ2, Ξ1, Ξ2 of two variables that generalize Kummer's confluent hypergeometric series 1F1 of one variable and the confluent hypergeometric limit function 0F1 of one variable. The first of these double series was introduced by . Definitions The Humbert series Φ1 is defined for |x| < 1 by the double series: where the Pochhammer symbol (q)n represents the rising factorial: where the second equality is true for all complex except . For other values of x the function Φ1 can be defined by analytic continuation. The Humbert series Φ1 can also be written as a one-dimensional Euler-type integral: This representation can be verified by means of Taylor expansion of the integrand, followed by termwise integration. Similarly, the function Φ2 is defined for all x, y by the series: the function Φ3 for all x, y by the series: the function Ψ1 for |x| < 1 by the series:",
                    "score": 0.9082564115524292
                },
                {
                    "id": 1280599,
                    "contents": "Taylor series\nHere is the th finite difference operator with step size . The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function is analytic at , the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series. In general, for any infinite sequence , the following power series identity holds: So in particular, The series on the right is the expectation value of , where is a Poisson-distributed random variable that takes the value with probability . Hence, The law of large numbers implies that the identity holds. List of Maclaurin series of some common functions Several important Maclaurin series expansions follow. All these expansions are valid for complex arguments . Exponential function The exponential function (with base ) has Maclaurin series . It converges for all . Natural logarithm",
                    "score": 0.9078887701034546
                },
                {
                    "id": 11646646,
                    "contents": "1 + 2 + 3 + 4 + ⋯\nThe second key insight is that the alternating series is the formal power series expansion of the function but with x defined as 1. Accordingly, Ramanujan writes Dividing both sides by −3, one gets c = . Generally speaking, it is incorrect to manipulate infinite series as if they were finite sums. For example, if zeroes are inserted into arbitrary positions of a divergent series, it is possible to arrive at results that are not self-consistent, let alone consistent with other methods. In particular, the step is not justified by the additive identity law alone. For an extreme example, appending a single zero to the front of the series can lead to a different result.",
                    "score": 0.9078435897827148
                },
                {
                    "id": 1790055,
                    "contents": "Series (mathematics)\nSeries of functions A series of real- or complex-valued functions converges pointwise on a set E, if the series converges for each x in E as an ordinary series of real or complex numbers. Equivalently, the partial sums converge to ƒ(x) as N → ∞ for each x ∈ E. A stronger notion of convergence of a series of functions is the uniform convergence. A series converges uniformly if it converges pointwise to the function ƒ(x), and the error in approximating the limit by the Nth partial sum, can be made minimal independently of x by choosing a sufficiently large N.",
                    "score": 0.9076903462409973
                },
                {
                    "id": 17337406,
                    "contents": "Appell series\nIn mathematics, Appell series are a set of four hypergeometric series F1, F2, F3, F4 of two variables that were introduced by and that generalize Gauss's hypergeometric series 2F1 of one variable. Appell established the set of partial differential equations of which these functions are solutions, and found various reduction formulas and expressions of these series in terms of hypergeometric series of one variable. Definitions The Appell series F1 is defined for |x| < 1, |y| < 1 by the double series where is the Pochhammer symbol. For other values of x and y the function F1 can be defined by analytic continuation. It can be shown that Similarly, the function F2 is defined for |x| + |y| < 1 by the series and it can be shown that Also the function F3 for |x| < 1, |y| < 1 can be defined by the series and the function F4 for |x|½ + |y|½ < 1 by the series",
                    "score": 0.9076500535011292
                },
                {
                    "id": 680906,
                    "contents": "Power series\nFor , there is no general statement on the convergence of the series. However, Abel's theorem states that if the series is convergent for some value such that , then the sum of the series for is the limit of the sum of the series for where is a real variable less than that tends to . Operations on power series Addition and subtraction When two functions f and g are decomposed into power series around the same center c, the power series of the sum or difference of the functions can be obtained by termwise addition and subtraction. That is, if and then It is not true that if two power series and have the same radius of convergence, then also has this radius of convergence. If and , then both series have the same radius of convergence of 1, but the series has a radius of convergence of 3. Multiplication and division With the same definitions for and , the power series of the product and quotient of the functions can be obtained as follows:",
                    "score": 0.9073991775512695
                },
                {
                    "id": 1790039,
                    "contents": "Series (mathematics)\nIn modern terminology, any (ordered) infinite sequence of terms (that is, numbers, functions, or anything that can be added) defines a series, which is the operation of adding the one after the other. To emphasize that there are an infinite number of terms, a series may be called an infinite series. Such a series is represented (or denoted) by an expression like or, using the summation sign, The infinite sequence of additions implied by a series cannot be effectively carried on (at least in a finite amount of time). However, if the set to which the terms and their finite sums belong has a notion of limit, it is sometimes possible to assign a value to a series, called the sum of the series. This value is the limit as tends to infinity (if the limit exists) of the finite sums of the first terms of the series, which are called the th partial sums of the series. That is,",
                    "score": 0.9071587324142456
                },
                {
                    "id": 12162085,
                    "contents": "1 + 2 + 4 + 8 + ⋯\nSummation The partial sums of are since these diverge to infinity, so does the series. Therefore, any totally regular summation method gives a sum of infinity, including the Cesàro sum and Abel sum. On the other hand, there is at least one generally useful method that sums to the finite value of −1. The associated power series has a radius of convergence around 0 of only so it does not converge at Nonetheless, the so-defined function has a unique analytic continuation to the complex plane with the point deleted, and it is given by the same rule Since the original series is said to be summable (E) to −1, and −1 is the (E) sum of the series. (The notation is due to G. H. Hardy in reference to Leonhard Euler's approach to divergent series). An almost identical approach (the one taken by Euler himself) is to consider the power series whose coefficients are all 1, that is, and plugging in These two series are related by the substitution",
                    "score": 0.9070538282394409
                },
                {
                    "id": 1790071,
                    "contents": "Series (mathematics)\nGeneralizations Asymptotic series Asymptotic series, otherwise asymptotic expansions, are infinite series whose partial sums become good approximations in the limit of some point of the domain. In general they do not converge, but they are useful as sequences of approximations, each of which provides a value close to the desired answer for a finite number of terms. The difference is that an asymptotic series cannot be made to produce an answer as exact as desired, the way that convergent series can. In fact, after a certain number of terms, a typical asymptotic series reaches its best approximation; if more terms are included, most such series will produce worse answers. Divergent series",
                    "score": 0.9068697094917297
                },
                {
                    "id": 1790058,
                    "contents": "Series (mathematics)\nis the Taylor series of at the origin and converges to it for every x. Unless it converges only at x=c, such a series converges on a certain open disc of convergence centered at the point c in the complex plane, and may also converge at some of the points of the boundary of the disc. The radius of this disc is known as the radius of convergence, and can in principle be determined from the asymptotics of the coefficients an. The convergence is uniform on closed and bounded (that is, compact) subsets of the interior of the disc of convergence: to wit, it is uniformly convergent on compact sets. Historically, mathematicians such as Leonhard Euler operated liberally with infinite series, even if they were not convergent. When calculus was put on a sound and correct foundation in the nineteenth century, rigorous proofs of the convergence of series were always required. Formal power series",
                    "score": 0.9068129658699036
                },
                {
                    "id": 11637866,
                    "contents": "1 − 2 + 3 − 4 + ⋯\nIn mathematics, 1 − 2 + 3 − 4 + ··· is an infinite series whose terms are the successive positive integers, given alternating signs. Using sigma summation notation the sum of the first m terms of the series can be expressed as The infinite series diverges, meaning that its sequence of partial sums, , does not tend towards any finite limit. Nonetheless, in the mid-18th century, Leonhard Euler wrote what he admitted to be a paradoxical equation: A rigorous explanation of this equation would not arrive until much later. Starting in 1890, Ernesto Cesàro, Émile Borel and others investigated well-defined methods to assign generalized sums to divergent series—including new interpretations of Euler's attempts. Many of these summability methods easily assign to a \"value\" of . Cesàro summation is one of the few methods that do not sum , so the series is an example where a slightly stronger method, such as Abel summation, is required.",
                    "score": 0.9063588380813599
                },
                {
                    "id": 1744069,
                    "contents": "Geometric series\nThe convergence of the geometric series depends on the value of the common ratio r: If |r| < 1, the terms of the series approach zero in the limit (becoming smaller and smaller in magnitude), and the series converges to the sum a / (1 - r). If |r| = 1, the series does not converge. When r = 1, all of the terms of the series are the same and the series is infinite. When r = −1, the terms take two values alternately (for example, 2, −2, 2, −2, 2,... ). The sum of the terms oscillates between two values (for example, 2, 0, 2, 0, 2,... ). This is a different type of divergence. See for example Grandi's series: 1 − 1 + 1 − 1 + ···. If |r| > 1, the terms of the series become larger and larger in magnitude. The sum of the terms also gets larger and larger, and the series does not converge to a sum. (The series diverges.)",
                    "score": 0.9062271118164062
                },
                {
                    "id": 1790061,
                    "contents": "Series (mathematics)\nLaurent series Laurent series generalize power series by admitting terms into the series with negative as well as positive exponents. A Laurent series is thus any series of the form If such a series converges, then in general it does so in an annulus rather than a disc, and possibly some boundary points. The series converges uniformly on compact subsets of the interior of the annulus of convergence. Dirichlet series A Dirichlet series is one of the form where s is a complex number. For example, if all an are equal to 1, then the Dirichlet series is the Riemann zeta function",
                    "score": 0.9061805605888367
                },
                {
                    "id": 1790040,
                    "contents": "Series (mathematics)\nWhen this limit exists, one says that the series is convergent or summable, or that the sequence is summable. In this case, the limit is called the sum of the series. Otherwise, the series is said to be divergent. The notation denotes both the series—that is the implicit process of adding the terms one after the other indefinitely—and, if the series is convergent, the sum of the series—the result of the process. This is a generalization of the similar convention of denoting by both the addition—the process of adding—and its result—the sum of and . Generally, the terms of a series come from a ring, often the field of the real numbers or the field of the complex numbers. In this case, the set of all series is itself a ring (and even an associative algebra), in which the addition consists of adding the series term by term, and the multiplication is the Cauchy product. Basic properties",
                    "score": 0.905998706817627
                },
                {
                    "id": 1790045,
                    "contents": "Series (mathematics)\nA geometric series is one where each successive term is produced by multiplying the previous term by a constant number (called the common ratio in this context). For example: In general, the geometric series converges if and only if , in which case it converges to . The harmonic series is the series The harmonic series is divergent. An alternating series is a series where terms alternate signs. Examples: (alternating harmonic series) and A telescoping series converges if the sequence bn converges to a limit L—as n goes to infinity. The value of the series is then b1 − L. An arithmetico-geometric series is a generalization of the geometric series, which has coefficients of the common ratio equal to the terms in an arithmetic sequence. Example: The p-series converges if p > 1 and diverges for p ≤ 1, which can be shown with the integral criterion described below in convergence tests. As a function of p, the sum of this series is Riemann's zeta function.",
                    "score": 0.9056720733642578
                },
                {
                    "id": 19430505,
                    "contents": "Series multisection\nIn mathematics, a multisection of a power series is a new power series composed of equally spaced terms extracted unaltered from the original series. Formally, if one is given a power series then its multisection is a power series of the form where p, q are integers, with 0 ≤ p < q. Multisection of analytic functions A multisection of the series of an analytic function has a closed-form expression in terms of the function : where is a primitive q-th root of unity. This solution was first discovered by Thomas Simpson. This expression is especially useful in that it can convert an infinite sum into a finite sum. It is used, for example, in a key step of a standard proof of Gauss's digamma theorem, which gives a closed-form solution to the digamma function evaluated at rational values p/q. Examples Bisection In general, the bisections of a series are the even and odd parts of the series. Geometric series Consider the geometric series",
                    "score": 0.9056352972984314
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_20",
        "question": "Calculate the percentage difference between $\\ln (1+x)$ and $x$ for $x=0.0050$",
        "golden_answers": [
            " 0.249"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 16681544,
                    "contents": "Three Dollars (novel)\nPlot summary",
                    "score": 0.9166204333305359
                },
                {
                    "id": 5419975,
                    "contents": "Ordinary least squares\nIn this table: The Value column gives the least squares estimates of parameters βj The Std error column shows standard errors of each coefficient estimate: The t-statistic and p-value columns are testing whether any of the coefficients might be equal to zero. The t-statistic is calculated simply as . If the errors ε follow a normal distribution, t follows a Student-t distribution. Under weaker conditions, t is asymptotically normal. Large values of t indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero. The second column, p-value, expresses the results of the hypothesis test as a significance level. Conventionally, p-values smaller than 0.05 are taken as evidence that the population coefficient is nonzero.",
                    "score": 0.9090077877044678
                },
                {
                    "id": 16863774,
                    "contents": "$100,000 infield\nEddie Collins",
                    "score": 0.9081029295921326
                },
                {
                    "id": 5419976,
                    "contents": "Ordinary least squares\nR-squared is the coefficient of determination indicating goodness-of-fit of the regression. This statistic will be equal to one if fit is perfect, and to zero when regressors X have no explanatory power whatsoever. This is a biased estimate of the population R-squared, and will never decrease if additional regressors are added, even if they are irrelevant. Adjusted R-squared is a slightly modified version of , designed to penalize for the excess number of regressors which do not add to the explanatory power of the regression. This statistic is always smaller than , can decrease as new regressors are added, and even be negative for poorly fitting models: Log-likelihood is calculated under the assumption that errors follow normal distribution. Even though the assumption is not very reasonable, this statistic may still find its use in conducting LR tests.",
                    "score": 0.9076149463653564
                },
                {
                    "id": 7753411,
                    "contents": "100 mexicanos dijeron\nIf one or both family members accumulate a total of 200 points or more, the family wins Dinero Rápido and MX$100,000. If the family members give the top answer for each question, they win a MX$25,000 bonus, regardless of the outcome. If only one family member named all five top answers, that bonus would double to MX$50,000.",
                    "score": 0.9063352346420288
                },
                {
                    "id": 24973000,
                    "contents": "The 4% Solution\nIt was also presented by Amity Shlaes and Brendan Miniter at the Harvard Club of New York. Moreover, the Philadelphia Media Network published an excerpt by Brendan Miniter. It was also promoted by George W. Bush's brother, Jeb Bush, who served as the Governor of Florida from 1999 to 2007. He suggested the book should be used by then presidential candidate Mitt Romney to offer a bold vision for leadership and win the election, instead of simply criticizing President Barack Obama's policies. Content The book contains essays by academics and businesspeople, five of which are Nobel Prize-winning economists: Robert Lucas, Gary Becker, Edward Prescott, Vernon Smith and Myron Scholes.",
                    "score": 0.9053115248680115
                },
                {
                    "id": 447182,
                    "contents": "Mean squared error\nIn regression analysis, \"mean squared error\", often referred to as mean squared prediction error or \"out-of-sample mean squared error\", can also refer to the mean value of the squared deviations of the predictions from the true values, over an out-of-sample test space, generated by a model estimated over a particular sample space. This also is a known, computed quantity, and it varies by sample and by out-of-sample test space. Examples Mean Suppose we have a random sample of size from a population, . Suppose the sample units were chosen with replacement. That is, the units are selected one at a time, and previously selected units are still eligible for selection for all draws. The usual estimator for the is the sample average which has an expected value equal to the true mean (so it is unbiased) and a mean squared error of where is the population variance.",
                    "score": 0.9042673110961914
                },
                {
                    "id": 6967794,
                    "contents": "Simple linear regression\nSubstituting in place of gives the regression through : where Cov and Var refer to the covariance and variance of the sample data (uncorrected for bias). The last form above demonstrates how moving the line away from the center of mass of the data points affects the slope. Numerical properties Model-based properties Description of the statistical properties of estimators from the simple linear regression estimates requires the use of a statistical model. The following is based on assuming the validity of a model under which the estimates are optimal. It is also possible to evaluate the properties under other assumptions, such as inhomogeneity, but this is discussed elsewhere. Unbiasedness The estimators and are unbiased.",
                    "score": 0.9037947654724121
                },
                {
                    "id": 1747596,
                    "contents": "Gini coefficient\nand L(x) is the Lorenz function: then the Lorenz curve L(F) may then be represented as a function parametric in L(x) and F(x) and the value of B can be found by integration: The Gini coefficient can also be calculated directly from the cumulative distribution function of the distribution F(y). Defining μ as the mean of the distribution, and specifying that F(y) is zero for all negative values, the Gini coefficient is given by: The latter result comes from integration by parts. (Note that this formula can be applied when there are negative values if the integration is taken from minus infinity to plus infinity.) The Gini coefficient may be expressed in terms of the quantile function Q(F) (inverse of the cumulative distribution function: Q(F(x)) = x)",
                    "score": 0.9037020802497864
                },
                {
                    "id": 2062263,
                    "contents": "Warhammer 40,000\nFirst edition (Warhammer 40,000: Rogue Trader) (1987)",
                    "score": 0.9034462571144104
                },
                {
                    "id": 8887770,
                    "contents": "Log-concave\nLog-concave may refer to: Logarithmically concave function Logarithmically concave measure Logarithmically concave sequence",
                    "score": 0.9033054113388062
                },
                {
                    "id": 3745146,
                    "contents": "Regression analysis\nOnce researchers determine their preferred statistical model, different forms of regression analysis provide tools to estimate the parameters . For example, least squares (including its most common variant, ordinary least squares) finds the value of that minimizes the sum of squared errors . A given regression method will ultimately provide an estimate of , usually denoted to distinguish the estimate from the true (unknown) parameter value that generated the data. Using this estimate, the researcher can then use the fitted value for prediction or to assess the accuracy of the model in explaining the data. Whether the researcher is intrinsically interested in the estimate or the predicted value will depend on context and their goals. As described in ordinary least squares, least squares is widely used because the estimated function approximates the conditional expectation . However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when",
                    "score": 0.9026361107826233
                },
                {
                    "id": 96396,
                    "contents": "Logistic regression\nAlso, as an analog to the error of the linear regression case, we may define the deviance of a logistic regression fit as: which will always be positive or zero. The reason for this choice is that not only is the deviance a good measure of the goodness of fit, it is also approximately chi-squared distributed, with the approximation improving as the number of data points (K) increases, becoming exactly chi-square distributed in the limit of an infinite number of data points. As in the case of linear regression, we may use this fact to estimate the probability that a random set of data points will give a better fit than the fit obtained by the proposed model, and so have an estimate how significantly the model is improved by including the xk data points in the proposed model.",
                    "score": 0.9023236632347107
                },
                {
                    "id": 96398,
                    "contents": "Logistic regression\nGoodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods including the following can be used instead. Deviance and likelihood ratio tests In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations – variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations. Deviance is analogous to the sum of squares calculations in linear regression and is a measure of the lack of fit to the data in a logistic regression model. When a \"saturated\" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model. This computation gives the likelihood-ratio test:",
                    "score": 0.9022852182388306
                },
                {
                    "id": 5227566,
                    "contents": "Probit\nIn probability theory and statistics, the probit function is the quantile function associated with the standard normal distribution. It has applications in data analysis and machine learning, in particular exploratory statistical graphics and specialized regression modeling of binary response variables. Mathematically, the probit is the inverse of the cumulative distribution function of the standard normal distribution, which is denoted as , so the probit is defined as . Largely because of the central limit theorem, the standard normal distribution plays a fundamental role in probability theory and statistics. If we consider the familiar fact that the standard normal distribution places 95% of probability between −1.96 and 1.96, and is symmetric around zero, it follows that The probit function gives the 'inverse' computation, generating a value of a standard normal random variable, associated with specified cumulative probability. Continuing the example, . In general, and",
                    "score": 0.9019174575805664
                },
                {
                    "id": 1938400,
                    "contents": "Herbert Spencer\nIn recent years, much more positive estimates have appeared, as well as a still highly negative estimate.",
                    "score": 0.9015597105026245
                },
                {
                    "id": 23431993,
                    "contents": "Linear regression\nBayesian linear regression applies the framework of Bayesian statistics to linear regression. (See also Bayesian multivariate linear regression.) In particular, the regression coefficients β are assumed to be random variables with a specified prior distribution. The prior distribution can bias the solutions for the regression coefficients, in a way similar to (but more general than) ridge regression or lasso regression. In addition, the Bayesian estimation process produces not a single point estimate for the \"best\" values of the regression coefficients but an entire posterior distribution, completely describing the uncertainty surrounding the quantity. This can be used to estimate the \"best\" coefficients using the mean, mode, median, any quantile (see quantile regression), or any other function of the posterior distribution.",
                    "score": 0.9014778733253479
                },
                {
                    "id": 22649702,
                    "contents": "Vector generalized linear model\nSee also generalized linear models R (software) Regression analysis Statistical model Natural exponential family References Further reading Actuarial science Regression models",
                    "score": 0.9009366035461426
                },
                {
                    "id": 7334173,
                    "contents": "Report on the Barnhouse Effect\nPlot summary",
                    "score": 0.9005380868911743
                },
                {
                    "id": 5327247,
                    "contents": "Score test\nSee also Fisher information Uniformly most powerful test Score (statistics) Sup-LM test References Further reading Statistical tests",
                    "score": 0.9005354642868042
                },
                {
                    "id": 3043570,
                    "contents": "Generalized linear model\nIn statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation of the model parameters. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian approaches and least squares fits to variance stabilized responses, have been developed.",
                    "score": 0.9003079533576965
                },
                {
                    "id": 1194505,
                    "contents": "Quantile\nClosely related is the subject of least absolute deviations, a method of regression that is more robust to outliers than is least squares, in which the sum of the absolute value of the observed errors is used in place of the squared error. The connection is that the mean is the single estimate of a distribution that minimizes expected squared error while the median minimizes expected absolute error. Least absolute deviations shares the ability to be relatively insensitive to large deviations in outlying observations, although even better methods of robust regression are available.",
                    "score": 0.9002382159233093
                },
                {
                    "id": 25419474,
                    "contents": "They Both Die at the End\nPlot summary",
                    "score": 0.9001071453094482
                },
                {
                    "id": 26864274,
                    "contents": "Edward Lopez\nEdward Lopez may refer to: Edward J. Lopez, American economist R. Edward Lopez (1953–2005), American newsman and morning radio personality",
                    "score": 0.9000810980796814
                },
                {
                    "id": 23432006,
                    "contents": "Linear regression\nFurther reading Mathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression. External links Least-Squares Regression, PhET Interactive simulations, University of Colorado at Boulder DIY Linear Fit Single-equation methods (econometrics) Estimation theory Parametric statistics",
                    "score": 0.900031566619873
                },
                {
                    "id": 3745166,
                    "contents": "Regression analysis\nSoftware All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized. Different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging. See also",
                    "score": 0.8997631072998047
                },
                {
                    "id": 6967802,
                    "contents": "Simple linear regression\nThus a seemingly small variation in the data has a real effect. See also Design matrix#Simple linear regression Line fitting Linear trend estimation Linear segmented regression Proofs involving ordinary least squares—derivation of all formulas used in this article in general multidimensional case References External links Wolfram MathWorld's explanation of Least Squares Fitting, and how to calculate it Mathematics of simple regression (Robert Nau, Duke University) zh-yue:簡單線性迴歸分析 Regression analysis Parametric statistics",
                    "score": 0.8996918201446533
                },
                {
                    "id": 23431951,
                    "contents": "Linear regression\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.",
                    "score": 0.8995961546897888
                },
                {
                    "id": 9217491,
                    "contents": "Nonparametric regression\nReferences Further reading External links HyperNiche, software for nonparametric multiplicative regression. Scale-adaptive nonparametric regression (with Matlab software).",
                    "score": 0.8995876312255859
                },
                {
                    "id": 12153818,
                    "contents": "The Minx\nPlot",
                    "score": 0.8992995023727417
                },
                {
                    "id": 5669271,
                    "contents": "Coefficient of determination\nR2 in logistic regression In the case of logistic regression, usually fit by maximum likelihood, there are several choices of pseudo-R2. One is the generalized R2 originally proposed by Cox & Snell, and independently by Magee: where is the likelihood of the model with only the intercept, is the likelihood of the estimated model (i.e., the model with a given set of parameter estimates) and n is the sample size. It is easily rewritten to: where D is the test statistic of the likelihood ratio test.",
                    "score": 0.8992183208465576
                },
                {
                    "id": 11121157,
                    "contents": "50%\n50% may refer to: One half, an irreducible fraction \"50%\", a 2006 song by Grandaddy from Just Like the Fambly Cat See also \"50% & 50%\", a 1993 song by Hide Middle 50% or interquartile range, a measure of statistical dispersion",
                    "score": 0.8991555571556091
                },
                {
                    "id": 5831273,
                    "contents": "Lenny Lane\nEarly years (1997–1999)",
                    "score": 0.898890495300293
                },
                {
                    "id": 3699544,
                    "contents": "Total least squares\nSee also Deming regression, a special case with two predictors and independent errors. Errors-in-variables model Gauss-Helmert model Linear regression Least squares Notes References",
                    "score": 0.8988597393035889
                },
                {
                    "id": 4458933,
                    "contents": "Nonlinear regression\nwhere . It follows from this that the least squares estimators are given by compare generalized least squares with covariance matrix proportional to the unit matrix. The nonlinear regression statistics are computed and used as in linear regression statistics, but using J in place of X in the formulas. The linear approximation introduces bias into the statistics. Therefore, more caution than usual is required in interpreting statistics derived from a nonlinear model. Ordinary and weighted least squares",
                    "score": 0.8988584280014038
                },
                {
                    "id": 870803,
                    "contents": "Least squares\nLimitations This regression formulation considers only observational errors in the dependent variable (but the alternative total least squares regression can account for errors in both variables). There are two rather different contexts with different implications:",
                    "score": 0.898847758769989
                },
                {
                    "id": 1544423,
                    "contents": "Z-test\ncan be used as a Z-test statistic. When using a Z-test for maximum likelihood estimates, it is important to be aware that the normal approximation may be poor if the sample size is not sufficiently large. Although there is no simple, universal rule stating how large the sample size must be to use a Z-test, simulation can give a good idea as to whether a Z-test is appropriate in a given situation. Z-tests are employed whenever it can be argued that a test statistic follows a normal distribution under the null hypothesis of interest. Many non-parametric test statistics, such as U statistics, are approximately normal for large enough sample sizes, and hence are often performed as Z-tests. See also Normal distribution Standard normal table Standard score Student's t-test References",
                    "score": 0.8987278342247009
                },
                {
                    "id": 96379,
                    "contents": "Logistic regression\nand the best fit is obtained for those choices of β0 and β1 where L is maximized. The maximum of L will also be the maximum of the log-likelihood &ell;, defined as the logarithm of L: Since &ell; is nonlinear in β0 and β1, determining their optimum values will require numerical methods. Note that one method of maximizing &ell; is to require the derivatives of &ell; with respect to β0 and β1 to be zero: and the maximization procedure can be accomplished by solving the above two equations for β0 and β1, which again, will generally require the use of numerical methods. The values of β0 and β1 which maximize &ell; and L using the above data are found to be: which yields a value for μ of: The logistic regression analysis gives the following output.",
                    "score": 0.8986831307411194
                },
                {
                    "id": 15526303,
                    "contents": "Outline of regression analysis\nVisualization Scatterplot Linear regression based on least squares General linear model Ordinary least squares Generalized least squares Simple linear regression Trend estimation Ridge regression Polynomial regression Segmented regression Nonlinear regression Generalized linear models Generalized linear models Logistic regression Multinomial logit Ordered logit Probit model Multinomial probit Ordered probit Poisson regression Maximum likelihood Cochrane–Orcutt estimation Computation Numerical methods for linear least squares Inference for regression models F-test t-test Lack-of-fit sum of squares Confidence band Coefficient of determination Multiple correlation Scheffé's method Challenges to regression modeling Autocorrelation Cointegration Multicollinearity Homoscedasticity and heteroscedasticity Lack of fit Non-normality of errors Outliers",
                    "score": 0.8986095190048218
                },
                {
                    "id": 22572707,
                    "contents": "Against All Odds (TV series)\nReferences 1990s American reality television series",
                    "score": 0.8985454440116882
                },
                {
                    "id": 7187011,
                    "contents": "Robust regression\nAlthough uptake of robust methods has been slow, modern mainstream statistics text books often include discussion of these methods (for example, the books by Seber and Lee, and by Faraway; for a good general description of how the various robust regression methods developed from one another see Andersen's book). Also, modern statistical software packages such as R, Statsmodels, Stata and S-PLUS include considerable functionality for robust estimation (see, for example, the books by Venables and Ripley, and by Maronna et al.). Methods for robust regression Least squares alternatives The simplest methods of estimating parameters in a regression model that are less sensitive to outliers than the least squares estimates, is to use least absolute deviations. Even then, gross outliers can still have a considerable impact on the model, motivating research into even more robust approaches.",
                    "score": 0.898492693901062
                },
                {
                    "id": 26872122,
                    "contents": "UFC 202\nRaquel Pennington: $46,000 (includes $23,000 win bonus) def. Elizabeth Phillips: $12,000 Artem Lobov: $26,000 (includes $13,000 win bonus) def. Chris Avila: $10,000 Cortney Casey: $40,000 (includes $20,000 win bonus) def. Randa Markos: $14,000 Lorenz Larkin: $78,000 (includes $39,000 win bonus) def. Neil Magny: $47,000 Colby Covington: $42,000 (includes $21,000 win bonus) def. Max Griffin: $10,000 Marvin Vettori: $20,000 (includes $10,000 win bonus) def. Alberto Uda: $10,000",
                    "score": 0.8984092473983765
                },
                {
                    "id": 96402,
                    "contents": "Logistic regression\nFour of the most commonly used indices and one less commonly used one are examined on this page: Likelihood ratio ² Cox and Snell ² Nagelkerke ² McFadden ² Tjur ² ²L is given by Cohen: This is the most analogous index to the squared multiple correlations in linear regression. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. One limitation of the likelihood ratio ² is that it is not monotonically related to the odds ratio, meaning that it does not necessarily increase as the odds ratio increases and does not necessarily decrease as the odds ratio decreases. ²CS is an alternative index of goodness of fit related to the ² value from linear regression. It is given by:",
                    "score": 0.8982976078987122
                },
                {
                    "id": 23431952,
                    "contents": "Linear regression\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories: If the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.",
                    "score": 0.8980478644371033
                },
                {
                    "id": 5669255,
                    "contents": "Coefficient of determination\nThis leads to the alternative approach of looking at the adjusted R2. The explanation of this statistic is almost the same as R2 but it penalizes the statistic as extra variables are included in the model. For cases other than fitting by ordinary least squares, the R2 statistic can be calculated as above and may still be a useful measure. If fitting is by weighted least squares or generalized least squares, alternative versions of R2 can be calculated appropriate to those statistical frameworks, while the \"raw\" R2 may still be useful if it is more easily interpreted. Values for R2 can be calculated for any type of predictive model, which need not have a statistical basis. In a multiple linear model Consider a linear model with more than a single explanatory variable, of the form",
                    "score": 0.8980242013931274
                },
                {
                    "id": 23431958,
                    "contents": "Linear regression\nThe values xij may be viewed as either observed values of random variables Xj or as fixed values chosen prior to observing the dependent variable. Both interpretations may be appropriate in different cases, and they generally lead to the same estimation procedures; however different approaches to asymptotic analysis are used in these two situations. is a -dimensional parameter vector, where is the intercept term (if one is included in the model—otherwise is p-dimensional). Its elements are known as effects or regression coefficients (although the latter term is sometimes reserved for the estimated effects). In simple linear regression, p=1, and the coefficient is known as regression slope. Statistical estimation and inference in linear regression focuses on β. The elements of this parameter vector are interpreted as the partial derivatives of the dependent variable with respect to the various independent variables.",
                    "score": 0.8980149626731873
                },
                {
                    "id": 23432005,
                    "contents": "Linear regression\nReferences Citations Sources Cohen, J., Cohen P., West, S.G., & Aiken, L.S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences. (2nd ed.) Hillsdale, NJ: Lawrence Erlbaum Associates Charles Darwin. The Variation of Animals and Plants under Domestication. (1868) (Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term \"reversion\".) Francis Galton. \"Regression Towards Mediocrity in Hereditary Stature,\" Journal of the Anthropological Institute, 15:246-263 (1886). (Facsimile at: ) Robert S. Pindyck and Daniel L. Rubinfeld (1998, 4h ed.). Econometric Models and Economic Forecasts'', ch. 1 (Intro, incl. appendices on Σ operators & derivation of parameter est.) & Appendix 4.3 (mult. regression in matrix form). Further reading Mathieu Rouaud, 2013: Probability, Statistics and Estimation Chapter 2: Linear Regression, Linear Regression with Error Bars and Nonlinear Regression. External links",
                    "score": 0.898006796836853
                },
                {
                    "id": 10088736,
                    "contents": "Mean absolute difference\n. But the expected value of any estimator R(S) of RMD(X(p)) will be of the form: where the r i are constants. So E(R(S)) can never equal RMD(X(p)) for all p between 0 and 1. Examples † is the regularized incomplete Beta function See also Mean absolute error Mean deviation Estimator Coefficient of variation L-moment References Statistical deviation and dispersion Summary statistics Theory of probability distributions Scale statistics Distance",
                    "score": 0.8979918360710144
                },
                {
                    "id": 8127792,
                    "contents": "Generalized additive model\nbetween saturated log likelihood and the model log likelihood) for the model. Minimizing the deviance by the usual iteratively re-weighted least squares would result in overfit, so we seek to minimize",
                    "score": 0.8979893922805786
                },
                {
                    "id": 3745147,
                    "contents": "Regression analysis\nis widely used because the estimated function approximates the conditional expectation . However, alternative variants (e.g., least absolute deviations or quantile regression) are useful when researchers want to model other functions .",
                    "score": 0.8976961374282837
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_21",
        "question": "Calculate the reduced mass of a nitrogen molecule in which both nitrogen atoms have an atomic mass of 14.00.",
        "golden_answers": [
            " 7.00"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 8226869,
                    "contents": "Nitrogen rule\nAn even nominal mass indicates that a net even number of covalent bonds have been broken or formed and an even number of nitrogen atoms are present, or that a net odd number of covalent bonds have been broken or formed and an odd number of nitrogen atoms are present. An odd nominal mass indicates that a net even number of covalent bonds have been broken or formed and an odd number of nitrogen atoms are present, or that a net odd number of covalent bonds have been broken or formed and an even number of nitrogen atoms are present. Inorganic molecules do not necessarily follow the rule. For example, the nitrogen oxides NO and NO2 have an odd number of nitrogens but even masses of 30 and 46, respectively. See also Mass (mass spectrometry) References Mass spectrometry Parity (mathematics) Rule Nuclear chemistry",
                    "score": 0.9001420140266418
                },
                {
                    "id": 8226867,
                    "contents": "Nitrogen rule\nFormulation of the rule This rule is derived from the fact that, perhaps coincidentally, for the most common chemical elements in neutral organic compounds (hydrogen, carbon, nitrogen, oxygen, silicon, phosphorus, sulfur, and the halogens), elements with even numbered nominal masses form even numbers of covalent bonds, while elements with odd numbered nominal masses form odd numbers of covalent bonds, with the exception of nitrogen, which has a nominal (or integer) mass of 14, but has a valency of 3. The nitrogen rule is only true for neutral structures in which all of the atoms in the molecule have a number of covalent bonds equal to their standard valency (counting each sigma bond and pi bond as a separate covalent bond for the purposes of the calculation). Therefore, the rule is typically only applied to the molecular ion signal in the mass spectrum.",
                    "score": 0.8992578983306885
                },
                {
                    "id": 1128586,
                    "contents": "Nitrogen\nGiven the great reactivity of atomic nitrogen, elemental nitrogen usually occurs as molecular N2, dinitrogen. This molecule is a colourless, odourless, and tasteless diamagnetic gas at standard conditions: it melts at −210 °C and boils at −196 °C. Dinitrogen is mostly unreactive at room temperature, but it will nevertheless react with lithium metal and some transition metal complexes. This is due to its bonding, which is unique among the diatomic elements at standard conditions in that it has an N≡N triple bond. Triple bonds have short bond lengths (in this case, 109.76 pm) and high dissociation energies (in this case, 945.41 kJ/mol), and are thus very strong, explaining dinitrogen's chemical inertness.",
                    "score": 0.893373966217041
                },
                {
                    "id": 1128562,
                    "contents": "Nitrogen\nNitrogen is the chemical element with the symbol N and atomic number 7. It was first discovered and isolated by Scottish physician Daniel Rutherford in 1772. Although Carl Wilhelm Scheele and Henry Cavendish had independently done so at about the same time, Rutherford is generally accorded the credit because his work was published first. The name nitrogène was suggested by French chemist Jean-Antoine-Claude Chaptal in 1790 when it was found that nitrogen was present in nitric acid and nitrates. Antoine Lavoisier suggested instead the name azote, from the \"no life\", as it is an asphyxiant gas; this name is used in several languages, including French, Italian, Russian, Romanian, Portuguese and Turkish, and appears in the English names of some nitrogen compounds such as hydrazine, azides and azo compounds.",
                    "score": 0.8901810050010681
                },
                {
                    "id": 8226866,
                    "contents": "Nitrogen rule\nThe nitrogen rule states that organic compounds containing exclusively hydrogen, carbon, nitrogen, oxygen, silicon, phosphorus, sulfur, and the halogens either have (1) an odd nominal mass that indicates an odd number of nitrogen atoms are present or (2) an even nominal mass that indicates an even number of nitrogen atoms in the molecular formula of the molecular ion. The nitrogen rule is not a rule as much as a general principle which may prove useful when attempting to solve organic mass spectrometry structures. Formulation of the rule",
                    "score": 0.8898129463195801
                },
                {
                    "id": 20873122,
                    "contents": "Atomicity (chemistry)\nAtomicity is defined as the total number of atoms present in a molecule. For example, each molecule of oxygen (O2) is composed of two oxygen atoms. So atomicity of oxygen is 2. In older contexts, atomicity is sometimes used in the same sense as valency. Some authors also use the term to refer to the maximum number of valencies observed for an element. On the basis of atomicity, molecules can be classified as: Monatomic – composed of one atom e.g. He, Ne, Ar, Kr (all noble gases are monatomic) Diatomic – composed of two atoms e.g. H2 , N2 , O2 , F2 , Cl2 (all halogens are usually diatomic) Triatomic – composed of three atoms e.g. O3 Polyatomic – composed of three or more atoms e.g. P4 , S8 All metals and some other elements, such as carbon, do not have a simple structure but consist of a very large and indefinite number of atoms bonded together. Their atomicity cannot be determined and is usually considered as 1.",
                    "score": 0.8878304958343506
                },
                {
                    "id": 1560678,
                    "contents": "Atom\nThe actual mass of an atom at rest is often expressed in daltons (Da), also called the unified atomic mass unit (u). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately . Hydrogen-1 (the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 Da. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the atomic mass unit (for example the mass of a nitrogen-14 is roughly 14 Da), but this number will not be exactly an integer except (by definition) in the case of carbon-12. The heaviest stable atom is lead-208, with a mass of .",
                    "score": 0.8871029615402222
                },
                {
                    "id": 1560642,
                    "contents": "Atom\nAs a final example: nitrous oxide is 63.3% nitrogen and 36.7% oxygen, nitric oxide is 44.05% nitrogen and 55.95% oxygen, and nitrogen dioxide is 29.5% nitrogen and 70.5% oxygen. Adjusting these figures, in nitrous oxide there is 80 g of oxygen for every 140 g of nitrogen, in nitric oxide there is about 160 g of oxygen for every 140 g of nitrogen, and in nitrogen dioxide there is 320 g of oxygen for every 140 g of nitrogen. 80, 160, and 320 form a ratio of 1:2:4. The respective formulas for these oxides are N2O, NO, and NO2. Kinetic theory of gases",
                    "score": 0.887056291103363
                },
                {
                    "id": 1128563,
                    "contents": "Nitrogen\nNitrogen is a nonmetal and the lightest member of group 15 of the periodic table, often called the pnictogens. It is a common element in the universe, estimated at seventh in total abundance in the Milky Way and the Solar System. At standard temperature and pressure, two atoms of the element bind to form N2, a colorless and odorless diatomic gas. N2 forms about 78% of Earth's atmosphere, making it the most abundant uncombined element. Nitrogen occurs in all organisms, primarily in amino acids (and thus proteins), in the nucleic acids (DNA and RNA) and in the energy transfer molecule adenosine triphosphate. The human body contains about 3% nitrogen by mass, the fourth most abundant element in the body after oxygen, carbon, and hydrogen. The nitrogen cycle describes movement of the element from the air, into the biosphere and organic compounds, then back into the atmosphere.",
                    "score": 0.8860254287719727
                },
                {
                    "id": 1131748,
                    "contents": "Nuclear physics\nand having it bounce off. The discovery, with Rutherford's analysis of the data in 1911, led to the Rutherford model of the atom, in which the atom had a very small, very dense nucleus containing most of its mass, and consisting of heavy positively charged particles with embedded electrons in order to balance out the charge (since the neutron was unknown). As an example, in this model (which is not the modern one) nitrogen-14 consisted of a nucleus with 14 protons and 7 electrons (21 total particles) and the nucleus was surrounded by 7 more orbiting electrons.",
                    "score": 0.8853199481964111
                },
                {
                    "id": 11242068,
                    "contents": "Timeline of chemistry\n1912Peter Debye develops the concept of molecular dipole to describe asymmetric charge distribution in some molecules. 1913Niels Bohr introduces concepts of quantum mechanics to atomic structure by proposing what is now known as the Bohr model of the atom, where electrons exist only in strictly defined orbitals. 1913Henry Moseley, working from Van den Broek's earlier idea, introduces concept of atomic number to fix inadequacies of Mendeleev's periodic table, which had been based on atomic weight. 1913Frederick Soddy proposes the concept of isotopes, that elements with the same chemical properties may have differing atomic weights. 1913J. J. Thomson expanding on the work of Wien, shows that charged subatomic particles can be separated by their mass-to-charge ratio, a technique known as mass spectrometry. 1916Gilbert N. Lewis publishes \"The Atom and the Molecule\", the foundation of valence bond theory.",
                    "score": 0.8851522207260132
                },
                {
                    "id": 1668666,
                    "contents": "Diatomic molecule\nHistorical significance Diatomic elements played an important role in the elucidation of the concepts of element, atom, and molecule in the 19th century, because some of the most common elements, such as hydrogen, oxygen, and nitrogen, occur as diatomic molecules. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed water's formula to be HO, giving the atomic weight of oxygen as eight times that of hydrogen, instead of the modern value of about 16. As a consequence, confusion existed regarding atomic weights and molecular formulas for about half a century.",
                    "score": 0.8849508762359619
                },
                {
                    "id": 1560677,
                    "contents": "Atom\nMass The large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called \"nucleons\") in a given atom is called the mass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is \"carbon-12,\" which has 12 nucleons (six protons and six neutrons).",
                    "score": 0.8837717771530151
                },
                {
                    "id": 3271809,
                    "contents": "Mass number\nThe mass number (symbol A, from the German word Atomgewicht [atomic weight]), also called atomic mass number or nucleon number, is the total number of protons and neutrons (together known as nucleons) in an atomic nucleus. It is approximately equal to the atomic (also known as isotopic) mass of the atom expressed in atomic mass units. Since protons and neutrons are both baryons, the mass number A is identical with the baryon number B of the nucleus (and also of the whole atom or ion). The mass number is different for each different isotope of a chemical element. Hence, the difference between the mass number and the atomic number Z gives the number of neutrons (N) in a given nucleus: .",
                    "score": 0.882599949836731
                },
                {
                    "id": 11167210,
                    "contents": "History of molecular theory\nAmedeo Avogadro created the word \"molecule\". His 1811 paper \"Essay on Determining the Relative Masses of the Elementary Molecules of Bodies\", he essentially states, i.e. according to Partington's A Short History of Chemistry, that: Note that this quote is not a literal translation. Avogadro uses the name \"molecule\" for both atoms and molecules. Specifically, he uses the name \"elementary molecule\" when referring to atoms and to complicate the matter also speaks of \"compound molecules\" and \"composite molecules\".",
                    "score": 0.8819191455841064
                },
                {
                    "id": 3015881,
                    "contents": "Valence bond theory\nIn chemistry, valence bond (VB) theory is one of the two basic theories, along with molecular orbital (MO) theory, that were developed to use the methods of quantum mechanics to explain chemical bonding. It focuses on how the atomic orbitals of the dissociated atoms combine to give individual chemical bonds when a molecule is formed. In contrast, molecular orbital theory has orbitals that cover the whole molecule. History Lothar Meyer in his 1864 book, Die modernen Theorien der Chemie, contained an early version of the periodic table containing 28 elements, classified elements into six families by their valence—for the first time, elements had been grouped according to their valence. Works on organizing the elements by atomic weight, until then had been stymied by the widespread use of equivalent weights for the elements, rather than atomic weights.",
                    "score": 0.8816951513290405
                },
                {
                    "id": 1128601,
                    "contents": "Nitrogen\nof nitrogen compared to oxygen and the presence of only one lone pair in NH3 rather than two in H2O. It is a weak base in aqueous solution (pKb 4.74); its conjugate acid is ammonium, . It can also act as an extremely weak acid, losing a proton to produce the amide anion, . It thus undergoes self-dissociation, similar to water, to produce ammonium and amide. Ammonia burns in air or oxygen, though not readily, to produce nitrogen gas; it burns in fluorine with a greenish-yellow flame to give nitrogen trifluoride. Reactions with the other nonmetals are very complex and tend to lead to a mixture of products. Ammonia reacts on heating with metals to give nitrides.",
                    "score": 0.8816707730293274
                },
                {
                    "id": 21806581,
                    "contents": "History of subatomic physics\nBy 1914, experiments by Ernest Rutherford, Henry Moseley, James Franck and Gustav Hertz had largely established the structure of an atom as a dense nucleus of positive charge surrounded by lower-mass electrons.",
                    "score": 0.881585955619812
                },
                {
                    "id": 1555851,
                    "contents": "Atomic number\nIn 1917, Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two \"nuclear electrons\" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of +79, consistent with its atomic number.",
                    "score": 0.8814853429794312
                },
                {
                    "id": 1861861,
                    "contents": "Molecule\nA molecule is an electrically neutral group of two or more atoms held together by chemical bonds. Molecules are distinguished from ions by their lack of electrical charge. In quantum physics, organic chemistry, and biochemistry, the distinction from ions is dropped and molecule is often used when referring to polyatomic ions. In the kinetic theory of gases, the term molecule is often used for any gaseous particle regardless of its composition. This relaxes the requirement that a molecule contains two or more atoms, since the noble gases are individual atoms. A molecule may be homonuclear, that is, it consists of atoms of one chemical element, e.g. two atoms in the oxygen molecule (O2); or it may be heteronuclear, a chemical compound composed of more than one element, e.g. water (two hydrogen atoms and one oxygen atom; H2O). Atoms and complexes connected by non-covalent interactions, such as hydrogen bonds or ionic bonds, are typically not considered single molecules.",
                    "score": 0.8813782930374146
                },
                {
                    "id": 1590158,
                    "contents": "Atomic theory\nSee also Spectroscopy History of molecular theory Timeline of chemical element discoveries Introduction to quantum mechanics Kinetic theory of gases Atomism The Physical Principles of the Quantum Theory Footnotes Bibliography Further reading Bernard Pullman (1998) The Atom in the History of Human Thought, trans. by Axel Reisinger. Oxford Univ. Press. Eric Scerri (2007) The Periodic Table, Its Story and Its Significance, Oxford University Press, New York. Charles Adolphe Wurtz (1881) The Atomic Theory, D. Appleton and Company, New York. Alan J. Rocke (1984) Chemical Atomism in the Nineteenth Century: From Dalton to Cannizzaro, Ohio State University Press, Columbus (open access full text at http://digital.case.edu/islandora/object/ksl%3Ax633gj985). External links Atomism by S. Mark Cohen. Atomic Theory - detailed information on atomic theory with respect to electrons and electricity. Statistical mechanics Chemistry theories Foundational quantum physics Amount of substance",
                    "score": 0.8811801671981812
                },
                {
                    "id": 1880752,
                    "contents": "Nitrogen dioxide\nNitrogen dioxide is a chemical compound with the formula . It is one of several nitrogen oxides. is an intermediate in the industrial synthesis of nitric acid, millions of tons of which are produced each year for use primarily in the production of fertilizers. At higher temperatures it is a reddish-brown gas. It can be fatal if inhaled in large quantity. Nitrogen dioxide is a paramagnetic, bent molecule with C2v point group symmetry. Properties Nitrogen dioxide is a reddish-brown gas above with a pungent, acrid odor, becomes a yellowish-brown liquid below , and converts to the colorless dinitrogen tetroxide () below . The bond length between the nitrogen atom and the oxygen atom is 119.7 pm. This bond length is consistent with a bond order between one and two.",
                    "score": 0.8811587691307068
                },
                {
                    "id": 1193069,
                    "contents": "Polyatomic ion\nA polyatomic ion, also known as a molecular ion, is a covalently bonded set of two or more atoms, or of a metal complex, that can be considered to behave as a single unit and that has a net charge that is not zero. Unlike a molecule, which has a net charge of zero, this chemical species is an ion. (The prefix poly- carries the meaning \"many\" in Greek, but even ions of two atoms are commonly described as polyatomic.) In older literature, a polyatomic ion may instead be referred to as a radical (or less commonly, as a radical group). (In contemporary usage, the term radical refers to various free radicals, which are species that have an unpaired electron and need not be charged.) A simple example of a polyatomic ion is the hydroxide ion, which consists of one oxygen atom and one hydrogen atom, jointly carrying a net charge of −1; its chemical formula is . In contrast, an ammonium ion consists of one nitrogen atom and four hydrogen atoms, with a charge of +1; its chemical formula is .",
                    "score": 0.8810354471206665
                },
                {
                    "id": 1668662,
                    "contents": "Diatomic molecule\nDiatomic molecules are molecules composed of only two atoms, of the same or different chemical elements. The prefix di- is of Greek origin, meaning \"two\". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H2) or oxygen (O2), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear. The bond in a homonuclear diatomic molecule is non-polar. The only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H2), nitrogen (N2), oxygen (O2), fluorine (F2), and chlorine (Cl2).",
                    "score": 0.8808810710906982
                },
                {
                    "id": 1128572,
                    "contents": "Nitrogen\nA nitrogen atom has seven electrons. In the ground state, they are arranged in the electron configuration 1s2s2p2p2p. It therefore has five valence electrons in the 2s and 2p orbitals, three of which (the p-electrons) are unpaired. It has one of the highest electronegativities among the elements (3.04 on the Pauling scale), exceeded only by chlorine (3.16), oxygen (3.44), and fluorine (3.98). (The light noble gases, helium, neon, and argon, would presumably also be more electronegative, and in fact are on the Allen scale.) Following periodic trends, its single-bond covalent radius of 71 pm is smaller than those of boron (84 pm) and carbon (76 pm), while it is larger than those of oxygen (66 pm) and fluorine (57 pm). The nitride anion, N3−, is much larger at 146 pm, similar to that of the oxide (O2−: 140 pm) and fluoride (F−: 133 pm) anions. The first three ionisation energies of nitrogen are 1.402, 2.856, and 4.577 MJ·mol−1, and the sum of the fourth and fifth is . Due to these very",
                    "score": 0.8803216218948364
                },
                {
                    "id": 15711745,
                    "contents": "Atomic mass\nHere are some values of the ratio of atomic mass to mass number: Measurement of atomic masses Direct comparison and measurement of the masses of atoms is achieved with mass spectrometry.",
                    "score": 0.8797636032104492
                },
                {
                    "id": 7487964,
                    "contents": "List of compounds with carbon number 20\nThis is a partial list of molecules that contain 20 carbon atoms. See also Carbon number References C20",
                    "score": 0.8792980909347534
                },
                {
                    "id": 15711733,
                    "contents": "Atomic mass\nThe atomic mass (ma or m) is the mass of an atom. Although the SI unit of mass is the kilogram (symbol: kg), atomic mass is often expressed in the non-SI unit atomic mass unit (amu) or unified mass (u) or dalton (symbol: Da), where 1 amu or 1 u or 1 Da is defined as of the mass of a single carbon-12 atom, at rest. The protons and neutrons of the nucleus account for nearly all of the total mass of atoms, with the electrons and nuclear binding energy making minor contributions. Thus, the numeric value of the atomic mass when expressed in daltons has nearly the same value as the mass number. Conversion between mass in kilograms and mass in daltons can be done using the atomic mass constant . The formula used for conversion is: where is the molar mass constant, is the Avogadro constant, and is the experimentally determined molar mass of carbon-12.",
                    "score": 0.87815260887146
                },
                {
                    "id": 5937335,
                    "contents": "Electronegativities of the elements (data page)\nCRC As quoted from these sources in an online version of: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003; Section 9, Molecular Structure and Spectroscopy; Electronegativity Pauling, L., The Nature of the Chemical Bond, Third Edition, Cornell University Press, Ithaca, New York, 1960. Allen, L.C., J. Am. Chem. Soc., 111, 9003, 1989. LNG As quoted from these sources in: J.A. Dean (ed), Lange's Handbook of Chemistry (15th Edition), McGraw-Hill, 1999; Section 4; Table 4.5, Electronegativities of the Elements. L. Pauling, The Chemical Bond, Cornell University Press, Ithaca, New York, 1967. L. C. Allen, J. Am. Chem. Soc. 111:9003 (1989). A. L. Allred J. Inorg. Nucl. Chem. 17:215 (1961).",
                    "score": 0.8773236870765686
                },
                {
                    "id": 7487961,
                    "contents": "List of compounds with carbon number 23\nThis is a partial list of molecules that contain 23 carbon atoms. See also Carbon number C23",
                    "score": 0.8770366907119751
                },
                {
                    "id": 16681008,
                    "contents": "Timeline of quantum mechanics\n1930 – Dirac hypothesizes the existence of the positron. 1930 – Dirac's textbook The Principles of Quantum Mechanics is published, becoming a standard reference book that is still used today. 1930 – Erich Hückel introduces the Hückel molecular orbital method, which expands on orbital theory to determine the energies of orbitals of pi electrons in conjugated hydrocarbon systems. 1930 – Fritz London explains van der Waals forces as due to the interacting fluctuating dipole moments between molecules 1930 – Pauli suggests in a famous letter that, in addition to electrons and protons, atoms also contain an extremely light neutral particle which he calls the \"neutron.\" He suggests that this \"neutron\" is also emitted during beta decay and has simply not yet been observed. Later it is determined that this particle is actually the almost massless neutrino. 1931 – John Lennard-Jones proposes the Lennard-Jones interatomic potential",
                    "score": 0.8768488168716431
                },
                {
                    "id": 18767739,
                    "contents": "Natural units\nStoney units are rarely used in modern physics for calculations, but they are of historical interest. Atomic units The Hartree atomic unit system uses the following constants to have numeric value 1 in terms of the resulting units: Coulomb's constant, , is generally expressed as when working with this system. These units are designed to simplify atomic and molecular physics and chemistry, especially the hydrogen atom, and are widely used in these fields. The Hartree units were first proposed by Douglas Hartree. The units are designed especially to characterize the behavior of an electron in the ground state of a hydrogen atom. For example, in Hartree atomic units, in the Bohr model of the hydrogen atom an electron in the ground state has orbital radius (the Bohr radius) 0 = 1 , orbital velocity = 1 ⋅, angular momentum = 1 ⋅⋅, ionization energy = ⋅⋅, etc.",
                    "score": 0.8767940402030945
                },
                {
                    "id": 7492447,
                    "contents": "List of compounds with carbon number 19\nThis is a partial list of molecules that contain 19 carbon atoms. See also Carbon number C19",
                    "score": 0.8766911029815674
                },
                {
                    "id": 7492451,
                    "contents": "List of compounds with carbon number 15\nThis is a partial list of molecules that contain 15 carbon atoms. See also Carbon number C15",
                    "score": 0.87665194272995
                },
                {
                    "id": 11167212,
                    "contents": "History of molecular theory\nAvogadro developed this hypothesis in order to reconcile Joseph Louis Gay-Lussac's 1808 law on volumes and combining gases with Dalton's 1803 atomic theory. The greatest difficulty Avogadro had to resolve was the huge confusion at that time regarding atoms and molecules—one of the most important contributions of Avogadro's work was clearly distinguishing one from the other, admitting that simple particles too could be composed of molecules, and that these are composed of atoms. Dalton, by contrast, did not consider this possibility. Curiously, Avogadro considers only molecules containing even numbers of atoms; he does not say why odd numbers are left out. In 1826, building on the work of Avogadro, the French chemist Jean-Baptiste Dumas states:",
                    "score": 0.8765941858291626
                },
                {
                    "id": 1560679,
                    "contents": "Atom\nAs even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about ). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the unified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 Da, and so a mole of carbon-12 atoms weighs exactly 0.012 kg. Shape and size",
                    "score": 0.8765801191329956
                },
                {
                    "id": 7487962,
                    "contents": "List of compounds with carbon number 22\nThis is a partial list of molecules that contain 22 carbon atoms. See also Carbon number C22",
                    "score": 0.8763394355773926
                },
                {
                    "id": 4905657,
                    "contents": "Nitride\nIn chemistry, a nitride is a compound of nitrogen where nitrogen has a formal oxidation state of −3. Nitrides are a large class of compounds with a wide range of properties and applications. The nitride ion, N3−, is never encountered in protic solution because it is so basic that it would be protonated immediately. Its ionic radius is estimated to be 140 pm.",
                    "score": 0.8757977485656738
                },
                {
                    "id": 1378695,
                    "contents": "Mole (unit)\nThe name mole is an 1897 translation of the German unit Mol, coined by the chemist Wilhelm Ostwald in 1894 from the German word Molekül (molecule). The related concept of equivalent mass had been in use at least a century earlier. Standardization Developments in mass spectrometry led to the adoption of oxygen-16 as the standard substance, in lieu of natural oxygen. The oxygen-16 definition was replaced with one based on carbon-12 during the 1960s. The mole was defined by International Bureau of Weights and Measures as \"the amount of substance of a system which contains as many elementary entities as there are atoms in 0.012 kilogram of carbon-12.\" Thus, by that definition, one mole of pure 12C had a mass of exactly 12 g. The four different definitions were equivalent to within 1%.",
                    "score": 0.87571781873703
                },
                {
                    "id": 7492448,
                    "contents": "List of compounds with carbon number 18\nThis is a partial list of molecules that contain 18 carbon atoms. See also Carbon number C18",
                    "score": 0.875686526298523
                },
                {
                    "id": 7530786,
                    "contents": "Atoms in molecules\nThe quantum theory of atoms in molecules (QTAIM) is a model of molecular and condensed matter electronic systems (such as crystals) in which the principal objects of molecular structure - atoms and bonds - are natural expressions of a system's observable electron density distribution function. An electron density distribution of a molecule is a probability distribution that describes the average manner in which the electronic charge is distributed throughout real space in the attractive field exerted by the nuclei. According to QTAIM, molecular structure is revealed by the stationary points of the electron density together with the gradient paths of the electron density that originate and terminate at these points. QTAIM was primarily developed by Professor Richard Bader and his research group at McMaster University over the course of decades, beginning with analyses of theoretically calculated electron densities of simple molecules in the early 1960s and culminating with analyses",
                    "score": 0.8755545020103455
                },
                {
                    "id": 7492450,
                    "contents": "List of compounds with carbon number 16\nThis is a partial list of molecules that contain 16 carbon atoms. See also Carbon number C16",
                    "score": 0.8754546642303467
                },
                {
                    "id": 3087165,
                    "contents": "Lewis structure\nNitrogen is the least electronegative atom of the two, so it is the central atom by multiple criteria. Count valence electrons. Nitrogen has 5 valence electrons; each oxygen has 6, for a total of (6 × 2) + 5 = 17. The ion has a charge of −1, which indicates an extra electron, so the total number of electrons is 18. Connect the atoms by single bonds. Each oxygen must be bonded to the nitrogen, which uses four electrons—two in each bond. Place lone pairs. The 14 remaining electrons should initially be placed as 7 lone pairs. Each oxygen may take a maximum of 3 lone pairs, giving each oxygen 8 electrons including the bonding pair. The seventh lone pair must be placed on the nitrogen atom. Satisfy the octet rule. Both oxygen atoms currently have 8 electrons assigned to them. The nitrogen atom has only 6 electrons assigned to it. One of the lone pairs on an oxygen atom must form a double bond, but either atom will work equally well. Therefore, there is a resonance structure.",
                    "score": 0.8753122091293335
                },
                {
                    "id": 7487928,
                    "contents": "List of compounds with carbon number 24\nThis is a partial list of molecules that contain 24 carbon atoms. See also Carbon number References C24",
                    "score": 0.8751487135887146
                },
                {
                    "id": 4016551,
                    "contents": "C2\nChemistry C2, Diatomic carbon, a molecule made of two carbon atoms C2=, ethylene, a two carbon alkene",
                    "score": 0.8751388192176819
                },
                {
                    "id": 7492449,
                    "contents": "List of compounds with carbon number 17\nThis is a partial list of molecules that contain 17 carbon atoms. See also Carbon number C17",
                    "score": 0.875105082988739
                },
                {
                    "id": 7487963,
                    "contents": "List of compounds with carbon number 21\nThis is a partial list of molecules that contain 21 carbon atoms. See also Carbon number C21",
                    "score": 0.8750926852226257
                },
                {
                    "id": 7768704,
                    "contents": "Effective atomic number\nEffective atomic number has two different meanings: one that is the effective nuclear charge of an atom, and one that calculates the average atomic number for a compound or mixture of materials. Both are abbreviated Zeff. For an atom The effective atomic number Zeff, (sometimes referred to as the effective nuclear charge) of an atom is the number of protons that an electron in the element effectively 'sees' due to screening by inner-shell electrons. It is a measure of the electrostatic interaction between the negatively charged electrons and positively charged protons in the atom. One can view the electrons in an atom as being 'stacked' by energy outside the nucleus; the lowest energy electrons (such as the 1s and 2s electrons) occupy the space closest to the nucleus, and electrons of higher energy are located further from the nucleus.",
                    "score": 0.8749409914016724
                },
                {
                    "id": 20873123,
                    "contents": "Atomicity (chemistry)\nAtomicity may vary in different allotropes of the same element. There is a simple way to determine the atomicity of any homonuclear molecule. It can be determined as a ratio of molecular weight and atomic weight. For example, the molecular weight of oxygen is 31.999, while it's atomic weight is 15.999. On dividing the two, we get atomicity as (31.999/15.999) which is roughly equal to 2. Examples The atomicity of the first 30 elements in the periodic table is as follows: References Molecules Stoichiometry Physical chemistry Inorganic chemistry",
                    "score": 0.874933660030365
                },
                {
                    "id": 1128610,
                    "contents": "Nitrogen\nOxides Nitrogen forms nine molecular oxides, some of which were the first gases to be identified: N2O (nitrous oxide), NO (nitric oxide), N2O3 (dinitrogen trioxide), NO2 (nitrogen dioxide), N2O4 (dinitrogen tetroxide), N2O5 (dinitrogen pentoxide), N4O (nitrosylazide), and N(NO2)3 (trinitramide). All are thermally unstable towards decomposition to their elements. One other possible oxide that has not yet been synthesised is oxatetrazole (N4O), an aromatic ring.",
                    "score": 0.8747594952583313
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_22",
        "question": "Two narrow slits are illuminated with red light of wavelength $694.3 \\mathrm{~nm}$ from a laser, producing a set of evenly placed bright bands on a screen located $3.00 \\mathrm{~m}$ beyond the slits. If the distance between the bands is $1.50 \\mathrm{~cm}$, then what is the distance between the slits?\r\n",
        "golden_answers": [
            " 0.139"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1681638,
                    "contents": "Double-slit experiment\nFor example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6 μm wavelength laser (), then at a distance of 1 m (), the spacing of the fringes will be 1.2 mm. If the width of the slits is appreciable compared to the wavelength, the Fraunhofer diffraction equation is needed to determine the intensity of the diffracted light as follows: where the sinc function is defined as sinc(x) = sin(x)/x for x ≠ 0, and sinc(0) = 1. This is illustrated in the figure above, where the first pattern is the diffraction pattern of a single slit, given by the function in this equation, and the second figure shows the combined intensity of the light diffracted from the two slits, where the function represents the fine structure, and the coarser structure represents diffraction by the individual slits as described by the function.",
                    "score": 0.9357046484947205
                },
                {
                    "id": 1681618,
                    "contents": "Double-slit experiment\nIf one illuminates two parallel slits, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of alternating light and dark bands. The width of the bands is a property of the frequency of the illuminating light. (See the bottom photograph to the right.) When Thomas Young (1773–1829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts. Young's experiment, performed in the early 1800s, played a crucial role in the understanding of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries. However, the later discovery of the photoelectric effect demonstrated that under different circumstances, light can behave as if it is composed of discrete particles. These",
                    "score": 0.9228194952011108
                },
                {
                    "id": 4008836,
                    "contents": "Fraunhofer diffraction\nDiffraction by a double slit In the double-slit experiment, the two slits are illuminated by a single light beam. If the width of the slits is small enough (less than the wavelength of the light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. These fringes are often known as Young's fringes. The angular spacing of the fringes is given by The spacing of the fringes at a distance from the slits is given by where is the separation of the slits. The fringes in the picture were obtained using the yellow light from a sodium light (wavelength = 589 nm), with slits separated by 0.25 mm, and projected directly onto the image plane of a digital camera.",
                    "score": 0.9175888895988464
                },
                {
                    "id": 1681637,
                    "contents": "Double-slit experiment\nWhere d is the distance between the two slits. When the two waves are in phase, i.e. the path difference is equal to an integral number of wavelengths, the summed amplitude, and therefore the summed intensity is maximum, and when they are in anti-phase, i.e. the path difference is equal to half a wavelength, one and a half wavelengths, etc., then the two waves cancel and the summed intensity is zero. This effect is known as interference. The interference fringe maxima occur at angles where λ is the wavelength of the light. The angular spacing of the fringes, , is given by The spacing of the fringes at a distance from the slits is given by For example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6 μm wavelength laser (), then at a distance of 1 m (), the spacing of the fringes will be 1.2 mm.",
                    "score": 0.9144555330276489
                },
                {
                    "id": 11980855,
                    "contents": "Diffraction from slits\nIf we now consider the situation where , the path length becomes This is the Fresnel approximation. To further simplify things: If the diffracting object is much smaller than the distance , the last term will contribute much less than a wavelength to the path length, and will then not change the phase appreciably. That is . The result is the Fraunhofer approximation, which is only valid very far away from the object Depending on the size of the diffraction object, the distance to the object and the wavelength of the wave, the Fresnel approximation, the Fraunhofer approximation or neither approximation may be valid. As the distance between the measured point of diffraction and the obstruction point increases, the diffraction patterns or results predicted converge towards those of Fraunhofer diffraction, which is more often observed in nature due to the extremely small wavelength of visible light. Multiple narrow slits A simple quantitative description",
                    "score": 0.9121137857437134
                },
                {
                    "id": 11943794,
                    "contents": "Slitless spectroscopy\nSlitless spectroscopy is astronomical spectroscopy done without a small slit to allow only light from a small region to be diffracted. It works best in sparsely populated fields, as it spreads each point source out into its spectrum, and crowded fields will be too confused to be useful. It also faces the problem that for extended sources, nearby emission lines will overlap. The Crossley telescope utilized a slitless spectrograph that was originally employed by Nicholas Mayall. Florence Cushman employed this method at the Harvard College Observatory to classify hundreds of thousands of stars in the Henry Draper Catalogue. References Cited sources See also Fine Guidance Sensor and Near Infrared Imager and Slitless Spectrograph (JWST component) Astronomical spectroscopy",
                    "score": 0.9109393358230591
                },
                {
                    "id": 1681617,
                    "contents": "Double-slit experiment\nOverview If light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this \"single-slit experiment\" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.",
                    "score": 0.9103292226791382
                },
                {
                    "id": 19024864,
                    "contents": "Fraunhofer diffraction equation\nSlits Two slits The pattern which occurs when light diffracted from two slits overlaps is of considerable interest in physics, firstly for its importance in establishing the wave theory of light through Young's interference experiment, and secondly because of its role as a thought experiment in double-slit experiment in quantum mechanics. Narrow slits right|thumb|Two slit interference using a red laserAssume we have two long slits illuminated by a plane wave of wavelength . The slits are in the plane, parallel to the axis, separated by a distance and are symmetrical about the origin. The width of the slits is small compared with the wavelength. Solution by integration The incident light is diffracted by the slits into uniform spherical waves. The waves travelling in a given direction from the two slits have differing phases. The phase of the waves from the upper and lower slits relative to the origin is given by and The complex amplitude of the summed waves is given by:",
                    "score": 0.9094752669334412
                },
                {
                    "id": 11980856,
                    "contents": "Diffraction from slits\nMultiple narrow slits A simple quantitative description Multiple-slit arrangements can be mathematically considered as multiple simple wave sources, if the slits are narrow enough. For light, a slit is an opening that is infinitely extended in one dimension, and this has the effect of reducing a wave problem in 3D-space to a simpler problem in 2D-space. The simplest case is that of two narrow slits, spaced a distance apart. To determine the maxima and minima in the amplitude we must determine the path difference to the first slit and to the second one. In the Fraunhofer approximation, with the observer far away from the slits, the difference in path length to the two slits can be seen from the image to be Maxima in the intensity occur if this path length difference is an integer number of wavelengths.",
                    "score": 0.9084086418151855
                },
                {
                    "id": 1324713,
                    "contents": "Wavelength\nwhere m is an integer, and for destructive interference is: Thus, if the wavelength of the light is known, the slit separation can be determined from the interference pattern or fringes, and vice versa. For multiple slits, the pattern is where q is the number of slits, and g is the grating constant. The first factor, I1, is the single-slit result, which modulates the more rapidly varying second factor that depends upon the number of slits and their spacing. In the figure I1 has been set to unity, a very rough approximation. The effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up. Single-slit diffraction",
                    "score": 0.9083283543586731
                },
                {
                    "id": 1324714,
                    "contents": "Wavelength\nThe effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up. Single-slit diffraction The notion of path difference and constructive or destructive interference used above for the double-slit experiment applies as well to the display of a single slit of light intercepted on a screen. The main result of this interference is to spread out the light from the narrow slit into a broader image on the screen. This distribution of wave energy is called diffraction. Two types of diffraction are distinguished, depending upon the separation between the source and the screen: Fraunhofer diffraction or far-field diffraction at large separations and Fresnel diffraction or near-field diffraction at close separations.",
                    "score": 0.9079059362411499
                },
                {
                    "id": 11980862,
                    "contents": "Diffraction from slits\nTaking results in: It can be noted through Euler's formula and its derivatives that and . where the (unnormalized) sinc function is defined by . Now, substituting in , the intensity (squared amplitude) of the diffracted waves at an angle θ is given by: Multiple slits Let us again start with the mathematical representation of Huygens' principle. Consider slits in the prime plane of equal size and spacing spread along the axis. As above, the distance from slit 1 is: To generalize this to slits, we make the observation that while and remain constant, shifts by Thus and the sum of all contributions to the wave function is: Again noting that is small, so , we have: Now, we can use the following identity Substituting into our equation, we find: We now make our substitution as before and represent all non-oscillating constants by the variable as in the 1-slit diffraction and bracket the result. Remember that",
                    "score": 0.9078689813613892
                },
                {
                    "id": 1680367,
                    "contents": "Diffraction\nIn the modern quantum mechanical understanding of light propagation through a slit (or slits) every photon has what is known as a wavefunction. The wavefunction is determined by the physical surroundings such as slit geometry, screen distance and initial conditions when the photon is created. In important experiments (A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, see double-slit experiment) the existence of the photon's wavefunction was demonstrated. In the quantum approach the diffraction pattern is created by the probability distribution, the observation of light and dark bands is the presence or absence of photons in these areas, where these particles were more or less likely to be detected. The quantum approach has some striking similarities to the Huygens-Fresnel principle; based on that principle, as light travels through slits and boundaries, secondary, point light sources are created near or along these obstacles, and the resulting",
                    "score": 0.9056620001792908
                },
                {
                    "id": 1681636,
                    "contents": "Double-slit experiment\nIn the double-slit experiment, the two slits are illuminated by the quasi-monochromatic light of a single laser. If the width of the slits is small enough (much less than the wavelength of the laser light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves. If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure below right. The path difference between two waves travelling at an angle is given by:",
                    "score": 0.9046459197998047
                },
                {
                    "id": 1324715,
                    "contents": "Wavelength\nIn the analysis of the single slit, the non-zero width of the slit is taken into account, and each point in the aperture is taken as the source of one contribution to the beam of light (Huygens' wavelets). On the screen, the light arriving from each position within the slit has a different path length, albeit possibly a very small difference. Consequently, interference occurs. In the Fraunhofer diffraction pattern sufficiently far from a single slit, within a small-angle approximation, the intensity spread S is related to position x via a squared sinc function: with where L is the slit width, R is the distance of the pattern (on the screen) from the slit, and λ is the wavelength of light used. The function S has zeros where u is a non-zero integer, where are at x values at a separation proportion to wavelength. Diffraction-limited resolution",
                    "score": 0.9042110443115234
                },
                {
                    "id": 17684228,
                    "contents": "N-slit interferometer\nThe N-slit interferometer is an extension of the double-slit interferometer also known as Young's double-slit interferometer. One of the first known uses of N-slit arrays in optics was illustrated by Newton. In the first part of last century, Michelson described various cases of N-slit diffraction. Feynman described thought experiments, of two-slit quantum interference, of electrons using Dirac's notation. This approach was extended to N-slit interferometers, by Duarte and colleagues in 1989, using narrow-linewidth laser illumination, that is, illumination by indistinguishable photons. The first application of the N-slit interferometer was the generation and measurement of complex interference patterns. These interferograms are accurately reproduced, or predicted, by the N-slit interferometric equation for either even (N = 2, 4, 6,…), or odd (N = 3, 5, 7,…), numbers of slits. N-slit laser interferometer",
                    "score": 0.9025755524635315
                },
                {
                    "id": 1681613,
                    "contents": "Double-slit experiment\nIn the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves); the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass",
                    "score": 0.9006928205490112
                },
                {
                    "id": 20044836,
                    "contents": "Long-slit spectroscopy\nIn astronomy, long-slit spectroscopy involves observing a celestial object using a spectrograph in which the entrance aperture is an elongated, narrow slit. Light entering the slit is then refracted using a prism, diffraction grating, or grism. The dispersed light is typically recorded on a charge-coupled device detector. Velocity profiles This technique can be used to observe the rotation curve of a galaxy, as those stars moving towards the observer are blue-shifted, while stars moving away are red-shifted.",
                    "score": 0.8991361856460571
                },
                {
                    "id": 1680373,
                    "contents": "Diffraction\nA slit that is wider than a wavelength produces interference effects in the space downstream of the slit. These can be explained by assuming that the slit behaves as though it has a large number of point sources spaced evenly across the width of the slit. The analysis of this system is simplified if we consider light of a single wavelength. If the incident light is coherent, these sources all have the same phase. Light incident at a given point in the space downstream of the slit is made up of contributions from each of these point sources and if the relative phases of these contributions vary by 2π or more, we may expect to find minima and maxima in the diffracted light. Such phase differences are caused by differences in the path lengths over which contributing rays reach the point from the slit.",
                    "score": 0.8990359902381897
                },
                {
                    "id": 11980859,
                    "contents": "Diffraction from slits\nSince we are for the moment only interested in the amplitude and relative phase, we can ignore any overall phase factors that are not dependent on or . We approximate . In the Fraunhofer limit we can neglect terms of order : in the exponential, and any terms involving or in the denominator. The sum becomes The sum has the form of a geometric sum and can be evaluated to give The intensity is given by the absolute value of the complex amplitude squared where denotes the complex conjugate of . Single slit As an example, an exact equation can now be derived for the intensity of the diffraction pattern as a function of angle in the case of single-slit diffraction. A mathematical representation of Huygens' principle can be used to start an equation. Consider a monochromatic complex plane wave of wavelength λ incident on a slit of width a.",
                    "score": 0.8983668684959412
                },
                {
                    "id": 11980863,
                    "contents": "Diffraction from slits\nWe now make our substitution as before and represent all non-oscillating constants by the variable as in the 1-slit diffraction and bracket the result. Remember that This allows us to discard the tailing exponent and we have our answer: General case for far field In the far field, where r is essentially constant, then the equation: is equivalent to doing a Fourier transform on the gaps in the barrier. See also Diffraction grating Envelope (waves) Fourier analysis N-slit interferometer Radio telescopes References Equations of physics Wave mechanics",
                    "score": 0.8972623944282532
                },
                {
                    "id": 1681621,
                    "contents": "Double-slit experiment\nA low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly non-overlapping. A double-slit experiment was not performed with anything other than light until 1961, when Claus Jönsson of the University of Tübingen performed it with electron beams. In 1974, the Italian physicists Pier Giorgio Merli, Gian Franco Missiroli, and Giulio Pozzi repeated the experiment using single electrons and biprism (instead of slits), showing that each electron interferes with itself as predicted by quantum theory. In 2002, the single-electron version of the experiment was voted \"the most beautiful experiment\" by readers of Physics World.",
                    "score": 0.8970553874969482
                },
                {
                    "id": 1681620,
                    "contents": "Double-slit experiment\nFeynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment. He also proposed (as a thought experiment) that if detectors were placed before each slit, the interference pattern would disappear. The Englert–Greenberger duality relation provides a detailed treatment of the mathematics of double-slit interference in the context of quantum mechanics. A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly non-overlapping.",
                    "score": 0.8960651755332947
                },
                {
                    "id": 1681639,
                    "contents": "Double-slit experiment\nSimilar calculations for the near field can be made by applying the Fresnel diffraction equation, which implies that as the plane of observation gets closer to the plane in which the slits are located, the diffraction patterns associated with each slit decrease in size, so that the area in which interference occurs is reduced, and may vanish altogether when there is no overlap in the two diffracted patterns. Interpretations of the experiment Like the Schrödinger's cat thought experiment, the double-slit experiment is often used to highlight the differences and similarities between the various interpretations of quantum mechanics. Copenhagen interpretation",
                    "score": 0.8955422639846802
                },
                {
                    "id": 1681630,
                    "contents": "Double-slit experiment\nOther variations In 1967, Pfleegor and Mandel demonstrated two-source interference using two separate lasers as light sources. It was shown experimentally in 1972 that in a double-slit system where only one slit was open at any time, interference was nonetheless observed provided the path difference was such that the detected photon could have come from either slit. The experimental conditions were such that the photon density in the system was much less than unity. In 1999, a quantum interference experiment (using a diffraction grating, rather than two slits) was successfully performed with buckyball molecules (each of which comprises 60 carbon atoms). A buckyball is large enough (diameter about 0.7 nm, nearly half a million times larger than a proton) to be seen under an electron microscope.",
                    "score": 0.8946146965026855
                },
                {
                    "id": 1680375,
                    "contents": "Diffraction\nwhere d is the width of the slit, is the angle of incidence at which the minimum intensity occurs, and is the wavelength of the light A similar argument can be used to show that if we imagine the slit to be divided into four, six, eight parts, etc., minima are obtained at angles θn given by where n is an integer other than zero. There is no such simple argument to enable us to find the maxima of the diffraction pattern. The intensity profile can be calculated using the Fraunhofer diffraction equation as where is the intensity at a given angle, is the intensity at the central maximum (), which is also a normalization factor of the intensity profile that can be determined by an integration from to and conservation of energy. is the unnormalized sinc function. This analysis applies only to the far field (Fraunhofer diffraction), that is, at a distance much larger than the width of the slit.",
                    "score": 0.8924341201782227
                },
                {
                    "id": 18112139,
                    "contents": "Young's interference experiment\nYoung's interference experiment, also called Young's double-slit interferometer, was the original version of the modern double-slit experiment, performed at the beginning of the nineteenth century by Thomas Young. This experiment played a major role in the general acceptance of the wave theory of light. In Young's own judgement, this was the most important of his many achievements.",
                    "score": 0.8919957280158997
                },
                {
                    "id": 19024858,
                    "contents": "Fraunhofer diffraction equation\nThe slit can be represented by the rect function as: The Fourier transform of this function is given by where is the Fourier transform frequency, and the function is here defined as sin(x)/(x) The Fourier transform frequency here is , giving Note that the function is here defined as sin(x)/(x) to maintain consistency. Intensity The intensity is proportional to the square of the amplitude, and is therefore Apertures Rectangular aperture When a rectangular slit of width W and height H is illuminated normally (the slit illuminated at the normal angle) by a monochromatic plane wave of wavelength λ, the complex amplitude can be found using similar analyses to those in the previous section, applied over two independent orthogonal dimensions as:Longhurst, 1967, p 217 . The intensity is given by",
                    "score": 0.8919470906257629
                },
                {
                    "id": 11980857,
                    "contents": "Diffraction from slits\nMaxima in the intensity occur if this path length difference is an integer number of wavelengths. {| |- | || || rowspan=4 | where is an integer that labels the order of each maximum, is the wavelength, is the distance between the slits and is the angle at which constructive interference occurs. |- | |} The corresponding minima are at path differences of an integer number plus one half of the wavelength: . For an array of slits, positions of the minima and maxima are not changed, the fringes visible on a screen however do become sharper, as can be seen in the image. Mathematical description To calculate this intensity pattern, one needs to introduce some more sophisticated methods. The mathematical representation of a radial wave is given by",
                    "score": 0.8904475569725037
                },
                {
                    "id": 901535,
                    "contents": "Diffraction grating\nAn idealized diffraction grating is made up of a set of slits of spacing , that must be wider than the wavelength of interest to cause diffraction. Assuming a plane wave of monochromatic light of wavelength at normal incidence on a grating (I.e., wavefronts of the incident wave are parallel to the grating main plane), each slit in the grating acts as a quasi point wave source from which light propagates in all directions (although this is typically limited to the forward hemisphere from the point source). Of course, every point on every slit to which the incident wave reaches plays as a point wave source for the diffraction wave and all these contributions to the diffraction wave determine the detailed diffraction wave light property distribution, but diffraction angles (at the grating) at which the diffraction wave intensity is highest are determined only by these quasi point sources corresponding the slits in the grating. After the incident light (wave) interacts with the grating,",
                    "score": 0.8900377750396729
                },
                {
                    "id": 4008837,
                    "contents": "Fraunhofer diffraction\nDouble-slit interference fringes can be observed by cutting two slits in a piece of card, illuminating with a laser pointer, and observing the diffracted light at a distance of 1 m. If the slit separation is 0.5 mm, and the wavelength of the laser is 600 nm, then the spacing of the fringes viewed at a distance of 1 m would be 1.2 mm. Semi-quantitative explanation of double-slit fringes The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves. If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure. The path difference between two waves travelling at an angle is given by",
                    "score": 0.8899458646774292
                },
                {
                    "id": 11980860,
                    "contents": "Diffraction from slits\nA mathematical representation of Huygens' principle can be used to start an equation. Consider a monochromatic complex plane wave of wavelength λ incident on a slit of width a. If the slit lies in the x′-y′ plane, with its center at the origin, then it can be assumed that diffraction generates a complex wave ψ, traveling radially in the r direction away from the slit, and this is given by: Let (x′,y′,0) be a point inside the slit over which it is being integrated. If (x,0,z) is the location at which the intensity of the diffraction pattern is being computed, the slit extends from to , and from to . The distance r from the slot is: Assuming Fraunhofer diffraction will result in the conclusion . In other words, the distance to the target is much larger than the diffraction width on the target. By the binomial expansion rule, ignoring terms quadratic and higher, the quantity on the right can be estimated to be:",
                    "score": 0.8893198370933533
                },
                {
                    "id": 1563838,
                    "contents": "Augustin-Jean Fresnel\nFor the experimental testing of his calculations, Fresnel used red light with a wavelength of 638nm, which he deduced from the diffraction pattern in the simple case in which light incident on a single slit was focused by a cylindrical lens. For a variety of distances from the source to the obstacle and from the obstacle to the field point, he compared the calculated and observed positions of the fringes for diffraction by a half-plane, a slit, and a narrow strip – concentrating on the minima, which were visually sharper than the maxima. For the slit and the strip, he could not use the previously computed table of maxima and minima; for each combination of dimensions, the intensity had to be expressed in terms of sums or differences of Fresnel integrals and calculated from the table of integrals, and the extrema had to be calculated anew. The agreement between calculation and measurement was better than 1.5% in almost every case.",
                    "score": 0.8890901803970337
                },
                {
                    "id": 19024860,
                    "contents": "Fraunhofer diffraction equation\nIn practice, all slits are of finite size so produce diffraction on the both transverse directions, along the (width W defined) and (height H defined) axes. If the height H of the slit is much greater than its width W, then the spacing of the vertical (along the height or the axis) diffraction fringes is much less than the spacing of the horizontal (along the width or axis) fringes. If the vertical fringe spacing is so less by a relatively so large H, then the observation of the vertical fringes is so hard that a person observing the diffracted wave intensity pattern on the plane of observation or the image plane recognizes only the horizontal fringes with their narrow height. This is the reason why a height-long slit or slit array such as a diffraction grating is typically analyzed only in the dimension along the width. If the illuminating beam does not illuminate the whole height of the slit, then the spacing of the vertical fringes is determined by the dimension of the laser",
                    "score": 0.8889468908309937
                },
                {
                    "id": 1681612,
                    "contents": "Double-slit experiment\nThe experiment belongs to a general class of \"double path\" experiments, in which a wave is split into two separate waves that later combine into a single wave. Changes in the path-lengths of both waves result in a phase shift, creating an interference pattern. Another version is the Mach–Zehnder interferometer, which splits the beam with a beam splitter.",
                    "score": 0.8887742757797241
                },
                {
                    "id": 23431583,
                    "contents": "Orders Of Coherence\nYoung Double Slit Experiment In Young's double slit experiment, light from a light source is allowed to pass through two pinholes separated by some distance, and a screen is placed some distance away from the pinholes where the interference between the light waves is observed (Figure. 1). Young's double slit experiment demonstrates the dependence of interference on coherence, specifically on the first-order correlation. This experiment is equivalent to the Mach-Zehnder interferometer with the caveat that Young's double slit experiment is concerned with spatial coherence, while Mach-Zehnder interferometer relies on temporal coherence. The intensity measured at the position at time is . Light field has highest degree of coherence when the corresponding interference pattern has the maximum contrast on the screen. The fringe contrast is defined as .",
                    "score": 0.8870108723640442
                },
                {
                    "id": 4008830,
                    "contents": "Fraunhofer diffraction\nExamples of Fraunhofer diffraction In each of these examples, the aperture is illuminated by a monochromatic plane wave at normal incidence. Diffraction by a narrow rectangular slit The width of the slit is . The Fraunhofer diffraction pattern is shown in the image together with a plot of the intensity vs. angle . The pattern has maximum intensity at , and a series of peaks of decreasing intensity. Most of the diffracted light falls between the first minima. The angle, , subtended by these two minima is given by: Thus, the smaller the aperture, the larger the angle subtended by the diffraction bands. The size of the central band at a distance is given by For example, when a slit of width 0.5 mm is illuminated by light of wavelength 0.6 μm, and viewed at a distance of 1000 mm, the width of the central band in the diffraction pattern is 2.4 mm. The fringes extend to infinity in the direction since the slit and illumination also extend to infinity.",
                    "score": 0.886440098285675
                },
                {
                    "id": 11980853,
                    "contents": "Diffraction from slits\nSeveral qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: the smaller the diffracting object, the wider the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object. When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The fourth figure, for example, shows a comparison of a double-slit pattern with a pattern formed by five slits, both sets of slits having the same spacing between the center of one slit and the next.",
                    "score": 0.8862426280975342
                },
                {
                    "id": 17684229,
                    "contents": "N-slit interferometer\nN-slit laser interferometer The N-slit laser interferometer, introduced by Duarte, uses prismatic beam expansion to illuminate a transmission grating, or N-slit array, and a photoelectric detector array (such as a CCD or CMOS) at the interference plane to register the interferometric signal. The expanded laser beam illuminating the N-slit array is single-transverse-mode and narrow-linewidth. This beam can also take the shape, via the introduction of a convex lens prior to the prismatic expander, of a beam extremely elongated in the propagation plane and extremely thin in the orthogonal plane. This use of one-dimensional (or line) illumination eliminates the need of point-by-point scanning in microscopy and microdensitometry. Thus, these instruments can be used as straight forward N-slit interferometers or as interferometric microscopes (see section on microscopy).",
                    "score": 0.8846649527549744
                },
                {
                    "id": 1681644,
                    "contents": "Double-slit experiment\nAs is always the case when calculating probability, the results must then be normalized by imposing: To summarize, the probability distribution of the outcome is the normalized square of the norm of the superposition, over all paths from the point of origin to the final point, of waves propagating proportionally to the action along each path. The differences in the cumulative action along the different paths (and thus the relative phases of the contributions) produces the interference pattern observed by the double-slit experiment. Feynman stressed that his formulation is merely a mathematical description, not an attempt to describe a real process that we can measure. Relational interpretation",
                    "score": 0.8839821815490723
                },
                {
                    "id": 1681641,
                    "contents": "Double-slit experiment\nThe probability \"wave\" can be said to \"pass through space\" because the probability values that one can compute from its mathematical representation are dependent on time. One cannot speak of the location of any particle such as a photon between the time it is emitted and the time it is detected simply because in order to say that something is located somewhere at a certain time one has to detect it. The requirement for the eventual appearance of an interference pattern is that particles be emitted, and that there be a screen with at least two distinct paths for the particle to take from the emitter to the detection screen. Experiments observe nothing whatsoever between the time of emission of the particle and its arrival at the detection screen. If a ray tracing is next made as if a light wave (as understood in classical physics) is wide enough to take both paths, then that ray tracing will accurately predict the appearance of maxima and minima on the detector screen when many",
                    "score": 0.8828074336051941
                },
                {
                    "id": 1681645,
                    "contents": "Double-slit experiment\nAccording to the relational interpretation of quantum mechanics, first proposed by Carlo Rovelli, observations such as those in the double-slit experiment result specifically from the interaction between the observer (measuring device) and the object being observed (physically interacted with), not any absolute property possessed by the object. In the case of an electron, if it is initially \"observed\" at a particular slit, then the observer–particle (photon–electron) interaction includes information about the electron's position. This partially constrains the particle's eventual location at the screen. If it is \"observed\" (measured with a photon) not at a particular slit but rather at the screen, then there is no \"which path\" information as part of the interaction, so the electron's \"observed\" position on the screen is determined strictly by its probability function. This makes the resulting pattern on the screen the same as if each individual electron had passed through both slits.",
                    "score": 0.8824558854103088
                },
                {
                    "id": 3383750,
                    "contents": "Monochromator\nAt the exit slit, the colors of the light are spread out (in the visible this shows the colors of the rainbow). Because each color arrives at a separate point in the exit-slit plane, there are a series of images of the entrance slit focused on the plane. Because the entrance slit is finite in width, parts of nearby images overlap. The light leaving the exit slit (G) contains the entire image of the entrance slit of the selected color plus parts of the entrance slit images of nearby colors. A rotation of the dispersing element causes the band of colors to move relative to the exit slit, so that the desired entrance slit image is centered on the exit slit. The range of colors leaving the exit slit is a function of the width of the slits. The entrance and exit slit widths are adjusted together.",
                    "score": 0.8821804523468018
                },
                {
                    "id": 11980854,
                    "contents": "Diffraction from slits\nApproximations The problem of calculating what a diffracted wave looks like, is the problem of determining the phase of each of the simple sources on the incoming wave front. It is mathematically easier to consider the case of far-field or Fraunhofer diffraction, where the point of observation is far from that of the diffracting obstruction, and as a result, involves less complex mathematics than the more general case of near-field or Fresnel diffraction. To make this statement more quantitative, consider a diffracting object at the origin that has a size . For definiteness let us say we are diffracting light and we are interested in what the intensity looks like on a screen a distance away from the object. At some point on the screen the path length to one side of the object is given by the Pythagorean theorem If we now consider the situation where , the path length becomes",
                    "score": 0.8820995688438416
                },
                {
                    "id": 1324716,
                    "contents": "Wavelength\nDiffraction-limited resolution Diffraction is the fundamental limitation on the resolving power of optical instruments, such as telescopes (including radiotelescopes) and microscopes. For a circular aperture, the diffraction-limited image spot is known as an Airy disk; the distance x in the single-slit diffraction formula is replaced by radial distance r and the sine is replaced by 2J1, where J1 is a first order Bessel function. The resolvable spatial size of objects viewed through a microscope is limited according to the Rayleigh criterion, the radius to the first null of the Airy disk, to a size proportional to the wavelength of the light used, and depending on the numerical aperture: where the numerical aperture is defined as for θ being the half-angle of the cone of rays accepted by the microscope objective.",
                    "score": 0.8820726871490479
                },
                {
                    "id": 19024857,
                    "contents": "Fraunhofer diffraction equation\nThese derivations can be found in most standard optics books, in slightly different forms using varying notations. A reference is given for each of the systems modelled here. The Fourier transforms used can be found here. Narrow rectangular slit The aperture is a slit of width which is located along the -axis, Solution by integration Assuming the centre of the slit is located at , the first equation above, for all values of , is: Using Euler's formula, this can be simplified to: where . The sinc function is sometimes defined as and this may cause confusion when looking at derivations in different texts. This can also be written as: where is the angle between z-axis and the line joining x to the origin and when . Fourier transform solution The slit can be represented by the rect function as: The Fourier transform of this function is given by where is the Fourier transform frequency, and the function is here defined as sin(x)/(x)",
                    "score": 0.8820213079452515
                },
                {
                    "id": 11980861,
                    "contents": "Diffraction from slits\nIt can be seen that 1/r in front of the equation is non-oscillatory, i.e. its contribution to the magnitude of the intensity is small compared to our exponential factors. Therefore, we will lose little accuracy by approximating it as 1/z. To make things cleaner, a placeholder 'C' is used to denote constants in the equation. It is important to keep in mind that C can contain imaginary numbers, thus the wave function will be complex. However, at the end, the ψ will be bracketed, which will eliminate any imaginary components. Now, in Fraunhofer diffraction, is small, so (note that participates in this exponential and it is being integrated). In contrast the term can be eliminated from the equation, since when bracketed it gives 1. (For the same reason we have also eliminated the term ) Taking results in: It can be noted through Euler's formula and its derivatives that and . where the (unnormalized) sinc function is defined by .",
                    "score": 0.881688117980957
                },
                {
                    "id": 18112142,
                    "contents": "Young's interference experiment\nIn 1801, Young presented a famous paper to the Royal Society entitled \"On the Theory of Light and Colours\" which describes various interference phenomena. In 1803, he described his famous interference experiment. Unlike the modern double-slit experiment, Young's experiment reflects sunlight (using a steering mirror) through a small hole, and splits the thin beam in half using a paper card. He also mentions the possibility of passing light through two slits in his description of the experiment:",
                    "score": 0.8815102577209473
                },
                {
                    "id": 1775076,
                    "contents": "Huygens–Fresnel principle\nareas, no photons are landing, and in bright areas, many photons are landing. The set of possible photon paths is determined by the surroundings: the photon's originating point (atom), the slit, and the screen. The wave function is a solution to this geometry. The wave function approach was further supported by additional double-slit experiments in Italy and Japan in the 1970s and 1980s with electrons.",
                    "score": 0.88087397813797
                },
                {
                    "id": 18344273,
                    "contents": "Interface Region Imaging Spectrograph\nExperiment Interface Region Imaging Spectrograph (IRIS) The IRIS instrument is a multi-channel imaging spectrograph with a ultraviolet telescope. IRIS obtains a spectra along a slit (1/3 arcsecond wide), and slit-jaw images. The charge-coupled device (CCD) detectors has 1/6 arcsecond pixels. IRIS will have an effective spatial resolution between 0.33 and 0.40 arcsecond and a maximum field of view (FoV) of 120 arcseconds. The far-ultraviolet channel covers 133.2-135.8 nm and 139.0-140.6 nm with a 4 nm resolution and an effective area of . The near-ultraviolet channel covers 278.5-283.5 nm with a 8 nm resolution and an effective area of . Slit-jaw imaging has four passbands: 133.5 nm and 140.0 nm with a 4 nm bandpass each; and 279.6 nm and 283.1 nm with a 0.4 nm bandpass each. IRIS has a high data rate (0.7 Mbit/s on average) so that the baseline cadence is 5 seconds for slit-jaw images and 1 second for six spectral windows, including rapid rastering to map solar regions.",
                    "score": 0.8807022571563721
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_23",
        "question": "Calculate the energy and wavelength associated with an $\\alpha$ particle that has fallen through a potential difference of $4.0 \\mathrm{~V}$. Take the mass of an $\\alpha$ particle to be $6.64 \\times 10^{-27} \\mathrm{~kg}$.",
        "golden_answers": [
            " 1.3"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1485600,
                    "contents": "Geiger–Marsden experiments\nAn alpha particle is a sub-microscopic, positively charged particle of matter. According to Thomson's Plum pudding model, if an alpha particle were to collide with an atom, it would just fly straight through, its path being deflected by at most a fraction of a degree. At the atomic scale, the concept of \"solid matter\" is meaningless. The Thomson atom is a sphere of positive electrical charge, anchored in place by its mass. Thus the alpha particle would not bounce off the atom like a ball, but might pass right through if the atom's electric fields are weak enough to permit it. Thomson's model predicted that the electric fields in an atom are too weak to affect a passing alpha particle much (alpha particles tend to move very fast). Both the negative and positive charges within the Thomson atom are spread out over the atom's entire volume. According to Coulomb's Law, the less concentrated a sphere of electric charge is, the weaker its electric field at its surface will be.",
                    "score": 0.8941081762313843
                },
                {
                    "id": 986659,
                    "contents": "Atomic, molecular, and optical physics\n. where E0 is the magnitude of the electric field amplitude, and E is the magnitude of the electric field at position x. From this basic, Planck's law was derived. In 1911, Ernest Rutherford concluded, based on alpha particle scattering, that an atom has a central pointlike proton. He also thought that an electron would be still attracted to the proton by Coulomb's law, which he had verified still held at small scales. As a result, he believed that electrons revolved around the proton. Niels Bohr, in 1913, combined the Rutherford model of the atom with the quantisation ideas of Planck. Only specific and well-defined orbits of the electron could exist, which also do not radiate light. In jumping orbit the electron would emit or absorb light corresponding to the difference in energy of the orbits. His prediction of the energy levels was then consistent with observation.",
                    "score": 0.8899206519126892
                },
                {
                    "id": 5380937,
                    "contents": "Radiation length\n, where is the atomic number and is mass number of the nucleus. For , a good approximation is , where is the number density of the nucleus, denotes the reduced Planck constant, the electron rest mass, the speed of light, and the fine structure constant. For electrons at lower energies (below few tens of MeV), the energy loss by ionization is predominant. While this definition may also be used for other electromagnetic interacting particles beyond leptons and photons, the presence of the stronger hadronic and nuclear interaction makes it a far less interesting characterisation of the material; the nuclear collision length and nuclear interaction length are more relevant. Comprehensive tables for radiation lengths and other properties of materials are available from the Particle Data Group. See also Mean free path Attenuation length Attenuation coefficient Attenuation Range (particle radiation) Stopping power (particle radiation) Electron energy loss spectroscopy",
                    "score": 0.8866366147994995
                },
                {
                    "id": 1485624,
                    "contents": "Geiger–Marsden experiments\nFinally, Geiger and Marsden tested how the scattering varied with the velocity of the alpha particles (i.e. if s ∝ 1/v4). Using the same apparatus again, they slowed the alpha particles by placing extra sheets of mica in front of the alpha particle source. They found that, within the range of experimental error, that the number of scinitillations was indeed proportional to 1/v4. Rutherford determines the nucleus is positively charged In his 1911 paper (see above), Rutherford assumed that the central charge of the atom was positive, but a negative charge would have fitted his scattering model just as well. In a 1913 paper, Rutherford declared that the \"nucleus\" (as he now called it) was indeed positively charged, based on the result of experiments exploring the scattering of alpha particles in various gases.",
                    "score": 0.8831347227096558
                },
                {
                    "id": 1208594,
                    "contents": "Rutherford scattering\nRearranging: For an alpha particle: (mass) = = (for helium) = 2 × = (for gold) = 79 × = (initial velocity) = (for this example) Substituting these in gives the value of about , or 27 fm. (The true radius is about 7.3 fm.) The true radius of the nucleus is not recovered in these experiments because the alphas do not have enough energy to penetrate to more than 27 fm of the nuclear center, as noted, when the actual radius of gold is 7.3 fm. Rutherford realized this, and also realized that actual impact of the alphas on gold causing any force-deviation from that of the coulomb potential would change the form of his scattering curve at high scattering angles (the smallest impact parameters) from a hyperbola to something else. This was not seen, indicating that the surface of the gold nucleus had not been \"touched\" so that Rutherford also knew the gold nucleus (or the sum of the gold and alpha radii) was smaller than 27 fm.",
                    "score": 0.8818369507789612
                },
                {
                    "id": 5380936,
                    "contents": "Radiation length\nIn physics, the radiation length is a characteristic of a material, related to the energy loss of high energy particles electromagnetically interacting with it. Definition In materials of high atomic number (e.g. W, U, Pu) the electrons of energies >~10 MeV predominantly lose energy by bremsstrahlung, and high-energy photons by pair production. The characteristic amount of matter traversed for these related interactions is called the radiation length , usually measured in g·cm−2. It is both the mean distance over which a high-energy electron loses all but of its energy by bremsstrahlung, and of the mean free path for pair production by a high-energy photon. It is also the appropriate length scale for describing high-energy electromagnetic cascades. The radiation length for a given material consisting of a single type of nucleus can be approximated by the following expression: , where is the atomic number and is mass number of the nucleus. For , a good approximation is ,",
                    "score": 0.8816710710525513
                },
                {
                    "id": 1324718,
                    "contents": "Wavelength\nA subwavelength particle is a particle smaller than the wavelength of light with which it interacts (see Rayleigh scattering). Subwavelength apertures are holes smaller than the wavelength of light propagating through them. Such structures have applications in extraordinary optical transmission, and zero-mode waveguides, among other areas of photonics. Subwavelength may also refer to a phenomenon involving subwavelength objects; for example, subwavelength imaging. Angular wavelength A quantity related to the wavelength is the angular wavelength (also known as reduced wavelength), usually symbolized by ƛ (lambda-bar). It is equal to the \"regular\" wavelength \"reduced\" by a factor of 2π (ƛ = λ/2π). It is usually encountered in quantum mechanics, where it is used in combination with the reduced Planck constant (symbol ħ, h-bar) and the angular frequency (symbol ω) or angular wavenumber (symbol k).",
                    "score": 0.8804662227630615
                },
                {
                    "id": 8502089,
                    "contents": "Stopping power (particle radiation)\nand where x = r/au, and a0 is the Bohr atomic radius = 0.529 Å. The standard deviation of the fit of the universal ZBL repulsive potential to the theoretically calculated pair-specific potentials it is fit to is 18% above 2 eV. Even more accurate repulsive potentials can be obtained from self-consistent total energy calculations using density-functional theory and the local-density approximation (LDA) for electronic exchange and correlation. Channeling In crystalline materials the ion may in some instances get \"channeled\", i.e., get focused into a channel between crystal planes where it experiences almost no collisions with nuclei. Also, the electronic stopping power may be weaker in the channel. Thus the nuclear and electronic stopping do not only depend on material type and density but also on its microscopic structure and cross-section.",
                    "score": 0.8794258832931519
                },
                {
                    "id": 5205023,
                    "contents": "Compton wavelength\nThus the uncertainty in position must be greater than half of the reduced Compton wavelength . The Compton wavelength can be contrasted with the de Broglie wavelength, which depends on the momentum of a particle and determines the cutoff between particle and wave behavior in quantum mechanics. Notably, de Broglie's derivation of the de Broglie wavelength is based on the assumption that an observed particle is associated with a periodic phenomenon of the particle's Compton frequency. Relationship to other constants Typical atomic lengths, wave numbers, and areas in physics can be related to the reduced Compton wavelength for the electron () and the electromagnetic fine structure constant (). The Bohr radius is related to the Compton wavelength by: The classical electron radius is about 3 times larger than the proton radius, and is written: The Rydberg constant, having dimensions of linear wavenumber, is written: This yields the sequence: .",
                    "score": 0.8786271810531616
                },
                {
                    "id": 9884593,
                    "contents": "Astroparticle physics\nHistory The field of astroparticle physics is evolved out of optical astronomy. With the growth of detector technology came the more mature astrophysics, which involved multiple physics subtopics, such as mechanics, electrodynamics, thermodynamics, plasma physics, nuclear physics, relativity, and particle physics. Particle physicists found astrophysics necessary due to difficulty in producing particles with comparable energy to those found in space. For example, the cosmic ray spectrum contains particles with energies as high as 1020 eV, where a proton-proton collision at the Large Hadron Collider occurs at an energy of ~1012 eV. The field can be said to have begun in 1910, when a German physicist named Theodor Wulf measured the ionization in the air, an indicator of gamma radiation, at the bottom and top of the Eiffel Tower. He found that there was far more ionization at the top than what was expected if only terrestrial sources were attributed for this radiation.",
                    "score": 0.8784106969833374
                },
                {
                    "id": 1485601,
                    "contents": "Geiger–Marsden experiments\nAs a worked example, consider an alpha particle passing along the edge of a gold atom, where it will experience the electric field at its strongest and thus experience the maximum deflection θ. Since the electrons are very light compared to the alpha particle, their influence can be neglected, so the atom can be seen as a heavy sphere of positive charge. Qn = positive charge of gold atom = = Qα = charge of alpha particle = = r = radius of a gold atom = vα = velocity of alpha particle = mα = mass of alpha particle = k = Coulomb's constant = Using classical physics, the alpha particle's lateral change in momentum Δp can be approximated using the impulse of force relationship and the Coulomb force expression:",
                    "score": 0.8775818943977356
                },
                {
                    "id": 17179316,
                    "contents": "Alpha particle\nBecause alpha particles occur naturally, but can have energy high enough to participate in a nuclear reaction, study of them led to much early knowledge of nuclear physics. Rutherford used alpha particles emitted by radium bromide to infer that J. J. Thomson's Plum pudding model of the atom was fundamentally flawed. In Rutherford's gold foil experiment conducted by his students Hans Geiger and Ernest Marsden, a narrow beam of alpha particles was established, passing through very thin (a few hundred atoms thick) gold foil. The alpha particles were detected by a zinc sulfide screen, which emits a flash of light upon an alpha particle collision. Rutherford hypothesized that, assuming the \"plum pudding\" model of the atom was correct, the positively charged alpha particles would be only slightly deflected, if at all, by the dispersed positive charge predicted.",
                    "score": 0.8771467208862305
                },
                {
                    "id": 5488556,
                    "contents": "Thermal de Broglie wavelength\nSince the energy levels are extremely close together, we can approximate this sum as an integral: Hence, where is the Planck constant, is the mass of a gas particle, is the Boltzmann constant, and is the temperature of the gas. This can also be expressed using the reduced Planck constant as Massless particles For massless (or highly relativistic) particles, the thermal wavelength is defined as where c is the speed of light. As with the thermal wavelength for massive particles, this is of the order of the average wavelength of the particles in the gas and defines a critical point at which quantum effects begin to dominate. For example, when observing the long-wavelength spectrum of black body radiation, the classical Rayleigh–Jeans law can be applied, but when the observed wavelengths approach the thermal wavelength of the photons in the black body radiator, the quantum Planck's law must be used. General definition",
                    "score": 0.8759975433349609
                },
                {
                    "id": 1162791,
                    "contents": "Particle physics\nSubatomic particles Modern particle physics research is focused on subatomic particles, including atomic constituents, such as electrons, protons, and neutrons (protons and neutrons are composite particles called baryons, made of quarks), that are produced by radioactive and scattering processes; such particles are photons, neutrinos, and muons, as well as a wide range of exotic particles. Dynamics of particles are also governed by quantum mechanics; they exhibit wave–particle duality, displaying particle-like behaviour under certain experimental conditions and wave-like behaviour in others. In more technical terms, they are described by quantum state vectors in a Hilbert space, which is also treated in quantum field theory. Following the convention of particle physicists, the term elementary particles is applied to those particles that are, according to current understanding, presumed to be indivisible and not composed of other particles.",
                    "score": 0.8754152059555054
                },
                {
                    "id": 10311466,
                    "contents": "Bethe formula\nThe formula For a particle with speed v, charge z (in multiples of the electron charge), and energy E, traveling a distance x into a target of electron number density n and mean excitation potential I, the relativistic version of the formula reads, in SI units: where c is the speed of light and ε0 the vacuum permittivity, , e and me the electron charge and rest mass respectively. Here, the electron density of the material can be calculated by where ρ is the density of the material, Z its atomic number, A its relative atomic mass, NA the Avogadro number and Mu the Molar mass constant. In the figure to the right, the small circles are experimental results obtained from measurements of various authors, while the red curve is Bethe's formula. Evidently, Bethe's theory agrees very well with experiment at high energy. The agreement is even better when corrections are applied (see below). For low energies, i.e., for small velocities of the particle β << 1, the Bethe formula reduces to",
                    "score": 0.8749884963035583
                },
                {
                    "id": 17179310,
                    "contents": "Alpha particle\nThe energy of alpha particles emitted varies, with higher energy alpha particles being emitted from larger nuclei, but most alpha particles have energies of between 3 and 7 MeV (mega-electron-volts), corresponding to extremely long and extremely short half-lives of alpha-emitting nuclides, respectively. The energies and ratios are often distinct and can be used to identify specific nuclides as in alpha spectrometry. With a typical kinetic energy of 5 MeV; the speed of emitted alpha particles is 15,000 km/s, which is 5% of the speed of light. This energy is a substantial amount of energy for a single particle, but their high mass means alpha particles have a lower speed than any other common type of radiation, e.g. β particles, neutrons.",
                    "score": 0.8739088177680969
                },
                {
                    "id": 22460208,
                    "contents": "Julius Ashkin\nIn 1953, with Bethe, his former director of theoretical work in Los Alamos, Ashkin published an article closely related to the work they had then done. This article, \"Passage of Radiations Through Matter,\" summarized the effects of particles and radiation as they passed through solids. In time it became a standard reference for physics experimenters. Using the CIT cyclotron and following on work done by Bethe and Robert E. Marshak, Ashkin conducted experiments to determine the characteristics of a short-lived particle — the pi-meson or pion — that is produced when high energy cosmic ray protons and other cosmic ray components interact with matter in the Earth's atmosphere. Ashkin served as chair of the physics department between 1961 and 1972. After he died, CMU created the Julius Ashkin Teaching Award in his honor.",
                    "score": 0.8738278746604919
                },
                {
                    "id": 17179317,
                    "contents": "Alpha particle\nIt was found that some of the alpha particles were deflected at much larger angles than expected (at a suggestion by Rutherford to check it) and some even bounced almost directly back. Although most of the alpha particles went straight through as expected, Rutherford commented that the few particles that were deflected was akin to shooting a fifteen-inch shell at tissue paper only to have it bounce off, again assuming the \"plum pudding\" theory was correct. It was determined that the atom's positive charge was concentrated in a small area in its center, making the positive charge dense enough to deflect any positively charged alpha particles that came close to what was later termed the nucleus.",
                    "score": 0.8736801147460938
                },
                {
                    "id": 5205024,
                    "contents": "Compton wavelength\nThe classical electron radius is about 3 times larger than the proton radius, and is written: The Rydberg constant, having dimensions of linear wavenumber, is written: This yields the sequence: . For fermions, the reduced Compton wavelength sets the cross-section of interactions. For example, the cross-section for Thomson scattering of a photon from an electron is equal to which is roughly the same as the cross-sectional area of an iron-56 nucleus. For gauge bosons, the Compton wavelength sets the effective range of the Yukawa interaction: since the photon has no mass, electromagnetism has infinite range. The Planck mass is the order of mass for which the Compton wavelength and the Schwarzschild radius are the same, when their value is close to the Planck length (). The Schwarzschild radius is proportional to the mass, whereas the Compton wavelength is proportional to the inverse of the mass. The Planck mass and length are defined by:",
                    "score": 0.8728704452514648
                },
                {
                    "id": 8502079,
                    "contents": "Stopping power (particle radiation)\nThe picture shows how the stopping power of 5.49 MeV alpha particles increases while the particle traverses air, until it reaches the maximum. This particular energy corresponds to that of the alpha particle radiation from naturally radioactive gas radon (222Rn) which is present in the air in minute amounts. The mean range can be calculated by integrating the reciprocal stopping power over energy: where: E0 is the initial kinetic energy of the particle Δx is the \"continuous slowing down approximation (CSDA)\" range and S(E) is the linear stopping power. The deposited energy can be obtained by integrating the stopping power over the entire path length of the ion while it moves in the material.",
                    "score": 0.872614324092865
                },
                {
                    "id": 1485618,
                    "contents": "Geiger–Marsden experiments\ns = the number of alpha particles falling on unit area at an angle of deflection Φ r = distance from point of incidence of α rays on scattering material X = total number of particles falling on the scattering material n = number of atoms in a unit volume of the material t = thickness of the foil Qn = positive charge of the atomic nucleus Qα = positive charge of the alpha particles m = mass of an alpha particle v = velocity of the alpha particle From the scattering data, Rutherford estimated the central charge Qn to be about +100 units (see Rutherford model)",
                    "score": 0.8712959289550781
                },
                {
                    "id": 1662444,
                    "contents": "Cross section (physics)\nwhile the impact area element is Using the relation for the solid angle in the spherical coordinates: and the trigonometric identity we obtain while the total cross section as we expected is As one can see, it also agrees with the result from the Example 1 if the photon is assumed to be a rigid sphere of zero radius. See also Cross section (geometry) Flow velocity Luminosity (scattering theory) Linear attenuation coefficient Mass attenuation coefficient Neutron cross section Nuclear cross section Gamma ray cross section Partial wave analysis Particle detector Radar cross-section Rutherford scattering Scattering amplitude References General references J. D. Bjorken, S. D. Drell, Relativistic Quantum Mechanics, 1964 P. Roman, Introduction to Quantum Theory, 1969 W. Greiner, J. Reinhardt, Quantum Electrodynamics, 1994 R. G. Newton. Scattering Theory of Waves and Particles. McGraw Hill, 1966.",
                    "score": 0.8707016110420227
                },
                {
                    "id": 1680388,
                    "contents": "Diffraction\nParticle diffraction According to quantum theory every particle exhibits wave properties. In particular, massive particles can interfere with themselves and therefore diffract. Diffraction of electrons and neutrons stood as one of the powerful arguments in favor of quantum mechanics. The wavelength associated with a particle is the de Broglie wavelength where h is Planck's constant and p is the momentum of the particle (mass × velocity for slow-moving particles). For most macroscopic objects, this wavelength is so short that it is not meaningful to assign a wavelength to them. A sodium atom traveling at about 30,000 m/s would have a De Broglie wavelength of about 50 pico meters.",
                    "score": 0.8702735900878906
                },
                {
                    "id": 11978413,
                    "contents": "Quantum concentration\nThe quantum concentration is the particle concentration (i.e. the number of particles per unit volume) of a system where the interparticle distance is equal to the thermal de Broglie wavelength. Quantum effects become appreciable when the particle concentration is greater than or equal to the quantum concentration, which is defined as: where: is the mass of the particles in the system is the Boltzmann constant is the temperature as measured in kelvins is the reduced Planck constant The quantum concentration for room temperature protons is about 1/cubic-Angstrom. As the quantum concentration depends on temperature, high temperatures will put most systems in the classical limit unless they have a very high density e.g. a White dwarf. For an ideal gas the Sackur–Tetrode equation can be written in terms of the quantum concentration as References Statistical mechanics",
                    "score": 0.8699706196784973
                },
                {
                    "id": 1208589,
                    "contents": "Rutherford scattering\nHowever, the intriguing results showed that around 1 in 20,000 alpha particles were deflected by very large angles (over 90°), while the rest passed through with little deflection. From this, Rutherford concluded that the majority of the mass was concentrated in a minute, positively-charged region (the nucleus) surrounded by electrons. When a (positive) alpha particle approached sufficiently close to the nucleus, it was repelled strongly enough to rebound at high angles. The small size of the nucleus explained the small number of alpha particles that were repelled in this way. Rutherford showed, using the method outlined below, that the size of the nucleus was less than about (how much less than this size, Rutherford could not tell from this experiment alone; see more below on this problem of lowest possible size). As a visual example, Figure 1 shows the deflection of an alpha particle by a nucleus in the gas of a cloud chamber.",
                    "score": 0.8698974847793579
                },
                {
                    "id": 5205017,
                    "contents": "Compton wavelength\nThe CODATA 2018 value for the Compton wavelength of the electron is . Other particles have different Compton wavelengths. Reduced Compton wavelength When the Compton wavelength is divided by , one obtains the \"reduced\" Compton wavelength (barred lambda), i.e. the Compton wavelength for radian instead of radians: where is the \"reduced\" Planck constant. Role in equations for massive particles The inverse reduced Compton wavelength is a natural representation for mass on the quantum scale, and as such, it appears in many of the fundamental equations of quantum mechanics. The reduced Compton wavelength appears in the relativistic Klein–Gordon equation for a free particle: It appears in the Dirac equation (the following is an explicitly covariant form employing the Einstein summation convention):",
                    "score": 0.8693613409996033
                },
                {
                    "id": 26107629,
                    "contents": "Uwe Thumm\nThumm's research focuses on numerical modeling interactions of intense pulses of laser light and of particle beams with atoms, small molecules, clusters, nanoparticles, surfaces, and thin films. He has written and co-authored more than 107 publications in peer-reviewed journals, three patents, and more than 280 non-refereed publications, abstracts, and press releases. His research has contributed to different areas of physics with noteworthy broader impacts, including: Light–matter interactions (work on laser-induced and laser-probed nuclear dynamics in molecules, coherent control, electronic dynamics in atoms and surfaces). Broader impacts: solar energy conversion, novel light sources, light detection. Highly-charged ion physics. Broader impacts: ion-lithography, controlled thermonuclear fusion, particle detection.",
                    "score": 0.8692065477371216
                },
                {
                    "id": 9810434,
                    "contents": "Threshold energy\nIn particle physics, the threshold energy for production of a particle is the minimum kinetic energy a pair of traveling particles must have when they collide. The threshold energy is always greater than or equal to the rest energy of the desired particle. In most cases, since momentum is also conserved, the threshold energy is significantly greater than the rest energy of the desired particle - and thus there will still be considerable kinetic energy in the final particles. The threshold energy should not be confused with the threshold displacement energy, which is the minimum energy needed to permanently displace an atom in a crystal to produce a crystal defect in radiation material science. Example Consider the collision of a mobile proton with a stationary proton so that a meson is produced:",
                    "score": 0.8689242601394653
                },
                {
                    "id": 10311467,
                    "contents": "Bethe formula\nFor low energies, i.e., for small velocities of the particle β << 1, the Bethe formula reduces to This can be seen by first replacing βc by v in eq. (1) and then neglecting β2 because of its small size. At low energy, the energy loss according to the Bethe formula therefore decreases approximately as v−2 with increasing energy. It reaches a minimum for approximately E = 3Mc2, where M is the mass of the particle (for protons, this would be about at 3000 MeV). For highly relativistic cases β ≈ 1, the energy loss increases again, logarithmically due to the transversal component of the electric field. The mean excitation potential In the Bethe theory, the material is completely described by a single number, the mean excitation potential I. In 1933 Felix Bloch showed that the mean ionization potential of atoms is approximately given by",
                    "score": 0.8684262037277222
                },
                {
                    "id": 1485617,
                    "contents": "Geiger–Marsden experiments\nRutherford mathematically models the scattering pattern Considering the results of the above experiments, Rutherford published a landmark paper in 1911 titled \"The Scattering of α and β Particles by Matter and the Structure of the Atom\" wherein he proposed that the atom contains at its center a volume of electric charge that is very small and intense (in fact, Rutherford treats it as a point charge in his calculations). For the purpose of his mathematical calculations he assumed this central charge was positive, but he admitted he could not prove this and that he had to wait for other experiments to develop his theory. Rutherford developed a mathematical equation that modeled how the foil should scatter the alpha particles if all the positive charge and most of the atomic mass was concentrated in a single point at the center of an atom.",
                    "score": 0.8682255148887634
                },
                {
                    "id": 4087929,
                    "contents": "Length scale\nExamples The atomic length scale is meters and is given by the size of hydrogen atom (i.e., the Bohr radius (approximately 53 pm)) which is set by the electron's Compton wavelength times the fine-structure constant: . The length scale for the strong interactions (or the one derived from QCD through dimensional transmutation) is around meters (or in natural units 1000 MeV or 1 GeV), and the \"radii\" of strongly interacting particles (such as the proton) are roughly comparable. This length scale is determined by the range of the Yukawa potential. The lifetimes of strongly interacting particles, such as the rho meson, are given by this length scale divided by the speed of light: seconds. The masses of strongly interacting particles are several times the associated energy scale (500 MeV to 3000 MeV).",
                    "score": 0.8681739568710327
                },
                {
                    "id": 3882728,
                    "contents": "Klein–Nishina formula\nThe Klein–Nishina formula gives the differential cross section of photons scattered from a single free electron in lowest order of quantum electrodynamics. At low frequencies (e.g., visible light) this yields Thomson scattering; at higher frequencies (e.g., x-rays and gamma-rays) this yields Compton scattering. For an incident unpolarized photon of energy , the differential cross section is: where is a differential cross section, is an infinitesimal solid angle element, is the fine structure constant (~1/137.04), is the scattering angle; is the \"reduced\" Compton wave length of the electron (~0.38616 pm); is the mass of an electron (~511 keV); and is the ratio of photon energy after and before the collision: Note that this result may also be expressed in terms of the classical electron radius :",
                    "score": 0.8676187992095947
                },
                {
                    "id": 1566371,
                    "contents": "Alpha decay\nAlpha particles have a typical kinetic energy of 5 MeV (or ≈ 0.13% of their total energy, 110 TJ/kg) and have a speed of about 15,000,000 m/s, or 5% of the speed of light. There is surprisingly small variation around this energy, due to the heavy dependence of the half-life of this process on the energy produced. Because of their relatively large mass, the electric charge of and relatively low velocity, alpha particles are very likely to interact with other atoms and lose their energy, and their forward motion can be stopped by a few centimeters of air. Approximately 99% of the helium produced on Earth is the result of the alpha decay of underground deposits of minerals containing uranium or thorium. The helium is brought to the surface as a by-product of natural gas production. History",
                    "score": 0.8675298690795898
                },
                {
                    "id": 5380698,
                    "contents": "Attenuation length\nIn physics, the attenuation length or absorption length is the distance into a material when the probability has dropped to that a particle has not been absorbed. Alternatively, if there is a beam of particles incident on the material, the attenuation length is the distance where the intensity of the beam has dropped to , or about 63% of the particles have been stopped. Mathematically, the probability of finding a particle at depth x into the material is calculated by Beer–Lambert law: . In general is material and energy dependent. See also Beer's Law Mean free path Attenuation coefficient Attenuation (electromagnetic radiation) Radiation length References https://web.archive.org/web/20050215215652/http://www.ct.infn.it/~rivel/Glossario/node2.html External links http://henke.lbl.gov/optical_constants/atten2.html Particle physics Experimental particle physics",
                    "score": 0.8675264120101929
                },
                {
                    "id": 23962586,
                    "contents": "List of examples of lengths\nmetres = 1 am = 1 attometre = zeptometres 1 am — sensitivity of the LIGO detector for gravitational waves metres = 10 am metres = 100 am 0.85 fm — approximate proton radius 1 fm to 1 pm metres = 1 fm = 1 femtometre = attometres 1.5 fm — diameter of the Scattering Cross Section of an 11 MeV proton with a target proton — classical electron radius 7 fm - the radius of the effective scattering cross section for a gold nucleus scattering a 6 MeV alpha particle over 140 degrees metres = 10 fm metres = 100 fm metres = 1 pm = 1 picometre = femtometres 1 picometre Lengths between 10−12 and 10−11 m (1 and 10 pm). 1 pm = 1 picometre = femtometres 1 pm = distance between atomic nuclei in a white dwarf star 2.4 pm — The Compton wavelength of the electron. 5 pm — shorter X-ray wavelengths (approx.)",
                    "score": 0.8668512105941772
                },
                {
                    "id": 5205019,
                    "contents": "Compton wavelength\nDividing through by , and rewriting in terms of the fine structure constant, one obtains: Distinction between reduced and non-reduced The reduced Compton wavelength is a natural representation of mass on the quantum scale. Equations that pertain to inertial mass like Klein-Gordon and Schrödinger's, use the reduced Compton wavelength. The non-reduced Compton wavelength is a natural representation for mass that has been converted into energy. Equations that pertain to the conversion of mass into energy, or to the wavelengths of photons interacting with mass, use the non-reduced Compton wavelength. A particle of mass has a rest energy of . The non-reduced Compton wavelength for this particle is the wavelength of a photon of the same energy. For photons of frequency , energy is given by which yields the non-reduced or standard Compton wavelength formula if solved for .",
                    "score": 0.8668268918991089
                },
                {
                    "id": 10723318,
                    "contents": "Alpha-particle spectroscopy\nThe size of T is dependent on the ratio of masses of the products and due to the conservation of momentum (the parent's momentum = 0 at the moment of decay) this can be calculated: and , The alpha particle, or 4He nucleus, is an especially strongly bound particle. This combined with the fact that the binding energy per nucleon has a maximum value near A=56 and systematically decreases for heavier nuclei, creates the situation that nuclei with A>150 have positive Qα-values for the emission of alpha particles. For example, one of the heaviest naturally occurring isotopes, ^238U -> ^234Th + ^4He (ignoring charges): Qα = -931.5 (234.043 601 + 4.002 603 254 13 - 238.050 788 2) = 4.2699 MeV Note that the decay energy will be divided between the alpha-particle and the heavy recoiling daughter so that the kinetic energy of the alpha particle (Tα) will be slightly less:",
                    "score": 0.8668114542961121
                },
                {
                    "id": 5186109,
                    "contents": "Hanbury Brown and Twiss effect\nAlso, in the field of particle physics, Goldhaber et al. performed an experiment in 1959 in Berkeley and found an unexpected angular correlation among identical pions, discovering the ρ0 resonance, by means of decay. From then on, the HBT technique started to be used by the heavy-ion community to determine the space–time dimensions of the particle emission source for heavy-ion collisions. For recent developments in this field, see for example the review article by Lisa. Wave mechanics The HBT effect can, in fact, be predicted solely by treating the incident electromagnetic radiation as a classical wave. Suppose we have a monochromatic wave with frequency on two detectors, with an amplitude that varies on timescales slower than the wave period . (Such a wave might be produced from a very distant point source with a fluctuating intensity.) Since the detectors are separated, say the second detector gets the signal delayed by a time , or equivalently, a phase ; that is,",
                    "score": 0.8667095303535461
                },
                {
                    "id": 20803,
                    "contents": "Subatomic particle\nIn physical sciences, a subatomic particle is a particle that is smaller than an atom. According to the Standard Model of particle physics, a subatomic particle can be either a composite particle, which is composed of other particles (for example, a proton, neutron, or meson), or an elementary particle, which is not composed of other particles (for example, an electron, photon, or muon). Particle physics and nuclear physics study these particles and how they interact. Experiments show that light could behave like a stream of particles (called photons) as well as exhibiting wave-like properties. This led to the concept of wave–particle duality to reflect that quantum-scale behave like both particles and waves (they are sometimes described as waveicles to reflect this).",
                    "score": 0.8665981292724609
                },
                {
                    "id": 597137,
                    "contents": "Mean free path\nIn physics, mean free path is an average distance over which a moving particle (such as an atom, a molecule, a photon) substantially changes its direction or energy (or, in a specific context, other properties), typically as a result of one or more successive collisions with other particles. Scattering theory Imagine a beam of particles being shot through a target, and consider an infinitesimally thin slab of the target (see the figure). The atoms (or particles) that might stop a beam particle are shown in red. The magnitude of the mean free path depends on the characteristics of the system. Assuming that all the target particles are at rest but only the beam particle is moving, that gives an expression for the mean free path: where is the mean free path, is the number of target particles per unit volume, and is the effective cross-sectional area for collision.",
                    "score": 0.8665745258331299
                },
                {
                    "id": 1485615,
                    "contents": "Geiger–Marsden experiments\nmeasure where the flashes of light appeared on the screen and thus calculate the particles' angles of deflection. The alpha particles emitted from A was narrowed to a beam by a small circular hole at D. Geiger placed a metal foil in the path of the rays at D and E to observe how the zone of flashes changed. He could also vary the velocity of the alpha particles by placing extra sheets of mica or aluminium at A.",
                    "score": 0.8663995862007141
                },
                {
                    "id": 11261969,
                    "contents": "Scattering length\nThe scattering length in quantum mechanics describes low-energy scattering. For potentials that decay faster than as , it is defined as the following low-energy limit: where is the scattering length, is the wave number, and is the phase shift of the outgoing spherical wave. The elastic cross section, , at low energies is determined solely by the scattering length:",
                    "score": 0.8661843538284302
                },
                {
                    "id": 5606481,
                    "contents": "History of chemistry\nHowever, the actual results surprised Rutherford. Although many of the alpha particles did pass through as expected, many others were deflected at small angles while others were reflected back to the alpha source. They observed that a very small percentage of particles were deflected through angles much larger than 90 degrees. The gold foil experiment showed large deflections for a small fraction of incident particles. Rutherford realized that, because some of the alpha particles were deflected or reflected, the atom had a concentrated centre of positive charge and of relatively large mass - Rutherford later termed this positive center the \"atomic nucleus\". The alpha particles had either hit the positive centre directly or passed by it close enough to be affected by its positive charge. Since many other particles passed through the gold foil, the positive centre would have to be a relatively small size compared to the rest of the atom - meaning that the atom is mostly open space. From",
                    "score": 0.8660550117492676
                },
                {
                    "id": 1693515,
                    "contents": "Electromagnetic radiation\nAt the quantum level, electromagnetic radiation is produced when the wavepacket of a charged particle oscillates or otherwise accelerates. Charged particles in a stationary state do not move, but a superposition of such states may result in a transition state that has an electric dipole moment that oscillates in time. This oscillating dipole moment is responsible for the phenomenon of radiative transition between quantum states of a charged particle. Such states occur (for example) in atoms when photons are radiated as the atom shifts from one stationary state to another. As a wave, light is characterized by a velocity (the speed of light), wavelength, and frequency. As particles, light is a stream of photons. Each has an energy related to the frequency of the wave given by Planck's relation E = hf, where E is the energy of the photon, h is Planck's constant, 6.626 × 10−34 J·s, and f is the frequency of the wave.",
                    "score": 0.8660240173339844
                },
                {
                    "id": 1561013,
                    "contents": "Alpha\nMathematics and science The letter alpha represents various concepts in physics and chemistry, including alpha radiation, angular acceleration, alpha particles, alpha carbon and strength of electromagnetic interaction (as Fine-structure constant). Alpha also stands for thermal expansion coefficient of a compound in physical chemistry. It is also commonly used in mathematics in algebraic solutions representing quantities such as angles. Furthermore, in mathematics, the letter alpha is used to denote the area underneath a normal curve in statistics to denote significance level when proving null and alternative hypotheses. In ethology, it is used to name the dominant individual in a group of animals. In aerodynamics, the letter is used as a symbol for the angle of attack of an aircraft and the word \"alpha\" is used as a synonym for this property. In mathematical logic, α is sometimes used as a placeholder for ordinal numbers.",
                    "score": 0.8660169243812561
                },
                {
                    "id": 1106319,
                    "contents": "Timeline of atomic and subatomic physics\n1902 Theodor Svedberg suggests that fluctuations in molecular bombardment cause the Brownian motion 1905 Albert Einstein explains the photoelectric effect 1906 Charles Barkla discovers that each element has a characteristic X-ray and that the degree of penetration of these X-rays is related to the atomic weight of the element 1909 Hans Geiger and Ernest Marsden discover large angle deflections of alpha particles by thin metal foils 1909 Ernest Rutherford and Thomas Royds demonstrate that alpha particles are doubly ionized helium atoms 1911 Ernest Rutherford explains the Geiger–Marsden experiment by invoking a nuclear atom model and derives the Rutherford cross section 1911 Jean Perrin proves the existence of atoms and molecules with experimental work to test Einstein's theoretical explanation of Brownian motion 1911 Ștefan Procopiu measures the magnetic dipole moment of the electron 1912 Max von Laue suggests using crystal lattices to diffract X-rays",
                    "score": 0.8659499287605286
                },
                {
                    "id": 1590142,
                    "contents": "Atomic theory\nconcentrated enough to produce an electric field strong enough to deflect an alpha particle, and the electrons are so lightweight they should be pushed aside effortlessly by the much heavier alpha particles. Yet there was scattering, so Rutherford and his colleagues decided to investigate this scattering carefully.",
                    "score": 0.865908145904541
                },
                {
                    "id": 22831098,
                    "contents": "Index of physics articles (A)\nAll gas-phase iodine laser Allais effect Allan Blaer Allan Boardman Allan Carswell Allan Mackintosh Allan McLeod Cormack Allan V. Cox Allen Shenstone Allyn Vine Allyne L. Merrill Aloyzas Sakalas Alpha-particle spectroscopy Alpha & Omega (book) Alpha Magnetic Spectrometer Alpha decay Alpha factor Alpha particle Alpha process Alpher–Bethe–Gamow paper Alternating Gradient Synchrotron Alternatives to general relativity Alternatives to the Standard Model Higgs Aluminium gallium phosphide Alv Egeland Alvarez Physics Memo Alvarez Physics Memos Alvin Andreas Herborg Nielsen Alvin C. Graves Alvin M. Weinberg Alvin Radkowsky Alwyn Van der Merwe Amagat Amagat's law Amal Kumar Raychaudhuri Amasa Stone Bishop Ambient noise level Ambient pressure Ambipolar diffusion Amer Iqbal American Association of Physicists in Medicine American Association of Physics Teachers American Astron. Soc. 215th Meeting American Astron. Soc. Meeting American Astron Soc 215th Meeting American Astronomical Society",
                    "score": 0.8655158281326294
                },
                {
                    "id": 5205021,
                    "contents": "Compton wavelength\nThis argument also shows that the reduced Compton wavelength is the cutoff below which quantum field theory – which can describe particle creation and annihilation – becomes important. The above argument can be made a bit more precise as follows. Suppose we wish to measure the position of a particle to within an accuracy . Then the uncertainty relation for position and momentum says that so the uncertainty in the particle's momentum satisfies",
                    "score": 0.8653818964958191
                },
                {
                    "id": 28050186,
                    "contents": "David B. Cline\nHigh-Energy Scattering, by David B. Cline and Vernon D. Barger and published in the December 1967 edition of Scientific American explains what it takes to study the properties of fundamental particles. By accelerating and colliding particles at high energies, researchers are able to observe the fundamental particles that composed the previous particles using a bubble chamber.",
                    "score": 0.865340530872345
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.4
            }
        }
    },
    {
        "id": "test_24",
        "question": "Calculate the number of photons in a $2.00 \\mathrm{~mJ}$ light pulse at (a) $1.06 \\mu \\mathrm{m}$\r\n",
        "golden_answers": [
            " 1.07"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1168440,
                    "contents": "Photon\nwhere is the rate constant for emitting a photon spontaneously, and is the rate constant for emissions in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state and those in state must, on average, be constant; hence, the rates and must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of and is where and are the degeneracy of the state and that of , respectively, and their energies, the Boltzmann constant and the system's temperature. From this, it is readily derived that and The and are collectively known as the Einstein coefficients.",
                    "score": 0.8846111297607422
                },
                {
                    "id": 23429417,
                    "contents": "Photon statistics\nWith the probability distribution above, we can now find the average intensity of the field (where several constants have been omitted for clarity) The instantaneous intensity of the field is given by Because the electric field and thus the intensity are dependent on the stochastic complex variable . The probability of obtaining an intensity in between and is where is the infinitesimal element on the complex plane. This infinitesimal element can be rewritten as The above intensity distribution can now be written as This last expression represents the intensity distribution for thermal light. The last step in showing thermal light satisfies the variance condition for super-Poisson statistics is to use Mandel's formula. The formula describes the probability of observing n photon counts and is given by",
                    "score": 0.8825493454933167
                },
                {
                    "id": 18326096,
                    "contents": "Photon energy\nThe photon energy at 1 μm wavelength, the wavelength of near infrared radiation, is approximately 1.2398 eV. In chemistry, quantum physics and optical engineering See where E is photon energy (joules), h is the Planck constant - 6.62607015 × 10−34 (m2kgs−1) The Greek letter ν (nu) is the photon's frequency. Examples An FM radio station transmitting at 100 MHz emits photons with an energy of about 4.1357 × 10−7 eV. This minuscule amount of energy is approximately 8 × 10−13 times the electron's mass (via mass-energy equivalence). Very-high-energy gamma rays have photon energies of 100 GeV to over 1 PeV (1011 to 1015 electronvolts) or 16 nanojoules to 160 microjoules. This corresponds to frequencies of 2.42 × 1025 to 2.42 × 1029 Hz.",
                    "score": 0.8778901100158691
                },
                {
                    "id": 381864,
                    "contents": "Planck's law\nIt was not till 1919 that Planck in the third edition of his monograph more or less accepted his 'third theory', that both emission and absorption of light were quantal. The colourful term \"ultraviolet catastrophe\" was given by Paul Ehrenfest in 1911 to the paradoxical result that the total energy in the cavity tends to infinity when the equipartition theorem of classical statistical mechanics is (mistakenly) applied to black-body radiation. But this had not been part of Planck's thinking, because he had not tried to apply the doctrine of equipartition: when he made his discovery in 1900, he had not noticed any sort of \"catastrophe\". It was first noted by Lord Rayleigh in 1900, and then in 1901 by Sir James Jeans; and later, in 1905, by Einstein when he wanted to support the idea that light propagates as discrete packets, later called 'photons', and by Rayleigh and by Jeans.",
                    "score": 0.876103937625885
                },
                {
                    "id": 18106721,
                    "contents": "Single-photon source\nSingle-photon sources generate single-photon states as described above. In other words, ideal single-photon sources generate radiation with a photon-number distribution that has a mean one and variance zero.",
                    "score": 0.8740068674087524
                },
                {
                    "id": 1168424,
                    "contents": "Photon\nThe Maxwell wave theory, however, does not account for all properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.",
                    "score": 0.8736553192138672
                },
                {
                    "id": 907505,
                    "contents": "Intensity (physics)\nand the local intensity is obtained by multiplying this expression by the wave velocity, c/n: where n is the refractive index, c is the speed of light in vacuum and is the vacuum permittivity. For non-monochromatic waves, the intensity contributions of different spectral components can simply be added. The treatment above does not hold for arbitrary electromagnetic fields. For example, an evanescent wave may have a finite electrical amplitude while not transferring any power. The intensity should then be defined as the magnitude of the Poynting vector.",
                    "score": 0.8732821345329285
                },
                {
                    "id": 18326095,
                    "contents": "Photon energy\nFormulas Physics Photon energy is directly proportional to frequency. where is energy (J) is Planck's constant: 6.62607015 × 10−34 (m2kgs−1) is frequency (Hz) This equation is known as the Planck-Einstein relation. Additionally, where E is photon energy (Joules), λ is the photon's wavelength (metres), c is the speed of light in vacuum: 299792458 metres per second h is the Planck constant: 6.62607015 × 10−34 (m2kgs−1) The photon energy at 1 Hz is equal to 6.62607015 × 10−34 J That is equal to 4.135667697 × 10−15 eV (electronvolts) Electronvolts Energy is often measured in electronvolts. To find the photon energy in electronvolts using the wavelength in micrometres, the equation is approximately This equation only holds if the wavelength is measured in picometers. The photon energy at 1 μm wavelength, the wavelength of near infrared radiation, is approximately 1.2398 eV. In chemistry, quantum physics and optical engineering See",
                    "score": 0.8722913861274719
                },
                {
                    "id": 18326094,
                    "contents": "Photon energy\nPhoton energy is the energy carried by a single photon. The amount of energy is directly proportional to the photon's electromagnetic frequency and thus, equivalently, is inversely proportional to the wavelength. The higher the photon's frequency, the higher its energy. Equivalently, the longer the photon's wavelength, the lower its energy. Photon energy can be expressed using any unit of energy. Among the units commonly used to denote photon energy are the electronvolt (eV) and the joule (as well as its multiples, such as the microjoule). As one joule equals 6.24 × 1018 eV, the larger units may be more useful in denoting the energy of photons with higher frequency and higher energy, such as gamma rays, as opposed to lower energy photons, such as those in the radio frequency region of the electromagnetic spectrum. Formulas Physics Photon energy is directly proportional to frequency. where is energy (J) is Planck's constant: 6.62607015 × 10−34 (m2kgs−1) is frequency (Hz)",
                    "score": 0.8716753125190735
                },
                {
                    "id": 23429414,
                    "contents": "Photon statistics\nwhere is the frequency of the field and is a time independent phase shift. The analogue in quantum mechanics is the coherent state By projecting the coherent state onto the Fock state , we can find the probability of finding photons using the Born rule, which gives The above result is a Poissonian distribution with which is a distinct feature of the coherent state. Super-Poissonian Light",
                    "score": 0.8713576793670654
                },
                {
                    "id": 1680365,
                    "contents": "Diffraction\nmust propagate as waves. Augustin-Jean Fresnel did more definitive studies and calculations of diffraction, made public in 1816 and 1818, and thereby gave great support to the wave theory of light that had been advanced by Christiaan Huygens and reinvigorated by Young, against Newton's particle theory.",
                    "score": 0.8686484694480896
                },
                {
                    "id": 3167414,
                    "contents": "Wave packet\nThe photon's energy is equal to Planck's constant, , multiplied by its frequency, . This resolved a problem in classical physics, called the ultraviolet catastrophe. The ideas of quantum mechanics continued to be developed throughout the 20th century. The picture that was developed was of a particulate world, with all phenomena and matter made of and interacting with discrete particles; however, these particles were described by a probability wave. The interactions, locations, and all of physics would be reduced to the calculations of these probability amplitudes.",
                    "score": 0.8677916526794434
                },
                {
                    "id": 1168439,
                    "contents": "Photon\nEinstein began by postulating simple proportionality relations for the different reaction rates involved. In his model, the rate for a system to absorb a photon of frequency and transition from a lower energy to a higher energy is proportional to the number of atoms with energy and to the energy density of ambient photons of that frequency, where is the rate constant for absorption. For the reverse process, there are two possibilities: spontaneous emission of a photon, or the emission of a photon initiated by the interaction of the atom with a passing photon and the return of the atom to the lower-energy state. Following Einstein's approach, the corresponding rate for the emission of photons of frequency and transition from a higher energy to a lower energy is",
                    "score": 0.8671927452087402
                },
                {
                    "id": 23429415,
                    "contents": "Photon statistics\nThe above result is a Poissonian distribution with which is a distinct feature of the coherent state. Super-Poissonian Light Light that is governed by super-Poissonian statistics exhibits a statistical distribution with variance . An example of light that exhibits super-Poissonian statistics is thermal light. The intensity of thermal light fluctuates randomly and the fluctuations give rise to super-Poissonian statistics, as shown below by calculating the distribution of the intensity fluctuations. Using the intensity distribution together with Mandel's formula which describes the probability of the number of photon counts registered by a photodetector, the statistical distribution of photons in thermal light can be obtained. Thermal light can be modeled as a collection of harmonic oscillators. Suppose the -th oscillator emits an electromagnetic field with phase . Using the theory of superposition of fields, the total field produced by the oscillators is",
                    "score": 0.8669729232788086
                },
                {
                    "id": 3271381,
                    "contents": "Quantum optics\na single atom emitting one photon at a time, further compelling evidence that light consists of photons. Previously unknown quantum states of light with characteristics unlike classical states, such as squeezed light were subsequently discovered.",
                    "score": 0.8665213584899902
                },
                {
                    "id": 381808,
                    "contents": "Planck's law\nThat is, 0.01% of the radiation is at a wavelength below µm, 20% below , etc. The wavelength and frequency peaks are in bold and occur at 25.0% and 64.6% respectively. The 41.8% point is the wavelength-frequency-neutral peak (i.e. the peak in power per unit change in logarithm of wavelength or frequency). These are the points at which the respective Planck-law functions , and , respectively, divided by attain their maxima. The much smaller gap in ratio of wavelengths between 0.1% and 0.01% (1110 is 22% more than 910) than between 99.9% and 99.99% (113374 is 120% more than 51613) reflects the exponential decay of energy at short wavelengths (left end) and polynomial decay at long.",
                    "score": 0.866269052028656
                },
                {
                    "id": 23429413,
                    "contents": "Photon statistics\nThree regimes of statistical distributions can be obtained depending on the properties of the light source: Poissonian, super-Poissonian, and sub-Poissonian. The regimes are defined by the relationship between the variance and average number of photon counts for the corresponding distribution. Both Poissonian and super-Poissonian light can be described by a semi-classical theory in which the light source is modeled as an electromagnetic wave and the atom is modeled according to quantum mechanics. In contrast, sub-Poissonian light requires the quantization of the electromagnetic field for a proper description and thus is a direct measure of the particle nature of light. Poissonian Light In classical electromagnetic theory, an ideal source of light with constant intensity can be modeled by a spatially and temporally coherent electromagnetic wave of a single frequency. Such a light source can be modeled by, where is the frequency of the field and is a time independent phase shift.",
                    "score": 0.8657429814338684
                },
                {
                    "id": 381814,
                    "contents": "Planck's law\nIf we measure the energy relative to the ground state, the total energy in the box follows by summing over all allowed single photon states. This can be done exactly in the thermodynamic limit as approaches infinity. In this limit, becomes continuous and we can then integrate over this parameter. To calculate the energy in the box in this way, we need to evaluate how many photon states there are in a given energy range. If we write the total number of single photon states with energies between and as , where is the density of states (which is evaluated below), then we can write: To calculate the density of states we rewrite equation () as follows: where is the norm of the vector :",
                    "score": 0.8656076788902283
                },
                {
                    "id": 1168412,
                    "contents": "Photon\nIn physics, a photon is usually denoted by the symbol (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by , which is the photon energy, where is Planck constant and the Greek letter (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by , where its frequency is denoted by .",
                    "score": 0.8654235601425171
                },
                {
                    "id": 5186105,
                    "contents": "Hanbury Brown and Twiss effect\nThis result was met with much skepticism in the physics community. The radio astronomy result was justified by Maxwell's equations, but there were concerns that the effect should break down at optical wavelengths, since the light would be quantised into a relatively small number of photons that induce discrete photoelectrons in the detectors. Many physicists worried that the correlation was inconsistent with the laws of thermodynamics. Some even claimed that the effect violated the uncertainty principle. Hanbury Brown and Twiss resolved the dispute in a neat series of articles (see References below) that demonstrated, first, that wave transmission in quantum optics had exactly the same mathematical form as Maxwell's equations, albeit with an additional noise term due to quantisation at the detector, and second, that according to Maxwell's equations, intensity interferometry should work. Others, such as Edward Mills Purcell immediately supported the technique, pointing out that the",
                    "score": 0.8645939826965332
                },
                {
                    "id": 7747990,
                    "contents": "Introduction to quantum mechanics\nFor centuries, scientists had debated between two possible theories of light: was it a wave or did it instead comprise a stream of tiny particles? By the 19th century, the debate was generally considered to have been settled in favor of the wave theory, as it was able to explain observed effects such as refraction, diffraction, interference, and polarization. James Clerk Maxwell had shown that electricity, magnetism, and light are all manifestations of the same phenomenon: the electromagnetic field. Maxwell's equations, which are the complete set of laws of classical electromagnetism, describe light as waves: a combination of oscillating electric and magnetic fields. Because of the preponderance of evidence in favor of the wave theory, Einstein's ideas were met initially with great skepticism. Eventually, however, the photon model became favored. One of the most significant pieces of evidence in its favor was its ability to explain several puzzling properties of the photoelectric",
                    "score": 0.8633750677108765
                },
                {
                    "id": 7747989,
                    "contents": "Introduction to quantum mechanics\nPhotons: the quantization of light In 1905, Albert Einstein took an extra step. He suggested that quantization was not just a mathematical construct, but that the energy in a beam of light actually occurs in individual packets, which are now called photons. The energy of a single photon of light of frequency is given by the frequency multiplied by Planck's constant (an extremely tiny positive number):",
                    "score": 0.8630493879318237
                },
                {
                    "id": 7747995,
                    "contents": "Introduction to quantum mechanics\nEinstein explained the effect by postulating that a beam of light is a stream of particles (\"photons\") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron. To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the work function and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:",
                    "score": 0.8628957867622375
                },
                {
                    "id": 1168441,
                    "contents": "Photon\nThe and are collectively known as the Einstein coefficients. Einstein could not fully justify his rate equations, but claimed that it should be possible to calculate the coefficients , and once physicists had obtained \"mechanics and electrodynamics modified to accommodate the quantum hypothesis\". Not long thereafter, in 1926, Paul Dirac derived the rate constants by using a semiclassical approach, and, in 1927, succeeded in deriving all the rate constants from first principles within the framework of quantum theory. Dirac's work was the foundation of quantum electrodynamics, i.e., the quantization of the electromagnetic field itself. Dirac's approach is also called second quantization or quantum field theory; earlier quantum mechanical treatments only treat material particles as quantum mechanical, not the electromagnetic field.",
                    "score": 0.8626459240913391
                },
                {
                    "id": 1168410,
                    "contents": "Photon\nNomenclature The word quanta (singular quantum, Latin for how much) was used before 1900 to mean particles or amounts of different quantities, including electricity. In 1900, the German physicist Max Planck was studying black-body radiation, and he suggested that the experimental observations, specifically at shorter wavelengths, would be explained if the energy stored within a molecule was a \"discrete quantity composed of an integral number of finite equal parts\", which he called \"energy elements\". In 1905, Albert Einstein published a paper in which he proposed that many light-related phenomena—including black-body radiation and the photoelectric effect—would be better explained by modelling electromagnetic waves as consisting of spatially localized, discrete wave-packets. He called such a wave-packet the light quantum (German: das Lichtquant).",
                    "score": 0.8622688055038452
                },
                {
                    "id": 381815,
                    "contents": "Planck's law\nTo calculate the density of states we rewrite equation () as follows: where is the norm of the vector : For every vector with integer components larger than or equal to zero, there are two photon states. This means that the number of photon states in a certain region of -space is twice the volume of that region. An energy range of corresponds to shell of thickness in -space. Because the components of have to be positive, this shell spans an octant of a sphere. The number of photon states , in an energy range , is thus given by: Inserting this in Eq. () gives: From this equation one can derive the spectral energy density as a function of frequency and as a function of wavelength : where And: where",
                    "score": 0.8619787096977234
                },
                {
                    "id": 4757586,
                    "contents": "QED: The Strange Theory of Light and Matter\nThe four lectures 1. Photons - Corpuscles of Light In the first lecture, which acts as a gentle lead-in to the subject of quantum electrodynamics, Feynman describes the basic properties of photons. He discusses how to measure the probability that a photon will reflect or transmit through a partially reflective piece of glass. 2. Fits of Reflection and Transmission - Quantum Behaviour In the second lecture, Feynman looks at the different paths a photon can take as it travels from one point to another and how this affects phenomena like reflection and diffraction. 3. Electrons and Their interactions The third lecture describes quantum phenomena such as the famous double-slit experiment and Werner Heisenberg's uncertainty principle, thus describing the transmission and reflection of photons. It also introduces his famous \"Feynman diagrams\" and how quantum electrodynamics describes the interactions of subatomic particles. 4. New Queries",
                    "score": 0.8615968823432922
                },
                {
                    "id": 7009537,
                    "contents": "History of optics\nmust propagate as waves. Augustin-Jean Fresnel did more definitive studies and calculations of diffraction, published in 1815 and 1818, and thereby gave great support to the wave theory of light that had been advanced by Christiaan Huygens and reinvigorated by Young, against Newton's particle theory.",
                    "score": 0.8614336252212524
                },
                {
                    "id": 10767594,
                    "contents": "Frank–Tamm formula\nCherenkov radiation does not have characteristic spectral peaks, as typical for fluorescence or emission spectra. The relative intensity of one frequency is approximately proportional to the frequency. That is, higher frequencies (shorter wavelengths) are more intense in Cherenkov radiation. This is why visible Cherenkov radiation is observed to be brilliant blue. In fact, most Cherenkov radiation is in the ultraviolet spectrum; the sensitivity of the human eye peaks at green, and is very low in the violet portion of the spectrum. The total amount of energy radiated per unit length is: This integral is done over the frequencies for which the particle's speed is greater than speed of light of the media . The integral is convergent (finite) because at high frequencies the refractive index becomes less than unity and for extremely high frequencies it becomes unity.",
                    "score": 0.8613138794898987
                },
                {
                    "id": 13262081,
                    "contents": "List of scientific publications by Albert Einstein\n| Verhandlungen der Deutschen Physikalischen Gesellschaft, 18, 318–323 || Photons. Seminal paper in which Einstein showed that Planck's quantum hypothesis E=hν could be derived from a kinetic rate equation. This paper introduced the idea of stimulated emission (which led to the laser and maser), and Einstein's A and B coefficients provided a guide for the development of quantum electrodynamics, the most accurately tested theory of physics at present. In this work, Einstein begins to realize that quantum mechanics seems to involve probabilities and a breakdown of causality. |- | Schilpp 93; CP 6, 38 || 1916 || Quantentheorie der Strahlung",
                    "score": 0.8611469268798828
                },
                {
                    "id": 1151283,
                    "contents": "Optics\nPhysical optics In physical optics, light is considered to propagate as a wave. This model predicts phenomena such as interference and diffraction, which are not explained by geometric optics. The speed of light waves in air is approximately 3.0×108 m/s (exactly 299,792,458 m/s in vacuum). The wavelength of visible light waves varies between 400 and 700 nm, but the term \"light\" is also often applied to infrared (0.7–300 μm) and ultraviolet radiation (10–400 nm).",
                    "score": 0.8610188961029053
                },
                {
                    "id": 16528559,
                    "contents": "Kramers' law\nTo obtain a simple expression for the energy flux, first change variables from (the wavelength) to (the angular frequency) using and also using . Now is that quantity which is integrated over from 0 to to get the total number (still infinite) of photons, where : The energy flux, which we will call (but which may also be referred to as the \"intensity\" in conflict with the above name of ) is obtained by multiplying the above by the energy : for for . It is a linear function that is zero at the maximum energy . References Spectroscopy X-rays",
                    "score": 0.8606917858123779
                },
                {
                    "id": 7747997,
                    "contents": "Introduction to quantum mechanics\nEinstein's description of light as being composed of particles extended Planck's notion of quantized energy, which is that a single photon of a given frequency, , delivers an invariant amount of energy, . In other words, individual photons can deliver more or less energy, but only depending on their frequencies. In nature, single photons are rarely encountered. The Sun and emission sources available in the 19th century emit vast numbers of photons every second, and so the importance of the energy carried by each photon was not obvious. Einstein's idea that the energy contained in individual units of light depends on their frequency made it possible to explain experimental results that had seemed counterintuitive. However, although the photon is a particle, it was still being described as having the wave-like property of frequency. Effectively, the account of light as a particle is insufficient, and its wave-like nature is still required.",
                    "score": 0.8602967262268066
                },
                {
                    "id": 18106728,
                    "contents": "Single-photon source\nReferences Bibliography R. Loudon, The Quantum Theory of Light,:Oxford University Press, 3rd edition (2000). Translated in Light sources Photonics",
                    "score": 0.8601636290550232
                },
                {
                    "id": 1254129,
                    "contents": "Speed of light\nIn a medium In a medium, light usually does not propagate at a speed equal to c; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity vp. A physical signal with a finite extent (a pulse of light) travels at a different speed. The overall envelope of the pulse travels at the group velocity vg, and its earliest part travels at the front velocity vf.",
                    "score": 0.8598794937133789
                },
                {
                    "id": 26845214,
                    "contents": "Walter Thompson Welford\n(47) (132) 1991 Useful optics (University of Chicago Lectures in Physics). University of Chicago Press References External links W. T. Welford Bibliography at WorldCat 20th-century British physicists Fellows of the Royal Society Academics of Imperial College London 1916 births 1990 deaths",
                    "score": 0.8595566749572754
                },
                {
                    "id": 1563843,
                    "contents": "Augustin-Jean Fresnel\nteaching the Huygens-Fresnel principle without committing himself to a wave basis. Third, Fresnel's theory did not adequately explain the mechanism of generation of secondary waves or why they had any significant angular spread; this issue particularly bothered Poisson. Fourth, the question that most exercised optical physicists at that time was not diffraction, but polarization – on which Fresnel had been working, but was yet to make his critical breakthrough.",
                    "score": 0.8594363331794739
                },
                {
                    "id": 25073779,
                    "contents": "Planck relation\nThe Planck relation (referred to as Planck's energy–frequency relation, the Planck relation, Planck equation, and Planck formula, though the latter might also refer to Planck's law) is a fundamental equation in quantum mechanics which states that the energy of a photon, , known as photon energy, is proportional to its frequency, : The constant of proportionality, , is known as the Planck constant. Several equivalent forms of the relation exist, including in terms of angular frequency, : where . The relation accounts for the quantized nature of light and plays a key role in understanding phenomena such as the photoelectric effect and black-body radiation (where the related Planck postulate can be used to derive Planck's law). Spectral forms Light can be characterized using several spectral quantities, such as frequency , wavelength , wavenumber , and their angular equivalents (angular frequency , angular wavelength , and angular wavenumber ). These quantities are related through",
                    "score": 0.8593764305114746
                },
                {
                    "id": 1168438,
                    "contents": "Photon\nStimulated and spontaneous emission In 1916, Albert Einstein showed that Planck's radiation law could be derived from a semi-classical, statistical treatment of photons and atoms, which implies a link between the rates at which atoms emit and absorb photons. The condition follows from the assumption that functions of the emission and absorption of radiation by the atoms are independent of each other, and that thermal equilibrium is made by way of the radiation's interaction with the atoms. Consider a cavity in thermal equilibrium with all parts of itself and filled with electromagnetic radiation and that the atoms can emit and absorb that radiation. Thermal equilibrium requires that the energy density of photons with frequency (which is proportional to their number density) is, on average, constant in time; hence, the rate at which photons of any particular frequency are emitted must equal the rate at which they are absorbed.",
                    "score": 0.8593078851699829
                },
                {
                    "id": 1168432,
                    "contents": "Photon\nnot a short pulse of electromagnetic radiation; a photon's Maxwell waves will diffract, but photon energy does not spread out as it propagates, nor does this energy divide when it encounters a beam splitter. Rather, the received photon acts like a point-like particle since it is absorbed or emitted as a whole by arbitrarily small systems, including systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron.",
                    "score": 0.8592192530632019
                },
                {
                    "id": 24468007,
                    "contents": "NA63 experiment\nEmission times Another line of enquiry for NA63 is the effect of strong electromagnetic fields on the duration of the process of photon emission. Specifically, fields of a critical magnitude have an intriguing effect on how long it takes for an electron to emit a photon.",
                    "score": 0.8591492176055908
                },
                {
                    "id": 6598362,
                    "contents": "Einstein coefficients\nwhere and are the Einstein coefficients for photon absorption and induced emission respectively. Like the coefficient , these are also fixed by the intrinsic properties of the relevant atom for the two relevant energy levels. For thermodynamics and for the application of Kirchhoff's law, it is necessary that the total absorption be expressed as the algebraic sum of two components, described respectively by and , which may be regarded as positive and negative absorption, which are, respectively, the direct photon absorption, and what is commonly called stimulated or induced emission. The above equations have ignored the influence of the spectroscopic line shape. To be accurate, the above equations need to be multiplied by the (normalized) spectral line shape, in which case the units will change to include a 1/Hz term.",
                    "score": 0.8589507937431335
                },
                {
                    "id": 3271384,
                    "contents": "Quantum optics\nSeveral Nobel prizes have been awarded for work in quantum optics. These were awarded: in 2012, Serge Haroche and David J. Wineland \"for ground-breaking experimental methods that enable measuring & manipulation of individual quantum systems\". in 2005, Theodor W. Hänsch, Roy J. Glauber and John L. Hall in 2001, Wolfgang Ketterle, Eric Allin Cornell and Carl Wieman in 1997, Steven Chu, Claude Cohen-Tannoudji and William Daniel Phillips Concepts According to quantum theory, light may be considered not only to be as an electro-magnetic wave but also as a \"stream\" of particles called photons which travel with c, the vacuum speed of light. These particles should not be considered to be classical billiard balls, but as quantum mechanical particles described by a wavefunction spread over a finite region.",
                    "score": 0.8589164018630981
                },
                {
                    "id": 9723351,
                    "contents": "Two-photon absorption\nCoefficients The two-photon absorption coefficient is defined by the relation so that Where is the two-photon absorption coefficient, is the absorption coefficient, is the transition rate for TPA per unit volume, is the irradiance, is the reduced Planck constant, is the photon frequency and the thickness of the slice is . is the number density of molecules per cm3, is the photon energy (J), is the two-photon absorption cross section (cm4s/molecule). The SI units of the beta coefficient are m/W. If (m/W) is multiplied by 10−9 it can be converted to the CGS system (cal/cm s/erg). Due to different laser pulses the TPA coefficients reported has differed as much as a factor 3. With the transition towards shorter laser pulses, from picosecond to subpicosecond durations, noticeably reduced TPA coefficient have been obtained. In water Laser induced TPA in water was discovered in 1980.",
                    "score": 0.8588935136795044
                },
                {
                    "id": 23429416,
                    "contents": "Photon statistics\nAfter pulling out all the variables that are independent of the summation index , a random complex amplitude can be defined by where was rewritten in terms of its magnitude and its phase . Because the oscillators are uncorrelated, the phase of the superposed field will be random. Therefore, the complex amplitude is a stochastic variable. It represents the sum of the uncorrelated phases of the oscillators which models the intensity fluctuations in thermal light. On the complex plane, it represents a two dimensional random walker with representing the steps taken. For large a random walker has a Gaussian probability distribution. Thus, the joint probability distribution for the real and imaginary parts of the complex random variable can be represented as, After steps, the expectation value of the radius squared is . The expectation value which can be thought of as all directions being equally likely. Rewriting the probability distribution in terms of results in",
                    "score": 0.8588353395462036
                },
                {
                    "id": 1775074,
                    "contents": "Huygens–Fresnel principle\nHuygens' theory and the modern photon wave function",
                    "score": 0.8587904572486877
                },
                {
                    "id": 1169165,
                    "contents": "Photoelectric effect\nIn 1900, while studying black-body radiation, the German physicist Max Planck suggested in his \"On the Law of Distribution of Energy in the Normal Spectrum\" paper that the energy carried by electromagnetic waves could only be released in packets of energy. In 1905, Albert Einstein published a paper advancing the hypothesis that light energy is carried in discrete quantized packets to explain experimental data from the photoelectric effect. Einstein theorized that the energy in each quantum of light was equal to the frequency of light multiplied by a constant, later called Planck's constant. A photon above a threshold frequency has the required energy to eject a single electron, creating the observed effect. This was a key step in the development of quantum mechanics. In 1914, Robert A. Millikan's highly accurate measurements of the Planck's constant from the photoelectric effect supported Einstein's model, even though a corpuscular theory of light was for Millikan, at the time, \"quite",
                    "score": 0.858721137046814
                },
                {
                    "id": 3952464,
                    "contents": "List of important publications in physics\nWork by Thomas Young and Fresnel provided a comprehensive picture of the propagation of light. Hamiltonian geometrical optics. Theory of Systems of Rays and three supplements. Reissued in ——. \"Supplement to an Essay on the Theory of Systems of Rays\" (Transactions of the Royal Irish Academy, volume 16, part 1 (1830), pp. 1–61.) ——. \"Second Supplement to an Essay on the Theory of Systems of Rays\" (Transactions of the Royal Irish Academy, volume 16, part 2 (1831), pp. 93–125.) ——. \"Third Supplement to an Essay on the Theory of Systems of Rays\" (Transactions of the Royal Irish Academy, volume 17 (1837), pp. 1–144.) A series of papers recording Hamilton's work in geometric optics. This would later become an inspiration for Hamiltonian mechanics. Maxwell, James Clerk (1861), (1865). See electromagnetism section. These three papers introduced the Frequency comb technique. The earlier presented the main idea but last is the one often cited. Nuclear and particle physics",
                    "score": 0.8584374785423279
                },
                {
                    "id": 11719110,
                    "contents": "History of quantum mechanics\nFounding experiments Thomas Young's double-slit experiment demonstrating the wave nature of light. (c. 1801) Henri Becquerel discovers radioactivity. (1896) J. J. Thomson's cathode ray tube experiments (discovers the electron and its negative charge). (1897) The study of black-body radiation between 1850 and 1900, which could not be explained without quantum concepts. The photoelectric effect: Einstein explained this in 1905 (and later received a Nobel prize for it) using the concept of photons, particles of light with quantized energy. Robert Millikan's oil-drop experiment, which showed that electric charge occurs as quanta (whole units). (1909) Ernest Rutherford's gold foil experiment disproved the plum pudding model of the atom which suggested that the mass and positive charge of the atom are almost uniformly distributed. This led to the planetary model of the atom (1911).",
                    "score": 0.8584097623825073
                },
                {
                    "id": 1918032,
                    "contents": "Frits Zernike\nZernike's work helped awaken interest in coherence theory, the study of partially coherent light sources. In 1938 he published a simpler derivation of Van Cittert's 1934 theorem on the coherence of radiation from distant sources, now known as the Van Cittert–Zernike theorem. Death He died in the hospital at Amersfoort, Netherlands in 1966 after suffering illness the last years of his life. His granddaughter is journalist Kate Zernike. Honours and awards In 1946, Zernike became member of the Royal Netherlands Academy of Arts and Sciences. In 1953, Zernike won the Nobel Prize in Physics, for his invention of the phase-contrast microscope, an instrument that permits the study of internal cell structure without the need to stain and thus kill the cells. In 1954, Zernike became an Honorary Member of The Optical Society (OSA). Zernike was elected a Foreign Member of the Royal Society (ForMemRS).",
                    "score": 0.8583545088768005
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_25",
        "question": "The force constant of ${ }^{35} \\mathrm{Cl}^{35} \\mathrm{Cl}$ is $319 \\mathrm{~N} \\cdot \\mathrm{m}^{-1}$. Calculate the fundamental vibrational frequency",
        "golden_answers": [
            " 556"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 8657438,
                    "contents": "Vibrational partition function\nThe vibrational partition function traditionally refers to the component of the canonical partition function resulting from the vibrational degrees of freedom of a system. The vibrational partition function is only well-defined in model systems where the vibrational motion is relatively uncoupled with the system's other degrees of freedom. Definition For a system (such as a molecule or solid) with uncoupled vibrational modes the vibrational partition function is defined by where is the absolute temperature of the system, is the Boltzmann constant, and is the energy of j'th mode when it has vibrational quantum number . For an isolated molecule of n atoms, the number of vibrational modes (i.e. values of j) is 3n − 5 for linear molecules and 3n − 6 for non-linear ones. In crystals, the vibrational normal modes are commonly known as phonons. Approximations",
                    "score": 0.8856810331344604
                },
                {
                    "id": 4258551,
                    "contents": "Molecular geometry\nTo get a feeling for the probability that the vibration of molecule may be thermally excited, we inspect the Boltzmann factor , where ΔE is the excitation energy of the vibrational mode, k the Boltzmann constant and T the absolute temperature. At 298 K (25 °C), typical values for the Boltzmann factor β are: β = 0.089 for ΔE = 500 cm−1 β = 0.008 for ΔE = 1000 cm−1 β = 0.0007 for ΔE = 1500 cm−1. (The reciprocal centimeter is an energy unit that is commonly used in infrared spectroscopy; 1 cm−1 corresponds to ). When an excitation energy is 500 cm−1, then about 8.9 percent of the molecules are thermally excited at room temperature. To put this in perspective: the lowest excitation vibrational energy in water is the bending mode (about 1600 cm−1). Thus, at room temperature less than 0.07 percent of all the molecules of a given amount of water will vibrate faster than at absolute zero.",
                    "score": 0.885002076625824
                },
                {
                    "id": 14762074,
                    "contents": "Edgar Bright Wilson\nIn 1955 Bright published a book Molecular Vibrations along with co-authors J.C Decius and P.C. Cross which discussed infrared and raman spectra of polyatomic molecules. In 1955 Bright studied the internal rotation of single bonds in molecules using microwave spectroscopy. In 1965 Bright studied the energy transfer in rotationally molecular inelastic collisions. In 1970, Bright began to study hydrogen bonding and the structure of hydrogen bonds using low resolution microwave spectroscopy. In 1979, Bright retired and was named an emeritus professor. The E. Bright Wilson Award in Spectroscopy was established in 1994 by the American Chemical Society. Personal life Wilson was born in Gallatin, Tennessee to mother Alma Lackey and father E. B. Wilson, a lawyer. His family soon moved to Yonkers, New York.",
                    "score": 0.8840764164924622
                },
                {
                    "id": 27911026,
                    "contents": "Eighth power\nThe Casimir–Polder force between two molecules decays as the inverse eighth power of the distance between them. See also Seventh power Sixth power Fifth power (algebra) Fourth power Cube (algebra) Square number References Integers Number theory Elementary arithmetic Integer sequences Unary operations Figurate numbers",
                    "score": 0.8809177279472351
                },
                {
                    "id": 8333673,
                    "contents": "Force field (chemistry)\nThe bond stretching constant can be determined from the experimental Infrared spectrum, Raman spectrum, or high-level quantum mechanical calculations. The constant determines vibrational frequencies in molecular dynamics simulations. The stronger the bond is between atoms, the higher is the value of the force constant, and the higher the wavenumber (energy) in the IR/Raman spectrum. The vibration spectrum according to a given force constant can be computed from short MD trajectories (5 ps) with ~1 fs time steps, calculation of the velocity autocorrelation function, and its Fourier transform.",
                    "score": 0.8789045810699463
                },
                {
                    "id": 986650,
                    "contents": "Atomic, molecular, and optical physics\nBoth subfields are primarily concerned with electronic structure and the dynamical processes by which these arrangements change. Generally this work involves using quantum mechanics. For molecular physics, this approach is known as quantum chemistry. One important aspect of molecular physics is that the essential atomic orbital theory in the field of atomic physics expands to the molecular orbital theory. Molecular physics is concerned with atomic processes in molecules, but it is additionally concerned with effects due to the molecular structure. Additionally to the electronic excitation states which are known from atoms, molecules are able to rotate and to vibrate. These rotations and vibrations are quantized; there are discrete energy levels. The smallest energy differences exist between different rotational states, therefore pure rotational spectra are in the far infrared region (about 30 - 150 µm wavelength) of the electromagnetic spectrum. Vibrational spectra are in the near",
                    "score": 0.8784893155097961
                },
                {
                    "id": 7471073,
                    "contents": "Camille Sandorfy\nHe carried out extensive research in molecular spectroscopy. In infrared spectroscopy he studied molecular vibrations as well as overtone bands in hydrogen-bonded systems, and the effect of hydrogen bonds on vibrational anharmonicity. In electronic spectroscopy he specialized in the far ultraviolet region where he observed a number of molecular Rydberg states. Some of his spectroscopic studies led to insights into biological processes, including the molecular mechanism of vision and the role of hydrogen bonding in anesthesia. Honours In 1967, he was made a Fellow of the Royal Society of Canada. In 1982, he was awarded the Quebec government's Prix Marie-Victorin. In 1993, he was made a Member of the Hungarian Academy of Sciences. In 1995, he was made an Officer of the Order of Canada. In 1995, he was made a Knight of the National Order of Quebec.",
                    "score": 0.8774673938751221
                },
                {
                    "id": 15425029,
                    "contents": "Ellis R. Lippincott Award\nThe Ellis R. Lippincott Award is awarded annually to recognize \"an individual who has made significant contributions to vibrational spectroscopy as judged by his or her influence on other scientists.\" It was jointly established in 1975 by The Optical Society, The Coblentz Society, and The Society for Applied Spectroscopy. The award honors Ellis R. Lippincott, a vibrational spectroscopist who worked at the University of Maryland. Lippincott was one of the developers of the Diamond anvil cell, which is used in high pressure research. Past winners of the Lippincott Award",
                    "score": 0.8765043020248413
                },
                {
                    "id": 782587,
                    "contents": "Van der Waals force\nDespite being the weakest of the weak chemical forces, with a strength within 0.4 and 4 kJ/mol (4 to 40 meV per bond), they may still support an integral structural load when multitudes of such interactions are present. The force results from a transient shift in electron density. Specifically, the electron density may temporarily shift more greatly to one side of the nucleus. This generates a transient charge which a nearby atom can be attracted or repelled by. When the interatomic distance of two atoms is greater than 0.6 nm the force is not strong enough to be observed, when the distance is below 0.4 nm the force becomes repulsive. Intermolecular forces have four major contributions:",
                    "score": 0.8756805062294006
                },
                {
                    "id": 14762069,
                    "contents": "Edgar Bright Wilson\nWilson made major contributions to the field of molecular spectroscopy. He developed the first rigorous quantum mechanical Hamiltonian in internal coordinates for a polyatomic molecule. He developed the theory of how rotational spectra are influenced by centrifugal distortion during rotation. He pioneered the use of group theory for the analysis and simplification normal mode analysis, particularly for high symmetry molecules, such as benzene. In 1955, Wilson published Molecular Vibrations along with J.C. Decius Paul C. Cross. Following the Second World War, Wilson was a pioneer in the application of microwave spectroscopy to the determination of molecular structure. Wilson wrote an influential introductory text Introduction to Scientific Research that provided an introduction of all the steps of scientific research, from defining a problem through the archival of data after publication.",
                    "score": 0.8749935626983643
                },
                {
                    "id": 19042079,
                    "contents": "Overtone band\nIn vibrational spectroscopy, an overtone band is the spectral band that occurs in a vibrational spectrum of a molecule when the molecule makes a transition from the ground state (v=0) to the second excited state (v=2), where v is the vibrational quantum number (a non-negative integer) obtained from solving the Schrödinger equation for the molecule.",
                    "score": 0.8743125796318054
                },
                {
                    "id": 14762071,
                    "contents": "Edgar Bright Wilson\nIn 1934, Bright was elected to the Society of Fellows at Harvard for his work done at the California Institute of Technology. His election meant he had a 3 year junior fellowship at Harvard during which he studied molecular motion and symmetry analysis. In 1936 the Harvard Chemistry department appointed Bright was an Assistant professor during his third year of his fellowship. He taught courses in chemistry and quantum mechanics and was promoted to an Associate Professor with tenure after three years. From 1934 to 1941, Bright, along with Harold Gershinowitz, constructed an automatic infrared spectrometer which was used to measure vibrational absorption spectra of various molecules.",
                    "score": 0.8735076785087585
                },
                {
                    "id": 13262039,
                    "contents": "List of scientific publications by Albert Einstein\n| Annalen der Physik (ser. 4), 4, 513–523, link || Intermolecular forces. The first of two papers in which Einstein proposed the (incorrect) theory that the interactions between all molecules are a universal function of distance, in analogy with the inverse-square force of gravity. Once parameterized, his theory makes reasonably accurate predictions for heavier hydrophobic molecules, but fails for lighter molecules. |- | Schilpp 2; CP 2, 2 || 1902 || Thermodynamische Theorie der Potentialdifferenz zwischen Metallen und vollständig dissoziierten Lösungen ihrer Salze, und eine elektrische Methode zur Erforschung der Molekularkräfte",
                    "score": 0.8735001683235168
                },
                {
                    "id": 782585,
                    "contents": "Van der Waals force\nIn molecular physics, the van der Waals force, named after Dutch physicist Johannes Diderik van der Waals, is a distance-dependent interaction between atoms or molecules. Unlike ionic or covalent bonds, these attractions do not result from a chemical electronic bond; they are comparatively weak and therefore more susceptible to disturbance. The van der Waals force quickly vanishes at longer distances between interacting molecules. Van der Waals force plays a fundamental role in fields as diverse as supramolecular chemistry, structural biology, polymer science, nanotechnology, surface science, and condensed matter physics. It also underlies many properties of organic compounds and molecular solids, including their solubility in polar and non-polar media.",
                    "score": 0.8725601434707642
                },
                {
                    "id": 21582853,
                    "contents": "Cho Minhaeng\nCho's research group actively studies nonlinear optical and vibrational spectroscopy, molecular dynamics simulations of chemical and biological systems in condensed phases, quantum dynamics of chemical reactions, linear and nonlinear chiroptical spectroscopy of biomolecules, quantum spectroscopy and imaging with high-precision laser technology, interferometric measurements of scattering fields for single particle tracking, chemically sensitive spectroscopy and imaging, surface-specific spectroscopy, and ultrafast vibrational microspectroscopy. Membership 2010: Member, Korean Academy of Science and Technology 2002-2009: Junior Member, Korean Academy of Science and Technology 1996–present: Permanent Member, Korean Chemical Society 1996–present: Member, American Chemical Society",
                    "score": 0.8723735809326172
                },
                {
                    "id": 28509185,
                    "contents": "Sergey Macheret\nAssumptions: 1. Translational-vibrational non equilibrium: or . 2. Collisions are collinear and rotational energy effects are negligible. 3. The duration of collision is much faster than the period of molecule vibration. 4. The vibrational energy obeys Boltzmann distribution. Formulas and relations: Nonequilibrium factor in the high temperature case (): Nonequilibrium factor in the low temperature case (): References External links Sergey Macheret at Google Scholar 1957 births Living people American physicists",
                    "score": 0.8717414140701294
                },
                {
                    "id": 10017712,
                    "contents": "GF method\nThe GF method, sometimes referred to as FG method, is a classical mechanical method introduced by Edgar Bright Wilson to obtain certain internal coordinates for a vibrating semi-rigid molecule, the so-called normal coordinates Qk. Normal coordinates decouple the classical vibrational motions of the molecule and thus give an easy route to obtaining vibrational amplitudes of the atoms as a function of time. In Wilson's GF method it is assumed that the molecular kinetic energy consists only of harmonic vibrations of the atoms, i.e., overall rotational and translational energy is ignored. Normal coordinates appear also in a quantum mechanical description of the vibrational motions of the molecule and the Coriolis coupling between rotations and vibrations.",
                    "score": 0.871502697467804
                },
                {
                    "id": 7113368,
                    "contents": "Anharmonicity\nIt is possible to use first-principles methods such as density-functional theory to map the anharmonic potential experienced by the atoms in both molecules and solids. Accurate anharmonic vibrational energies can then be obtained by solving the anharmonic vibrational equations for the atoms within a mean-field theory. Finally, it is possible to use Møller–Plesset perturbation theory to go beyond the mean-field formalism.",
                    "score": 0.8712928891181946
                },
                {
                    "id": 15776451,
                    "contents": "William Klemperer\nAwards Bill Klemperer has had many awards and honors, which include: Inducted a Fellow of the American Physical Society, 1954 Elected to the American Academy of Arts and Sciences, 1963 Elected to the National Academy of Sciences, 1969 John Price Wetherill Medal, awarded by the Franklin Institute, 1978 Irving Langmuir Award, awarded by the American Chemical Society, 1980 The Distinguished Service Medal, awarded by the U.S. National Science Foundation, 1981 The Earle K. Plyler Prize for Molecular Spectroscopy, awarded by the American Physical Society, 1983 The Bomem-Michelson Award for the advancement of the field of vibrational spectroscopy. awarded by the Coblentz Society, 1990 Inaugural George C. Pimentel Memorial Lecturer, Chemistry Department, UC Berkeley. 1991-2. The Remsen Award from the Maryland Section of the American Chemical Society, 1992 The Peter Debye Award in Physical Chemistry, awarded by the American Chemical Society, 1994",
                    "score": 0.8710386157035828
                },
                {
                    "id": 29815424,
                    "contents": "Sylvie Roke\nTo elucidate molecular surface structures, morphologies and chirality of nano- and microscopic objects in solutions, Roke invented vibrational sum frequency scattering (SFS), a method that allows the recording the vibrational spectrum of the molecular interfacial layer around objects. She used SFS to specify the molecular interfaces of complex systems: polymer particles in a solid matrix, particles in solution, oil droplets in water (emulsions), lipid droplet like systems, water droplets, a micro-jet and liposomes in aqueous solution. Her studies indicate that objects on nano- and microscale show different behaviors as model planar interfaces.",
                    "score": 0.8710365295410156
                },
                {
                    "id": 19260536,
                    "contents": "Vibrational temperature\nThe vibrational temperature is commonly used in thermodynamics, to simplify certain equations. It has units of temperature and is defined as where is Boltzmann's constant, is the speed of light, and (Greek letter nu) is the characteristic frequency of the oscillator. The vibrational temperature is used commonly when finding the vibrational partition function. References Statistical thermodynamics University Arizona See also Rotational temperature Rotational spectroscopy Vibrational spectroscopy Infrared spectroscopy Spectroscopy Atomic physics Molecular physics",
                    "score": 0.870729923248291
                },
                {
                    "id": 8333697,
                    "contents": "Force field (chemistry)\nWidely used force fields Different force fields are designed for different purposes. All are implemented in various computers software. MM2 was developed by Norman Allinger mainly for conformational analysis of hydrocarbons and other small organic molecules. It is designed to reproduce the equilibrium covalent geometry of molecules as precisely as possible. It implements a large set of parameters that is continuously refined and updated for many different classes of organic compounds (MM3 and MM4). CFF was developed by Arieh Warshel, Lifson, and coworkers as a general method for unifying studies of energies, structures, and vibration of general molecules and molecular crystals. The CFF program, developed by Levitt and Warshel, is based on the Cartesian representation of all the atoms, and it served as the basis for many subsequent simulation programs.",
                    "score": 0.8695669174194336
                },
                {
                    "id": 10291323,
                    "contents": "Molecular vibration\nIntensities In an infrared spectrum the intensity of an absorption band is proportional to the derivative of the molecular dipole moment with respect to the normal coordinate. Likewise, the intensity of Raman bands depends on the derivative of polarizability with respect to the normal coordinate. There is also a dependence on the fourth-power of the wavelength of the laser used. See also Coherent anti-Stokes Raman spectroscopy Eckart conditions Fermi resonance GF method Infrared spectroscopy of metal carbonyls Lennard–Jones potential Near infrared spectroscopy Nuclear resonance vibrational spectroscopy Resonance Raman spectroscopy Transition dipole moment References Further reading External links Free Molecular Vibration code developed by Zs. Szabó and R. Scipioni Molecular vibration and absorption small explanation of vibrational spectra and a table including force constants. Character tables for chemically important point groups Chemical physics Spectroscopy",
                    "score": 0.8689284324645996
                },
                {
                    "id": 10535941,
                    "contents": "Table of thermodynamic equations\n|- |} More relations include the following. {| class=\"wikitable\" |- | | | |- | | | |- | | | |- |} Other differential equations are: {| class=\"wikitable\" |- ! Name ! H ! U ! G |- !Gibbs–Helmholtz equation | | | |- | | | | |- |} Quantum properties Indistinguishable Particles where N is number of particles, h is Planck's constant, I is moment of inertia, and Z is the partition function, in various forms: {| class=\"wikitable\" |- !Degree of freedom !Partition function |- !Translation | |- !Vibration | |- !Rotation | where: σ = 1 (heteronuclear molecules) σ = 2 (homonuclear) |} Thermal properties of matter {| class=\"wikitable\" |- ! Coefficients ! Equation |- !Joule-Thomson coefficient | |- !Compressibility (constant temperature) | |- ! Coefficient of thermal expansion (constant pressure) | |- ! Heat capacity (constant pressure) | |- ! Heat capacity (constant volume) | |- |}",
                    "score": 0.8687822818756104
                },
                {
                    "id": 7844100,
                    "contents": "Grüneisen parameter\nA proper description of the Grüneisen parameter represents a stringent test for any type of interatomic potential. Microscopic definition via the phonon frequencies The physical meaning of the parameter can also be extended by combining thermodynamics with a reasonable microphysics model for the vibrating atoms within a crystal. When the restoring force acting on an atom displaced from its equilibrium position is linear in the atom's displacement, the frequencies ωi of individual phonons do not depend on the volume of the crystal or on the presence of other phonons, and the thermal expansion (and thus γ) is zero. When the restoring force is non-linear in the displacement, the phonon frequencies ωi change with the volume . The Grüneisen parameter of an individual vibrational mode can then be defined as (the negative of) the logarithmic derivative of the corresponding frequency :",
                    "score": 0.8685160875320435
                },
                {
                    "id": 10291305,
                    "contents": "Molecular vibration\nA molecular vibration is excited when the molecule absorbs energy, ΔE, corresponding to the vibration's frequency, ν, according to the relation ΔE = hν, where h is Planck's constant. A fundamental vibration is evoked when one such quantum of energy is absorbed by the molecule in its ground state. When multiple quanta are absorbed, the first and possibly higher overtones are excited.",
                    "score": 0.8684484958648682
                },
                {
                    "id": 14078251,
                    "contents": "Molecular Spectra and Molecular Structure IV. Constants of Diatomic Molecules\nDiscusses Physics, engineering, mathematics, kinetics, spectroscopy, astronomy, astrophysics, aeronautics, astronautics, radiation, optics, energy, photometry, spectrometry, electromagnetics, oscillators, thermochemistry, thermodynamics, ionization, X-rays, ESR, photoelectrons, electronics, industry, science and technology. Science books",
                    "score": 0.8682814836502075
                },
                {
                    "id": 27308861,
                    "contents": "Jeanette Grasselli Brown\nGrasselli Brown edited the international journal, Vibrational Spectroscopy. She is a member of the American Chemical Society, Coblentz Society, Federation of Analytical Chemistry and Spectroscopy Societies, and the American Association for the Advancement of Science. She was the president of the Society for Applied Spectroscopy in 1970. She is active in promoting women's careers as a member of the International Women's Forum and National Research Council's Committee on Women in Science and Engineering. She is an avid supporter of women in the workplace and defends part-time work for women, equal salaries, and corporate child-care facilities.",
                    "score": 0.8682131767272949
                },
                {
                    "id": 9653728,
                    "contents": "Henry Margenau\nMargenau's work embraced investigation of intermolecular forces, spectroscopy, nuclear physics and electronics. He was also interested in parapsychology. He co-authored parapsychological papers with his friend Lawrence LeShan. He was married to Liesel Noe and the couple parented two sons and a daughter. Margenau died in Hamden, Connecticut. Honours and awards Guggenheim Fellowship Fulbright Fellowship William Clyde DeVane Medal from the Yale chapter of Phi Beta Kappa for outstanding teaching and scholarship (1970). Works This book and Margenau each receive a mention in a December 28, 1992 Time magazine article: Galileo And Other Faithful Scientists 1943 1956 See also Life Science Library List of science and religion scholars Notes External links",
                    "score": 0.8681483268737793
                },
                {
                    "id": 10526932,
                    "contents": "Eckart conditions\nThe Eckart conditions, named after Carl Eckart, simplify the nuclear motion (rovibrational) Hamiltonian that arises in the second step of the Born–Oppenheimer approximation. They make it possible to approximately separate rotation from vibration. Although the rotational and vibrational motions of the nuclei in a molecule cannot be fully separated, the Eckart conditions minimize the coupling close to a reference (usually equilibrium) configuration. The Eckart condition are explained by Louck and Galbraith and in Section 10.2 of the textbook by Bunker and Jensen, where a numerical example is given. Definition of Eckart conditions The Eckart conditions can only be formulated for a semi-rigid molecule, which is a molecule with a potential energy surface V(R1, R2,..RN) that has a well-defined minimum for RA0 (). These equilibrium coordinates of the nuclei—with masses MA—are expressed with respect to a fixed orthonormal principal axes frame and hence satisfy the relations",
                    "score": 0.8680577278137207
                },
                {
                    "id": 7113367,
                    "contents": "Anharmonicity\nAnharmonicity plays a role in lattice and molecular vibrations, in quantum oscillations, and in acoustics. The atoms in a molecule or a solid vibrate about their equilibrium positions. When these vibrations have small amplitudes they can be described by harmonic oscillators. However, when the vibrational amplitudes are large, for example at high temperatures, anharmonicity becomes important. An example of the effects of anharmonicity is the thermal expansion of solids, which is usually studied within the quasi-harmonic approximation. Studying vibrating anharmonic systems using quantum mechanics is a computationally demanding task because anharmonicity not only makes the potential experienced by each oscillator more complicated, but also introduces coupling between the oscillators. It is possible to use first-principles methods such as density-functional theory to map the anharmonic potential experienced by the atoms in both molecules and solids. Accurate anharmonic vibrational",
                    "score": 0.8676807880401611
                },
                {
                    "id": 986668,
                    "contents": "Atomic, molecular, and optical physics\nNotes References Solid State Physics (2nd Edition), J.R. Hook, H.E. Hall, Manchester Physics Series, John Wiley & Sons, 2010, Light and Matter: Electromagnetism, Optics, Spectroscopy and Lasers, Y.B. Band, John Wiley & Sons, 2010, The Light Fantastic – Introduction to Classic and Quantum Optics, I.R. Kenyon, Oxford University Press, 2008, Handbook of atomic, molecular, and optical physics, Editor: Gordon Drake, Springer, Various authors, 1996, External links ScienceDirect - Advances In Atomic, Molecular, and Optical Physics Journal of Physics B: Atomic, Molecular and Optical Physics Institutions",
                    "score": 0.8673096895217896
                },
                {
                    "id": 21676810,
                    "contents": "Dunham expansion\nIn quantum chemistry, the Dunham expansion is an expression for the rotational-vibrational energy levels of a diatomic molecule: where and are the vibrational and rotational quantum numbers, and is the projection of along the internuclear axis in the body-fixed frame. The constant coefficients are called Dunham parameters with representing the electronic energy. The expression derives from a semiclassical treatment of a perturbational approach to deriving the energy levels. The Dunham parameters are typically calculated by a least-squares fitting procedure of energy levels with the quantum numbers. Relation to conventional band spectrum constants This table adapts the sign conventions from the book of Huber and Herzberg. See also Rotational-vibrational spectroscopy References Spectroscopy Molecular vibration",
                    "score": 0.867303192615509
                },
                {
                    "id": 11242070,
                    "contents": "Timeline of chemistry\n1926Erwin Schrödinger proposes the Schrödinger equation, which provides a mathematical basis for the wave model of atomic structure. 1927Werner Heisenberg develops the uncertainty principle which, among other things, explains the mechanics of electron motion around the nucleus. 1927Fritz London and Walter Heitler apply quantum mechanics to explain covalent bonding in the hydrogen molecule, which marked the birth of quantum chemistry. 1929 Linus Pauling publishes Pauling's rules, which are key principles for the use of X-ray crystallography to deduce molecular structure. 1931Erich Hückel proposes Hückel's rule, which explains when a planar ring molecule will have aromatic properties. 1931Harold Urey discovers deuterium by fractionally distilling liquid hydrogen. 1932James Chadwick discovers the neutron. 1932–1934Linus Pauling and Robert Mulliken quantify electronegativity, devising the scales that now bear their names.",
                    "score": 0.8670116066932678
                },
                {
                    "id": 19036026,
                    "contents": "Hund's cases\nIn rotational-vibrational and electronic spectroscopy of diatomic molecules, Hund's coupling cases are idealized descriptions of rotational states in which specific terms in the molecular Hamiltonian and involving couplings between angular momenta are assumed to dominate over all other terms. There are five cases, proposed by Friedrich Hund in 1926-27 and traditionally denoted by the letters (a) through (e). Most diatomic molecules are somewhere between the idealized cases (a) and (b). Angular momenta To describe the Hund's coupling cases, we use the following angular momenta (where boldface letters indicate vector quantities): , the electronic orbital angular momentum , the electronic spin angular momentum , the total electronic angular momentum , the rotational angular momentum of the nuclei , the total angular momentum of the system (exclusive of nuclear spin) , the total angular momentum exclusive of electron (and nuclear) spin",
                    "score": 0.8665893077850342
                },
                {
                    "id": 5311713,
                    "contents": "Franck–Condon principle\nThe Franck–Condon principle is a rule in spectroscopy and quantum chemistry that explains the intensity of vibronic transitions. Vibronic transitions are the simultaneous changes in electronic and vibrational energy levels of a molecule due to the absorption or emission of a photon of the appropriate energy. The principle states that during an electronic transition, a change from one vibrational energy level to another will be more likely to happen if the two vibrational wave functions overlap more significantly. Overview",
                    "score": 0.8665826916694641
                },
                {
                    "id": 15776443,
                    "contents": "William Klemperer\nScience Klemperer's early work concentrated on the infrared spectroscopy of small molecules that are only stable in the gas phase at high temperatures. Among these are the alkali halides, for many of which he obtained the first vibrational spectra. The work provided basic structural data for many oxides and fluorides, and gave remarkable insight into the details of the bonding. It also led Klemperer to recognize the immense potential of molecular beams in spectroscopy, and in particular the use of the electric resonance technique to address fundamental problems in structural chemistry. An important result was his benchmark measurement of the electric dipole moment of LiH, at a date when this was the largest molecule for which quantum chemical calculations had any hope of getting useful results in a sensible length of time. Klemperer has always been enthusiastic about molecular beams; he writes: \"Molecular beams are fun for a chemist. They give one a sense of power.\"",
                    "score": 0.8665717244148254
                },
                {
                    "id": 10991325,
                    "contents": "Pauli equation\nSee also Semiclassical physics Atomic, molecular, and optical physics Group contraction Gordon decomposition Footnotes References Books Quantum mechanics",
                    "score": 0.8664798736572266
                },
                {
                    "id": 16374823,
                    "contents": "Coblentz Society\nThe Awards sponsored by The Coblentz Society include: The Coblentz Award is to recognize the contributions by a young professional spectroscopist to the fundamental understanding of vibrational spectroscopy The Craver Award is to recognize the efforts of young professional spectroscopists in the field of applied analytical spectroscopy The Williams–Wright Award is unique in that it recognizes the lifetime accomplishments of an industrial spectroscopist The Bomem-Michelson Award is currently inactive but was designed to recognize advancements in the field of vibrational spectroscopy The Lippincott Award for the advancement of spectroscopy from an optical perspective (co-sponsored by Optica and the Society for Applied Spectroscopy). References External links Official homepage Scientific societies based in the United States Spectroscopy",
                    "score": 0.8664730191230774
                },
                {
                    "id": 20595434,
                    "contents": "Rydberg–Klein–Rees method\nThe Rydberg–Klein–Rees method is a procedure used in the analysis of rotational-vibrational spectra of diatomic molecules to obtain a potential energy curve from the experimentally-known line positions. Atomic physics",
                    "score": 0.8663198947906494
                },
                {
                    "id": 106278,
                    "contents": "Gerhard Herzberg\nThe main building of John Abbott College in Montreal is named after him. Carleton University named the Herzberg Laboratories building after him. A public park in the College Park neighbourhood of Saskatoon also bears his name. Books and publications Herzberg authored some classic works in the field of spectroscopy, including Atomic Spectra and Atomic Structure and the encyclopaedic four volume work: Molecular Spectra and Molecular Structure, which is often called the spectroscopist's bible. The three volumes of Molecular Spectra and Molecular Structure were re-issued by Krieger in 1989, including extensive new footnotes by Herzberg. Volume IV of the series, \"Constants of diatomic molecules\" is purely a reference work, a compendium of known spectroscopic constants (and therefore a bibliography of molecular spectroscopy) of diatomic molecules up until 1978.",
                    "score": 0.8659448027610779
                },
                {
                    "id": 3478652,
                    "contents": "Molecular mechanics\nIn addition to the functional form of each energy term, a useful energy function must be assigned parameters for force constants, van der Waals multipliers, and other constant terms. These terms, together with the equilibrium bond, angle, and dihedral values, partial charge values, atomic masses and radii, and energy function definitions, are collectively termed a force field. Parameterization is typically done through agreement with experimental values and theoretical calculations results. Norman L. Allinger's force field in the last MM4 version calculate for hydrocarbons heats of formation with a rms error of 0.35 kcal/mol, vibrational spectra with a rms error of 24 cm−1, rotational barriers with a rms error of 2.2, C-C bond lengths within 0.004 Å and C-C-C angles within 1. Later MM4 versions cover also compounds with heteroatoms such as aliphatic amines.",
                    "score": 0.8657838106155396
                },
                {
                    "id": 10291319,
                    "contents": "Molecular vibration\nNewtonian mechanics Perhaps surprisingly, molecular vibrations can be treated using Newtonian mechanics to calculate the correct vibration frequencies. The basic assumption is that each vibration can be treated as though it corresponds to a spring. In the harmonic approximation the spring obeys Hooke's law: the force required to extend the spring is proportional to the extension. The proportionality constant is known as a force constant, k. The anharmonic oscillator is considered elsewhere. By Newton's second law of motion this force is also equal to a reduced mass, μ, times acceleration. Since this is one and the same force the ordinary differential equation follows. The solution to this equation of simple harmonic motion is A is the maximum amplitude of the vibration coordinate Q. It remains to define the reduced mass, μ. In general, the reduced mass of a diatomic molecule, AB, is expressed in terms of the atomic masses, mA and mB, as",
                    "score": 0.8656389713287354
                },
                {
                    "id": 534723,
                    "contents": "Force spectroscopy\nTechniques that can be used to perform force spectroscopy include atomic force microscopy, optical tweezers, magnetic tweezers, acoustic force spectroscopy, microneedles, and biomembranes. Force spectroscopy measures the behavior of a molecule under stretching or torsional mechanical force. In this way a great deal has been learned in recent years about the mechanochemical coupling in the enzymes responsible for muscle contraction, transport in the cell, energy generation (F1-ATPase), DNA replication and transcription (polymerases), DNA unknotting and unwinding (topoisomerases and helicases). As a single-molecule technique, as opposed to typical ensemble spectroscopies, it allows a researcher to determine properties of the particular molecule under study. In particular, rare events such as conformational change, which are masked in an ensemble, may be observed. Experimental techniques",
                    "score": 0.865388810634613
                },
                {
                    "id": 1297220,
                    "contents": "Theoretical chemistry\nIn recent years, it has consisted primarily of quantum chemistry, i.e., the application of quantum mechanics to problems in chemistry. Other major components include molecular dynamics, statistical thermodynamics and theories of electrolyte solutions, reaction networks, polymerization, catalysis, molecular magnetism and spectroscopy. Modern theoretical chemistry may be roughly divided into the study of chemical structure and the study of chemical dynamics. The former includes studies of: electronic structure, potential energy surfaces, and force fields; vibrational-rotational motion; equilibrium properties of condensed-phase systems and macro-molecules. Chemical dynamics includes: bimolecular kinetics and the collision theory of reactions and energy transfer; unimolecular rate theory and metastable states; condensed-phase and macromolecular aspects of dynamics.",
                    "score": 0.8653783798217773
                },
                {
                    "id": 4039815,
                    "contents": "Molecular physics\nIn addition to the electronic excitation states which are known from atoms, molecules exhibit rotational and vibrational modes whose energy levels are quantized. The smallest energy differences exist between different rotational states: pure rotational spectra are in the far infrared region (about 30 - 150 μm wavelength) of the electromagnetic spectrum. Vibrational spectra are in the near infrared (about 1 - 5 μm) and spectra resulting from electronic transitions are mostly in the visible and ultraviolet regions. From measuring rotational and vibrational spectra properties of molecules like the distance between the nuclei can be specifically calculated. One important aspect of molecular physics is that the essential atomic orbital theory in the field of atomic physics expands to the molecular orbital theory. See also Born–Oppenheimer approximation Electrostatic deflection (molecular physics/nanotechnology) Molecular energy state Molecular modeling Rigid rotor Spectroscopy",
                    "score": 0.865168571472168
                },
                {
                    "id": 2296386,
                    "contents": "Astrophysics\nof physics and chemistry, including classical mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.",
                    "score": 0.8645901083946228
                },
                {
                    "id": 23978892,
                    "contents": "Vibronic spectroscopy\nNotes References Bibliography Chapter: Molecular Spectroscopy 2. Spectroscopy",
                    "score": 0.8639932870864868
                },
                {
                    "id": 534722,
                    "contents": "Force spectroscopy\nForce spectroscopy is a set of techniques for the study of the interactions and the binding forces between individual molecules. These methods can be used to measure the mechanical properties of single polymer molecules or proteins, or individual chemical bonds. The name \"force spectroscopy\", although widely used in the scientific community, is somewhat misleading, because there is no true matter-radiation interaction. Techniques that can be used to perform force spectroscopy include atomic force microscopy, optical tweezers, magnetic tweezers, acoustic force spectroscopy, microneedles, and biomembranes.",
                    "score": 0.863886296749115
                },
                {
                    "id": 7303299,
                    "contents": "The Journal of Physical Chemistry A\nThe Journal of Physical Chemistry A is a scientific journal which reports research on the chemistry of molecules - including their dynamics, spectroscopy, kinetics, structure, bonding, and quantum chemistry. It is published weekly by the American Chemical Society. Before 1997 the title was simply Journal of Physical Chemistry. Owing to the ever-growing amount of research in the area, in 1997 the journal was split into Journal of Physical Chemistry A (molecular theoretical and experimental physical chemistry) and The Journal of Physical Chemistry B (solid state, soft matter, liquids, etc.). Beginning in 2007, the latter underwent a further split, with The Journal of Physical Chemistry C now being dedicated to nanotechnology, molecular electronics, and related subjects. Editors-in-chief 1896–1932 Wilder Dwight Bancroft, Joseph E. Trevor 1933–1951 S. C. Lind 1952–1964 William A. Noyes 1965–1969 F. T. Wall 1970–1980 Bryce Crawford 1980–2004 Mostafa El-Sayed",
                    "score": 0.8638818860054016
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_26",
        "question": "$$\r\n\\text {Calculate the energy of a photon for a wavelength of } 100 \\mathrm{pm} \\text { (about one atomic diameter). }\r\n$$\r\n",
        "golden_answers": [
            " 2"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 18326096,
                    "contents": "Photon energy\nThe photon energy at 1 μm wavelength, the wavelength of near infrared radiation, is approximately 1.2398 eV. In chemistry, quantum physics and optical engineering See where E is photon energy (joules), h is the Planck constant - 6.62607015 × 10−34 (m2kgs−1) The Greek letter ν (nu) is the photon's frequency. Examples An FM radio station transmitting at 100 MHz emits photons with an energy of about 4.1357 × 10−7 eV. This minuscule amount of energy is approximately 8 × 10−13 times the electron's mass (via mass-energy equivalence). Very-high-energy gamma rays have photon energies of 100 GeV to over 1 PeV (1011 to 1015 electronvolts) or 16 nanojoules to 160 microjoules. This corresponds to frequencies of 2.42 × 1025 to 2.42 × 1029 Hz.",
                    "score": 0.9191268682479858
                },
                {
                    "id": 18326094,
                    "contents": "Photon energy\nPhoton energy is the energy carried by a single photon. The amount of energy is directly proportional to the photon's electromagnetic frequency and thus, equivalently, is inversely proportional to the wavelength. The higher the photon's frequency, the higher its energy. Equivalently, the longer the photon's wavelength, the lower its energy. Photon energy can be expressed using any unit of energy. Among the units commonly used to denote photon energy are the electronvolt (eV) and the joule (as well as its multiples, such as the microjoule). As one joule equals 6.24 × 1018 eV, the larger units may be more useful in denoting the energy of photons with higher frequency and higher energy, such as gamma rays, as opposed to lower energy photons, such as those in the radio frequency region of the electromagnetic spectrum. Formulas Physics Photon energy is directly proportional to frequency. where is energy (J) is Planck's constant: 6.62607015 × 10−34 (m2kgs−1) is frequency (Hz)",
                    "score": 0.9154350757598877
                },
                {
                    "id": 18326095,
                    "contents": "Photon energy\nFormulas Physics Photon energy is directly proportional to frequency. where is energy (J) is Planck's constant: 6.62607015 × 10−34 (m2kgs−1) is frequency (Hz) This equation is known as the Planck-Einstein relation. Additionally, where E is photon energy (Joules), λ is the photon's wavelength (metres), c is the speed of light in vacuum: 299792458 metres per second h is the Planck constant: 6.62607015 × 10−34 (m2kgs−1) The photon energy at 1 Hz is equal to 6.62607015 × 10−34 J That is equal to 4.135667697 × 10−15 eV (electronvolts) Electronvolts Energy is often measured in electronvolts. To find the photon energy in electronvolts using the wavelength in micrometres, the equation is approximately This equation only holds if the wavelength is measured in picometers. The photon energy at 1 μm wavelength, the wavelength of near infrared radiation, is approximately 1.2398 eV. In chemistry, quantum physics and optical engineering See",
                    "score": 0.9149980545043945
                },
                {
                    "id": 1168412,
                    "contents": "Photon\nIn physics, a photon is usually denoted by the symbol (the Greek letter gamma). This symbol for the photon probably derives from gamma rays, which were discovered in 1900 by Paul Villard, named by Ernest Rutherford in 1903, and shown to be a form of electromagnetic radiation in 1914 by Rutherford and Edward Andrade. In chemistry and optical engineering, photons are usually symbolized by , which is the photon energy, where is Planck constant and the Greek letter (nu) is the photon's frequency. Much less commonly, the photon can be symbolized by , where its frequency is denoted by .",
                    "score": 0.9133908152580261
                },
                {
                    "id": 1168410,
                    "contents": "Photon\nNomenclature The word quanta (singular quantum, Latin for how much) was used before 1900 to mean particles or amounts of different quantities, including electricity. In 1900, the German physicist Max Planck was studying black-body radiation, and he suggested that the experimental observations, specifically at shorter wavelengths, would be explained if the energy stored within a molecule was a \"discrete quantity composed of an integral number of finite equal parts\", which he called \"energy elements\". In 1905, Albert Einstein published a paper in which he proposed that many light-related phenomena—including black-body radiation and the photoelectric effect—would be better explained by modelling electromagnetic waves as consisting of spatially localized, discrete wave-packets. He called such a wave-packet the light quantum (German: das Lichtquant).",
                    "score": 0.9114692807197571
                },
                {
                    "id": 18106728,
                    "contents": "Single-photon source\nReferences Bibliography R. Loudon, The Quantum Theory of Light,:Oxford University Press, 3rd edition (2000). Translated in Light sources Photonics",
                    "score": 0.9057905673980713
                },
                {
                    "id": 1168432,
                    "contents": "Photon\nnot a short pulse of electromagnetic radiation; a photon's Maxwell waves will diffract, but photon energy does not spread out as it propagates, nor does this energy divide when it encounters a beam splitter. Rather, the received photon acts like a point-like particle since it is absorbed or emitted as a whole by arbitrarily small systems, including systems much smaller than its wavelength, such as an atomic nucleus (≈10−15 m across) or even the point-like electron.",
                    "score": 0.9049138426780701
                },
                {
                    "id": 3271384,
                    "contents": "Quantum optics\nSeveral Nobel prizes have been awarded for work in quantum optics. These were awarded: in 2012, Serge Haroche and David J. Wineland \"for ground-breaking experimental methods that enable measuring & manipulation of individual quantum systems\". in 2005, Theodor W. Hänsch, Roy J. Glauber and John L. Hall in 2001, Wolfgang Ketterle, Eric Allin Cornell and Carl Wieman in 1997, Steven Chu, Claude Cohen-Tannoudji and William Daniel Phillips Concepts According to quantum theory, light may be considered not only to be as an electro-magnetic wave but also as a \"stream\" of particles called photons which travel with c, the vacuum speed of light. These particles should not be considered to be classical billiard balls, but as quantum mechanical particles described by a wavefunction spread over a finite region.",
                    "score": 0.904442548751831
                },
                {
                    "id": 1151283,
                    "contents": "Optics\nPhysical optics In physical optics, light is considered to propagate as a wave. This model predicts phenomena such as interference and diffraction, which are not explained by geometric optics. The speed of light waves in air is approximately 3.0×108 m/s (exactly 299,792,458 m/s in vacuum). The wavelength of visible light waves varies between 400 and 700 nm, but the term \"light\" is also often applied to infrared (0.7–300 μm) and ultraviolet radiation (10–400 nm).",
                    "score": 0.903899073600769
                },
                {
                    "id": 3271381,
                    "contents": "Quantum optics\na single atom emitting one photon at a time, further compelling evidence that light consists of photons. Previously unknown quantum states of light with characteristics unlike classical states, such as squeezed light were subsequently discovered.",
                    "score": 0.9032011032104492
                },
                {
                    "id": 16071795,
                    "contents": "Branches of physics\nFor example, the light, or electromagnetic radiation emitted or absorbed by an atom has only certain frequencies (or wavelengths), as can be seen from the line spectrum associated with the chemical element represented by that atom. The quantum theory shows that those frequencies correspond to definite energies of the light quanta, or photons, and result from the fact that the electrons of the atom can have only certain allowed energy values, or levels; when an electron changes from one allowed level to another, a quantum of energy is emitted or absorbed whose frequency is directly proportional to the energy difference between the two levels. The photoelectric effect further confirmed the quantization of light.",
                    "score": 0.9029306769371033
                },
                {
                    "id": 11719110,
                    "contents": "History of quantum mechanics\nFounding experiments Thomas Young's double-slit experiment demonstrating the wave nature of light. (c. 1801) Henri Becquerel discovers radioactivity. (1896) J. J. Thomson's cathode ray tube experiments (discovers the electron and its negative charge). (1897) The study of black-body radiation between 1850 and 1900, which could not be explained without quantum concepts. The photoelectric effect: Einstein explained this in 1905 (and later received a Nobel prize for it) using the concept of photons, particles of light with quantized energy. Robert Millikan's oil-drop experiment, which showed that electric charge occurs as quanta (whole units). (1909) Ernest Rutherford's gold foil experiment disproved the plum pudding model of the atom which suggested that the mass and positive charge of the atom are almost uniformly distributed. This led to the planetary model of the atom (1911).",
                    "score": 0.9024223685264587
                },
                {
                    "id": 15462837,
                    "contents": "Angstrom\nThe angstrom (, ; , ) or ångström is a metric unit of length equal to m; that is, one ten-billionth (US) of a metre, a hundred-millionth of a centimetre, 0.1 nanometre, or 100 picometres. Its symbol is Å, a letter of the Swedish alphabet. The unit is named after the Swedish physicist Anders Jonas Ångström (1814–1874). The angstrom is often used in the natural sciences and technology to express sizes of atoms, molecules, microscopic biological structures, and lengths of chemical bonds, arrangement of atoms in crystals, wavelengths of electromagnetic radiation, and dimensions of integrated circuit parts. The atomic (covalent) radii of phosphorus, sulfur, and chlorine are about 1 angstrom, while that of hydrogen is about 0.5 angstroms. Visible light has wavelengths in the range of 4000–7000 Å.",
                    "score": 0.9016650319099426
                },
                {
                    "id": 7748002,
                    "contents": "Introduction to quantum mechanics\nIn 1885 the Swiss mathematician Johann Balmer discovered that each wavelength (lambda) in the visible spectrum of hydrogen is related to some integer by the equation where is a constant Balmer determined is equal to 364.56 nm. In 1888 Johannes Rydberg generalized and greatly increased the explanatory utility of Balmer's formula. He predicted that is related to two integers and according to what is now known as the Rydberg formula: where R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m.",
                    "score": 0.9012125134468079
                },
                {
                    "id": 7747995,
                    "contents": "Introduction to quantum mechanics\nEinstein explained the effect by postulating that a beam of light is a stream of particles (\"photons\") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron. To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the work function and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:",
                    "score": 0.9011626243591309
                },
                {
                    "id": 9766133,
                    "contents": "Herwig Schopper\nScientific publications Schopper wrote more than 200 original publications in optics, nuclear physics, elementary particle physics and accelerator technology. Some of the most important works are: Fleischmann R. and H. Schopper, The determination of the optical constants and thickness of the layer of absorbent layers by means of the measurement of the absolute phase change Z.Physik 129.285 (1951) (first method for the measurement of the absolute phase upon reflection of light on the thin metal layers) H. Schopper, The interpretation of the optical constants of alkali metals, Z.Physik 135, 163 (1953) (the abnormal optical behaviour of thin alkali metal layers does not require a special physical state of the metal)",
                    "score": 0.9011163711547852
                },
                {
                    "id": 552407,
                    "contents": "Wavenumber\nFor example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula: where R is the Rydberg constant, and ni and nf are the principal quantum numbers of the initial and final levels respectively (ni is greater than nf for emission). A spectroscopic wavenumber can be converted into energy per photon E by Planck's relation: It can also be converted into wavelength of light: where n is the refractive index of the medium. Note that the wavelength of light changes as it passes through different media, however, the spectroscopic wavenumber (i.e., frequency) remains constant. Conventionally, inverse centimeter (cm−1) units are used for , so often that such spatial frequencies are stated by some authors \"in wavenumbers\", incorrectly transferring the name of the quantity to the CGS unit cm−1 itself. See also Spatial frequency Refractive index Zonal wavenumber References Wave mechanics Physical quantities Units of frequency",
                    "score": 0.900854766368866
                },
                {
                    "id": 13262097,
                    "contents": "List of scientific publications by Albert Einstein\n| Naturwissenschaften, 12, 601–602 || History of physics. |- | Schilpp 185; Weil *142 || 1924 || Quantentheorie des einatomigen idealen Gases | Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-mathematische Klasse, 1924, 261–267 || Photons and statistical mechanics. First of two seminal papers (see reference #194), in which Einstein creates the theory of identical particles in quantum mechanics. In 1924, Satyendra Nath Bose derived Planck's law of black-body radiation from a modification of coarse-grained counting of phase space. Einstein shows that this modification is equivalent to assuming that photons are rigorously identical, leading to the concept of coherent states. Einstein also extends Bose's formalism to material particles (bosons), predicting that they condense at sufficiently low temperatures, as verified experimentally. |- | Schilpp 186 || 1924 || Über den Äther",
                    "score": 0.9006712436676025
                },
                {
                    "id": 381800,
                    "contents": "Planck's law\nThe absorption coefficient is the fractional change in the intensity of the light beam as it travels the distance , and has units of length−1. It is composed of two parts, the decrease due to absorption and the increase due to stimulated emission. Stimulated emission is emission by the material body which is caused by and is proportional to the incoming radiation. It is included in the absorption term because, like absorption, it is proportional to the intensity of the incoming radiation. Since the amount of absorption will generally vary linearly as the density of the material, we may define a \"mass absorption coefficient\" which is a property of the material itself. The change in intensity of a light beam due to absorption as it traverses a small distance will then be",
                    "score": 0.9006549715995789
                },
                {
                    "id": 1168424,
                    "contents": "Photon\nThe Maxwell wave theory, however, does not account for all properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.",
                    "score": 0.9005356431007385
                },
                {
                    "id": 16528559,
                    "contents": "Kramers' law\nTo obtain a simple expression for the energy flux, first change variables from (the wavelength) to (the angular frequency) using and also using . Now is that quantity which is integrated over from 0 to to get the total number (still infinite) of photons, where : The energy flux, which we will call (but which may also be referred to as the \"intensity\" in conflict with the above name of ) is obtained by multiplying the above by the energy : for for . It is a linear function that is zero at the maximum energy . References Spectroscopy X-rays",
                    "score": 0.8999435901641846
                },
                {
                    "id": 907505,
                    "contents": "Intensity (physics)\nand the local intensity is obtained by multiplying this expression by the wave velocity, c/n: where n is the refractive index, c is the speed of light in vacuum and is the vacuum permittivity. For non-monochromatic waves, the intensity contributions of different spectral components can simply be added. The treatment above does not hold for arbitrary electromagnetic fields. For example, an evanescent wave may have a finite electrical amplitude while not transferring any power. The intensity should then be defined as the magnitude of the Poynting vector.",
                    "score": 0.8997050523757935
                },
                {
                    "id": 7747989,
                    "contents": "Introduction to quantum mechanics\nPhotons: the quantization of light In 1905, Albert Einstein took an extra step. He suggested that quantization was not just a mathematical construct, but that the energy in a beam of light actually occurs in individual packets, which are now called photons. The energy of a single photon of light of frequency is given by the frequency multiplied by Planck's constant (an extremely tiny positive number):",
                    "score": 0.8995429277420044
                },
                {
                    "id": 24044901,
                    "contents": "Harvey Elliott White\nPrivate life White had a lifelong interest in ham radio; his call sign was 6KS. He married Adeline Dally in 1928; they had three children. White dedicated his textbook Modern College Physics to his son Don. He died in Modesto, California. Selected bibliography White, Harvey Elliott. Fundamentals of Physical Optics (McGraw-Hill, 1937) White, Harvey Elliott. Introduction to Atomic Spectra (McGraw-Hill, 1934) White, Harvey Elliott. Modern College Physics (1948) Jenkins, Francis and White, Harvey Elliott. Fundamentals of Physical Optics (1937); revised ed. Fundamentals of Optics (1950) References 1902 births People from Parkersburg, West Virginia 1988 deaths Occidental College alumni Cornell University alumni University of California, Berkeley faculty",
                    "score": 0.8994088172912598
                },
                {
                    "id": 986656,
                    "contents": "Atomic, molecular, and optical physics\nLater, the connection between atomic physics and optical physics became apparent, by the discovery of spectral lines and attempts to describe the phenomenon - notably by Joseph von Fraunhofer, Fresnel, and others in the 19th century. From that time to the 1920s, physicists were seeking to explain atomic spectra and blackbody radiation. One attempt to explain hydrogen spectral lines was the Bohr atom model. Experiments including electromagnetic radiation and matter - such as the photoelectric effect, Compton effect, and spectra of sunlight the due to the unknown element of Helium, the limitation of the Bohr model to Hydrogen, and numerous other reasons, lead to an entirely new mathematical model of matter and light: quantum mechanics.",
                    "score": 0.8993595242500305
                },
                {
                    "id": 7747982,
                    "contents": "Introduction to quantum mechanics\nLight behaves in some aspects like particles and in other aspects like waves. Matter—the \"stuff\" of the universe consisting of particles such as electrons and atoms—exhibits wavelike behavior too. Some light sources, such as neon lights, give off only certain specific frequencies of light, a small set of distinct pure colors determined by neon's atomic structure. Quantum mechanics shows that light, along with all other forms of electromagnetic radiation, comes in discrete units, called photons, and predicts its spectral energies (corresponding to pure colors), and the intensities of its light beams. A single photon is a quantum, or smallest observable particle, of the electromagnetic field. A partial photon is never experimentally observed. More broadly, quantum mechanics shows that many properties of objects, such as position, speed, and angular momentum, that appeared continuous in the zoomed-out view of classical mechanics, turn out to be (in the very tiny, zoomed-in scale of",
                    "score": 0.8993228673934937
                },
                {
                    "id": 16680977,
                    "contents": "Timeline of quantum mechanics\n1801 – Thomas Young establishes that light made up of waves with his Double-slit experiment. 1859 – Gustav Kirchhoff introduces the concept of a blackbody and proves that its emission spectrum depends only on its temperature. 1860-1900 – Ludwig Eduard Boltzmann, James Clerk Maxwell and others develop the theory of statistical mechanics. Boltzmann argues that entropy is a measure of disorder. 1877 – Boltzmann suggests that the energy levels of a physical system could be discrete based on statistical mechanics and mathematical arguments; also produces the first circle diagram representation, or atomic model of a molecule (such as an iodine gas molecule) in terms of the overlapping terms α and β, later (in 1928) called molecular orbitals, of the constituting atoms. 1885 – Johann Jakob Balmer discovers a numerical relationship between visible spectral lines of hydrogen, the Balmer series.",
                    "score": 0.8991089463233948
                },
                {
                    "id": 937603,
                    "contents": "Wien's displacement law\nThe total radiance is the integral of the distribution over all positive values, and that is invariant for a given temperature under any parameterization. Additionally, for a given temperature the radiance consisting of all photons between two wavelengths must be the same regardless of which distribution you use. That is to say, integrating the wavelength distribution from λ1 to λ2 will result in the same value as integrating the frequency distribution between the two frequencies that correspond to λ1 and λ2, namely from c/λ2 to c/λ1. However, the distribution shape depends on the parameterization, and for a different parameterization the distribution will typically have a different peak density, as these calculations demonstrate.",
                    "score": 0.8988955020904541
                },
                {
                    "id": 1168440,
                    "contents": "Photon\nwhere is the rate constant for emitting a photon spontaneously, and is the rate constant for emissions in response to ambient photons (induced or stimulated emission). In thermodynamic equilibrium, the number of atoms in state and those in state must, on average, be constant; hence, the rates and must be equal. Also, by arguments analogous to the derivation of Boltzmann statistics, the ratio of and is where and are the degeneracy of the state and that of , respectively, and their energies, the Boltzmann constant and the system's temperature. From this, it is readily derived that and The and are collectively known as the Einstein coefficients.",
                    "score": 0.8988012671470642
                },
                {
                    "id": 552406,
                    "contents": "Wavenumber\nwhich remains essentially the same in air, and so the spectroscopic wavenumber is directly related to the angles of light scattered from diffraction gratings and the distance between fringes in interferometers, when those instruments are operated in air or vacuum. Such wavenumbers were first used in the calculations of Johannes Rydberg in the 1880s. The Rydberg–Ritz combination principle of 1908 was also formulated in terms of wavenumbers. A few years later spectral lines could be understood in quantum theory as differences between energy levels, energy being proportional to wavenumber, or frequency. However, spectroscopic data kept being tabulated in terms of spectroscopic wavenumber rather than frequency or energy. For example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula:",
                    "score": 0.8985604643821716
                },
                {
                    "id": 154982,
                    "contents": "Rydberg formula\nBut the Rydberg formula also provides correct wavelengths for distant electrons, where the effective nuclear charge can be estimated as the same as that for hydrogen, since all but one of the nuclear charges have been screened by other electrons, and the core of the atom has an effective positive charge of +1.",
                    "score": 0.8983201384544373
                },
                {
                    "id": 3086292,
                    "contents": "Arthur Compton\nwhere is the initial wavelength, is the wavelength after scattering, is the Planck constant, is the electron rest mass, is the speed of light, and is the scattering angle. The quantity is known as the Compton wavelength of the electron; it is equal to . The wavelength shift lies between zero (for ) and twice the Compton wavelength of the electron (for ). He found that some X-rays experienced no wavelength shift despite being scattered through large angles; in each of these cases the photon failed to eject an electron. Thus the magnitude of the shift is related not to the Compton wavelength of the electron, but to the Compton wavelength of the entire atom, which can be upwards of 10,000 times smaller.",
                    "score": 0.8982977867126465
                },
                {
                    "id": 7748003,
                    "contents": "Introduction to quantum mechanics\nwhere R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m. Rydberg's formula accounts for the four visible wavelengths of hydrogen by setting and . It also predicts additional wavelengths in the emission spectrum: for and for , the emission spectrum should contain certain ultraviolet wavelengths, and for and , it should also contain certain infrared wavelengths. Experimental observation of these wavelengths came two decades later: in 1908 Louis Paschen found some of the predicted infrared wavelengths, and in 1914 Theodore Lyman found some of the predicted ultraviolet wavelengths. Both Balmer and Rydberg's formulas involve integers: in modern terms, they imply that some property of the atom is quantized. Understanding exactly what this property was, and why it was quantized, was a major part of the development of quantum mechanics, as shown in the rest of this article.",
                    "score": 0.8981914520263672
                },
                {
                    "id": 1184214,
                    "contents": "Outline of physics\nHistory of atomic, molecular, and optical physics – history of the study of how matter and light interact History of biophysics – history of the study of physical processes relating to biology History of medical physics – history of the application of physics concepts, theories and methods to medicine. History of neurophysics – history of the branch of biophysics dealing with the nervous system. History of chemical physics – history of the branch of physics that studies chemical processes from the point of view of physics. History of computational physics – history of the study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists. History of condensed matter physics – history of the study of the physical properties of condensed phases of matter.",
                    "score": 0.8979209661483765
                },
                {
                    "id": 701061,
                    "contents": "Max Born\nBibliography During his life, Born wrote several semi-popular and technical books. His volumes on topics like atomic physics and optics were very well received. They are considered classics in their fields, and are still in print. The following is a chronological listing of his major works:",
                    "score": 0.8977774381637573
                },
                {
                    "id": 13536555,
                    "contents": "Spectral flux density\nIn spectroscopy, spectral flux density is the quantity that describes the rate at which energy is transferred by electromagnetic radiation through a real or virtual surface, per unit surface area and per unit wavelength (or, equivalently, per unit frequency). It is a radiometric rather than a photometric measure. In SI units it is measured in W m−3, although it can be more practical to use W m−2 nm−1 (1 W m−2 nm−1 = 1 GW m−3 = 1 W mm−3) or W m−2 μm−1 (1 W m−2 μm−1 = 1 MW m−3), and respectively by W·m−2·Hz−1, Jansky or solar flux units. The terms irradiance, radiant exitance, radiant emittance, and radiosity are closely related to spectral flux density.",
                    "score": 0.8977062702178955
                },
                {
                    "id": 381847,
                    "contents": "Planck's law\nFor long wavelengths, Rayleigh's 1900 heuristic formula approximately meant that energy was proportional to temperature, . It is known that and this leads to and thence to for long wavelengths. But for short wavelengths, the Wien formula leads to and thence to for short wavelengths. Planck perhaps patched together these two heuristic formulas, for long and for short wavelengths, to produce a formula This led Planck to the formula where Planck used the symbols and to denote empirical fitting constants.",
                    "score": 0.8975468873977661
                },
                {
                    "id": 1168408,
                    "contents": "Photon\nLike all elementary particles, photons are currently best explained by quantum mechanics, and exhibit wave–particle duality, their behavior featuring properties of both waves and particles. The modern photon concept originated during the first two decades of the 20th century with the work of Albert Einstein, who built upon the research of Max Planck. While trying to explain how matter and electromagnetic radiation could be in thermal equilibrium with one another, Planck proposed that the energy stored within a material object should be regarded as composed of an integer number of discrete, equal-sized parts. To explain the photoelectric effect, Einstein introduced the idea that light itself is made of discrete units of energy. In 1926, Gilbert N. Lewis popularized the term photon for these energy units. Subsequently, many other experiments validated Einstein's approach.",
                    "score": 0.8975344300270081
                },
                {
                    "id": 6598375,
                    "contents": "Einstein coefficients\nReferences Cited bibliography Chandrasekhar, S. (1950). Radiative Transfer, Oxford University Press, Oxford. Garrison, J. C., Chiao, R. Y. (2008). Quantum Optics, Oxford University Press, Oxford UK, . Goody, R. M., Yung, Y. L. (1989). Atmospheric Radiation: Theoretical Basis, 2nd edition, Oxford University Press, Oxford, New York, 1989, . Translated as \"Quantum-theoretical Re-interpretation of kinematic and mechanical relations\" in Herzberg, G. (1950). Molecular Spectroscopy and Molecular Structure, vol. 1, Diatomic Molecules, second edition, Van Nostrand, New York. Loudon, R. (1973/2000). The Quantum Theory of Light, (first edition 1973), third edition 2000, Oxford University Press, Oxford UK, . Mihalas, D., Weibel-Mihalas, B. (1984). Foundations of Radiation Hydrodynamics, Oxford University Press, New York . Yariv, A. (1967/1989). Quantum Electronics, third edition, John Wiley & sons, New York, . Other reading",
                    "score": 0.8975287675857544
                },
                {
                    "id": 552400,
                    "contents": "Wavenumber\nWavenumber can be used to specify quantities other than spatial frequency. In optical spectroscopy, it is often used as a unit of temporal frequency assuming a certain speed of light. Definition Wavenumber, as used in spectroscopy and most chemistry fields, is defined as the number of wavelengths per unit distance, typically centimeters (cm−1): where λ is the wavelength. It is sometimes called the \"spectroscopic wavenumber\". It equals the spatial frequency. A wavenumber in inverse cm can be converted to a frequency in GHz by multiplying by 29.9792458 (the speed of light in centimeters per nanosecond). An electromagnetic wave at 29.9792458 GHz has a wavelength of 1 cm in free space. In theoretical physics, a wave number defined as the number of radians per unit distance, sometimes called \"angular wavenumber\", is more often used:",
                    "score": 0.897480845451355
                },
                {
                    "id": 5205018,
                    "contents": "Compton wavelength\nIt appears in the Dirac equation (the following is an explicitly covariant form employing the Einstein summation convention): The reduced Compton wavelength also appears in Schrödinger's equation, although its presence is obscured in traditional representations of the equation. The following is the traditional representation of Schrödinger's equation for an electron in a hydrogen-like atom: Dividing through by , and rewriting in terms of the fine structure constant, one obtains:",
                    "score": 0.8970649838447571
                },
                {
                    "id": 16681001,
                    "contents": "Timeline of quantum mechanics\n1926 – Paul Epstein reconsiders the linear and quadratic Stark effect from the point of view of the new quantum theory, using the equations of Schrödinger and others. The derived equations for the line intensities are a decided improvement over previous results obtained by Hans Kramers. 1926 to 1932 – John von Neumann lays the mathematical foundations of Quantum Mechanics in terms of Hermitian operators on Hilbert spaces, subsequently published in 1932 as a basic textbook of quantum mechanics. 1927 – Werner Heisenberg formulates the quantum uncertainty principle. 1927 – Niels Bohr and Werner Heisenberg develops the Copenhagen interpretation of the probabilistic nature of wavefunctions. 1927 – Born and J. Robert Oppenheimer introduce the Born–Oppenheimer approximation, which allows the quick approximation of the energy and wavefunctions of smaller molecules.",
                    "score": 0.8967854976654053
                },
                {
                    "id": 8920454,
                    "contents": "Curtis J. Humphreys\nOther works include: T.L.De Bruin, C.J.Humphreys, and W.F.Meggers, J. Res. NBS (U.S.) 11, 409 (1933). \"The 29 and 30 electron-system spectra of arsenic and selenium\" Curtis J Humphreys, 1928. \"Element Ne I\" Meggers, W. F., and Humphreys, C. J. 1933, J. Res. N. B. S. 10, 427. [EA, 7724-18549, a UMT and RMTsource] C.J.Humphreys, J. Res. NBS (U.S.) 22, 19 (1939). C.J.Humphreys, J. Opt. Soc. Am. 43, 1027 (1953). \"Humphreys Series\" Humphreys, C.J., J. Research Natl. Bur. Standards 1953, 50, 1. \"Interferometric measurement of wavelengths of infrared atomic emission lines in the extraphotographic region\" Applied Optics, 1963. Co-authored Rao, K. Narahari; Curtis J. Humphreys; D.H. Rank, \"Wavelength Standards in the Infrared\", Academic Press, 1966. Humphreys, C. J., & Paul, E. 1970, J. Opt. Soc. Am., 60, 1302. H.H. Li and C.J. Humphreys and J. Opt. Soc. Am. 64 (1974) 1072. C.J. Humphreys, Rep. Prog. Phys. 42 (1979) 122. References",
                    "score": 0.8966844081878662
                },
                {
                    "id": 9723351,
                    "contents": "Two-photon absorption\nCoefficients The two-photon absorption coefficient is defined by the relation so that Where is the two-photon absorption coefficient, is the absorption coefficient, is the transition rate for TPA per unit volume, is the irradiance, is the reduced Planck constant, is the photon frequency and the thickness of the slice is . is the number density of molecules per cm3, is the photon energy (J), is the two-photon absorption cross section (cm4s/molecule). The SI units of the beta coefficient are m/W. If (m/W) is multiplied by 10−9 it can be converted to the CGS system (cal/cm s/erg). Due to different laser pulses the TPA coefficients reported has differed as much as a factor 3. With the transition towards shorter laser pulses, from picosecond to subpicosecond durations, noticeably reduced TPA coefficient have been obtained. In water Laser induced TPA in water was discovered in 1980.",
                    "score": 0.8962491750717163
                },
                {
                    "id": 7747990,
                    "contents": "Introduction to quantum mechanics\nFor centuries, scientists had debated between two possible theories of light: was it a wave or did it instead comprise a stream of tiny particles? By the 19th century, the debate was generally considered to have been settled in favor of the wave theory, as it was able to explain observed effects such as refraction, diffraction, interference, and polarization. James Clerk Maxwell had shown that electricity, magnetism, and light are all manifestations of the same phenomenon: the electromagnetic field. Maxwell's equations, which are the complete set of laws of classical electromagnetism, describe light as waves: a combination of oscillating electric and magnetic fields. Because of the preponderance of evidence in favor of the wave theory, Einstein's ideas were met initially with great skepticism. Eventually, however, the photon model became favored. One of the most significant pieces of evidence in its favor was its ability to explain several puzzling properties of the photoelectric",
                    "score": 0.8959331512451172
                },
                {
                    "id": 6598018,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path of electrons can roughly be described by a universal curve that is the same for all materials. See also Scattering theory Beer-Lambert law References Atomic, molecular, and optical physics",
                    "score": 0.8958709836006165
                },
                {
                    "id": 18146259,
                    "contents": "2019 redefinition of the SI base units\nThe speed of light is exactly ; The ground state hyperfine structure transition frequency of the caesium-133 atom is exactly ; The luminous efficacy of monochromatic radiation of frequency () – a frequency of green-colored light at approximately the peak sensitivity of the human eye – is exactly . The seven definitions above are rewritten below with the derived units (joule, coulomb, hertz, lumen, and watt) expressed in terms of the seven base units: second, metre, kilogram, ampere, kelvin, mole, and candela, according to the 9th SI Brochure. In the list that follows, the symbol sr stands for the dimensionless unit steradian. = = = = = = = =",
                    "score": 0.8957803249359131
                },
                {
                    "id": 4757586,
                    "contents": "QED: The Strange Theory of Light and Matter\nThe four lectures 1. Photons - Corpuscles of Light In the first lecture, which acts as a gentle lead-in to the subject of quantum electrodynamics, Feynman describes the basic properties of photons. He discusses how to measure the probability that a photon will reflect or transmit through a partially reflective piece of glass. 2. Fits of Reflection and Transmission - Quantum Behaviour In the second lecture, Feynman looks at the different paths a photon can take as it travels from one point to another and how this affects phenomena like reflection and diffraction. 3. Electrons and Their interactions The third lecture describes quantum phenomena such as the famous double-slit experiment and Werner Heisenberg's uncertainty principle, thus describing the transmission and reflection of photons. It also introduces his famous \"Feynman diagrams\" and how quantum electrodynamics describes the interactions of subatomic particles. 4. New Queries",
                    "score": 0.8956746459007263
                },
                {
                    "id": 986659,
                    "contents": "Atomic, molecular, and optical physics\n. where E0 is the magnitude of the electric field amplitude, and E is the magnitude of the electric field at position x. From this basic, Planck's law was derived. In 1911, Ernest Rutherford concluded, based on alpha particle scattering, that an atom has a central pointlike proton. He also thought that an electron would be still attracted to the proton by Coulomb's law, which he had verified still held at small scales. As a result, he believed that electrons revolved around the proton. Niels Bohr, in 1913, combined the Rutherford model of the atom with the quantisation ideas of Planck. Only specific and well-defined orbits of the electron could exist, which also do not radiate light. In jumping orbit the electron would emit or absorb light corresponding to the difference in energy of the orbits. His prediction of the energy levels was then consistent with observation.",
                    "score": 0.8955694437026978
                },
                {
                    "id": 21143392,
                    "contents": "Pierre-Michel Duffieux\nDuffieux was also interested in philosophy, music, and the meaning and interpretation of quantum theory. References Bibliography Max Born and Emil Wolf (1999). Principles of Optics: Electromagnetic Theory of Propagation, Interference and Diffraction of Light, seventh edition (Cambridge University Press), p. 543. P.-M. Duffieux (1983). The Fourier Transform and its Applications to Optics (Wiley, New York). Peter Hawkes and Noël Bonnet (1997). A symposium in honour of Pierre-Michel Duffieux. Microscopy, Microanalysis, Microstructures vol. 8, no. 1, pp. ix - xiv. André Maréchal (1976). Pierre-Michel Duffieux. Physics Today, vol. 29, No. 11 (November), p. 85. External links Former laboratory of P. M. Duffieux FEMTO-ST : Département d'Optique P. M. Duffieux 1891 births 1976 deaths Scientists from Bordeaux French physicists Optical physicists",
                    "score": 0.8954052925109863
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.6
            }
        }
    },
    {
        "id": "test_27",
        "question": "A proton and a negatively charged $\\mu$ meson (called a muon) can form a short-lived species called a mesonic atom. The charge of a muon is the same as that on an electron and the mass of a muon is $207 m_{\\mathrm{e}}$. Assume that the Bohr theory can be applied to such a mesonic atom and calculate the ground-state energy, the radius of the first Bohr orbit, and the energy and frequency associated with the $n=1$ to $n=2$ transition in a mesonic atom.",
        "golden_answers": [
            " 1.69"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 10704624,
                    "contents": "Mesonic molecule\nA mesonic molecule is a set of two or more mesons bound together by the strong force. Unlike baryonic molecules, which form the nuclei of all elements in nature save hydrogen-1, a mesonic molecule has yet to be definitively observed. The X(3872) discovered in 2003 and the Z(4430) discovered in 2007 by the Belle experiment are the best candidates for such an observation. See also Meson Tetraquark Pionium References Particle physics Subatomic particles Hypothetical composite particles",
                    "score": 0.8986412286758423
                },
                {
                    "id": 1872171,
                    "contents": "Muon\nMuonic atoms The muon was the first elementary particle discovered that does not appear in ordinary atoms. Negative muon atoms Negative muons can, however, form muonic atoms (previously called mu-mesic atoms), by replacing an electron in ordinary atoms. Muonic hydrogen atoms are much smaller than typical hydrogen atoms because the much larger mass of the muon gives it a much more localized ground-state wavefunction than is observed for the electron. In multi-electron atoms, when only one of the electrons is replaced by a muon, the size of the atom continues to be determined by the other electrons, and the atomic size is nearly unchanged. However, in such cases the orbital of the muon continues to be smaller and far closer to the nucleus than the atomic orbitals of the electrons.",
                    "score": 0.8907230496406555
                },
                {
                    "id": 1872174,
                    "contents": "Muon\nPositive muon atoms A positive muon, when stopped in ordinary matter, cannot be captured by a proton since the two positive charges can only repel. The positive muon is also not attracted to the nucleus of atoms. Instead, it binds a random electron and with this electron forms an exotic atom known as muonium (mu) atom. In this atom, the muon acts as the nucleus. The positive muon, in this context, can be considered a pseudo-isotope of hydrogen with one ninth of the mass of the proton. Because the mass of the electron is much smaller than the mass of both the proton and the muon, the reduced mass of muonium, and hence its Bohr radius, is very close to that of hydrogen. Therefore this bound muon-electron pair can be treated to a first approximation as a short-lived \"atom\" that behaves chemically like the isotopes of hydrogen (protium, deuterium and tritium).",
                    "score": 0.8886058330535889
                },
                {
                    "id": 1868230,
                    "contents": "Meson\nIn particle physics, mesons ( or ) are hadronic subatomic particles composed of an equal number of quarks and antiquarks, usually one of each, bound together by strong interactions. Because mesons are composed of quark subparticles, they have a meaningful physical size, a diameter of roughly one femtometer (1×10 m), which is about 0.6 times the size of a proton or neutron. All mesons are unstable, with the longest-lived lasting for only a few hundredths of a microsecond. Heavier mesons decay to lighter mesons and ultimately to stable electrons, neutrinos and photons. Outside the nucleus, mesons appear in nature only as short-lived products of very high-energy collisions between particles made of quarks, such as cosmic rays (high-energy protons and neutrons) and baryonic matter. Mesons are routinely produced artificially in cyclotrons or other accelerators in the collisions of protons, antiprotons, or other particles.",
                    "score": 0.8851668834686279
                },
                {
                    "id": 1872175,
                    "contents": "Muon\nBoth positive and negative muons can be part of a short-lived pi-mu atom consisting of a muon and an oppositely charged pion. These atoms were observed in the 1970s in experiments at Brookhaven and Fermilab. Use in measurement of the proton charge radius",
                    "score": 0.8706504106521606
                },
                {
                    "id": 1372112,
                    "contents": "Pion\nHistory Theoretical work by Hideki Yukawa in 1935 had predicted the existence of mesons as the carrier particles of the strong nuclear force. From the range of the strong nuclear force (inferred from the radius of the atomic nucleus), Yukawa predicted the existence of a particle having a mass of about 100 MeV/c. Initially after its discovery in 1936, the muon (initially called the \"mu meson\") was thought to be this particle, since it has a mass of 106 MeV/c. However, later experiments showed that the muon did not participate in the strong nuclear interaction. In modern terminology, this makes the muon a lepton, and not a meson. However, some communities of astrophysicists continue to call the muon a \"mu-meson\". The pions, which turned out to be examples of Yukawa's proposed mesons, were discovered later: the charged pions in 1947, and the neutral pion in 1950.",
                    "score": 0.869297206401825
                },
                {
                    "id": 1044066,
                    "contents": "Exotic atom\nThe true analogs of positronium in the theory of strong interactions, however, are not exotic atoms but certain mesons, the quarkonium states, which are made of a heavy quark such as the charm or bottom quark and its antiquark. (Top quarks are so heavy that they decay through the weak force before they can form bound states.) Exploration of these states through non-relativistic quantum chromodynamics (NRQCD) and lattice QCD are increasingly important tests of quantum chromodynamics. Muonium, despite its name, is not an onium containing a muon and an antimuon, because IUPAC assigned that name to the system of an antimuon bound with an electron. However, the production of a muon–antimuon bound state, which is an onium (called true muonium), has been theorized. Hypernuclear atoms",
                    "score": 0.8680355548858643
                },
                {
                    "id": 1872159,
                    "contents": "Muon\nAs more types of mesons were discovered in accelerator experiments later, it was eventually found that the mu meson significantly differed not only from the pi meson (of about the same mass), but also from all other types of mesons. The difference, in part, was that mu mesons did not interact with the nuclear force, as pi mesons did (and were required to do, in Yukawa's theory). Newer mesons also showed evidence of behaving like the pi meson in nuclear interactions, but not like the mu meson. Also, the mu meson's decay products included both a neutrino and an antineutrino, rather than just one or the other, as was observed in the decay of other charged mesons.",
                    "score": 0.8659583926200867
                },
                {
                    "id": 1872154,
                    "contents": "Muon\nLike all elementary particles, the muon has a corresponding antiparticle of opposite charge (+1 e) but equal mass and spin: the antimuon (also called a positive muon). Muons are denoted by and antimuons by . Formerly, muons were called \"mu mesons, but are not classified as mesons by modern particle physicists (see ), and that name is no longer used by the physics community. Muons have a mass of , which is approximately 207 times that of the electron, m. More precisely, it is . Due to their greater mass, muons accelerate more slowly than electrons in electromagnetic fields, and emit less bremsstrahlung (deceleration radiation). This allows muons of a given energy to penetrate far deeper into matter because the deceleration of electrons and muons is primarily due to energy loss by the bremsstrahlung mechanism. For example, so-called \"secondary muons\", created by cosmic rays hitting the atmosphere, can penetrate the atmosphere and reach Earth's land surface and even into deep mines.",
                    "score": 0.865099310874939
                },
                {
                    "id": 1372109,
                    "contents": "Pion\nIn particle physics, a pion (or a pi meson, denoted with the Greek letter pi: ) is any of three subatomic particles: , , and . Each pion consists of a quark and an antiquark and is therefore a meson. Pions are the lightest mesons and, more generally, the lightest hadrons. They are unstable, with the charged pions and decaying after a mean lifetime of 26.033 nanoseconds ( seconds), and the neutral pion decaying after a much shorter lifetime of 85 attoseconds ( seconds). Charged pions most often decay into muons and muon neutrinos, while neutral pions generally decay into gamma rays.",
                    "score": 0.8630004525184631
                },
                {
                    "id": 17832045,
                    "contents": "Phi meson\nwhere is the nonet mixing angle, and . The mixing angle at which the components decouple completely can be calculated to be . The mixing angle of the and states is calculated from the masses of each state to be about 35˚, which is very close to maximum decoupling. Therefore, the meson is nearly a pure state. History The existence of the meson was first proposed by the Japanese American particle physicist, J. J. Sakurai, in 1962 as a resonance state between the and the . It was discovered later in 1962 by Connolly, et al. in a 20-inch hydrogen bubble chamber at the Alternating Gradient Synchrotron (AGS) in Brookhaven National Laboratory in Uptown, NY while they were studying collisions at approximately 2.23GeV/c. In essence, the reaction involved a beam of s being accelerated to high energies to collide with protons.",
                    "score": 0.8624249696731567
                },
                {
                    "id": 1872157,
                    "contents": "Muon\nA particle with a mass in the meson range had been predicted before the discovery of any mesons, by theorist Hideki Yukawa: It seems natural to modify the theory of Heisenberg and Fermi in the following way. The transition of a heavy particle from neutron state to proton state is not always accompanied by the emission of light particles. The transition is sometimes taken up by another heavy particle. Because of its mass, the mu meson was initially thought to be Yukawa's particle and some scientists, including Niels Bohr, originally named it the yukon. Yukawa's predicted particle, the pi meson, was finally identified in 1947 (again from cosmic ray interactions), and was shown to differ from the mu meson by having the properties of a particle that mediated the nuclear force.",
                    "score": 0.8611695170402527
                },
                {
                    "id": 4321978,
                    "contents": "J/psi meson\nThe (J/psi) meson or psion is a subatomic particle, a flavor-neutral meson consisting of a charm quark and a charm antiquark. Mesons formed by a bound state of a charm quark and a charm anti-quark are generally known as \"charmonium\". The is the most common form of charmonium, due to its spin of 1 and its low rest mass. The has a rest mass of , just above that of the (), and a mean lifetime of . This lifetime was about a thousand times longer than expected.",
                    "score": 0.8601227402687073
                },
                {
                    "id": 1872160,
                    "contents": "Muon\nIn the eventual Standard Model of particle physics codified in the 1970s, all mesons other than the mu meson were understood to be hadrons – that is, particles made of quarks – and thus subject to the nuclear force. In the quark model, a meson was no longer defined by mass (for some had been discovered that were very massive – more than nucleons), but instead were particles composed of exactly two quarks (a quark and antiquark), unlike the baryons, which are defined as particles composed of three quarks (protons and neutrons were the lightest baryons). Mu mesons, however, had shown themselves to be fundamental particles (leptons) like electrons, with no quark structure. Thus, mu \"mesons\" were not mesons at all, in the new sense and use of the term meson used with the quark model of particle structure.",
                    "score": 0.860100269317627
                },
                {
                    "id": 1868235,
                    "contents": "Meson\nHistory From theoretical considerations, in 1934 Hideki Yukawa predicted the existence and the approximate mass of the \"meson\" as the carrier of the nuclear force that holds atomic nuclei together. If there were no nuclear force, all nuclei with two or more protons would fly apart due to electromagnetic repulsion. Yukawa called his carrier particle the meson, from μέσος mesos, the Greek word for \"intermediate\", because its predicted mass was between that of the electron and that of the proton, which has about 1,836 times the mass of the electron. Yukawa or Carl David Anderson, who discovered the muon, had originally named the particle the \"mesotron\", but he was corrected by the physicist Werner Heisenberg (whose father was a professor of Greek at the University of Munich). Heisenberg pointed out that there is no \"tr\" in the Greek word \"mesos\".",
                    "score": 0.8594928979873657
                },
                {
                    "id": 1872158,
                    "contents": "Muon\nWith two particles now known with the intermediate mass, the more general term meson was adopted to refer to any such particle within the correct mass range between electrons and nucleons. Further, in order to differentiate between the two different types of mesons after the second meson was discovered, the initial mesotron particle was renamed the mu meson (the Greek letter μ [mu] corresponds to m), and the new 1947 meson (Yukawa's particle) was named the pi meson.",
                    "score": 0.8587509393692017
                },
                {
                    "id": 22670786,
                    "contents": "Meson bomb\nOrigins Mesons (hadronic subatomic particles composed of one quark and one antiquark, bound together by the strong interaction) were proposed to form a nuclear weapon as early as the 1940s. Early speculation suggested that the resulting bomb would be the most powerful nuclear weapon yet to have been developed. American physicist Ernest Lawrence used the potential military applications of mesons to obtain funding for its synchrocyclotron built between 1940 and 1946 at the University of California, Berkeley. Soon, however, the scientific consensus was that construction of such a bomb would be impossible; and in 1968 physicist M. Stanley Livingston wrote that \"no responsible scientists would attempt to justify support in this field with predictions of an 'anti-matter engine,' or a super 'meson bomb,' or a 'hyper-drive' for spaceships.\"",
                    "score": 0.8582406640052795
                },
                {
                    "id": 1876729,
                    "contents": "Muonium\nBecause the muon is a lepton, the atomic energy levels of muonium can be calculated with great precision from quantum electrodynamics (QED), unlike in the case of hydrogen, where the precision is limited by uncertainties related to the internal structure of the proton. For this reason, muonium is an ideal system for studying bound-state QED and also for searching for physics beyond the standard model. Nomenclature Normally in the nomenclature of particle physics, an atom composed of a positively charged particle bound to an electron is named after the positive particle with \"-ium\" appended, in this case \"muium\". The suffix \"-onium\" is mostly used for bound states of a particle with its own antiparticle. The exotic atom consisting of a muon and an antimuon (which is yet to be observed) is known as true muonium. See also Muonic hydrogen Muon-catalyzed fusion References Exotic atoms",
                    "score": 0.8581985235214233
                },
                {
                    "id": 1868233,
                    "contents": "Meson\nBecause mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. Mesons are classified according to their quark content, total angular momentum, parity and various other properties, such as C-parity and G-parity. Although no meson is stable, those of lower mass are nonetheless more stable than the more massive, and hence are easier to observe and study in particle accelerators or in cosmic ray experiments. The lightest group of mesons is less massive than the lightest group of baryons, meaning that they are more easily produced in experiments, and thus exhibit certain higher-energy phenomena more readily than do baryons. But mesons can be quite massive: for example, the J/Psi meson () containing the charm quark, first seen 1974, is about three times as massive as a proton, and the upsilon meson () containing the bottom quark, first seen in 1977, is about ten times",
                    "score": 0.85579514503479
                },
                {
                    "id": 1372122,
                    "contents": "Pion\nCharged pion decays The mesons have a mass of and a mean lifetime of . They decay due to the weak interaction. The primary decay mode of a pion, with a branching fraction of 0.999877, is a leptonic decay into a muon and a muon neutrino: {| | || → || || + || |-- | || → || || + || |} The second most common decay mode of a pion, with a branching fraction of 0.000123, is also a leptonic decay into an electron and the corresponding electron antineutrino. This \"electronic mode\" was discovered at CERN in 1958: {| | || → || || + || |-- | || → || || + || |} The suppression of the electronic decay mode with respect to the muonic one is given approximately (up to a few percent effect of the radiative corrections) by the ratio of the half-widths of the pion–electron and the pion–muon decay reactions, and is a spin effect known as helicity suppression.",
                    "score": 0.854421079158783
                },
                {
                    "id": 1872153,
                    "contents": "Muon\nThe muon is an unstable subatomic particle with a mean lifetime of , much longer than many other subatomic particles. As with the decay of the non-elementary neutron (with a lifetime around 15 minutes), muon decay is slow (by subatomic standards) because the decay is mediated only by the weak interaction (rather than the more powerful strong interaction or electromagnetic interaction), and because the mass difference between the muon and the set of its decay products is small, providing few kinetic degrees of freedom for decay. Muon decay almost always produces at least three particles, which must include an electron of the same charge as the muon and two types of neutrinos.",
                    "score": 0.8544195890426636
                },
                {
                    "id": 1044059,
                    "contents": "Exotic atom\nAn exotic atom is an otherwise normal atom in which one or more sub-atomic particles have been replaced by other particles of the same charge. For example, electrons may be replaced by other negatively charged particles such as muons (muonic atoms) or pions (pionic atoms). Because these substitute particles are usually unstable, exotic atoms typically have very short lifetimes and no exotic atom observed so far can persist under normal conditions. Muonic atoms In a muonic atom (previously called a mu-mesic atom, now known to be a misnomer as muons are not mesons), an electron is replaced by a muon, which, like the electron, is a lepton. Since leptons are only sensitive to weak, electromagnetic and gravitational forces, muonic atoms are governed to very high precision by the electromagnetic interaction.",
                    "score": 0.8523426651954651
                },
                {
                    "id": 1868237,
                    "contents": "Meson\nThere were years of delays in the subatomic particle research during World War II (1939–1945), with most physicists working in applied projects for wartime necessities. When the war ended in August 1945, many physicists gradually returned to peacetime research. The first true meson to be discovered was what would later be called the \"pi meson\" (or pion). This discovery was made in 1947, by Cecil Powell, Hugh Muirhead, César Lattes, and Giuseppe Occhialini, who were investigating cosmic ray products at the University of Bristol in England, based on photographic films placed in the Andes mountains. Some of those mesons had about the same mass as the already-known mu \"meson\", yet seemed to decay into it, leading physicist Robert Marshak to hypothesize in 1947 that it was actually a new and different meson. Over the next few years, more experiments showed that the pion was indeed involved in strong interactions. The pion (as a virtual particle) is also believed to be the primary force",
                    "score": 0.8518794178962708
                },
                {
                    "id": 7262497,
                    "contents": "List of mesons\nEach meson has a corresponding antiparticle (antimeson) where quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark. Although tetraquarks with two quarks and two antiquarks can be considered mesons they are not listed here. The symbols encountered in these lists are: I (isospin), J (total angular momentum), P (parity), C (C-parity), G (G-parity), u (up quark), d (down quark), s (strange quark), c (charm quark), b (bottom quark), Q (charge), B (baryon number), S (strangeness), C (charm), and B′ (bottomness), as well as a wide array of subatomic particles (hover mouse for name).",
                    "score": 0.8491995334625244
                },
                {
                    "id": 22670785,
                    "contents": "Meson bomb\nThe meson bomb was a proposed nuclear weapon that would derive its destructive force from meson interactions with fissionable material like uranium. The idea behind the bomb was rejected by most scientists, but during the Cold War, American intelligence managed to trick the Soviet Union into conducting research on this topic, which resulted in several years of wasted labor by one of the Soviet nuclear weapon research bureaus.",
                    "score": 0.8490544557571411
                },
                {
                    "id": 1872172,
                    "contents": "Muon\nMuonic helium is created by substituting a muon for one of the electrons in helium-4. The muon orbits much closer to the nucleus, so muonic helium can therefore be regarded like an isotope of helium whose nucleus consists of two neutrons, two protons and a muon, with a single electron outside. Colloquially, it could be called \"helium 4.1\", since the mass of the muon is slightly greater than 0.1 amu. Chemically, muonic helium, possessing an unpaired valence electron, can bond with other atoms, and behaves more like a hydrogen atom than an inert helium atom. Muonic heavy hydrogen atoms with a negative muon may undergo nuclear fusion in the process of muon-catalyzed fusion, after the muon may leave the new atom to induce fusion in another hydrogen molecule. This process continues until the negative muon is captured by a helium nucleus, and cannot escape until it decays.",
                    "score": 0.84831702709198
                },
                {
                    "id": 2534939,
                    "contents": "List of particles\nMesons Ordinary mesons are made up of a valence quark and a valence antiquark. Because mesons have spin of 0 or 1 and are not themselves elementary particles, they are \"composite\" bosons. Examples of mesons include the pion, kaon, and the J/ψ. In quantum hadrodynamics, mesons mediate the residual strong force between nucleons. At one time or another, positive signatures have been reported for all of the following exotic mesons but their existences have yet to be confirmed. A tetraquark consists of two valence quarks and two valence antiquarks; A glueball is a bound state of gluons with no valence quarks; Hybrid mesons consist of one or more valence quark–antiquark pairs and one or more real gluons. Atomic nuclei",
                    "score": 0.8478441834449768
                },
                {
                    "id": 2578594,
                    "contents": "Val Logsdon Fitch\nconcerning mu-mesic atoms, atoms in which an electron is replaced by a muon. These had never been observed; they were completely theoretical and there was no evidence that they existed, but it made a good thesis topic.",
                    "score": 0.8473096489906311
                },
                {
                    "id": 1872166,
                    "contents": "Muon\nMuon decay Muons are unstable elementary particles and are heavier than electrons and neutrinos but lighter than all other matter particles. They decay via the weak interaction. Because leptonic family numbers are conserved in the absence of an extremely unlikely immediate neutrino oscillation, one of the product neutrinos of muon decay must be a muon-type neutrino and the other an electron-type antineutrino (antimuon decay produces the corresponding antiparticles, as detailed below). Because charge must be conserved, one of the products of muon decay is always an electron of the same charge as the muon (a positron if it is a positive muon). Thus all muons decay to at least an electron, and two neutrinos. Sometimes, besides these necessary products, additional other particles that have no net charge and spin of zero (e.g., a pair of photons, or an electron-positron pair), are produced.",
                    "score": 0.8456180095672607
                },
                {
                    "id": 1872173,
                    "contents": "Muon\nNegative muons bound to conventional atoms can be captured (muon capture) through the weak force by protons in nuclei, in a sort of electron-capture-like process. When this happens, nuclear transmutation results: The proton becomes a neutron and a muon neutrino is emitted.",
                    "score": 0.8447211384773254
                },
                {
                    "id": 1044065,
                    "contents": "Exotic atom\nOnium An onium (plural: onia) is the bound state of a particle and its antiparticle. The classic onium is positronium, which consists of an electron and a positron bound together as a metastable state, with a relatively long lifetime of 142 ns in the triplet state. Positronium has been studied since the 1950s to understand bound states in quantum field theory. A recent development called non-relativistic quantum electrodynamics (NRQED) used this system as a proving ground. Pionium, a bound state of two oppositely-charged pions, is useful for exploring the strong interaction. This should also be true of protonium, which is a proton–antiproton bound state. Understanding bound states of pionium and protonium is important in order to clarify notions related to exotic hadrons such as mesonic molecules and pentaquark states. Kaonium, which is a bound state of two oppositely charged kaons, has not been observed experimentally yet.",
                    "score": 0.844587504863739
                },
                {
                    "id": 7262495,
                    "contents": "List of mesons\nThis list is of all known and predicted scalar, pseudoscalar and vector mesons. See list of particles for a more detailed list of particles found in particle physics. This article contains a list of mesons, unstable subatomic particles composed of one quark and one antiquark. They are part of the hadron particle family—particles made of quarks. The other members of the hadron family are the baryons—subatomic particles composed of three quarks. The main difference between mesons and baryons is that mesons have integer spin (thus are bosons) while baryons are fermions (half-integer spin). Because mesons are bosons, the Pauli exclusion principle does not apply to them. Because of this, they can act as force mediating particles on short distances, and thus play a part in processes such as the nuclear interaction.",
                    "score": 0.8439146876335144
                },
                {
                    "id": 5711246,
                    "contents": "Nobel Prize controversies\n1950 The 1950 prize went to Cecil Powell for \"his development of the photographic method of studying nuclear processes and his discoveries regarding mesons made with this method\". However, Brazilian physicist César Lattes was the main researcher and the first author of the historical Nature journal article describing the subatomic particle meson pi (pion). Lattes was solely responsible for the improvement of the nuclear emulsion used by Powell (by asking Kodak Co. to add more boron to it—and in 1947, he made with them his great experimental discovery). This result was explained by the Nobel Committee policy (ended in 1960) to award the prize to the research group head only. Lattes calculated the pion's mass and, with USA physicist Eugene Gardner, demonstrated the existence of this particle after atomic collisions in a synchrotron. Gardner was denied a prize because he died soon thereafter.",
                    "score": 0.8432928323745728
                },
                {
                    "id": 1044062,
                    "contents": "Exotic atom\nThe symbol 4.1H (Hydrogen-4.1) has been used to describe the exotic atom muonic helium (4He-μ), which is like helium-4 in having 2 protons and 2 neutrons. However one of its electrons is replaced by a muon, which also has charge –1. Because the muon's orbital radius is less than 1/200th the electron's orbital radius (due to the mass ratio), the muon can be considered as a part of the nucleus. The atom then has a nucleus with 2 protons, 2 neutrons and 1 muon, with total nuclear charge +1 (from 2 protons and 1 muon) and only one electron outside, so that it is effectively an isotope of hydrogen instead of an isotope of helium. A muon's weight is approximately 0.1 amu so the isotopic mass is 4.1. Since there is only one electron outside the nucleus, the hydrogen-4.1 atom can react with other atoms. Its chemical behavior is that of a hydrogen atom and not a noble helium atom. The only radioactive part of the atom is the muon. Therefore, the atom decays with the muon's half-life, 1.52",
                    "score": 0.8430631160736084
                },
                {
                    "id": 1868231,
                    "contents": "Meson\nHigher-energy (more massive) mesons were created momentarily in the Big Bang, but are not thought to play a role in nature today. However, such heavy mesons are regularly created in particle accelerator experiments, in order to understand the nature of the heavier types of quark that compose the heavier mesons. Mesons are part of the hadron particle family, which are defined simply as particles composed of two or more quarks. The other members of the hadron family are the baryons: subatomic particles composed of odd numbers of valence quarks (at least 3), and some experiments show evidence of exotic mesons, which do not have the conventional valence quark content of two quarks (one quark and one antiquark), but 4 or more. Because quarks have a spin , the difference in quark number between mesons and baryons results in conventional two-quark mesons being bosons, whereas baryons are fermions.",
                    "score": 0.8429667949676514
                },
                {
                    "id": 2226860,
                    "contents": "Annihilation\nWhen a proton encounters its antiparticle (and more generally, if any species of baryon encounters the corresponding antibaryon), the reaction is not as simple as electron–positron annihilation. Unlike an electron, a proton is a composite particle consisting of three \"valence quarks\" and an indeterminate number of \"sea quarks\" bound by gluons. Thus, when a proton encounters an antiproton, one of its quarks, usually a constituent valence quark, may annihilate with an antiquark (which more rarely could be a sea quark) to produce a gluon, after which the gluon together with the remaining quarks, antiquarks, and gluons will undergo a complex process of rearrangement (called hadronization or fragmentation) into a number of mesons, (mostly pions and kaons), which will share the total energy and momentum. The newly created mesons are unstable, and unless they encounter and interact with some other material, they will decay in a series of reactions that ultimately produce only photons,",
                    "score": 0.8429101705551147
                },
                {
                    "id": 1868239,
                    "contents": "Meson\nIn the past, the word meson was sometimes used to mean any force carrier, such as \"the Z0 meson\", which is involved in mediating the weak interaction. However, this use has fallen out of favor, and mesons are now defined as particles composed of pairs of quarks and antiquarks. Overview Spin, orbital angular momentum, and total angular momentum Spin (quantum number ) is a vector quantity that represents the \"intrinsic\" angular momentum of a particle. It comes in increments of . The is often dropped because it is the \"fundamental\" unit of spin, and it is implied that \"spin 1\" means \"spin 1 \". (In some systems of natural units, is chosen to be 1, and therefore does not appear in equations.)",
                    "score": 0.8423094749450684
                },
                {
                    "id": 1330635,
                    "contents": "Weak interaction\nAll mesons are unstable because of weak decay. In the process known as beta decay, a down quark in the neutron can change into an up quark by emitting a virtual boson which is then converted into an electron and an electron antineutrino. Another example is electron capture, a common variant of radioactive decay, wherein a proton and an electron within an atom interact, and are changed to a neutron (an up quark is changed to a down quark) and an electron neutrino is emitted.",
                    "score": 0.8420593738555908
                },
                {
                    "id": 1868232,
                    "contents": "Meson\nBecause quarks have a spin , the difference in quark number between mesons and baryons results in conventional two-quark mesons being bosons, whereas baryons are fermions. Each type of meson has a corresponding antiparticle (antimeson) in which quarks are replaced by their corresponding antiquarks and vice versa. For example, a positive pion () is made of one up quark and one down antiquark; and its corresponding antiparticle, the negative pion (), is made of one up antiquark and one down quark.",
                    "score": 0.8402788639068604
                },
                {
                    "id": 1044064,
                    "contents": "Exotic atom\nHadronic atoms A hadronic atom is an atom in which one or more of the orbital electrons are replaced by a negatively charged hadron. Possible hadrons include mesons such as the pion or kaon, yielding a pionic atom or a kaonic atom (see Kaonic hydrogen), collectively called mesonic atoms; antiprotons, yielding an antiprotonic atom; and the particle, yielding a or sigmaonic atom. Unlike leptons, hadrons can interact via the strong force, so the orbitals of hadronic atoms are influenced by nuclear forces between the nucleus and the hadron. Since the strong force is a short-range interaction, these effects are strongest if the atomic orbital involved is close to the nucleus, when the energy levels involved may broaden or disappear because of the absorption of the hadron by the nucleus. Hadronic atoms, such as pionic hydrogen and kaonic hydrogen, thus provide experimental probes of the theory of strong interactions, quantum chromodynamics. Onium",
                    "score": 0.8397878408432007
                },
                {
                    "id": 1868236,
                    "contents": "Meson\nThe first candidate for Yukawa's meson, in modern terminology known as the muon, was discovered in 1936 by Carl David Anderson and others in the decay products of cosmic ray interactions. The \"mu meson\" had about the right mass to be Yukawa's carrier of the strong nuclear force, but over the course of the next decade, it became evident that it was not the right particle. It was eventually found that the \"mu meson\" did not participate in the strong nuclear interaction at all, but rather behaved like a heavy version of the electron, and was eventually classed as a lepton like the electron, rather than a meson. Physicists in making this choice decided that properties other than particle mass should control their classification.",
                    "score": 0.8385293483734131
                },
                {
                    "id": 17832046,
                    "contents": "Phi meson\nThe meson has several possible decay modes. The most energetically favored mode involves the meson decaying into 3 pions, which is what would naïvely be expected. However, we instead observe that it decays most frequently into 2 kaons. Between 1963 and 1966, 3 people, Susumu Okubo, George Zweig and Jugoro Iizuka, each independently proposed a rule to account for the observed suppression of the 3 pion decay. This rule is now known as the OZI rule and is also the currently accepted explanation for the unusually long lifetimes of the and mesons. Namely, on average they last and respectively. This is compared to the normal mean lifetime of a meson decaying via the strong force, which is on the order of .",
                    "score": 0.8381447196006775
                },
                {
                    "id": 1131753,
                    "contents": "Nuclear physics\nYukawa's meson postulated to bind nuclei In 1935 Hideki Yukawa proposed the first significant theory of the strong force to explain how the nucleus holds together. In the Yukawa interaction a virtual particle, later called a meson, mediated a force between all nucleons, including protons and neutrons. This force explained why nuclei did not disintegrate under the influence of proton repulsion, and it also gave an explanation of why the attractive strong force had a more limited range than the electromagnetic repulsion between protons. Later, the discovery of the pi meson showed it to have the properties of Yukawa's particle.",
                    "score": 0.8381274342536926
                },
                {
                    "id": 20612328,
                    "contents": "Photo-meson\nIn high-energy astrophysics, a photo-meson is a meson (most often a pion) produced in the interaction of a photon with a nucleon within an astrophysical object. This interaction is commonly referred to as photo-hadronic process. The decay of charged mesons ultimately results in the production of neutrinos and electrons, with muons as an intermediate state. The decay of neutral mesons produces high-energy gamma-rays. Photo-meson production is one of the hadronic processes that can occur in cosmic ray sources as gamma-ray bursts and active galactic nuclei, and that can result in an observable multi-messenger signature. References Astrophysics",
                    "score": 0.8379355669021606
                },
                {
                    "id": 1868246,
                    "contents": "Meson\nIf then, the meson is \" even\" ( = +1). On the other hand, if then the meson is \" odd\" ( = −1). Isospin and charge Original isospin model The concept of isospin was first proposed by Werner Heisenberg in 1932 to explain the similarities between protons and neutrons under the strong interaction. Although they had different electric charges, their masses were so similar that physicists believed that they were actually the same particle. The different electric charges were explained as being the result of some unknown excitation similar to spin. This unknown excitation was later dubbed isospin by Eugene Wigner in 1937. When the first mesons were discovered, they too were seen through the eyes of isospin and so the three pions were believed to be the same particle, but in different isospin states.",
                    "score": 0.8379115462303162
                },
                {
                    "id": 1868234,
                    "contents": "Meson\nmeson () containing the charm quark, first seen 1974, is about three times as massive as a proton, and the upsilon meson () containing the bottom quark, first seen in 1977, is about ten times as massive.",
                    "score": 0.8377821445465088
                },
                {
                    "id": 1044060,
                    "contents": "Exotic atom\nSince a muon is more massive than an electron, the Bohr orbits are closer to the nucleus in a muonic atom than in an ordinary atom, and corrections due to quantum electrodynamics are more important. Study of muonic atoms' energy levels as well as transition rates from excited states to the ground state therefore provide experimental tests of quantum electrodynamics. Muon-catalyzed fusion is a technical application of muonic atoms.",
                    "score": 0.8368483185768127
                },
                {
                    "id": 7262496,
                    "contents": "List of mesons\nSince mesons are composed of quarks, they participate in both the weak and strong interactions. Mesons with net electric charge also participate in the electromagnetic interaction. They are classified according to their quark content, total angular momentum, parity, and various other properties such as C-parity and G-parity. While no meson is stable, those of lower mass are nonetheless more stable than the most massive mesons, and are easier to observe and study in particle accelerators or in cosmic ray experiments. They are also typically less massive than baryons, meaning that they are more easily produced in experiments, and will exhibit higher-energy phenomena sooner than baryons would. For example, the charm quark was first seen in the J/Psi meson () in 1974, and the bottom quark in the upsilon meson () in 1977. The top quark (the last and heaviest quark to be discovered to date) was first observed at Fermilab in 1995.",
                    "score": 0.836520791053772
                },
                {
                    "id": 4321987,
                    "contents": "J/psi meson\nMuch of the scientific community considered it unjust to give one of the two discoverers priority, so most subsequent publications have referred to the particle as the \"\". The first excited state of the was called the ψ′; it is now called the ψ(2S), indicating its quantum state. The next excited state was called the ψ″; it is now called ψ(3770), indicating mass in MeV. Other vector charm-anticharm states are denoted similarly with ψ and the quantum state (if known) or the mass. The \"J\" is not used, since Richter's group alone first found excited states. The name charmonium is used for the and other charm-anticharm bound states. This is by analogy with positronium, which also consists of a particle and its antiparticle (an electron and positron in the case of positronium). See also OZI rule List of multiple discoveries Footnotes References Sources Mesons Onia History of physics",
                    "score": 0.8363275527954102
                },
                {
                    "id": 940623,
                    "contents": "Positronium\nPositronium in very weakly bound (extremely large n) states has been predicted to be the dominant form of atomic matter in the universe in the far future if proton decay occurs. Although any positrons and electrons left over from the decay of matter would be initially moving far too fast to bind together, the expansion of the universe slows free particles, so much so that eventually (in years, when electrons and positrons are typically 1 quintillion parsecs apart) their kinetic energy will actually fall below the Coulomb attraction potential, and thus they will be weakly bound (positronium). The resulting weakly bound electron and positron spiral inwards and eventually annihilate, with an estimated lifetime of years. See also Breit equation Antiprotonic helium Di-positronium Quantum electrodynamics Protonium Two-body Dirac equations References",
                    "score": 0.8362293243408203
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_28",
        "question": "$$\r\n\\beta=2 \\pi c \\tilde{\\omega}_{\\mathrm{obs}}\\left(\\frac{\\mu}{2 D}\\right)^{1 / 2}\r\n$$\r\nGiven that $\\tilde{\\omega}_{\\mathrm{obs}}=2886 \\mathrm{~cm}^{-1}$ and $D=440.2 \\mathrm{~kJ} \\cdot \\mathrm{mol}^{-1}$ for $\\mathrm{H}^{35} \\mathrm{Cl}$, calculate $\\beta$.",
        "golden_answers": [
            " 1.81"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 28310051,
                    "contents": "Kappa Beta Pi\n1955 - Beta Xi - Houston 1956 - Beta Omicron - Toledo 1958 - Beta Pi - Boston Law 1960 - Beta Rho - Kentucky 1962 - Beta Sigma - Seton Hall 1962 - Beta Tau - Rutgers (dormant 1970) 1963 - Beta Upsilon - Howard University (dormant 1974) 1963 - Beta Phi - Fordham (dormant 1970) 1963 - Beta Chi - Kentucky (dormant 1971) 1964 - Beta Psi - Puerto Rico (dormant 1973) 1966 - Beta Omega - Ohio Northern 1966 - Gamma Alpha - Colorado (dormant 1972) 1967 - Gamma Beta - Franklin (OH) (dormant 1973) 1967 - Gamma Gamma - University of Arkansas 1967 - Gamma Delta - Arkansas, Little Rock 1968 - Gamma Epsilon - New Brunswick, Canada (dormant 1973) 1970 - Gamma Zeta - Mississippi 1971 - Gamma Eta - Texas Tech 1972 - Gamma Theta - Pitt (dormant 1973) 1972 - Gamma Iota - Duquesne (dormant 1973) 1973 - Gamma Kappa - Windsor, Ontario, Canada 1974 - Gamma Lambda - Ottawa, Ontario, Canada",
                    "score": 0.8624769449234009
                },
                {
                    "id": 28310050,
                    "contents": "Kappa Beta Pi\n1927 - Alpha Omicron - Paris, France 1930 - Alpha Pi - Tulane 1931 - Alpha Rho - Alabama 1931 - Alpha Sigma - Denver 1931 - Alpha Tau - North Dakota 1932 - Alpha Upsilon - West Virginia University 1932 - Alpha Phi - University of Arizona 1934 - Alpha Chi - Columbia (dormant 1938-57) 1940 - Alpha Psi - Southern Methodist 1940 - Alpha Omega - Drake 1940 - Beta Alpha - London, England 1946 - Beta Beta - Missouri (dormant 1950-51) 1946 - Beta Gamma - Columbus Law (DC) 1946 - Beta Delta - San Francisco 19xx - Beta Epsilon - ? 1947 - Beta Zeta - Shanghai (China) 1948 - Beta Eta - South Dakota 1949 - Beta Theta - Miami (FL) 1949 - Beta Iota - Emory (dormant 1952-53) 1950 - Beta Kappa - Virginia 1952 - Beta Lambda - St. Mary's San Antonio 1953 - Beta Mu - Georgetown 1954 - Beta Nu, West Germany 1955 - Beta Xi - Houston 1956 - Beta Omicron - Toledo 1958 - Beta Pi - Boston Law 1960 - Beta Rho - Kentucky 1962 - Beta Sigma - Seton Hall 1962 - Beta Tau - Rutgers (dormant 1970)",
                    "score": 0.8623687028884888
                },
                {
                    "id": 21346741,
                    "contents": "Beta Phi Mu Award\n1954-1959 1959.Anita Miller Hostetter 1958. Florence Van Hoesen 1957. Lucy M. Crissey 1956. Margaret I Rufsvold 1955. Gretchen Knief Schenk 1954. Rudolph Hjalmar Gjelsness, University of Michigan References Librarians Library-related organizations Library science awards Awards established in 1954 Honor societies",
                    "score": 0.8586146235466003
                },
                {
                    "id": 21346740,
                    "contents": "Beta Phi Mu Award\n1970-1979 1979. Conrad Rawski, Case Western Reserve. University 1978. Frances E. Henne, Columbia University 1977. Russell E. Bidlack, University of Michigan 1976. Carolyn Whitenack, Purdue University 1975. Kenneth R. Shaffer, Simmons College 1974. Martha Boaz, University of Southern California 1973. Lester Asheim, University of Chicago, University of North Carolina 1972. Margaret E. Monroe, University of Wisconsin-Madison 1971. Leon Carnovsky University of Chicago 1970. Raynard C. Swank, University of California Berkeley 1969-1969 1969. Ethel M. Fair 1968. Sarah R. Reed 1967. Louis Shores, Florida State University 1966. James J. Kortendick 1965. Jesse H. Shera, University of Chicago, Case Western Reserve University 1964. Charles C. Williamson, Columbia University 1963 Ernest J. Reece 1962 Florrinell F. Morton, Louisiana State University 1961. Robert L. Gitler 1960. Louis Round Wilson, University of Chicago, University of North Carolina 1954-1959",
                    "score": 0.8558819890022278
                },
                {
                    "id": 6146617,
                    "contents": "Alpha Beta\nyou're at Alpha Beta or you're not\" next to Bugs Bunny. In the 90s, Looney Tunes were in the commercial of Alpha Beta grocery store commercial and Daffy Duck ended up saying \"Join Looney Tunes at Alpha Beta\", \"Either you're at Alpha Beta or you're not\", and \"Tell A Friend\".",
                    "score": 0.8537575602531433
                },
                {
                    "id": 28310049,
                    "contents": "Kappa Beta Pi\n1921 - Tau - Boston 1921 - Upsilon - Syracuse 1921 - Phi - Illinois 1921 - Chi - Oregon 1921 - Psi - Wisconsin, Madison 1923 - Omega - University of Southern California (dormant 1949) 1923 - Alpha Alpha - John Marshall Law, Cleveland 1923 - Alpha Beta - Minnesota (dormant in 1958) 1924 - Alpha Gamma - Southwestern, Los Angeles 1924 - Alpha Delta - Buffalo 1924 - Alpha Epsilon - Chicago-Kent Law (school closed in 1935) 1924 - Alpha Zeta - Marquette University 1924 - Alpha Eta - Hastings Law (dormant in 1957) 1924 - Alpha Theta - Loyola, Chicago 1925 - Alpha Iota - St. Louis 1925 - Alpha Kappa - Creighton 1925 - Alpha Lambda - Nebraska (dormant 1942-1951 1925 - Alpha Mu - Osgoode, Toronto, Canada 1926 - Alpha Nu - Ohio State University 1926 - Alpha Xi- University of Oklahoma 1927 - Alpha Omicron - Paris, France 1930 - Alpha Pi - Tulane 1931 - Alpha Rho - Alabama 1931 - Alpha Sigma - Denver 1931 - Alpha Tau - North Dakota 1932 - Alpha Upsilon - West Virginia University",
                    "score": 0.8518092632293701
                },
                {
                    "id": 27342327,
                    "contents": "Pi Alpha Xi\nAlpha – Cornell University, chartered June 1, 1923 Beta – University of Illinois at Urbana–Champaign, chartered April 2, 1924 Gamma – Penn State University, chartered May 29, 1926 Delta – Michigan State University, chartered April 12, 1929 Epsilon – Ohio State University, chartered January 6, 1929 Zeta – Rutgers University, chartered November 16, 1933 Eta – Washington State University, chartered March 12, 1949 Theta – University of Maryland, chartered September 14, 1949 Iota – North Carolina State University, chartered February 15, 1957 Kappa – Virginia Polytechnic & State University, chartered June 1, 1968 Lambda – University of Minnesota, chartered May 28, 1968 Mu – University of Florida, chartered May 16, 1974 Nu – Auburn University, chartered May 30, 1974 Xi – New Mexico State University, chartered December 1, 1974 Omicron – Purdue University, chartered February 23, 1975 Pi – Clemson University, chartered April 17, 1975",
                    "score": 0.8505052924156189
                },
                {
                    "id": 4515429,
                    "contents": "Pi Lambda Phi\nHistory Very little is known about the early foundings of the fraternity. After groups of men were denied admission to other fraternities at Yale University because of their religious and racial backgrounds in 1895, Frederick Manfred Werner, Louis Samter Levy, and Henry Mark Fisher were determined to start something new. They decided to start the first fraternity that was \"a fraternity in which all men were brothers, no matter what their religion; a fraternity in which ability, open-mindedness, farsightedness, and a progressive, forward-looking attitude would be recognized as the basic attributes.\" Chapters at other universities started soon after. While non-sectarian, it was predominantly Jewish until the end of World War II. During its history, three national fraternities merged with Pi Lambda Phi: Phi Beta Delta, Beta Sigma Tau and Beta Sigma Rho. Merging fraternities Phi Beta Delta",
                    "score": 0.8479511141777039
                },
                {
                    "id": 6146616,
                    "contents": "Alpha Beta\nIn the late 1970s and early 1980s, Alan Hamel was the television spokesman for the Alpha Beta grocery stores in California. Although the chain used various slogans such as \"You Can't Lose\" and \"The Savings Don't Stop\", every commercial featuring Hamel ended with him saying to the audience \"tell a friend\". Popeye was in the Alpha Beta grocery store commercial with Mickey Mouse in the 80s featuring the end of the Alpha Beta commercial with Mickey Mouse saying next to Popeye to the audience \"tell a friend\". Olive Oyl's voiceover ended up saying \"Go shopping with Popeye at Alpha Beta\" and \"Tell A Friend\" at the end of the Alpha Beta commercial in the 80s. Bugs Bunny, Donald Duck, and Hulk Hogan were in the commercial of Alpha Beta grocery store in the 90s featuring Hulk Hogan ended up saying \"Either you're at Alpha Beta or you're not\" next to Bugs Bunny. In the 90s, Looney Tunes were in the commercial of Alpha Beta grocery store commercial and Daffy Duck ended up saying \"Join Looney Tunes",
                    "score": 0.8478079438209534
                },
                {
                    "id": 5736171,
                    "contents": "Greek letters used in mathematics, science, and engineering\nΒ represents the beta function represents: the thermodynamic beta, equal to (kBT)−1, where kB is Boltzmann's constant and T is the absolute temperature. the second angle in a triangle, opposite the side B the standardized regression coefficient for predictor or independent variables in linear regression (unstandardized regression coefficients are represented with the lower-case Latin b, but are often called \"betas\" as well) the ratio of collector current to base current in a bipolar junction transistor (BJT) in electronics (current gain) the false negative rate in statistics (\"Type II\" error) the beta coefficient, the non-diversifiable risk, of an asset in mathematical finance the sideslip angle of an airplane a beta particle (e− or e+) the beta brain wave in brain or cognitive sciences ecliptic latitude in astronomy the ratio of plasma pressure to magnetic pressure in plasma physics β-reduction in lambda calculus",
                    "score": 0.8468562960624695
                },
                {
                    "id": 9750761,
                    "contents": "Beta Beta Beta\nChapter list References External links Beta Beta Beta http://www.bio.umb.edu/TriBetaHome2.html Student societies in the United States Student organizations established in 1922 1922 establishments in Oklahoma",
                    "score": 0.8462487459182739
                },
                {
                    "id": 21346739,
                    "contents": "Beta Phi Mu Award\n1980-1989 1989. Charles D. Patterson, Louisiana State University 1988. Samuel Rothstein, University of British Columbia (first Canadian to hold Ph.D. in Librarianship) 1987. Sarah K. Vann, University of Hawaii 1986. Agnes Lytton Reagan, ALA Accreditation Officer 1985. Robert M. Hayes, University of California Los Angeles 1984. Jane Anne Hannigan, Columbia University, Rutgers University 1983. J. Periam Danton, University of California, Berkeley 1982. David K. Berninghausen, University of Minnesota 1981. Haynes McMullen, University of North Carolina at Chapel 1980. Virginia Lacy Jones, Atlanta University 1970-1979",
                    "score": 0.8461595177650452
                },
                {
                    "id": 4568055,
                    "contents": "Limited Edition (Magic: The Gathering)\nThe print run of Beta is given as 7.3 million or 7.8 million depending on the source. Despite the set's print run being about three times as big as Alpha's, Beta sold out as quickly as its predecessor.",
                    "score": 0.8438417911529541
                },
                {
                    "id": 12960139,
                    "contents": "Alpha Ralpha Boulevard\ndo to help him. They continue upward until they finally reach the Abba-dingo, which seems to be an ancient computer system. It has a machine marked \"Food\", but they are disappointed to find that this no longer works. A machine marked \"Meteorological\" displays a sign which reads \"Typhoon coming\". A machine marked \"Predictions\" is surrounded by mysterious white objects which Paul slowly realizes are the bones of long-dead humans. Virginia puts her hand in a slot marked \"Put paper here\", which cuts words into her skin: \"You will love Paul all your life.\" After bandaging her hand with a strip torn from his clothing, Paul inserts a strip into the slot. The machine prints \"You will love Virginia twenty-one more minutes\". Paul \"accidentally\" loses the strip to the wind and pretends his prediction was the same as hers.",
                    "score": 0.8433603644371033
                },
                {
                    "id": 6059206,
                    "contents": "Phi Beta Sigma\n#19 Alvin J. McNeil, 1966–1970 #20 Parlette L. Moore, 1971–1973 #21 John E. Westberry, 1974–1976 #22 Richard M. Ballard, Jr., 1977–1979 #23 Charles B. Wright, 1980–1981 #24 Demetrius Newton, 1981–1984 #25 James T. Floyd 1984–1987 #26 Moses C. McClendon, 1987–1989 #27 Carter D. Womack, 1989–1993 #28 William E. Stanley, Jr., 1993–1995 #29 Carter D. Womack, 1995–1997 #30 Peter M. Adams, 1997-2001 #31 Arthur R. Thomas, 2001–2005 #32 Paul L. Griffin, Jr., 2005–2009 #33 Jimmy Hammock, 2009–2013 #34 Jonathan A. Mason, Sr., 2013–2017 #35 Michael E. Cristal, 2017–2021 #36 Chris V. Rey, 2021–Present Conclave",
                    "score": 0.8430994153022766
                },
                {
                    "id": 7723687,
                    "contents": "Sigma Alpha Iota\nBeta Beta - University of Kansas, Lawrence, Kansas Beta Chi - McNeese State University, Lake Charles, Louisiana Beta Delta - University of Puget Sound, Tacoma, Washington Beta Epsilon - University of Evansville, Evansville, Indiana Beta Eta - Western Michigan University, Kalamazoo, Michigan Beta Gamma - Bradley University, Peoria, Illinois Beta Iota - Northwestern State University, Natchitoches, Louisiana Beta Kappa - Texas Wesleyan University, Fort Worth, Texas* Beta Lambda - Valparaiso University, Valparaiso, Indiana Beta Mu - Northern Illinois University, DeKalb, Illinois* Beta Nu - Immaculate Heart College, Los Angeles, California** Beta Omega - Mount St. Mary's College, Los Angeles, California** Beta Omicron - Hartwick College, Oneonta, New York Beta Phi - The Catholic University of America, Washington, DC* Beta Pi - University of Houston, Houston, Texas Beta Psi - East Carolina University, Greenville, North Carolina",
                    "score": 0.8430079221725464
                },
                {
                    "id": 29357080,
                    "contents": "Alpha Phi Beta\nBefore the end of the millennium, the fraternity produced three bar topnotchers, one Student Regent, five Chairpersons of the University Student Council (USC), 15 Councilors of the USC, eight College Representatives, one Editor-in-Chief of the Philippine Collegian, and eight staff members of the Philippine Collegian.",
                    "score": 0.8429797291755676
                },
                {
                    "id": 27232142,
                    "contents": "Beta (The Walking Dead)\nTelevision series Season 9",
                    "score": 0.842322826385498
                },
                {
                    "id": 1921595,
                    "contents": "Beta function\nSimilarly, betainc (incomplete beta function) in MATLAB and GNU Octave, pbeta (probability of beta distribution) in R, or special.betainc in Python's SciPy package compute the regularized incomplete beta function—which is, in fact, the cumulative beta distribution—and so, to get the actual incomplete beta function, one must multiply the result of betainc by the result returned by the corresponding beta function. In Mathematica, Beta[x, a, b] and BetaRegularized[x, a, b] give and , respectively. See also Beta distribution and Beta prime distribution, two probability distributions related to the beta function Jacobi sum, the analogue of the beta function over finite fields. Nörlund–Rice integral Yule–Simon distribution References External links Arbitrarily accurate values can be obtained from: The Wolfram Functions Site: Evaluate Beta Regularized Incomplete beta danielsoper.com: Incomplete Beta Function Calculator, Regularized Incomplete Beta Function Calculator",
                    "score": 0.8413414359092712
                },
                {
                    "id": 27576177,
                    "contents": "Beta Sigma Omega Phi\nMisamis Oriental Institute of Science and Technology (MOIST), Balingasag - Beta Gamma Chapter University of Mindanao Digos College - Beta Delta Chapter Legacy College of Compostela - Beta Epsilon Chapter Cor Jesu College, Digos City DVO del Sur - Beta Zeta Chapter University of Southeastern Philippines (Bislig Campus) - Beta Eta Chapter SPAMAST, Digos City - Beta Theta Chapter Green Valley College Foundation, Koronadal City - Beta Iota Chapter University of Southern Mindanao (Kidapawan City Campus) - Beta Kappa Chapter De La Salle John Bosco College, Bislig City - Beta Lambda Chapter Notra Dame of Cotabato, Cotabato City - Beta Mu Chapter Saint Vincent de Paul College, Bislig City - Beta Nu Chapter Cotabato City State Polytechnic College - Beta Xi Chapter Northlink Technological College, Panabo City - Beta Omicron Chapter St. Mary's of Bansalan College - Beta Pi Chapter University of Mindanao Bansalan College - Beta Rho Chapter",
                    "score": 0.8411909937858582
                },
                {
                    "id": 15858739,
                    "contents": "Pi Alpha Tau\nChapter List Baird's showed most of these chapters in the 12th ed., supplemented by information from the Baird's Manual Online Archive. Alpha - Hunter College - 1919 Beta - New York University - 1920 Gamma - Adelphi College - 1918-1938 Delta - St. Lawrence University - 192x-19xx (?) Epsilon - New York State Teachers College at Albany - 1925 Zeta - New Jersey Law College - 192x-19xx Eta - University of Cincinnati - 1927-1941 ) Theta - Long Island University - 1929-193x (?) Iota - University of Wisconsin–Madison - 1929-193x (?) Kappa - St. John's University - 1929 Lambda - CUNY, Brooklyn College - 1930-1960 (?) ? - Brooklyn Law School - 1923 Mu - Syracuse University - 1950 See also List of Jewish fraternities and sororities References Further reading",
                    "score": 0.8403798341751099
                },
                {
                    "id": 21346738,
                    "contents": "Beta Phi Mu Award\n1990-1999 1999. D. W. Krummel, University of Illinois 1998. Elizabeth W. Stone, Catholic University 1997. Charles Bunge, University of Wisconsin 1996. Robert N. Broadus, Northern Illinois University 1995. Elizabeth Futas, University of Rhode Island 1994. Jane B. Robbins, University of Wisconsin-Madison 1993. Kathryn Luther Henderson, University of Illinois 1992. Guy Garrison, Drexel University 1991. Edward G. Holley, University of North Carolina 1990. Robert D. Stueart, Simmons College, Boston 1980-1989",
                    "score": 0.8386817574501038
                },
                {
                    "id": 8628118,
                    "contents": "Lambda Pi Chi\nRho chapter— North Carolina State University Sigma chapter— Johns Hopkins University Tau chapter— Union College Upsilon chapter— Long Island University C. W. Post Campus Phi chapter— University of North Carolina at Chapel Hill Chi chapter— University of Chicago Psi chapter— St. Thomas Aquinas College Omega chapter— Reserved for deceased Hermanas Alpha Alpha chapter— Saint Leo University Alpha Beta chapter— North Carolina Central University Alpha Gamma chapter— Davidson College Alpha Delta chapter— Rutgers University-New Brunswick Alpha Epsilon chapter— Campbell University Alpha Zeta chapter— High Point University Alpha Eta chapter— Western Carolina University Rhodes College Provisional chapter— Rhodes College Virginia Tech Provisional chapter— Virginia Tech",
                    "score": 0.8366192579269409
                },
                {
                    "id": 16982081,
                    "contents": "Kappa Beta Phi\nThe Wall Street chapter of Kappa Beta Phi was founded in 1929 prior to the stock market crash, and is the only remaining chapter of the society. The stated purpose of the Wall Street chapter is to \"keep alive the spirit of the 'good old days of 1928–29.'\"\"Hobart College: Kappa Beta Phi Men May Pay a Visit to Lehigh\". New York Times. December 8, 1912. pSM19",
                    "score": 0.8365830183029175
                },
                {
                    "id": 14504318,
                    "contents": "Beta Phi Alpha\nColors were Kelly green and gold. The flower was the yellow tea rose. The open motto was Scientia, Virtus, Amicitia - \"Knowledge, Virtue, Friendship\" The publication was Aldebaran. Chapters Baird's Manual (1940) notes that more than thirty chapters were established \"with a total membership of 3,295.\" Inactive chapters at the time of merger listed in italic. Delta Zeta's history (1983) notes that a total of eight new chapters were gained in the merger, with some groups combined and one released: References Baird's Manual of American College Fraternities (multiple volumes, with an online article here: The Baird's Manual Online Archive homepage.) Miner, Florence Hood (1983). Delta Zeta Sorority 1902- 1982: Building on Yesterday, Reaching for Tomorrow. Delta Zeta Sorority, Comploith Graphics, Muary Boyd and Associates, Inc., Indianapolis, Indiana. University of California, University of California Chronicle, University of California Press, 1920, v. 22.",
                    "score": 0.8361822962760925
                },
                {
                    "id": 27342329,
                    "contents": "Pi Alpha Xi\nAlpha Epsilon – University of California, Davis, chartered Spring 1984 Alpha Zeta – University of Wisconsin–River Falls, chartered May 16, 1985 Alpha Eta – Delaware Valley University, chartered Spring 1985 Alpha Theta – Iowa State University, chartered November 6, 1986 Alpha Iota – Texas Tech University, chartered March 24, 1988 Alpha Kappa – Mississippi State University, chartered Spring 1990 Alpha Lambda – Florida A&M University, chartered February 1, 1992 Alpha Mu – Temple University, chartered Spring 1992 Alpha Nu – Utah State University, chartered June 2, 1995 Alpha Xi – Texas State University, chartered April 18, 2005 Alpha Omicron – University of Arkansas, chartered May 2, 2006 Alpha Pi – University of Wisconsin–Platteville, chartered May 4, 2006 Alpha Rho – Oregon State University, chartered 2011 Alpha Sigma – University of Wyoming, chartered October 13, 2015 Alpha Tau – University of Maine, chartered September 21, 2015",
                    "score": 0.8358330130577087
                },
                {
                    "id": 568644,
                    "contents": "Phi Beta Kappa\nIn the 1960s, Vanderbilt University professor Donald Davidson claimed that Phi Beta Kappa was under the influence of Communists. In 1988, the United Chapters of Phi Beta Kappa officially changed its name to The Phi Beta Kappa Society, recalling the name under which the organization had been established in 1776. Today, Phi Beta Kappa participates in a more loosely coordinated lobbying association of four of the nation's oldest and most prestigious honor societies, called the Honor Society Caucus. Its members include Phi Beta Kappa, Phi Kappa Phi, Sigma Xi, and Omicron Delta Kappa.",
                    "score": 0.834963321685791
                },
                {
                    "id": 3496717,
                    "contents": "Tau Beta Pi\nThe Tau Beta Pi Association (commonly Tau Beta Pi, , or TBP) is the oldest engineering honor society and the second oldest collegiate honor society in the United States. It honors engineering students in American universities who have shown a history of academic achievement as well as a commitment to personal and professional integrity. Specifically, the association was founded \"to mark in a fitting manner those who have conferred honor upon their Alma Mater by distinguished scholarship and exemplary character as students in engineering, or by their attainments as alumni in the field of engineering, and to foster a spirit of liberal culture in engineering colleges\".",
                    "score": 0.8347384929656982
                },
                {
                    "id": 5420374,
                    "contents": "Phi Mu\nBusiness Pat Mitchell (Alpha Alpha) - president, PBS Evett Simmons (Alpha Tau) - president of the National Bar Association (2000) Toria Tolley (Beta Nu) - VP/consultant, The Psychological Advantage, former CNN weekend anchor Science, technology, engineering and math Jerrie Mock (Psi) - first woman to fly solo around the world Kathy Pham (Theta Zeta) - computer scientist, First Lady Michelle Obama's Guest to the 2015 State of the Union Address, Nguoi Viet 40 under 40. Mary Weber (Delta Epsilon) - astronaut",
                    "score": 0.8346330523490906
                },
                {
                    "id": 7723672,
                    "contents": "Sigma Alpha Iota\nThe next chapter of the fraternity, Beta, was chartered in 1904 at Northwestern University in Evanston, Illinois. Chapters have now been chartered at over 300 universities, conservatories, and colleges.",
                    "score": 0.8346279263496399
                },
                {
                    "id": 13850841,
                    "contents": "List of Phi Kappa Sigma chapters\nAlpha Omicron, University of Michigan Alpha Pi, University of Chicago Alpha Rho, Cornell University Alpha Sigma, University of Minnesota Alpha Tau, Stanford University Alpha Phi, University of Iowa Alpha Chi, Ohio State University Alpha Omega, University of British Columbia Alpha Zeta, University of Maryland Beta Alpha, University of Oregon Beta Beta, University of Kansas Beta Gamma, University of Denver Beta Epsilon, Oregon State University Beta Zeta, Ohio University Beta Eta, University of North Texas Beta Iota, St. Lawrence University Beta Kappa, Drury College Beta Nu, Adrian College Beta Omicron, Virginia Polytechnic Institute Beta Pi, Louisiana Tech University Beta Sigma, Salisbury State University Beta Tau, Towson University Beta Upsilon, SUNY Potsdam Beta Phi, SUNY Geneseo Gamma Alpha, SUNY-Buffalo Gamma Beta, Drexel University Gamma Delta, Texas A&M Gamma Epsilon Seton Hall University Gamma Zeta, California University of Pennsylvania Gamma Eta, SUNY Fredonia",
                    "score": 0.8340601325035095
                },
                {
                    "id": 29303431,
                    "contents": "Delta Beta Phi\nDelta Beta Phi () was a small national men's fraternity founded on 1878 at Cornell University, soon forming six chapters. The national disbanded in 1882, but may have been briefly restored through the 1920s. History Delta Beta Phi was formed in at Cornell University. Its four founders were: J.D. Hamrick I.W. Kelly J.S. Monroe Willard Olney The original Cornell chapter was cheekily nicknamed the \"Dead Bits\" on campus on account of the first two letters of its name. It expanded quickly to form six chapters in the Northeastern and Mid-Atlantic states, creating five in its first year. Two of its chapters, Psi and Delta came from earlier, local societies.",
                    "score": 0.8340042233467102
                },
                {
                    "id": 2384841,
                    "contents": "Clemson University\nThe College Panhellenic Council Chapters at Clemson University include Alpha Chi Omega, Alpha Delta Pi, Alpha Phi, Chi Omega, Delta Delta Delta, Delta Gamma (2021), Delta Zeta, Gamma Phi Beta, Kappa Delta, Kappa Kappa Gamma, Pi Beta Phi, Sigma Kappa, and Zeta Tau Alpha. The Interfraternity Council Chapters include Alpha Gamma Rho, Alpha Sigma Phi, Alpha Tau Omega, Beta Theta Pi, Beta Upsilon Chi, Chi Phi, Delta Chi, Delta Tau Delta, FarmHouse, Kappa Alpha Order, Kappa Sigma, Pi Kappa Phi, Phi Gamma Delta, Phi Delta Theta, Phi Sigma Kappa, Sigma Nu, Theta Chi, Tau Kappa Epsilon, Psi Upsilon, and Triangle. As of the fall 2017 semester there are twenty IFC Fraternities, thirteen NPC Sororities, eight NPHC Chapters, and four MGC Chapters, which make up approximately 23 percent of the undergraduate student body.",
                    "score": 0.8337810039520264
                },
                {
                    "id": 3496734,
                    "contents": "Tau Beta Pi\nNotable members Tau Beta Pi's membership includes some famous figures in engineering and technology, including 19 Nobel laureates: Buzz Aldrin, second astronaut to walk on the moon Charles Bachman, computer scientist and database technology pioneer John Bardeen, engineer and physicist, two time Nobel prize winner Jeff Bezos, Amazon.com founder Michael Bloomberg, founder of Bloomberg L.P. and mayor of New York from 2002 to 2013 Stephen G. Bowen, astronaut Wernher von Braun, rocket scientist Frank Capra, movie director Leon Cordero, former president of Ecuador Seymour Cray, supercomputer pioneer Francis deSouza, CEO of Illumina Donn Eisele, astronaut Thomas Francis Farrell, Major General, United States Army Ernie Fletcher and Paul E. Patton, former governors of Kentucky C. Gordon Fullerton, astronaut Fred Haise, astronaut Lee Iacocca, former Chrysler CEO Kelly Johnson (engineer), American systems engineer and aeronautical innovator Donald Knuth, computer scientist",
                    "score": 0.8337038159370422
                },
                {
                    "id": 27249693,
                    "contents": "Alpha Phi Gamma (honor society)\nChapters Chapters of Alpha Phi Gamma: Alpha - Ohio Northern University Beta - University of Akron Gamma - Wilmington College Delta - Baldwin-Wallace College Epsilon - Muskingum College Eta - University of Toledo Theta - Cotner College Iota - Louisiana State Normal College Kappa - New York State Teacher's College, Albany Lambda - Redlands University Nu - Southwestern University (Los Angeles) Omicron - State College of Fresno Pi - Santa Barbara State Teacher's College Rho - Hanover College Sigma - Pennsylvania State Teacher's College - Indiana Tau - Albion College Chi - Georgetown College Psi - College of Puget Sound Omega - Ball State Teacher's College Alpha Beta - Gustavus Adolphus College Alpha Gamma - San Francisco State Teacher's College",
                    "score": 0.83347487449646
                },
                {
                    "id": 568633,
                    "contents": "Phi Beta Kappa\nPhi Beta Kappa () stands for (), which means \"Wisdom [lit. love of knowledge] is the guide [lit. helmsman] of life\". Membership Phi Beta Kappa has chapters in only about 10% of American higher learning institutions, and only about 10% of these schools' Arts and Sciences graduates are invited to join the society. Although most students are elected in their senior year, many colleges elect a limited number of extremely select students in their junior year, generally less than 2% of the class. Each chapter sets its own academic standards, but all inductees must have studied the liberal arts and sciences, demonstrated \"good moral character\", and, usually, earned grades placing them in the top tenth of their class. (However, at least one school, Princeton University, includes Bachelor of Science in Engineering (BSE) students in Phi Beta Kappa.) There is a mandatory initiation fee (between US$50 and US$95, as of 2005), which is sometimes covered by the inductee's university.",
                    "score": 0.8332661390304565
                },
                {
                    "id": 5479540,
                    "contents": "Alpha Lambda Delta\nThe Society has over 1 million lifetime members. ALD initiates between 25,000 and 30,000 new members each year. Awards Each year, National Alpha Lambda Delta awards over $210,000 in scholarships and fellowships to members. Undergraduate members may also be awarded the Jo Anne Trow Scholarship; 50 scholarships are given annually. The James G. Stemler Scholarship was designed for members who wish to study abroad, and 20 scholarships are awarded each year. Twenty six graduate fellowships are awarded annually to members pursuing graduate education. Additionally, members are entitled to wear Alpha Lambda Delta honor cords at their graduation exercises. Students in each chapter who have succeeded in maintaining the highest GPA from among their school's membership may be awarded the Maria Leonard Senior Book Award, the presentation of a book with an inscribed bookplate.",
                    "score": 0.8332486152648926
                },
                {
                    "id": 28310047,
                    "contents": "Kappa Beta Pi\nThe organization publishes a quarterly, called The Kappa Beta Pi Quarterly, one edition annually in an esoteric form, called The Secret Bulletin. A fifty-year history was also written by Ms. Edgerton, who served as its first Grand Dean, or president, and published by the organization.",
                    "score": 0.8331565260887146
                },
                {
                    "id": 10344262,
                    "contents": "The Ballad of Beta-2\nIt turns out that in the early generations, the voyage went well, the fleet of generation ships proceeding as planned. It was at this time that Earth-bound terms got new meanings, \"A City\" being one of the ships and \"The Desert\" being the space between them. However. in later generations a fanatic religious ideology arose – its main tenets being that the ships' mission was \"To Bring Human Beings to the Stars\", that the term \"Human Being\" was to be defined according to a very strict \"Norm\" covering both physical characteristics and social behavior – and that anyone not fitting that \"Norm\" was not a true \"Human Being\" and had to be weeded out. Fanatic Judges were set up to judge such misfits and almost invariably sentence them to death, with the Judges increasingly usurping the authority of the Captains. Misfits escaped to the weightless areas at the core of the ships, where they could easier avoid capture, and which in effect became a kind of ghetto.",
                    "score": 0.833089292049408
                },
                {
                    "id": 10836804,
                    "contents": "Beta Phi Mu\nBeta Phi Mu Award The Beta Phi Mu Award is an annual award to a library school faculty member or to an individual for distinguished service to education for librarianship. The first award was made in 1954 to Rudolph Hjalmar Gjelsness Dean of the University of Michigan's Library Science Department from 1940 to 1964. References External links ACHS Beta Phi Mu entry Beta Phi Mu chapter list, ACHS Student organizations established in 1948 Association of College Honor Societies Library-related organizations 1948 establishments in Illinois",
                    "score": 0.833012044429779
                },
                {
                    "id": 7723699,
                    "contents": "Sigma Alpha Iota\nKappa Mu - Alabama State University, Montgomery, Alabama Kappa Nu - Lindenwood University, St. Charles, Missouri Kappa Omega - Elmhurst College, Elmhurst, Illinois* Kappa Omicron - Texas Southern University, Houston* Kappa Phi - Auburn University, Alabama Kappa Pi - Middle Tennessee State University, Murfreesboro, Tennessee* Kappa Psi - California State University - Fresno Kappa Rho - Langston University, Langston Oklahoma Kappa Sigma - Saint Mary's University of Minnesota, Winona, Minnesota Kappa Tau - Capital University, Columbus, Ohio Kappa Theta - University of Nevada, Las Vegas, Las Vegas, Nevada* Kappa Upsilon - Kansas State University, Manhattan, Kansas Kappa Xi - Morgan State University, Baltimore, Maryland* Kappa Zeta - New Mexico State University, Las Cruces, New Mexico* Lambda - New England Conservatory, Boston, Massachusetts* Lambda Alpha - East Tennessee State University - Johnson City, Tennessee* Lambda Beta - Mars Hill College, Mars Hill, North Carolina",
                    "score": 0.8327256441116333
                },
                {
                    "id": 3422787,
                    "contents": "Pi Beta Phi\nSee also National Panhellenic Conference List of social fraternities and sororities List of Pi Beta Phi sisters List of Pi Beta Phi chapters References External links Pi Beta Phi Fraternity For Women 1867 establishments in Illinois International student societies National Panhellenic Conference Student societies in the United States Student organizations established in 1867 Monmouth College",
                    "score": 0.8324835300445557
                },
                {
                    "id": 2297747,
                    "contents": "Theta Tau\nRandall J. Scheetz, O '79, was first elected Grand Regent in 1986. The fraternity experienced significant growth during his tenure with the installation of eight chapters and the certification of thirteen colonies. This extension effort was sparked by Jerome R. Palardy, EB '90, (then Student Member of the Executive Council) in the Detroit area (Xi Beta, Omicron Beta, and Phi Beta Chapters resulting, the latter installed in 1991). Highlighting extension at other schools was the reestablishment of Pi and Gamma Beta Chapters (inactive since the late 1970s). Other chapters installed were Pi Beta, Rho Beta, Sigma Beta, and Tau Beta; and four new alumni clubs were authorized.",
                    "score": 0.8318557739257812
                },
                {
                    "id": 29303435,
                    "contents": "Delta Beta Phi\nAlpha - Cornell University Beta - University of Pennsylvania Gamma or Pi - Columbia University 1921-193x Delta - City University of New York Epsilon - Harvard University Iota - DePaul University 192x-193x Kappa - New York University Mu - Ohio State University Nu - Northwestern University Omicron - Columbia Dental College Pi - Johns Hopkins University Rho - University of Washington Phi - Lehigh University Chi - Crane College 1923-1933 Psi - University of Virginia Psi Deuteron - Lafayette College Omega - Stetson University September 1926 Symbols and publications The official badge of the society was a diamond-shaped lozenge, displaying the letters , and , with these surmounting a pair of crossed keys. There was a star at each corner of the badge.",
                    "score": 0.8316649794578552
                },
                {
                    "id": 21346737,
                    "contents": "Beta Phi Mu Award\n2000-2009 2009. C. James Schmidt, San Jose State University, CA. 2008. Ching-chih Chen, Simmons College, Boston 2007. Barbara Immroth, University of Texas-Austin 2006. Lois Mai Chan, University of Kentucky 2005. Lynn Akin, Texas Woman's University 2004. Linda C. Smith, University of Illinois 2003. Kathleen de la Peña McCook, Louisiana State University, University of South Florida. 2002. Leigh Stewart Estabrook, University of Illinois 2001. Lotsee Patterson, University of Oklahoma 2000. Shirley Fitzgibbons, Indiana University 1990-1999",
                    "score": 0.8314037919044495
                },
                {
                    "id": 15810956,
                    "contents": "Phi Beta Delta (honor society)\nChapters The organization has chapters in over 179 campuses worldwide. International chapters are in Bulgaria, Canada, Mexico, Switzerland, and the United States of America. Publications The Medallion (the Society newsletter) Proceedings (a record of ideas, issues, and topics of interest to the Society) International Research and Review (the peer reviewed journal of the Society) 52 Weekly Thoughts Notable and Honorary Members",
                    "score": 0.831290066242218
                },
                {
                    "id": 265578,
                    "contents": "Monmouth University\nSororities Alpha Kappa Alpha (Tau Eta chapter) Alpha Omicron Pi (Iota Theta chapter) Alpha Sigma Tau (Beta Omega chapter) Alpha Xi Delta (Iota Nu chapter) Chi Upsilon Sigma (Gamma Beta chapter) Delta Phi Epsilon (Delta Omega chapter) Lambda Theta Alpha (Tau Chapter) Phi Sigma Sigma (Delta Phi chapter) Student residences Beechwood Hall Cedar Hall University Bluffs Elmwood Hall Garden Apartments Great Lawn Apartments Laurel Hall Maplewood Apartments Mullaney Hall Hesse Hall Oakwood Hall Pinewood Hall Redwood Hall Spruce Hall Willow Hall athletics_affiliations Athletics",
                    "score": 0.8312221765518188
                },
                {
                    "id": 18429886,
                    "contents": "List of Gamma Phi Beta chapters\nGamma Phi Beta has chartered 187 collegiate chapters. Chapters Lists of chapters of United States student societies by society chapters",
                    "score": 0.8306980133056641
                },
                {
                    "id": 27130137,
                    "contents": "Beta Sigma Omicron\nAlpha Iota - 1927 - University of California, Berkeley Beta Beta - 1933 - Samford University (m. ΖΤΑ 1964) ...",
                    "score": 0.8301801085472107
                },
                {
                    "id": 21477623,
                    "contents": "Alpha & Omega (book)\n\"Beyond the Third Revolution: Voyage to the Ends of Time\" \"Appendix A: Tired Light Retired\" \"Appendix B: Where Does Matter Come From?\" \"Appendix C: Nobel Prizes in Physics—Past and Future\" Seife predicts which scientists are likely to win a Nobel Prize for their work in cosmology. \"Appendix D: Some Experiments to Watch\" Glossary, Select Bibliography, Acknowledgements, Index",
                    "score": 0.8300004005432129
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_29",
        "question": "Two narrow slits separated by $0.10 \\mathrm{~mm}$ are illuminated by light of wavelength $600 \\mathrm{~nm}$. What is the angular position of the first maximum in the interference pattern? If a detector is located $2.00 \\mathrm{~m}$ beyond the slits, what is the distance between the central maximum and the first maximum?",
        "golden_answers": [
            " 12"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1681638,
                    "contents": "Double-slit experiment\nFor example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6 μm wavelength laser (), then at a distance of 1 m (), the spacing of the fringes will be 1.2 mm. If the width of the slits is appreciable compared to the wavelength, the Fraunhofer diffraction equation is needed to determine the intensity of the diffracted light as follows: where the sinc function is defined as sinc(x) = sin(x)/x for x ≠ 0, and sinc(0) = 1. This is illustrated in the figure above, where the first pattern is the diffraction pattern of a single slit, given by the function in this equation, and the second figure shows the combined intensity of the light diffracted from the two slits, where the function represents the fine structure, and the coarser structure represents diffraction by the individual slits as described by the function.",
                    "score": 0.937214732170105
                },
                {
                    "id": 11980855,
                    "contents": "Diffraction from slits\nIf we now consider the situation where , the path length becomes This is the Fresnel approximation. To further simplify things: If the diffracting object is much smaller than the distance , the last term will contribute much less than a wavelength to the path length, and will then not change the phase appreciably. That is . The result is the Fraunhofer approximation, which is only valid very far away from the object Depending on the size of the diffraction object, the distance to the object and the wavelength of the wave, the Fresnel approximation, the Fraunhofer approximation or neither approximation may be valid. As the distance between the measured point of diffraction and the obstruction point increases, the diffraction patterns or results predicted converge towards those of Fraunhofer diffraction, which is more often observed in nature due to the extremely small wavelength of visible light. Multiple narrow slits A simple quantitative description",
                    "score": 0.9259775876998901
                },
                {
                    "id": 1681637,
                    "contents": "Double-slit experiment\nWhere d is the distance between the two slits. When the two waves are in phase, i.e. the path difference is equal to an integral number of wavelengths, the summed amplitude, and therefore the summed intensity is maximum, and when they are in anti-phase, i.e. the path difference is equal to half a wavelength, one and a half wavelengths, etc., then the two waves cancel and the summed intensity is zero. This effect is known as interference. The interference fringe maxima occur at angles where λ is the wavelength of the light. The angular spacing of the fringes, , is given by The spacing of the fringes at a distance from the slits is given by For example, if two slits are separated by 0.5 mm (), and are illuminated with a 0.6 μm wavelength laser (), then at a distance of 1 m (), the spacing of the fringes will be 1.2 mm.",
                    "score": 0.9233474731445312
                },
                {
                    "id": 1680373,
                    "contents": "Diffraction\nA slit that is wider than a wavelength produces interference effects in the space downstream of the slit. These can be explained by assuming that the slit behaves as though it has a large number of point sources spaced evenly across the width of the slit. The analysis of this system is simplified if we consider light of a single wavelength. If the incident light is coherent, these sources all have the same phase. Light incident at a given point in the space downstream of the slit is made up of contributions from each of these point sources and if the relative phases of these contributions vary by 2π or more, we may expect to find minima and maxima in the diffracted light. Such phase differences are caused by differences in the path lengths over which contributing rays reach the point from the slit.",
                    "score": 0.9200325608253479
                },
                {
                    "id": 19024864,
                    "contents": "Fraunhofer diffraction equation\nSlits Two slits The pattern which occurs when light diffracted from two slits overlaps is of considerable interest in physics, firstly for its importance in establishing the wave theory of light through Young's interference experiment, and secondly because of its role as a thought experiment in double-slit experiment in quantum mechanics. Narrow slits right|thumb|Two slit interference using a red laserAssume we have two long slits illuminated by a plane wave of wavelength . The slits are in the plane, parallel to the axis, separated by a distance and are symmetrical about the origin. The width of the slits is small compared with the wavelength. Solution by integration The incident light is diffracted by the slits into uniform spherical waves. The waves travelling in a given direction from the two slits have differing phases. The phase of the waves from the upper and lower slits relative to the origin is given by and The complex amplitude of the summed waves is given by:",
                    "score": 0.9199303388595581
                },
                {
                    "id": 4008836,
                    "contents": "Fraunhofer diffraction\nDiffraction by a double slit In the double-slit experiment, the two slits are illuminated by a single light beam. If the width of the slits is small enough (less than the wavelength of the light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. These fringes are often known as Young's fringes. The angular spacing of the fringes is given by The spacing of the fringes at a distance from the slits is given by where is the separation of the slits. The fringes in the picture were obtained using the yellow light from a sodium light (wavelength = 589 nm), with slits separated by 0.25 mm, and projected directly onto the image plane of a digital camera.",
                    "score": 0.9198020100593567
                },
                {
                    "id": 1324713,
                    "contents": "Wavelength\nwhere m is an integer, and for destructive interference is: Thus, if the wavelength of the light is known, the slit separation can be determined from the interference pattern or fringes, and vice versa. For multiple slits, the pattern is where q is the number of slits, and g is the grating constant. The first factor, I1, is the single-slit result, which modulates the more rapidly varying second factor that depends upon the number of slits and their spacing. In the figure I1 has been set to unity, a very rough approximation. The effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up. Single-slit diffraction",
                    "score": 0.9197323322296143
                },
                {
                    "id": 11980860,
                    "contents": "Diffraction from slits\nA mathematical representation of Huygens' principle can be used to start an equation. Consider a monochromatic complex plane wave of wavelength λ incident on a slit of width a. If the slit lies in the x′-y′ plane, with its center at the origin, then it can be assumed that diffraction generates a complex wave ψ, traveling radially in the r direction away from the slit, and this is given by: Let (x′,y′,0) be a point inside the slit over which it is being integrated. If (x,0,z) is the location at which the intensity of the diffraction pattern is being computed, the slit extends from to , and from to . The distance r from the slot is: Assuming Fraunhofer diffraction will result in the conclusion . In other words, the distance to the target is much larger than the diffraction width on the target. By the binomial expansion rule, ignoring terms quadratic and higher, the quantity on the right can be estimated to be:",
                    "score": 0.9173519611358643
                },
                {
                    "id": 11980859,
                    "contents": "Diffraction from slits\nSince we are for the moment only interested in the amplitude and relative phase, we can ignore any overall phase factors that are not dependent on or . We approximate . In the Fraunhofer limit we can neglect terms of order : in the exponential, and any terms involving or in the denominator. The sum becomes The sum has the form of a geometric sum and can be evaluated to give The intensity is given by the absolute value of the complex amplitude squared where denotes the complex conjugate of . Single slit As an example, an exact equation can now be derived for the intensity of the diffraction pattern as a function of angle in the case of single-slit diffraction. A mathematical representation of Huygens' principle can be used to start an equation. Consider a monochromatic complex plane wave of wavelength λ incident on a slit of width a.",
                    "score": 0.9166960716247559
                },
                {
                    "id": 11980862,
                    "contents": "Diffraction from slits\nTaking results in: It can be noted through Euler's formula and its derivatives that and . where the (unnormalized) sinc function is defined by . Now, substituting in , the intensity (squared amplitude) of the diffracted waves at an angle θ is given by: Multiple slits Let us again start with the mathematical representation of Huygens' principle. Consider slits in the prime plane of equal size and spacing spread along the axis. As above, the distance from slit 1 is: To generalize this to slits, we make the observation that while and remain constant, shifts by Thus and the sum of all contributions to the wave function is: Again noting that is small, so , we have: Now, we can use the following identity Substituting into our equation, we find: We now make our substitution as before and represent all non-oscillating constants by the variable as in the 1-slit diffraction and bracket the result. Remember that",
                    "score": 0.9141132831573486
                },
                {
                    "id": 11980856,
                    "contents": "Diffraction from slits\nMultiple narrow slits A simple quantitative description Multiple-slit arrangements can be mathematically considered as multiple simple wave sources, if the slits are narrow enough. For light, a slit is an opening that is infinitely extended in one dimension, and this has the effect of reducing a wave problem in 3D-space to a simpler problem in 2D-space. The simplest case is that of two narrow slits, spaced a distance apart. To determine the maxima and minima in the amplitude we must determine the path difference to the first slit and to the second one. In the Fraunhofer approximation, with the observer far away from the slits, the difference in path length to the two slits can be seen from the image to be Maxima in the intensity occur if this path length difference is an integer number of wavelengths.",
                    "score": 0.9139342308044434
                },
                {
                    "id": 1680367,
                    "contents": "Diffraction\nIn the modern quantum mechanical understanding of light propagation through a slit (or slits) every photon has what is known as a wavefunction. The wavefunction is determined by the physical surroundings such as slit geometry, screen distance and initial conditions when the photon is created. In important experiments (A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, see double-slit experiment) the existence of the photon's wavefunction was demonstrated. In the quantum approach the diffraction pattern is created by the probability distribution, the observation of light and dark bands is the presence or absence of photons in these areas, where these particles were more or less likely to be detected. The quantum approach has some striking similarities to the Huygens-Fresnel principle; based on that principle, as light travels through slits and boundaries, secondary, point light sources are created near or along these obstacles, and the resulting",
                    "score": 0.9131751656532288
                },
                {
                    "id": 1324715,
                    "contents": "Wavelength\nIn the analysis of the single slit, the non-zero width of the slit is taken into account, and each point in the aperture is taken as the source of one contribution to the beam of light (Huygens' wavelets). On the screen, the light arriving from each position within the slit has a different path length, albeit possibly a very small difference. Consequently, interference occurs. In the Fraunhofer diffraction pattern sufficiently far from a single slit, within a small-angle approximation, the intensity spread S is related to position x via a squared sinc function: with where L is the slit width, R is the distance of the pattern (on the screen) from the slit, and λ is the wavelength of light used. The function S has zeros where u is a non-zero integer, where are at x values at a separation proportion to wavelength. Diffraction-limited resolution",
                    "score": 0.9116369485855103
                },
                {
                    "id": 11980857,
                    "contents": "Diffraction from slits\nMaxima in the intensity occur if this path length difference is an integer number of wavelengths. {| |- | || || rowspan=4 | where is an integer that labels the order of each maximum, is the wavelength, is the distance between the slits and is the angle at which constructive interference occurs. |- | |} The corresponding minima are at path differences of an integer number plus one half of the wavelength: . For an array of slits, positions of the minima and maxima are not changed, the fringes visible on a screen however do become sharper, as can be seen in the image. Mathematical description To calculate this intensity pattern, one needs to introduce some more sophisticated methods. The mathematical representation of a radial wave is given by",
                    "score": 0.9106013178825378
                },
                {
                    "id": 901535,
                    "contents": "Diffraction grating\nAn idealized diffraction grating is made up of a set of slits of spacing , that must be wider than the wavelength of interest to cause diffraction. Assuming a plane wave of monochromatic light of wavelength at normal incidence on a grating (I.e., wavefronts of the incident wave are parallel to the grating main plane), each slit in the grating acts as a quasi point wave source from which light propagates in all directions (although this is typically limited to the forward hemisphere from the point source). Of course, every point on every slit to which the incident wave reaches plays as a point wave source for the diffraction wave and all these contributions to the diffraction wave determine the detailed diffraction wave light property distribution, but diffraction angles (at the grating) at which the diffraction wave intensity is highest are determined only by these quasi point sources corresponding the slits in the grating. After the incident light (wave) interacts with the grating,",
                    "score": 0.9106006622314453
                },
                {
                    "id": 1681618,
                    "contents": "Double-slit experiment\nIf one illuminates two parallel slits, the light from the two slits again interferes. Here the interference is a more pronounced pattern with a series of alternating light and dark bands. The width of the bands is a property of the frequency of the illuminating light. (See the bottom photograph to the right.) When Thomas Young (1773–1829) first demonstrated this phenomenon, it indicated that light consists of waves, as the distribution of brightness can be explained by the alternately additive and subtractive interference of wavefronts. Young's experiment, performed in the early 1800s, played a crucial role in the understanding of the wave theory of light, vanquishing the corpuscular theory of light proposed by Isaac Newton, which had been the accepted model of light propagation in the 17th and 18th centuries. However, the later discovery of the photoelectric effect demonstrated that under different circumstances, light can behave as if it is composed of discrete particles. These",
                    "score": 0.9098155498504639
                },
                {
                    "id": 17684228,
                    "contents": "N-slit interferometer\nThe N-slit interferometer is an extension of the double-slit interferometer also known as Young's double-slit interferometer. One of the first known uses of N-slit arrays in optics was illustrated by Newton. In the first part of last century, Michelson described various cases of N-slit diffraction. Feynman described thought experiments, of two-slit quantum interference, of electrons using Dirac's notation. This approach was extended to N-slit interferometers, by Duarte and colleagues in 1989, using narrow-linewidth laser illumination, that is, illumination by indistinguishable photons. The first application of the N-slit interferometer was the generation and measurement of complex interference patterns. These interferograms are accurately reproduced, or predicted, by the N-slit interferometric equation for either even (N = 2, 4, 6,…), or odd (N = 3, 5, 7,…), numbers of slits. N-slit laser interferometer",
                    "score": 0.9092466831207275
                },
                {
                    "id": 1680375,
                    "contents": "Diffraction\nwhere d is the width of the slit, is the angle of incidence at which the minimum intensity occurs, and is the wavelength of the light A similar argument can be used to show that if we imagine the slit to be divided into four, six, eight parts, etc., minima are obtained at angles θn given by where n is an integer other than zero. There is no such simple argument to enable us to find the maxima of the diffraction pattern. The intensity profile can be calculated using the Fraunhofer diffraction equation as where is the intensity at a given angle, is the intensity at the central maximum (), which is also a normalization factor of the intensity profile that can be determined by an integration from to and conservation of energy. is the unnormalized sinc function. This analysis applies only to the far field (Fraunhofer diffraction), that is, at a distance much larger than the width of the slit.",
                    "score": 0.9081034660339355
                },
                {
                    "id": 11980863,
                    "contents": "Diffraction from slits\nWe now make our substitution as before and represent all non-oscillating constants by the variable as in the 1-slit diffraction and bracket the result. Remember that This allows us to discard the tailing exponent and we have our answer: General case for far field In the far field, where r is essentially constant, then the equation: is equivalent to doing a Fourier transform on the gaps in the barrier. See also Diffraction grating Envelope (waves) Fourier analysis N-slit interferometer Radio telescopes References Equations of physics Wave mechanics",
                    "score": 0.9048668742179871
                },
                {
                    "id": 1324714,
                    "contents": "Wavelength\nThe effect of interference is to redistribute the light, so the energy contained in the light is not altered, just where it shows up. Single-slit diffraction The notion of path difference and constructive or destructive interference used above for the double-slit experiment applies as well to the display of a single slit of light intercepted on a screen. The main result of this interference is to spread out the light from the narrow slit into a broader image on the screen. This distribution of wave energy is called diffraction. Two types of diffraction are distinguished, depending upon the separation between the source and the screen: Fraunhofer diffraction or far-field diffraction at large separations and Fresnel diffraction or near-field diffraction at close separations.",
                    "score": 0.9040337204933167
                },
                {
                    "id": 11980854,
                    "contents": "Diffraction from slits\nApproximations The problem of calculating what a diffracted wave looks like, is the problem of determining the phase of each of the simple sources on the incoming wave front. It is mathematically easier to consider the case of far-field or Fraunhofer diffraction, where the point of observation is far from that of the diffracting obstruction, and as a result, involves less complex mathematics than the more general case of near-field or Fresnel diffraction. To make this statement more quantitative, consider a diffracting object at the origin that has a size . For definiteness let us say we are diffracting light and we are interested in what the intensity looks like on a screen a distance away from the object. At some point on the screen the path length to one side of the object is given by the Pythagorean theorem If we now consider the situation where , the path length becomes",
                    "score": 0.9027082920074463
                },
                {
                    "id": 1681636,
                    "contents": "Double-slit experiment\nIn the double-slit experiment, the two slits are illuminated by the quasi-monochromatic light of a single laser. If the width of the slits is small enough (much less than the wavelength of the laser light), the slits diffract the light into cylindrical waves. These two cylindrical wavefronts are superimposed, and the amplitude, and therefore the intensity, at any point in the combined wavefronts depends on both the magnitude and the phase of the two wavefronts. The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves. If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure below right. The path difference between two waves travelling at an angle is given by:",
                    "score": 0.9024754762649536
                },
                {
                    "id": 17267833,
                    "contents": "N-slit interferometric equation\nwhere and after some algebra, the corresponding probability becomes where is the total number of slits in the array, or transmission grating, and the term in parentheses represents the phase that is directly related to the exact path differences derived from the geometry of the -slit array (), the intra interferometric distance, and the interferometric plane . In its simplest version, the phase term can be related to the geometry using where is the wavenumber, and and represent the exact path differences. Here the Dirac–Duarte (DD) interferometric equation is a probability distribution that is related to the intensity distribution measured experimentally. The calculations are performed numerically.",
                    "score": 0.9014625549316406
                },
                {
                    "id": 19024860,
                    "contents": "Fraunhofer diffraction equation\nIn practice, all slits are of finite size so produce diffraction on the both transverse directions, along the (width W defined) and (height H defined) axes. If the height H of the slit is much greater than its width W, then the spacing of the vertical (along the height or the axis) diffraction fringes is much less than the spacing of the horizontal (along the width or axis) fringes. If the vertical fringe spacing is so less by a relatively so large H, then the observation of the vertical fringes is so hard that a person observing the diffracted wave intensity pattern on the plane of observation or the image plane recognizes only the horizontal fringes with their narrow height. This is the reason why a height-long slit or slit array such as a diffraction grating is typically analyzed only in the dimension along the width. If the illuminating beam does not illuminate the whole height of the slit, then the spacing of the vertical fringes is determined by the dimension of the laser",
                    "score": 0.9014332294464111
                },
                {
                    "id": 19024858,
                    "contents": "Fraunhofer diffraction equation\nThe slit can be represented by the rect function as: The Fourier transform of this function is given by where is the Fourier transform frequency, and the function is here defined as sin(x)/(x) The Fourier transform frequency here is , giving Note that the function is here defined as sin(x)/(x) to maintain consistency. Intensity The intensity is proportional to the square of the amplitude, and is therefore Apertures Rectangular aperture When a rectangular slit of width W and height H is illuminated normally (the slit illuminated at the normal angle) by a monochromatic plane wave of wavelength λ, the complex amplitude can be found using similar analyses to those in the previous section, applied over two independent orthogonal dimensions as:Longhurst, 1967, p 217 . The intensity is given by",
                    "score": 0.89958256483078
                },
                {
                    "id": 1681617,
                    "contents": "Double-slit experiment\nOverview If light consisted strictly of ordinary or classical particles, and these particles were fired in a straight line through a slit and allowed to strike a screen on the other side, we would expect to see a pattern corresponding to the size and shape of the slit. However, when this \"single-slit experiment\" is actually performed, the pattern on the screen is a diffraction pattern in which the light is spread out. The smaller the slit, the greater the angle of spread. The top portion of the image shows the central portion of the pattern formed when a red laser illuminates a slit and, if one looks carefully, two faint side bands. More bands can be seen with a more highly refined apparatus. Diffraction explains the pattern as being the result of the interference of light waves from the slit.",
                    "score": 0.8993656635284424
                },
                {
                    "id": 1681639,
                    "contents": "Double-slit experiment\nSimilar calculations for the near field can be made by applying the Fresnel diffraction equation, which implies that as the plane of observation gets closer to the plane in which the slits are located, the diffraction patterns associated with each slit decrease in size, so that the area in which interference occurs is reduced, and may vanish altogether when there is no overlap in the two diffracted patterns. Interpretations of the experiment Like the Schrödinger's cat thought experiment, the double-slit experiment is often used to highlight the differences and similarities between the various interpretations of quantum mechanics. Copenhagen interpretation",
                    "score": 0.8988877534866333
                },
                {
                    "id": 1680374,
                    "contents": "Diffraction\nWe can find the angle at which a first minimum is obtained in the diffracted light by the following reasoning. The light from a source located at the top edge of the slit interferes destructively with a source located at the middle of the slit, when the path difference between them is equal to λ/2. Similarly, the source just below the top of the slit will interfere destructively with the source located just below the middle of the slit at the same angle. We can continue this reasoning along the entire height of the slit to conclude that the condition for destructive interference for the entire slit is the same as the condition for destructive interference between two narrow slits a distance apart that is half the width of the slit. The path difference is approximately so that the minimum intensity occurs at an angle θmin given by where d is the width of the slit, is the angle of incidence at which the minimum intensity occurs, and is the wavelength of the light",
                    "score": 0.898523211479187
                },
                {
                    "id": 1681642,
                    "contents": "Double-slit experiment\na light wave (as understood in classical physics) is wide enough to take both paths, then that ray tracing will accurately predict the appearance of maxima and minima on the detector screen when many particles pass through the apparatus and gradually \"paint\" the expected interference pattern.",
                    "score": 0.8984076976776123
                },
                {
                    "id": 17267835,
                    "contents": "N-slit interferometric equation\nGeneralized diffraction and refraction The -slit interferometric equation has been applied to describe classical phenomena such as interference, diffraction, refraction (Snell's law), and reflection, in a rational and unified approach, using quantum mechanics principles. In particular, this interferometric approach has been used to derive generalized refraction equations for both positive and negative refraction, thus providing a clear link between diffraction theory and generalized refraction. From the phase term, of the interferometric equation, the expression can be obtained, where . For , this equation can be written as which is the generalized diffraction grating equation. Here, is the angle of incidence, is the angle of diffraction, is the wavelength, and is the order of diffraction. Under certain conditions, , which can be readily obtained experimentally, the phase term becomes",
                    "score": 0.8978872299194336
                },
                {
                    "id": 11980852,
                    "contents": "Diffraction from slits\nThe simplest descriptions of diffraction are those in which the situation can be reduced to a two-dimensional problem. For water waves, this is already the case, as water waves propagate only on the surface of the water. For light, we can often neglect one dimension if the diffracting object extends in that direction over a distance far greater than the wavelength. In the case of light shining through small circular holes we will have to take into account the full three-dimensional nature of the problem.",
                    "score": 0.8977846503257751
                },
                {
                    "id": 18112139,
                    "contents": "Young's interference experiment\nYoung's interference experiment, also called Young's double-slit interferometer, was the original version of the modern double-slit experiment, performed at the beginning of the nineteenth century by Thomas Young. This experiment played a major role in the general acceptance of the wave theory of light. In Young's own judgement, this was the most important of his many achievements.",
                    "score": 0.8968507051467896
                },
                {
                    "id": 20576908,
                    "contents": "List of optics equations\nwhere |- !Single slit diffraction intensity | I0 = source intensity Wave phase through apertures | |- !N-slit diffraction (N ≥ 2) | d = centre-to-centre separation of slits N = number of slits Phase between N waves emerging from each slit | |- !N-slit diffraction (all N) | | |- !Circular aperture intensity | a = radius of the circular aperture J1 is a Bessel function | |- !Amplitude for a general planar aperture |Cartesian and spherical polar coordinates are used, xy plane contains aperture A, amplitude at position r r' = source point in the aperture Einc, magnitude of incident electric field at aperture | Near-field (Fresnel) Far-field (Fraunhofer)",
                    "score": 0.8968070149421692
                },
                {
                    "id": 11980858,
                    "contents": "Diffraction from slits\nMathematical description To calculate this intensity pattern, one needs to introduce some more sophisticated methods. The mathematical representation of a radial wave is given by where , is the wavelength, is frequency of the wave and is the phase of the wave at the slits at time t=0. The wave at a screen some distance away from the plane of the slits is given by the sum of the waves emanating from each of the slits. To make this problem a little easier, we introduce the complex wave , the real part of which is equal to The absolute value of this function gives the wave amplitude, and the complex phase of the function corresponds to the phase of the wave. is referred to as the complex amplitude. With slits, the total wave at point on the screen is .",
                    "score": 0.8963951468467712
                },
                {
                    "id": 1681613,
                    "contents": "Double-slit experiment\nIn the basic version of this experiment, a coherent light source, such as a laser beam, illuminates a plate pierced by two parallel slits, and the light passing through the slits is observed on a screen behind the plate. The wave nature of light causes the light waves passing through the two slits to interfere, producing bright and dark bands on the screen – a result that would not be expected if light consisted of classical particles. However, the light is always found to be absorbed at the screen at discrete points, as individual particles (not waves); the interference pattern appears via the varying density of these particle hits on the screen. Furthermore, versions of the experiment that include detectors at the slits find that each detected photon passes through one slit (as would a classical particle), and not through both slits (as would a wave). However, such experiments demonstrate that particles do not form the interference pattern if one detects which slit they pass",
                    "score": 0.8962100148200989
                },
                {
                    "id": 1324712,
                    "contents": "Wavelength\nInterference and diffraction Double-slit interference When sinusoidal waveforms add, they may reinforce each other (constructive interference) or cancel each other (destructive interference) depending upon their relative phase. This phenomenon is used in the interferometer. A simple example is an experiment due to Young where light is passed through two slits. As shown in the figure, light is passed through two slits and shines on a screen. The path of the light to a position on the screen is different for the two slits, and depends upon the angle θ the path makes with the screen. If we suppose the screen is far enough from the slits (that is, s is large compared to the slit separation d) then the paths are nearly parallel, and the path difference is simply d sin θ. Accordingly, the condition for constructive interference is: where m is an integer, and for destructive interference is:",
                    "score": 0.8960505127906799
                },
                {
                    "id": 17267831,
                    "contents": "N-slit interferometric equation\nQuantum mechanics was first applied to optics, and interference in particular, by Paul Dirac. Richard Feynman, in his Lectures on Physics, uses Dirac's notation to describe thought experiments on double-slit interference of electrons. Feynman's approach was extended to -slit interferometers for either single-photon illumination, or narrow-linewidth laser illumination, that is, illumination by indistinguishable photons, by Frank Duarte. The -slit interferometer was first applied in the generation and measurement of complex interference patterns. In this article the generalized -slit interferometric equation, derived via Dirac's notation, is described. Although originally derived to reproduce and predict -slit interferograms, this equation also has applications to other areas of optics. Probability amplitudes and the -slit interferometric equation",
                    "score": 0.8946211934089661
                },
                {
                    "id": 4008837,
                    "contents": "Fraunhofer diffraction\nDouble-slit interference fringes can be observed by cutting two slits in a piece of card, illuminating with a laser pointer, and observing the diffracted light at a distance of 1 m. If the slit separation is 0.5 mm, and the wavelength of the laser is 600 nm, then the spacing of the fringes viewed at a distance of 1 m would be 1.2 mm. Semi-quantitative explanation of double-slit fringes The difference in phase between the two waves is determined by the difference in the distance travelled by the two waves. If the viewing distance is large compared with the separation of the slits (the far field), the phase difference can be found using the geometry shown in the figure. The path difference between two waves travelling at an angle is given by",
                    "score": 0.8946081399917603
                },
                {
                    "id": 23431583,
                    "contents": "Orders Of Coherence\nYoung Double Slit Experiment In Young's double slit experiment, light from a light source is allowed to pass through two pinholes separated by some distance, and a screen is placed some distance away from the pinholes where the interference between the light waves is observed (Figure. 1). Young's double slit experiment demonstrates the dependence of interference on coherence, specifically on the first-order correlation. This experiment is equivalent to the Mach-Zehnder interferometer with the caveat that Young's double slit experiment is concerned with spatial coherence, while Mach-Zehnder interferometer relies on temporal coherence. The intensity measured at the position at time is . Light field has highest degree of coherence when the corresponding interference pattern has the maximum contrast on the screen. The fringe contrast is defined as .",
                    "score": 0.8945472836494446
                },
                {
                    "id": 19024868,
                    "contents": "Fraunhofer diffraction equation\nThe form of the pattern for =50 is shown in the first figure . The detailed structure for 20 and 50 slits gratings are illustrated in the second diagram. Finite width slit grating The grating now has N slits of width and spacing Solution using integration The amplitude is given by: Solution using Fourier transform The aperture function can be written as: Using the convolution theorem, which says that if we have two functions and , and we have where ∗ denotes the convolution operation, then we also have we can write the aperture function as The amplitude is then given by the Fourier transform of this expression as: Intensity The intensity is given by: The diagram shows the diffraction pattern for a grating with 20 slits, where the width of the slits is 1/5th of the slit separation. The size of the main diffracted peaks is modulated with the diffraction pattern of the individual slits. Other gratings",
                    "score": 0.8944811224937439
                },
                {
                    "id": 4008830,
                    "contents": "Fraunhofer diffraction\nExamples of Fraunhofer diffraction In each of these examples, the aperture is illuminated by a monochromatic plane wave at normal incidence. Diffraction by a narrow rectangular slit The width of the slit is . The Fraunhofer diffraction pattern is shown in the image together with a plot of the intensity vs. angle . The pattern has maximum intensity at , and a series of peaks of decreasing intensity. Most of the diffracted light falls between the first minima. The angle, , subtended by these two minima is given by: Thus, the smaller the aperture, the larger the angle subtended by the diffraction bands. The size of the central band at a distance is given by For example, when a slit of width 0.5 mm is illuminated by light of wavelength 0.6 μm, and viewed at a distance of 1000 mm, the width of the central band in the diffraction pattern is 2.4 mm. The fringes extend to infinity in the direction since the slit and illumination also extend to infinity.",
                    "score": 0.8942302465438843
                },
                {
                    "id": 11980853,
                    "contents": "Diffraction from slits\nSeveral qualitative observations can be made of diffraction in general: The angular spacing of the features in the diffraction pattern is inversely proportional to the dimensions of the object causing the diffraction. In other words: the smaller the diffracting object, the wider the resulting diffraction pattern, and vice versa. (More precisely, this is true of the sines of the angles.) The diffraction angles are invariant under scaling; that is, they depend only on the ratio of the wavelength to the size of the diffracting object. When the diffracting object has a periodic structure, for example in a diffraction grating, the features generally become sharper. The fourth figure, for example, shows a comparison of a double-slit pattern with a pattern formed by five slits, both sets of slits having the same spacing between the center of one slit and the next.",
                    "score": 0.893781840801239
                },
                {
                    "id": 19024866,
                    "contents": "Fraunhofer diffraction equation\nand We have or This is the same expression as was derived by integration. Intensity The intensity is given by: It can be seen that the form of the intensity pattern is the product of the individual slit diffraction pattern, and the interference pattern which would be obtained with slits of negligible width. This is illustrated in the image at the right which shows single slit diffraction by a laser beam, and also the diffraction/interference pattern given by two identical slits. Gratings A grating is defined in Born and Wolf as \"any arrangement which imposes on an incident wave a periodic variation of amplitude or phase, or both\". Narrow slit grating A simple grating consists of a screen with N slits whose width is significantly less than the wavelength of the incident light with slit separation of . Solution by integration The complex amplitude of the diffracted wave at an angle is given by: since this is the sum of a geometric series.",
                    "score": 0.8934211730957031
                },
                {
                    "id": 1681624,
                    "contents": "Double-slit experiment\nThe probability of detection is the square of the amplitude of the wave and can be calculated with classical waves (see below). Ever since the origination of quantum mechanics, some theorists have searched for ways to incorporate additional determinants or \"hidden variables\" that, were they to become known, would account for the location of each individual impact with the target. Mach-Zehnder interferometer The Mach–Zehnder interferometer can be seen as a simplified version of the double-slit experiment. Instead of propagating through free space after the two slits, and hitting any position in an extended screen, in the interferometer the photons can only propagate via two paths, and hit two discrete photodetectors. This makes it possible to describe it via simple linear algebra in dimension 2, rather than differential equations.",
                    "score": 0.8929166197776794
                },
                {
                    "id": 901536,
                    "contents": "Diffraction grating\nat which the diffraction wave intensity is highest are determined only by these quasi point sources corresponding the slits in the grating. After the incident light (wave) interacts with the grating, the resulting diffracted light from the grating is composed of the sum of interfering wave components emanating from each slit in the grating; At any given point in space through which the diffracted light may pass, typically called observation point, the path length from each slit in the grating to the given point varies, so the phase of the wave emanating from each of the slits at that point also varies. As a result, the sum of the diffracted waves from the grating slits at the given observation point creates a peak, valley, or some degree between them in light intensity through additive and destructive interference. When the difference between the light paths from adjacent slits to the observation point is equal to an odd integer-multiple of the half of the wavelength, l with an odd",
                    "score": 0.8928887248039246
                },
                {
                    "id": 11980861,
                    "contents": "Diffraction from slits\nIt can be seen that 1/r in front of the equation is non-oscillatory, i.e. its contribution to the magnitude of the intensity is small compared to our exponential factors. Therefore, we will lose little accuracy by approximating it as 1/z. To make things cleaner, a placeholder 'C' is used to denote constants in the equation. It is important to keep in mind that C can contain imaginary numbers, thus the wave function will be complex. However, at the end, the ψ will be bracketed, which will eliminate any imaginary components. Now, in Fraunhofer diffraction, is small, so (note that participates in this exponential and it is being integrated). In contrast the term can be eliminated from the equation, since when bracketed it gives 1. (For the same reason we have also eliminated the term ) Taking results in: It can be noted through Euler's formula and its derivatives that and . where the (unnormalized) sinc function is defined by .",
                    "score": 0.8925580978393555
                },
                {
                    "id": 1681620,
                    "contents": "Double-slit experiment\nFeynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment. He also proposed (as a thought experiment) that if detectors were placed before each slit, the interference pattern would disappear. The Englert–Greenberger duality relation provides a detailed treatment of the mathematics of double-slit interference in the context of quantum mechanics. A low-intensity double-slit experiment was first performed by G. I. Taylor in 1909, by reducing the level of incident light until photon emission/absorption events were mostly non-overlapping.",
                    "score": 0.8924756050109863
                },
                {
                    "id": 4008834,
                    "contents": "Fraunhofer diffraction\nIf the illuminating beam does not illuminate the whole vertical length of the slit, the spacing of the vertical fringes is determined by the dimensions of the illuminating beam. Close examination of the double-slit diffraction pattern below shows that there are very fine horizontal diffraction fringes above and below the main spot, as well as the more obvious horizontal fringes. Diffraction by a circular aperture The diffraction pattern given by a circular aperture is shown in the figure on the right. This is known as the Airy diffraction pattern. It can be seen that most of the light is in the central disk. The angle subtended by this disk, known as the Airy disk, is where is the diameter of the aperture. The Airy disk can be an important parameter in limiting the ability of an imaging system to resolve closely located objects. Diffraction by an aperture with a Gaussian profile",
                    "score": 0.8915424346923828
                },
                {
                    "id": 18112144,
                    "contents": "Young's interference experiment\nwhere is the distance between the slit and screen, is the wavelength of light and is the slit separation as shown in figure. The angular spacing of the fringes, , is then given by where <<1, and λ is the wavelength of the light. It can be seen that the spacing of the fringes depends on the wavelength, the separation of the holes, and the distance between the slits and the observation plane, as noted by Young.",
                    "score": 0.8913772702217102
                },
                {
                    "id": 17267832,
                    "contents": "N-slit interferometric equation\nProbability amplitudes and the -slit interferometric equation In this approach the probability amplitude for the propagation of a photon from a source to an interference plane , via an array of slits , is given using Dirac's bra–ket notation as This equation represents the probability amplitude of a photon propagating from to via an array of slits. Using a wavefunction representation for probability amplitudes, and defining the probability amplitudes as where and are the incidence and diffraction phase angles, respectively. Thus, the overall probability amplitude can be rewritten as where and after some algebra, the corresponding probability becomes",
                    "score": 0.8911440372467041
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.4
            }
        }
    },
    {
        "id": "test_30",
        "question": "$$\r\n\\text { If we locate an electron to within } 20 \\mathrm{pm} \\text {, then what is the uncertainty in its speed? }\r\n$$",
        "golden_answers": [
            " 3.7"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 7102653,
                    "contents": "Heisenberg's microscope\nQuantum mechanics questions whether an electron actually has a determinate position before it is disturbed by the measurement used to establish said determinate position. Under a more thorough quantum mechanical analysis, an electron has some probability of showing up at any point in the universe, though the probability that it will be far from where one expects becomes very low at great distances from the neighborhood in which it is originally found. In other words, the \"position\" of an electron can only be stated in terms of a probability distribution, as can predictions of where it may move. See also Atom localization Quantum mechanics Basics of quantum mechanics Interpretation of quantum mechanics Philosophical interpretation of classical physics Schrödinger's cat Uncertainty principle Quantum field theory Electromagnetic radiation References Sources External links History of Heisenberg's Microscope Lectures on Heisenberg's Microscope",
                    "score": 0.8971662521362305
                },
                {
                    "id": 14311955,
                    "contents": "Electron localization function\nThe ELF was originally defined by Becke and Edgecombe in 1990. They first argued that a measure of the electron localization is provided by where ρ is the electron spin density and τ the kinetic energy density. The second term (negative term) is the bosonic kinetic energy density, so D is the contribution due to fermions. D is expected to be small in those regions of space where localized electrons are to be found. Given the arbitrariness of the magnitude of the localization measure provided by D, it is compared to the corresponding value for a uniform electron gas with spin density equal to ρ(r), which is given by The ratio, is a dimensionless localization index that expresses electron localization for the uniform electron gas. In the final step, the ELF is defined in terms of χ by mapping its values on to the range 0 ≤ ELF ≤ 1 by defining the electron localization function as ELF = 1 corresponding to perfect localization and ELF = ½ corresponding to the electron gas.",
                    "score": 0.8940305709838867
                },
                {
                    "id": 7748007,
                    "contents": "Introduction to quantum mechanics\nwhere , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit. The energy of the electron can also be calculated, and is given by . Thus Bohr's assumption that angular momentum is quantized means that an electron can inhabit only certain orbits around the nucleus and that it can have only certain energies. A consequence of these constraints is that the electron does not crash into the nucleus: it cannot continuously emit energy, and it cannot come closer to the nucleus than a0 (the Bohr radius). An electron loses energy by jumping instantaneously from its original orbit to a lower orbit; the extra energy is emitted in the form of a photon. Conversely, an electron that absorbs a photon gains energy, hence it jumps to an orbit that is farther from the nucleus.",
                    "score": 0.891834020614624
                },
                {
                    "id": 154648,
                    "contents": "Electron density\nIn quantum chemistry, electron density or electronic density is the measure of the probability of an electron being present at an infinitesimal element of space surrounding any given point. It is a scalar quantity depending upon three spatial variables and is typically denoted as either or . The density is determined, through definition, by the normalised -electron wavefunction which itself depends upon variables ( spatial and spin coordinates). Conversely, the density determines the wave function modulo up to a phase factor, providing the formal foundation of density functional theory. According to quantum mechanics, due to the uncertainty principle on an atomic scale the exact location of an electron cannot be predicted, only the probability of its being at a given position; therefore electrons in atoms and molecules act as if they are \"smeared out\" in space. For one-electron systems, the electron density at any point is proportional to the square magnitude of the wavefunction.",
                    "score": 0.8913707733154297
                },
                {
                    "id": 1694120,
                    "contents": "Electron\nIn 1947, Willis Lamb, working in collaboration with graduate student Robert Retherford, found that certain quantum states of the hydrogen atom, which should have the same energy, were shifted in relation to each other; the difference came to be called the Lamb shift. About the same time, Polykarp Kusch, working with Henry M. Foley, discovered the magnetic moment of the electron is slightly larger than predicted by Dirac's theory. This small difference was later called anomalous magnetic dipole moment of the electron. This difference was later explained by the theory of quantum electrodynamics, developed by Sin-Itiro Tomonaga, Julian Schwinger and Richard Feynman in the late 1940s.",
                    "score": 0.8908824920654297
                },
                {
                    "id": 1694125,
                    "contents": "Electron\nElectrons have an electric charge of coulombs, which is used as a standard unit of charge for subatomic particles, and is also called the elementary charge. Within the limits of experimental accuracy, the electron charge is identical to the charge of a proton, but with the opposite sign. As the symbol e is used for the elementary charge, the electron is commonly symbolized by , where the minus sign indicates the negative charge. The positron is symbolized by because it has the same properties as the electron but with a positive rather than negative charge.",
                    "score": 0.8893481492996216
                },
                {
                    "id": 7748017,
                    "contents": "Introduction to quantum mechanics\nRalph Kronig originated the theory that particles such as atoms or electrons behave as if they rotate, or \"spin\", about an axis. Spin would account for the missing magnetic moment, and allow two electrons in the same orbital to occupy distinct quantum states if they \"spun\" in opposite directions, thus satisfying the exclusion principle. The quantum number represented the sense (positive or negative) of spin. The choice of the orientation of the magnetic field used in the Stern–Gerlach experiment is arbitrary. In the animation shown here, the field is vertical and so the atoms are deflected either up or down. If the magnet is rotated a quarter turn, the atoms are deflected either left or right. Using a vertical field shows that the spin along the vertical axis is quantized, and using a horizontal field shows that the spin along the horizontal axis is quantized.",
                    "score": 0.8887476921081543
                },
                {
                    "id": 456203,
                    "contents": "Orders of magnitude (length)\n10 picometres To help compare different orders of magnitude this section lists lengths between 10−11 and 10−10 m (10 pm and 100 pm). 25 pm – approximate radius of a helium atom, the smallest neutral atom 50 pm – radius of a hydrogen atom 50 pm – bohr radius: approximate radius of a hydrogen atom ~50 pm – best resolution of a high-resolution transmission electron microscope 60 pm – radius of a carbon atom 93 pm – length of a diatomic carbon molecule 96 pm – H–O bond length in a water molecule",
                    "score": 0.8867599368095398
                },
                {
                    "id": 7747996,
                    "contents": "Introduction to quantum mechanics\nIf is greater than , the energy is enough to remove an electron. The ejected electron has a kinetic energy, , which is, at most, equal to the photon's energy minus the energy needed to dislodge the electron from the metal:",
                    "score": 0.8866178393363953
                },
                {
                    "id": 777314,
                    "contents": "J. J. Thomson\nDiscovery of the electron",
                    "score": 0.8862617015838623
                },
                {
                    "id": 5205023,
                    "contents": "Compton wavelength\nThus the uncertainty in position must be greater than half of the reduced Compton wavelength . The Compton wavelength can be contrasted with the de Broglie wavelength, which depends on the momentum of a particle and determines the cutoff between particle and wave behavior in quantum mechanics. Notably, de Broglie's derivation of the de Broglie wavelength is based on the assumption that an observed particle is associated with a periodic phenomenon of the particle's Compton frequency. Relationship to other constants Typical atomic lengths, wave numbers, and areas in physics can be related to the reduced Compton wavelength for the electron () and the electromagnetic fine structure constant (). The Bohr radius is related to the Compton wavelength by: The classical electron radius is about 3 times larger than the proton radius, and is written: The Rydberg constant, having dimensions of linear wavenumber, is written: This yields the sequence: .",
                    "score": 0.8851909637451172
                },
                {
                    "id": 2367222,
                    "contents": "Wave function\nSee also Remarks Citations General sources Online copy (French) Online copy (English) Online copy Further reading External links Quantum Mechanics for Engineers Spin wave functions NYU Identical Particles Revisited, Michael Fowler The Nature of Many-Electron Wavefunctions Quantum Mechanics and Quantum Computation at BerkeleyX Einstein, The quantum theory of radiation Quantum states Waves",
                    "score": 0.8848749995231628
                },
                {
                    "id": 7748006,
                    "contents": "Introduction to quantum mechanics\nSome fundamental assumptions of the Bohr model were soon proven wrong—but the key result that the discrete lines in emission spectra are due to some property of the electrons in atoms being quantized is correct. The way that the electrons actually behave is strikingly different from Bohr's atom, and from what we see in the world of our everyday experience; this modern quantum mechanical model of the atom is discussed below. Bohr theorized that the angular momentum, , of an electron is quantized: where is an integer and is the Planck constant. Starting from this assumption, Coulomb's law and the equations of circular motion show that an electron with units of angular momentum orbits a proton at a distance given by , where is the Coulomb constant, is the mass of an electron, and is the charge on an electron. For simplicity this is written as where , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit.",
                    "score": 0.8841773271560669
                },
                {
                    "id": 986659,
                    "contents": "Atomic, molecular, and optical physics\n. where E0 is the magnitude of the electric field amplitude, and E is the magnitude of the electric field at position x. From this basic, Planck's law was derived. In 1911, Ernest Rutherford concluded, based on alpha particle scattering, that an atom has a central pointlike proton. He also thought that an electron would be still attracted to the proton by Coulomb's law, which he had verified still held at small scales. As a result, he believed that electrons revolved around the proton. Niels Bohr, in 1913, combined the Rutherford model of the atom with the quantisation ideas of Planck. Only specific and well-defined orbits of the electron could exist, which also do not radiate light. In jumping orbit the electron would emit or absorb light corresponding to the difference in energy of the orbits. His prediction of the energy levels was then consistent with observation.",
                    "score": 0.8837372064590454
                },
                {
                    "id": 1694091,
                    "contents": "Electron\nThe electron is a subatomic particle (denoted by the symbol or ) whose electric charge is negative one elementary charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, ħ. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence",
                    "score": 0.8832746744155884
                },
                {
                    "id": 1195811,
                    "contents": "Quantum electrodynamics\nin practice and involves integration.) But there is another possibility, which is that the electron first moves to G, where it emits a photon, which goes on to D, while the electron moves on to H, where it absorbs the first photon, before moving on to C. Again, we can calculate the probability amplitude of these possibilities (for all points G and H). We then have a better estimation for the total probability amplitude by adding the probability amplitudes of these two possibilities to our original simple estimate. Incidentally, the name given to this process of a photon interacting with an electron in this way is Compton scattering.",
                    "score": 0.8832022547721863
                },
                {
                    "id": 632879,
                    "contents": "Bohr radius\nRelated units The Bohr radius of the electron is one of a trio of related units of length, the other two being the Compton wavelength of the electron and the classical electron radius . The Bohr radius is built from the electron mass , Planck's constant and the electron charge . The Compton wavelength is built from , and the speed of light . The classical electron radius is built from , and . Any one of these three lengths can be written in terms of any other using the fine-structure constant : The Bohr radius is about 19,000 times bigger than the classical electron radius (i.e. the common scale of atoms is angstrom, while the scale of particles is femtometer). The electron's Compton wavelength is about 20 times smaller than the Bohr radius, and the classical electron radius is about 1000 times smaller than the electron's Compton wavelength. Hydrogen atom and similar systems The Bohr radius including the effect of reduced mass in the hydrogen atom is given by",
                    "score": 0.88259357213974
                },
                {
                    "id": 1694117,
                    "contents": "Electron\nDe Broglie's prediction of a wave nature for electrons led Erwin Schrödinger to postulate a wave equation for electrons moving under the influence of the nucleus in the atom. In 1926, this equation, the Schrödinger equation, successfully described how electron waves propagated. Rather than yielding a solution that determined the location of an electron over time, this wave equation also could be used to predict the probability of finding an electron near a position, especially a position near where the electron was bound in space, for which the electron wave equations did not change in time. This approach led to a second formulation of quantum mechanics (the first by Heisenberg in 1925), and solutions of Schrödinger's equation, like Heisenberg's, provided derivations of the energy states of an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction",
                    "score": 0.8820052742958069
                },
                {
                    "id": 15698732,
                    "contents": "Classical mechanics\nThe classical approximation to quantum mechanics The ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is where h is Planck's constant and p is the momentum. Again, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.",
                    "score": 0.8817163109779358
                },
                {
                    "id": 3415863,
                    "contents": "Density of states\nSee also References Further reading Chen, Gang. Nanoscale Energy Transport and Conversion. New York: Oxford, 2005 Streetman, Ben G. and Sanjay Banerjee. Solid State Electronic Devices. Upper Saddle River, NJ: Prentice Hall, 2000. Muller, Richard S. and Theodore I. Kamins. Device Electronics for Integrated Circuits. New York: John Wiley and Sons, 2003. Kittel, Charles and Herbert Kroemer. Thermal Physics. New York: W.H. Freeman and Company, 1980 Sze, Simon M. Physics of Semiconductor Devices. New York: John Wiley and Sons, 1981 External links Online lecture:ECE 606 Lecture 8: Density of States by M. Alam Scientists shed light on glowing materials How to measure the Photonic LDOS Statistical mechanics Physical quantities Electronic band structures",
                    "score": 0.8813631534576416
                },
                {
                    "id": 7102650,
                    "contents": "Heisenberg's microscope\nHeisenberg's argument Heisenberg supposes that an electron is like a classical particle, moving in the direction along a line below the microscope. Let the cone of light rays leaving the microscope lens and focusing on the electron make an angle with the electron. Let be the wavelength of the light rays. Then, according to the laws of classical optics, the microscope can only resolve the position of the electron up to an accuracy of",
                    "score": 0.8807002305984497
                },
                {
                    "id": 1696331,
                    "contents": "Electronvolt\nIt is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics, and high-energy astrophysics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion (109) electronvolts; it is equivalent to the GeV. Definition An electronvolt is the amount of kinetic energy gained or lost by a single electron accelerating from rest through an electric potential difference of one volt in vacuum. Hence, it has a value of one volt, , multiplied by the electron's elementary charge e, Therefore, one electronvolt is equal to The electronvolt, as opposed to the volt, is not an SI unit. The electronvolt (eV) is a unit of energy whereas the volt (V) is the derived SI unit of electric potential. The SI unit for energy is the joule (J).",
                    "score": 0.8806199431419373
                },
                {
                    "id": 11719093,
                    "contents": "History of quantum mechanics\nThe history of quantum mechanics is a fundamental part of the history of modern physics. Quantum mechanics' history, as it interlaces with the history of quantum chemistry, began essentially with a number of different scientific discoveries: the 1838 discovery of cathode rays by Michael Faraday; the 1859–60 winter statement of the black-body radiation problem by Gustav Kirchhoff; the 1877 suggestion by Ludwig Boltzmann that the energy states of a physical system could be discrete; the discovery of the photoelectric effect by Heinrich Hertz in 1887; and the 1900 quantum hypothesis by Max Planck that any energy-radiating atomic system can theoretically be divided into a number of discrete \"energy elements\" ε (Greek letter epsilon) such that each of these energy elements is proportional to the frequency ν with which each of them individually radiate energy, as defined by the following formula: where h is a numerical value called Planck's constant.",
                    "score": 0.8804633617401123
                },
                {
                    "id": 17803751,
                    "contents": "Runge–Gross theorem\nAt any given time, the N-electron wavefunction, which depends upon 3N spatial and N spin coordinates, determines the electronic density through integration as Two external potentials differing only by an additive time-dependent, spatially independent, function, c(t), give rise to wavefunctions differing only by a phase factor exp(-i α(t)), with dα(t)/dt = c(t), and therefore the same electronic density. These constructions provide a mapping from an external potential to the electronic density: The Runge–Gross theorem shows that this mapping is invertible, modulo c(t). Equivalently, that the density is a functional of the external potential and of the initial wavefunction on the space of potentials differing by more than the addition of c(t): Proof",
                    "score": 0.880370557308197
                },
                {
                    "id": 3271384,
                    "contents": "Quantum optics\nSeveral Nobel prizes have been awarded for work in quantum optics. These were awarded: in 2012, Serge Haroche and David J. Wineland \"for ground-breaking experimental methods that enable measuring & manipulation of individual quantum systems\". in 2005, Theodor W. Hänsch, Roy J. Glauber and John L. Hall in 2001, Wolfgang Ketterle, Eric Allin Cornell and Carl Wieman in 1997, Steven Chu, Claude Cohen-Tannoudji and William Daniel Phillips Concepts According to quantum theory, light may be considered not only to be as an electro-magnetic wave but also as a \"stream\" of particles called photons which travel with c, the vacuum speed of light. These particles should not be considered to be classical billiard balls, but as quantum mechanical particles described by a wavefunction spread over a finite region.",
                    "score": 0.8801541924476624
                },
                {
                    "id": 636486,
                    "contents": "Elementary charge\nThis method is not how the most accurate values are measured today. Nevertheless, it is a legitimate and still quite accurate method, and experimental methodologies are described below. The value of the Avogadro constant NA was first approximated by Johann Josef Loschmidt who, in 1865, estimated the average diameter of the molecules in air by a method that is equivalent to calculating the number of particles in a given volume of gas. Today the value of NA can be measured at very high accuracy by taking an extremely pure crystal (often silicon), measuring how far apart the atoms are spaced using X-ray diffraction or another method, and accurately measuring the density of the crystal. From this information, one can deduce the mass (m) of a single atom; and since the molar mass (M) is known, the number of atoms in a mole can be calculated: NA = M/m.",
                    "score": 0.8801324963569641
                },
                {
                    "id": 3437460,
                    "contents": "Principal quantum number\nIn quantum mechanics, the principal quantum number (symbolized n) is one of four quantum numbers assigned to each electron in an atom to describe that electron's state. Its values are natural numbers (from 1) making it a discrete variable. Apart from the principal quantum number, the other quantum numbers for bound electrons are the azimuthal quantum number ℓ, the magnetic quantum number ml, and the spin quantum number s. Overview and history As n increases, the electron is also at a higher energy and is, therefore, less tightly bound to the nucleus. For higher n the electron is farther from the nucleus, on average. For each value of n there are n accepted ℓ (azimuthal) values ranging from 0 to n − 1 inclusively, hence higher-n electron states are more numerous. Accounting for two states of spin, each n-shell can accommodate up to 2n2 electrons.",
                    "score": 0.8799954652786255
                },
                {
                    "id": 20846456,
                    "contents": "The Principles of Quantum Mechanics\nIn 1947 the third edition of the book was published, in which the chapter on quantum electrodynamics was rewritten particularly with the inclusion of electron-positron creation. In the fourth edition, 1958, the same chapter was revised, adding new sections on interpretation and applications. Later a revised fourth edition appeared in 1967. Beginning with the third edition (1947), the mathematical descriptions of quantum states and operators were changed to use the Bra–ket notation, introduced in 1939 and largely developed by Dirac himself. Laurie Brown wrote an article describing the book's evolution through its different editions, and Helge Kragh surveyed reviews by physicists (including Heisenberg, Pauli, and others) from the time of Dirac's book's publication.",
                    "score": 0.8797345161437988
                },
                {
                    "id": 11719110,
                    "contents": "History of quantum mechanics\nFounding experiments Thomas Young's double-slit experiment demonstrating the wave nature of light. (c. 1801) Henri Becquerel discovers radioactivity. (1896) J. J. Thomson's cathode ray tube experiments (discovers the electron and its negative charge). (1897) The study of black-body radiation between 1850 and 1900, which could not be explained without quantum concepts. The photoelectric effect: Einstein explained this in 1905 (and later received a Nobel prize for it) using the concept of photons, particles of light with quantized energy. Robert Millikan's oil-drop experiment, which showed that electric charge occurs as quanta (whole units). (1909) Ernest Rutherford's gold foil experiment disproved the plum pudding model of the atom which suggested that the mass and positive charge of the atom are almost uniformly distributed. This led to the planetary model of the atom (1911).",
                    "score": 0.8797146081924438
                },
                {
                    "id": 7748039,
                    "contents": "Introduction to quantum mechanics\nThe fourth quantum number, the spin quantum number (pertaining to the \"orientation\" of the electron's spin) is denoted , with values + or −. The chemist Linus Pauling wrote, by way of example: It is the underlying structure and symmetry of atomic orbitals, and the way that electrons fill them, that leads to the organization of the periodic table. The way the atomic orbitals on different atoms combine to form molecular orbitals determines the structure and strength of chemical bonds between atoms. Dirac wave equation",
                    "score": 0.8796861171722412
                },
                {
                    "id": 1113710,
                    "contents": "Energy level\nAn electron farther from the nucleus has higher potential energy than an electron closer to the nucleus, thus it becomes less bound to the nucleus, since its potential energy is negative and inversely dependent on its distance from the nucleus.",
                    "score": 0.8796401619911194
                },
                {
                    "id": 7747995,
                    "contents": "Introduction to quantum mechanics\nEinstein explained the effect by postulating that a beam of light is a stream of particles (\"photons\") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron. To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the work function and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:",
                    "score": 0.8790496587753296
                },
                {
                    "id": 6598018,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path of electrons can roughly be described by a universal curve that is the same for all materials. See also Scattering theory Beer-Lambert law References Atomic, molecular, and optical physics",
                    "score": 0.8787217736244202
                },
                {
                    "id": 1694102,
                    "contents": "Electron\nStoney initially coined the term electrolion in 1881. Ten years later, he switched to electron to describe these elementary charges, writing in 1894: \"... an estimate was made of the actual amount of this most remarkable fundamental unit of electricity, for which I have since ventured to suggest the name electron\". A 1906 proposal to change to electrion failed because Hendrik Lorentz preferred to keep electron. The word electron is a combination of the words electric and ion. The suffix -on which is now used to designate other subatomic particles, such as a proton or neutron, is in turn derived from electron. Discovery of free electrons outside matter",
                    "score": 0.8786667585372925
                },
                {
                    "id": 7747993,
                    "contents": "Introduction to quantum mechanics\nIn 1887, Heinrich Hertz observed that when light with sufficient frequency hits a metallic surface, the surface emits electrons. In 1902, Philipp Lenard discovered that the maximum possible energy of an ejected electron is related to the frequency of the light, not to its intensity: if the frequency is too low, no electrons are ejected regardless of the intensity. Strong beams of light toward the red end of the spectrum might produce no electrical potential at all, while weak beams of light toward the violet end of the spectrum would produce higher and higher voltages. The lowest frequency of light that can cause electrons to be emitted, called the threshold frequency, is different for different metals. This observation is at odds with classical electromagnetism, which predicts that the electron's energy should be proportional to the intensity of the incident radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a",
                    "score": 0.8785136342048645
                },
                {
                    "id": 27118359,
                    "contents": "Direct methods (electron microscopy)\nFor randomly distributed atoms, the following holds true: Meaning that if: Then: In the above equation, and the moduli are known on the right hand side. The only unknown terms are contained in the cosine term that includes the phases. The central limit theorem can be applied here, which establishes that distributions tend to be Gaussian in form. By combining the terms of the known moduli, a distribution function can be written that is dependent on the phases: This distribution is known as the Cochran distribution. The standard deviation for this Gaussian function scales with the reciprocal of the unitary structure factors. If they are large, then the sum in the cosine term must be: This is called the triplet phase relationship (). If the phases and are known, then the phase can be estimated. Tangent Formula",
                    "score": 0.8775889873504639
                },
                {
                    "id": 1590155,
                    "contents": "Atomic theory\nin classical electromagnetism), but rather gave the probability that an electron would, when measured, be found at a particular point. This reconciled the ideas of wave-like and particle-like electrons: the behavior of an electron, or of any other subatomic entity, has both wave-like and particle-like aspects, and whether one aspect or the other is more apparent depends upon the situation.",
                    "score": 0.8773306608200073
                },
                {
                    "id": 5097557,
                    "contents": "Asher Peres\nQuantum Theory textbook He authored a textbook, Quantum Theory: Concepts and Methods, of which he wrote, The purpose of this book is to clarify the conceptual meaning of quantum theory, and to explain some of the mathematical methods that it utilizes. This text is not concerned with specialized topics such as atomic structure, or strong or weak interactions, but with the very foundations of the theory. This is not, however, a book on the philosophy of science. The approach is pragmatic and strictly instrumentalist. This attitude will undoubtedly antagonize some readers, but it has its own logic: quantum phenomena do not occur in a Hilbert space, they occur in a laboratory.",
                    "score": 0.8768923282623291
                },
                {
                    "id": 14311953,
                    "contents": "Electron localization function\nIn quantum chemistry, the electron localization function (ELF) is a measure of the likelihood of finding an electron in the neighborhood space of a reference electron located at a given point and with the same spin. Physically, this measures the extent of spatial localization of the reference electron and provides a method for the mapping of electron pair probability in multielectronic systems.",
                    "score": 0.8764030933380127
                },
                {
                    "id": 7748021,
                    "contents": "Introduction to quantum mechanics\nIn the same year, building on de Broglie's hypothesis, Erwin Schrödinger developed the equation that describes the behavior of a quantum-mechanical wave. The mathematical model, called the Schrödinger equation after its creator, is central to quantum mechanics, defines the permitted stationary states of a quantum system, and describes how the quantum state of a physical system changes in time. The wave itself is described by a mathematical function known as a \"wave function\". Schrödinger said that the wave function provides the \"means for predicting the probability of measurement results\". Schrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's electron as a classical wave, moving in a well of the electrical potential created by the proton. This calculation accurately reproduced the energy levels of the Bohr model.",
                    "score": 0.8760368824005127
                },
                {
                    "id": 15462837,
                    "contents": "Angstrom\nThe angstrom (, ; , ) or ångström is a metric unit of length equal to m; that is, one ten-billionth (US) of a metre, a hundred-millionth of a centimetre, 0.1 nanometre, or 100 picometres. Its symbol is Å, a letter of the Swedish alphabet. The unit is named after the Swedish physicist Anders Jonas Ångström (1814–1874). The angstrom is often used in the natural sciences and technology to express sizes of atoms, molecules, microscopic biological structures, and lengths of chemical bonds, arrangement of atoms in crystals, wavelengths of electromagnetic radiation, and dimensions of integrated circuit parts. The atomic (covalent) radii of phosphorus, sulfur, and chlorine are about 1 angstrom, while that of hydrogen is about 0.5 angstroms. Visible light has wavelengths in the range of 4000–7000 Å.",
                    "score": 0.8759417533874512
                },
                {
                    "id": 1194739,
                    "contents": "Quantum mechanics\nOverview and fundamental concepts Quantum mechanics allows the calculation of properties and behaviour of physical systems. It is typically applied to microscopic systems: molecules, atoms and sub-atomic particles. It has been demonstrated to hold for complex molecules with thousands of atoms, but its application to human beings raises philosophical problems, such as Wigner's friend, and its application to the universe as a whole remains speculative. Predictions of quantum mechanics have been verified experimentally to an extremely high degree of accuracy.",
                    "score": 0.8758439421653748
                },
                {
                    "id": 5879541,
                    "contents": "Crookes tube\nbut a new particle, the first subatomic particle to be discovered, which was later named the electron. It was quickly realized that these particles were also responsible for electric currents in wires, and carried the negative charge in the atom.",
                    "score": 0.8756964206695557
                },
                {
                    "id": 16992313,
                    "contents": "Electron mass\nThe electron relative atomic mass is an adjusted parameter in the CODATA set of fundamental physical constants, while the electron rest mass in kilograms is calculated from the values of the Planck constant, the fine-structure constant and the Rydberg constant, as detailed above. Relationship to other physical constants The electron mass is used to calculate the Avogadro constant NA: Hence it is also related to the atomic mass constant mu: where Mu is the molar mass constant (defined in SI) and Ar(e) is a directly measured quantity, the relative atomic mass of the electron. Note that mu is defined in terms of Ar(e), and not the other way round, and so the name \"electron mass in atomic mass units\" for Ar(e) involves a circular definition (at least in terms of practical measurements).",
                    "score": 0.8756201863288879
                },
                {
                    "id": 8739813,
                    "contents": "Robert Karplus\nof the electron. This was an extremely difficult calculation, requiring more than a year of intense effort from both men; the agreement between their result and the experimental measurements was the first, dramatic confirmation of QED.",
                    "score": 0.8754652142524719
                },
                {
                    "id": 20846455,
                    "contents": "The Principles of Quantum Mechanics\nThe Principles of Quantum Mechanics is an influential monograph on quantum mechanics written by Paul Dirac and first published by Oxford University Press in 1930. Dirac gives an account of quantum mechanics by \"demonstrating how to construct a completely new theoretical framework from scratch\"; \"problems were tackled top-down, by working on the great principles, with the details left to look after themselves\". It leaves classical physics behind after the first chapter, presenting the subject with a logical structure. Its 82 sections contain 785 equations with no diagrams. Dirac is credited with developing the subject \"particularly in Cambridge and Göttingen between 1925–1927\" (Farmelo). History The first and second editions of the book were published in 1930 and 1935. In 1947 the third edition of the book was published, in which the chapter on quantum electrodynamics was rewritten particularly with the inclusion of electron-positron creation.",
                    "score": 0.8753355741500854
                },
                {
                    "id": 18101454,
                    "contents": "Basil Hiley\nDirac: \"Each electron only interferes with itself\" and adds: \"Somehow the ‘quantum force’ is a ‘private’ force. It thus cannot be regarded as a distortion of some underlying sub-quantum medium as was originally suggested by de Broglie\". It is independent of field intensity, thus fulfilling a precondition for non-locality, and it carries information about the whole experimental arrangement in which the particle finds itself.",
                    "score": 0.8752644062042236
                },
                {
                    "id": 27118358,
                    "contents": "Direct methods (electron microscopy)\nThis can be converted to the unitary structure factor by dividing by N (the number of atoms) and : This can be alternatively rewritten in real and reciprocal space as: This equation is a variation of the Sayre equation. Based on this equation, if the phases of and are known, then the phase of is known. Triplet Phase Relationship The triplet phase relationship is an equation directly relating two known phases of diffracted beams to the unknown phase of another. This relationship can be easily derived via the Sayre equation, but it may also be demonstrated through statistical relationships between the diffracted beams, as shown here. For randomly distributed atoms, the following holds true: Meaning that if: Then:",
                    "score": 0.8752601146697998
                },
                {
                    "id": 7747992,
                    "contents": "Introduction to quantum mechanics\nThe photoelectric effect",
                    "score": 0.8745302557945251
                },
                {
                    "id": 10045498,
                    "contents": "Kaufmann–Bucherer–Neumann experiments\nKaufmann also made a calculation mistake in deriving the deflection curves. Those errors were corrected by him in 1902. In 1902 and 1903 Kaufmann performed another series of tests with updated and improved experimental techniques. The results were interpreted by him as a confirmation of Abraham's theory and of the assumption that the electron's mass is completely of electromagnetic origin. Hermann Starke conducted similar measurements in 1903, although he used cathode rays limited to 0.3c. The results that he obtained were interpreted by him as being in agreement with those of Kaufmann. Competing theories In 1902, Max Abraham published a theory based on the assumption that the electron was a rigid, perfect sphere, with its charge being distributed evenly on its surface. As explained above, he introduced the so-called \"transverse electromagnetic mass\" besides the \"longitudinal electromagnetic mass\", and argued that the entire electron mass is of electromagnetic origin.",
                    "score": 0.8744708299636841
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_31",
        "question": "The mean temperature of the earth's surface is $288 \\mathrm{~K}$. What is the maximum wavelength of the earth's blackbody radiation?",
        "golden_answers": [
            " 1.01"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1568589,
                    "contents": "Absolute zero\nVery low temperatures The average temperature of the universe today is approximately , or about -270.42 degrees celsius, based on measurements of cosmic microwave background radiation.",
                    "score": 0.9023920297622681
                },
                {
                    "id": 1367644,
                    "contents": "Svante Arrhenius\n\"To a certain extent the temperature of the earth's surface, as we shall presently see, is conditioned by the properties of the atmosphere surrounding it, and particularly by the permeability of the latter for the rays of heat.\" (p. 46) \"That the atmospheric envelopes limit the heat losses from the planets had been suggested about 1800 by the great French physicist Fourier. His ideas were further developed afterwards by Pouillet and Tyndall. Their theory has been styled the hot-house theory, because they thought that the atmosphere acted after the manner of the glass panes of hot-houses.\" (p. 51)",
                    "score": 0.8996842503547668
                },
                {
                    "id": 1041492,
                    "contents": "Stefan–Boltzmann law\nThe above temperature is Earth's as seen from space, not ground temperature but an average over all emitting bodies of Earth from surface to high altitude. Because of the greenhouse effect, the Earth's actual average surface temperature is about 288 K (15 °C), which is higher than the 255 K effective temperature, and even higher than the 279 K temperature that a black body would have. In the above discussion, we have assumed that the whole surface of the earth is at one temperature. Another interesting question is to ask what the temperature of a blackbody surface on the earth would be assuming that it reaches equilibrium with the sunlight falling on it. This of course depends on the angle of the sun on the surface and on how much air the sunlight has gone through. When the sun is at the zenith and the surface is horizontal, the irradiance can be as high as 1120 W/m2. The Stefan–Boltzmann law then gives a temperature of",
                    "score": 0.896233081817627
                },
                {
                    "id": 1689226,
                    "contents": "Earth\nEarth receives 1361 W/m2 of solar irradiance. The amount of solar energy that reaches the Earth's surface decreases with increasing latitude. At higher latitudes, the sunlight reaches the surface at lower angles, and it must pass through thicker columns of the atmosphere. As a result, the mean annual air temperature at sea level decreases by about per degree of latitude from the equator. Earth's surface can be subdivided into specific latitudinal belts of approximately homogeneous climate. Ranging from the equator to the polar regions, these are the tropical (or equatorial), subtropical, temperate and polar climates.",
                    "score": 0.8883504271507263
                },
                {
                    "id": 3638564,
                    "contents": "Earth's energy budget\nEarth's energy budget accounts for the balance between the energy that Earth receives from the Sun and the energy the Earth loses back into outer space. Smaller energy sources, such as Earth's internal heat, are taken into consideration, but make a tiny contribution compared to solar energy. The energy budget also accounts for how energy moves through the climate system. Because the Sun heats the equatorial tropics more than the polar regions, received solar irradiance is unevenly distributed. As the energy seeks equilibrium across the planet, it drives interactions in Earth's climate system, i.e., Earth's water, ice, atmosphere, rocky crust, and all living things. The result is Earth's climate.",
                    "score": 0.8878786563873291
                },
                {
                    "id": 15576707,
                    "contents": "Temperature\nThe United States commonly uses the Fahrenheit scale, on which water freezes at and boils at at sea-level atmospheric pressure. Absolute zero At the absolute zero of temperature, no energy can be removed from matter as heat, a fact expressed in the third law of thermodynamics. At this temperature, matter contains no macroscopic thermal energy, but still has quantum-mechanical zero-point energy as predicted by the uncertainty principle, although this does not enter into the definition of absolute temperature. Experimentally, absolute zero can be approached only very closely; it can never be reached (least temperature attained by experiment is 100 pK). Theoretically, in a body at absolute zero temperature, all classical motion of its particles has ceased and they are at complete rest in this classical sense. The absolute zero, defined as , is exactly equal to , or .",
                    "score": 0.8876371383666992
                },
                {
                    "id": 2225046,
                    "contents": "Solar constant\nThe solar constant (GSC) is a flux density measuring mean solar electromagnetic radiation (total solar irradiance) per unit area. It is measured on a surface perpendicular to the rays, one astronomical unit (au) from the Sun (roughly the distance from the Sun to the Earth). The solar constant includes radiation over the entire electromagnetic spectrum. It is measured by satellite as being 1.361 kilowatts per square meter (kW/m2) at solar minimum (the time in the 11-year solar cycle when the number of sunspots is minimal) and approximately 0.1% greater (roughly 1.362 kW/m2) at solar maximum. The solar \"constant\" is not a physical constant in the modern CODATA scientific sense; that is, it is not like the Planck constant or the speed of light which are absolutely constant in physics. The solar constant is an average of a varying value. In the past 400 years it has varied less than 0.2 percent. Billions of years ago, it was significantly lower.",
                    "score": 0.8875351548194885
                },
                {
                    "id": 149230,
                    "contents": "Timeline of meteorology\n1804 – Sir John Leslie observes that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black-body radiation. 1806 – Francis Beaufort introduces his system for classifying wind speeds. 1808 – John Dalton defends caloric theory in A New System of Chemistry and describes how it combines with matter, especially gases; he proposes that the heat capacity of gases varies inversely with atomic weight. 1810 – Sir John Leslie freezes water to ice artificially. 1817 – Alexander von Humboldt publishes a global map of average temperature, the first global climate analysis. 1819 – Pierre Louis Dulong and Alexis Thérèse Petit give the Dulong-Petit law for the specific heat capacity of a crystal. 1820 – Heinrich Wilhelm Brandes publishes the first synoptic weather maps.",
                    "score": 0.8873246908187866
                },
                {
                    "id": 22812818,
                    "contents": "Thermal history of Earth\nSee also Earth's inner core Earth's magnetic field Earth's structure Geologic temperature record List of periods and events in climate history Paleothermometer Radiative forcing Timeline of glaciation References Further readings Geophysics Heat transfer",
                    "score": 0.8853620290756226
                },
                {
                    "id": 4992608,
                    "contents": "Surface temperature\nSurface temperature is the temperature at or near a surface. Specifically, it may refer to: Surface air temperature, the temperature of the air near the surface of the earth Sea surface temperature, the temperature of water close to the ocean's surface Global surface temperature, the combined global average of Surface air temperature and Sea surface temperature Surface temperature of a star, often the effective temperature See also Instrumental temperature record, the historical record of in situ measurements of surface air and sea temperatures Pyrometer, a device that remotely determines the temperature of a surface Infrared thermometer, a type of pyrometer",
                    "score": 0.8851982951164246
                },
                {
                    "id": 2406609,
                    "contents": "Milutin Milanković\nHe began working on it in 1912, after he had realized that \"most of meteorology is nothing but a collection of innumerable empirical findings, mainly numerical data, with traces of physics used to explain some of them... Mathematics was even less applied, nothing more than elementary calculus... Advanced mathematics had no role in that science...\" His first work described the present climate on Earth and how the Sun's rays determine the temperature on Earth's surface after passing through the atmosphere. He published the first paper on the subject entitled \"Contribution to the mathematical theory of climate\" in Belgrade on 5 April 1912. His next paper was entitled \"Distribution of the sun radiation on the earth's surface\" and was published on 5 June 1913. He correctly calculates the intensity of insolation and developed a mathematical theory describing Earth's climate zones. His aim was an integral, mathematically accurate theory which connects thermal regimes of the planets to their",
                    "score": 0.8851552605628967
                },
                {
                    "id": 15579619,
                    "contents": "Earth science\nThe troposphere, stratosphere, mesosphere, thermosphere, and exosphere are the five layers which make up Earth's atmosphere. 75% of the gases in the atmosphere are located within the troposphere, the lowest layer. In all, the atmosphere is made up of about 78.0% nitrogen, 20.9% oxygen, and 0.92% argon, and small amounts of other gases including CO2 and water vapor. Water vapor and CO2 allow the Earth's atmosphere to catch and hold the Sun's energy through the greenhouse effect. This allows Earth's surface to be warm enough to have liquid water and support life. In addition to storing heat, the atmosphere also protects living organisms by shielding the Earth's surface from cosmic rays—which are often incorrectly thought to be deflected by the magnetic field. The magnetic field—created by the internal motions of the core—produces the magnetosphere which protects Earth's atmosphere from the solar wind. As the Earth is 4.5 billion years old, it would have lost its atmosphere by now if",
                    "score": 0.883476972579956
                },
                {
                    "id": 1979244,
                    "contents": "John Tyndall\nTyndall explained the heat in the Earth's atmosphere in terms of the capacities of the various gases in the air to absorb radiant heat, in the form of infrared radiation. His measuring device, which used thermopile technology, is an early landmark in the history of absorption spectroscopy of gases. He was the first to correctly measure the relative infrared absorptive powers of the gases nitrogen, oxygen, water vapour, carbon dioxide, ozone, methane, and other trace gases and vapours. He concluded that water vapour is the strongest absorber of radiant heat in the atmosphere and is the principal gas controlling air temperature. Absorption by the other gases is not negligible but relatively small. Prior to Tyndall it was widely surmised that the Earth's atmosphere warms the surface in what was later called a greenhouse effect, but he was the first to prove it. The proof was that water vapour strongly absorbed infrared radiation. Three years earlier, in 1856, the American scientist",
                    "score": 0.8823031187057495
                },
                {
                    "id": 2225050,
                    "contents": "Solar constant\nThe actual direct solar irradiance at the top of the atmosphere fluctuates by about 6.9% during a year (from 1.412 kW/m2 in early January to 1.321 kW/m2 in early July) due to the Earth's varying distance from the Sun, and typically by much less than 0.1% from day to day. Thus, for the whole Earth (which has a cross section of 127,400,000 km2), the power is 1.730×1017 W (or 173,000 terawatts), plus or minus 3.5% (half the approximately 6.9% annual range). The solar constant does not remain constant over long periods of time (see Solar variation), but over a year the solar constant varies much less than the solar irradiance measured at the top of the atmosphere. This is because the solar constant is evaluated at a fixed distance of 1 Astronomical Unit (au) while the solar irradiance will be affected by the eccentricity of the Earth's orbit. Its distance to the Sun varies annually between 147.1·106 km at perihelion and 152.1·106 km at aphelion. In addition, several long term (tens to",
                    "score": 0.8818800449371338
                },
                {
                    "id": 3638570,
                    "contents": "Earth's energy budget\nDespite multiple other influences, the Stefan-Boltzmann law of radiation describes the fundamental dependence of OLR upon Earth's surface skin temperature (Tskin): Tskin has been globally measured from satellite observations of OLR in the infrared and microwave bands, and is approximated by in-situ surface temperatures. The strong (fourth-power) temperature sensitivity acts to maintain a near-balance of the outgoing energy flow to the incoming flow via small changes in absolute temperature. Earth's internal heat sources and other small effects The geothermal heat flow from the Earth's interior is estimated to be 47 terawatts (TW) and split approximately equally between radiogenic heat and heat left over from the Earth's formation. This corresponds to an average flux of 0.087 W/m2 and represents only 0.027% of Earth's total energy budget at the surface, being dwarfed by the 173,000 TW of incoming solar radiation.",
                    "score": 0.8812520503997803
                },
                {
                    "id": 453471,
                    "contents": "Atmosphere\nThe atmosphere of Earth is composed of layers with different properties, such as specific gaseous composition, temperature, and pressure. The lowest layer of the atmosphere is the troposphere, which extends from the planetary surface to the bottom of the stratosphere. The troposphere contains 75 per cent of the mass of the atmosphere, and is the atmospheric layer wherein the weather occurs; the height of the troposphere varies between 17 km at the equator and 7.0 km at the poles. The stratosphere extends from the top of the troposphere to the bottom of the mesosphere, and contains the ozone layer, at an altitude between 15 km and 35 km. It is the atmospheric layer that absorbs most of the ultraviolet radiation that Earth receives from the Sun. The mesosphere ranges from 50 km to 85 km, and is the layer wherein most meteors are incinerated before reaching the surface. The thermosphere extends from an altitude of 85 km to the base of the exosphere at 690 km and contains the ionosphere,",
                    "score": 0.8799059391021729
                },
                {
                    "id": 3953021,
                    "contents": "Sydney Chapman (mathematician)\nSydney Chapman (29 January 1888 – 16 June 1970) was a British mathematician and geophysicist. His work on the kinetic theory of gases, solar-terrestrial physics, and the Earth's ozone layer has inspired a broad range of research over many decades.",
                    "score": 0.8799008131027222
                },
                {
                    "id": 3638566,
                    "contents": "Earth's energy budget\nEarth's energy flows In spite of the enormous transfers of energy into and from the Earth, it maintains a relatively constant temperature because, as a whole, there is little net gain or loss: Earth emits via atmospheric and terrestrial radiation (shifted to longer electromagnetic wavelengths) to space about the same amount of energy as it receives via solar insolation (all forms of electromagnetic radiation). Incoming solar energy (shortwave radiation)",
                    "score": 0.8797025680541992
                },
                {
                    "id": 905763,
                    "contents": "Troposphere\nTemperature The planetary surface of the Earth heats the troposphere by means of latent heat, thermal radiation, and sensible heat.",
                    "score": 0.8793882131576538
                },
                {
                    "id": 453445,
                    "contents": "Atmosphere of Earth\nThe average temperature of the atmosphere at Earth's surface is or , depending on the reference. Physical properties Pressure and thickness The average atmospheric pressure at sea level is defined by the International Standard Atmosphere as . This is sometimes referred to as a unit of standard atmospheres (atm). Total atmospheric mass is 5.1480×1018 kg (1.135×1019 lb), about 2.5% less than would be inferred from the average sea level pressure and Earth's area of 51007.2 megahectares, this portion being displaced by Earth's mountainous terrain. Atmospheric pressure is the total weight of the air above unit area at the point where the pressure is measured. Thus air pressure varies with location and weather.",
                    "score": 0.878879725933075
                },
                {
                    "id": 24461338,
                    "contents": "Solar activity and climate\nThree to four billion years ago the Sun emitted only 70% of its current power. Under the present atmospheric composition, this past solar luminosity would have been insufficient to prevent water from uniformly freezing. There is nonetheless evidence that liquid water was already present in the Hadean and Archean eons, leading to what is known as the faint young Sun paradox. Hypothesized solutions to this paradox include a vastly different atmosphere, with much higher concentrations of greenhouse gases than currently exist.",
                    "score": 0.8788489103317261
                },
                {
                    "id": 453448,
                    "contents": "Atmosphere of Earth\nTemperature The division of the atmosphere into layers mostly by reference to temperature is discussed above. Temperature decreases with altitude starting at sea level, but variations in this trend begin above 11 km, where the temperature stabilizes over a large vertical distance through the rest of the troposphere. In the stratosphere, starting above about 20 km, the temperature increases with height, due to heating within the ozone layer caused by the capture of significant ultraviolet radiation from the Sun by the dioxygen and ozone gas in this region. Still another region of increasing temperature with altitude occurs at very high altitudes, in the aptly-named thermosphere above 90 km. Speed of sound",
                    "score": 0.8786592483520508
                },
                {
                    "id": 905523,
                    "contents": "Thermodynamic temperature\n1742: Anders Celsius (1701–1744) created a “backwards” version of the modern Celsius temperature scale. In Celsius's original scale, zero represented the boiling point of water and 100 represented the melting point of ice. In his paper Observations of two persistent degrees on a thermometer, he recounted his experiments showing that ice's melting point was effectively unaffected by pressure. He also determined with remarkable precision how water's boiling point varied as a function of atmospheric pressure. He proposed that zero on his temperature scale (water's boiling point) would be calibrated at the mean barometric pressure at mean sea level.",
                    "score": 0.8785459399223328
                },
                {
                    "id": 5179312,
                    "contents": "Effective temperature\nAlso note here that this equation does not take into account any effects from internal heating of the planet, which can arise directly from sources such as radioactive decay and also be produced from frictions resulting from tidal forces. Earth effective temperature The Earth has an albedo of about 0.306. The emissivity is dependent on the type of surface and many climate models set the value of the Earth's emissivity to 1. However, a more realistic value is 0.96. The Earth is a fairly fast rotator so the area ratio can be estimated as . The other variables are constant. This calculation gives us an effective temperature of the Earth of . The average temperature of the Earth is . One reason for the difference between the two values is due to the greenhouse effect, which increases the average temperature of the Earth's surface. See also Brightness temperature Color temperature List of hottest stars References",
                    "score": 0.8781863451004028
                },
                {
                    "id": 1041490,
                    "contents": "Stefan–Boltzmann law\nEffective temperature of the Earth Similarly we can calculate the effective temperature of the Earth T⊕ by equating the energy received from the Sun and the energy radiated by the Earth, under the black-body approximation (Earth's own production of energy being small enough to be negligible). The luminosity of the Sun, L⊙, is given by: At Earth, this energy is passing through a sphere with a radius of a0, the distance between the Earth and the Sun, and the irradiance (received power per unit area) is given by The Earth has a radius of R⊕, and therefore has a cross-section of . The radiant flux (i.e. solar power) absorbed by the Earth is thus given by: Because the Stefan–Boltzmann law uses a fourth power, it has a stabilizing effect on the exchange and the flux emitted by Earth tends to be equal to the flux absorbed, close to the steady state where: T⊕ can then be found:",
                    "score": 0.8778868317604065
                },
                {
                    "id": 27795135,
                    "contents": "Opus 300\nPart 2. Earth Sciences \"Below Earth's Atmosphere\" (chapter 6 of Exploring Earth and the Cosmos (1982)) \"Explosion at Thera\" (excerpt from How Did We Find Out About Volcanoes? (1981)) \"The Floating Crystal Palace\" (complete science essay from The Road to Infinity (1979)) Part 3. Mathematics \"To Ungild Refined Gold\" (complete mathematics essay from X Stands for Unknown (1984)) Excerpt from The Measure of the Universe (1983) \"1 to 999\" (complete mystery story from The Union Club Mysteries (1983)) Part 4. Physics \"Let Einstein Be!\" (complete science essay from Counting the Eons (1983)) \"Converting It All\" (complete science essay from Change! (1981)) Part 5. Chemistry \"Big Brother\" (complete science essay from X Stands for Unknown) \"Bread and Stone\" (complete science essay from X Stands for Unknown)",
                    "score": 0.8774877786636353
                },
                {
                    "id": 24996835,
                    "contents": "Earth's internal heat budget\nDespite its geological significance, Earth's interior heat contributes only 0.03% of Earth's total energy budget at the surface, which is dominated by 173,000 TW of incoming solar radiation. This external energy source powers most of the planet's atmospheric, oceanic, and biologic processes. Nevertheless on land and at the ocean floor, the sensible heat absorbed from non-reflected insolation flows inward only by means of thermal conduction, and thus penetrates only several tens of centimeters on the daily cycle and only several tens of meters on the annual cycle. This renders solar radiation minimally relevant for processes internal to Earth's crust. Global data on heat-flow density are collected and compiled by the International Heat Flow Commission of the International Association of Seismology and Physics of the Earth's Interior.",
                    "score": 0.877070426940918
                },
                {
                    "id": 1041493,
                    "contents": "Stefan–Boltzmann law\nor 102 °C. (Above the atmosphere, the result is even higher: 394 K.) We can think of the earth's surface as \"trying\" to reach equilibrium temperature during the day, but being cooled by the atmosphere, and \"trying\" to reach equilibrium with starlight and possibly moonlight at night, but being warmed by the atmosphere. Origination Thermodynamic derivation of the energy density The fact that the energy density of the box containing radiation is proportional to can be derived using thermodynamics. This derivation uses the relation between the radiation pressure p and the internal energy density , a relation that can be shown using the form of the electromagnetic stress–energy tensor. This relation is: Now, from the fundamental thermodynamic relation we obtain the following expression, after dividing by and fixing : The last equality comes from the following Maxwell relation: From the definition of energy density it follows that",
                    "score": 0.8757732510566711
                },
                {
                    "id": 15576752,
                    "contents": "Temperature\nUnits The basic unit of temperature in the International System of Units (SI) is the Kelvin. It has the symbol K. For everyday applications, it is often convenient to use the Celsius scale, in which corresponds very closely to the freezing point of water and is its boiling point at sea level. Because liquid droplets commonly exist in clouds at sub-zero temperatures, is better defined as the melting point of ice. In this scale, a temperature difference of 1 degree Celsius is the same as a increment, but the scale is offset by the temperature at which ice melts ().",
                    "score": 0.8757111430168152
                },
                {
                    "id": 19793276,
                    "contents": "Fritz Vahrenholt\n, criticised the book and considered its underlying assumptions to be either outdated or highly speculative. The later events showed that in spite of a quite low activity of the sun during the Solar cycle 24, as had been forecast in principle by Vahrenholt, the result of global cooling forecast by Vahrenholt did not occur; the earth rather heated up even more.",
                    "score": 0.8748674988746643
                },
                {
                    "id": 27882786,
                    "contents": "Franz Richarz\nWith Otto Krigar-Menzel, he conducted a series of experiments for determination of the gravitational constant and the Earth's mean density. Selected works Bestimmung der Gravitationsconstante und der mittleren Dichtigkeit der Erde durch Wägungen, 1898 – Determination of the gravitational constant and the mean density of the earth. Neuere fortschritte auf dem gebiete der elektrizität, 1899; Recent advances in the field of electricity. Ueber Temperaturänderungen in Künstlich auf- und Abbewegter Luft, 1902. Vorlesungen über Theorie der Wärme (as editor; 1903) – Hermann Helmholtz' lectures on the theory of heat. Zur Erinnerung an Paul Drude zwei Ansprachen (with Walter König, 1906); In memory of Paul Drude; two speeches. Anfangsgründe der Maxwellschen Theorie verknüpft mit der Elektronentheorie, 1909 – The rudiments of Maxwell's theory combined with the electron theory. References",
                    "score": 0.8748291730880737
                },
                {
                    "id": 3060725,
                    "contents": "Frank Washington Very\nVery's most important work was in measuring the temperatures of the surfaces of the Moon and other planets using a bolometer. Samuel Pierpont Langley published in 1890 a widely read paper that included Very's Moon observations, but for unknown reasons omitted his name from the list of contributors. In 1891, Very published his own paper, \"Distribution of the Moon's Heat,\" which also included measurements taken during a lunar eclipse. Infrared observations by Langley and Very, published in 1890, were used to make the first calculations of the greenhouse effect. Very crater on Mars and Very crater on the Moon are named in his honor. Published works References External links Discussion of Langley's 1890 publication 1852 births 1927 deaths American astronomers People from Salem, Massachusetts Massachusetts Institute of Technology alumni University of Pittsburgh faculty",
                    "score": 0.8746963143348694
                },
                {
                    "id": 1238338,
                    "contents": "Solar energy\nPotential The Earth receives 174 petawatts (PW) of incoming solar radiation (insolation) at the upper atmosphere. Approximately 30% is reflected back to space while the rest is absorbed by clouds, oceans and land masses. The spectrum of solar light at the Earth's surface is mostly spread across the visible and near-infrared ranges with a small part in the near-ultraviolet. Most of the world's population live in areas with insolation levels of 150–300 watts/m2, or 3.5–7.0 kWh/m2 per day.",
                    "score": 0.8744926452636719
                },
                {
                    "id": 453455,
                    "contents": "Atmosphere of Earth\nThe refractive index of air depends on temperature, giving rise to refraction effects when the temperature gradient is large. An example of such effects is the mirage. Circulation Atmospheric circulation is the large-scale movement of air through the troposphere, and the means (with ocean circulation) by which heat is distributed around Earth. The large-scale structure of the atmospheric circulation varies from year to year, but the basic structure remains fairly constant because it is determined by Earth's rotation rate and the difference in solar radiation between the equator and poles. Evolution of Earth's atmosphere Earliest atmosphere The first atmosphere consisted of gases in the solar nebula, primarily hydrogen. There were probably simple hydrides such as those now found in the gas giants (Jupiter and Saturn), notably water vapor, methane and ammonia.",
                    "score": 0.8743067979812622
                },
                {
                    "id": 905766,
                    "contents": "Troposphere\nThe difference in temperature derives from the planetary surface absorbing most of the energy from the sun, which then radiates outwards and heats the troposphere (the first layer of the atmosphere of Earth) while the radiation of surface heat to the upper atmosphere results in the cooling of that layer of the atmosphere. The ELR equation also assumes that the atmosphere is static, but heated air becomes buoyant, expands, and rises. The dry adiabatic lapse rate (DALR) accounts for the effect of the expansion of dry air as it rises in the atmosphere, and the wet adiabatic lapse rate (WALR) includes the effect of the condensation-rate of water vapor upon the environmental lapse rate.",
                    "score": 0.8742653131484985
                },
                {
                    "id": 15707664,
                    "contents": "Celsius\nHistory In 1742, Swedish astronomer Anders Celsius (1701–1744) created a temperature scale that was the reverse of the scale now known as \"Celsius\": 0 represented the boiling point of water, while 100 represented the freezing point of water. In his paper Observations of two persistent degrees on a thermometer, he recounted his experiments showing that the melting point of ice is essentially unaffected by pressure. He also determined with remarkable precision how the boiling point of water varied as a function of atmospheric pressure. He proposed that the zero point of his temperature scale, being the boiling point, would be calibrated at the mean barometric pressure at mean sea level. This pressure is known as one standard atmosphere. The BIPM's 10th General Conference on Weights and Measures (CGPM) in 1954 defined one standard atmosphere to equal precisely 1,013,250 dynes per square centimeter (101.325 kPa).",
                    "score": 0.8741908073425293
                },
                {
                    "id": 1367639,
                    "contents": "Svante Arrhenius\ntemperature variation between glacial and inter-glacial periods. Arrhenius used infrared observations of the moon – by Frank Washington Very and Samuel Pierpont Langley at the Allegheny Observatory in Pittsburgh – to calculate how much of infrared (heat) radiation is captured by CO2 and water (H2O) vapour in Earth's atmosphere. Using 'Stefan's law' (better known as the Stefan–Boltzmann law), he formulated what he referred to as a 'rule'.",
                    "score": 0.8732182383537292
                },
                {
                    "id": 1157970,
                    "contents": "Physics\nHistory The word \"physics\" comes from , meaning \"knowledge of nature\". Ancient astronomy Astronomy is one of the oldest natural sciences. Early civilizations dating back before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilisation, had a predictive knowledge and a basic awareness of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which however did not explain the positions of the planets.",
                    "score": 0.8720306158065796
                },
                {
                    "id": 24996837,
                    "contents": "Earth's internal heat budget\nGlobal internal heat flow Estimates of the total heat flow from Earth's interior to surface span a range of 43 to 49 terawatts (TW) (a terawatt is 1012 watts). One recent estimate is 47 TW, equivalent to an average heat flux of 91.6 mW/m2, and is based on more than 38,000 measurements. The respective mean heat flows of continental and oceanic crust are 70.9 and 105.4 mW/m2. While the total internal Earth heat flow to the surface is well constrained, the relative contribution of the two main sources of Earth's heat, radiogenic and primordial heat, are highly uncertain because their direct measurement is difficult. Chemical and physical models give estimated ranges of 15–41 TW and 12–30 TW for radiogenic heat and primordial heat, respectively.",
                    "score": 0.8713600039482117
                },
                {
                    "id": 6704746,
                    "contents": "History of thermodynamics\nCarl Wilhelm Scheele distinguished heat transfer by thermal radiation (radiant heat) from that by convection and conduction in 1777. In 1791, Pierre Prévost showed that all bodies radiate heat, no matter how hot or cold they are. In 1804, Leslie observed that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black-body radiation. Though it had become to be suspected even from Scheele's work, in 1831 Macedonio Melloni demonstrated that black-body radiation could be reflected, refracted and polarised in the same way as light. James Clerk Maxwell's 1862 insight that both light and radiant heat were forms of electromagnetic wave led to the start of the quantitative analysis of thermal radiation. In 1879, Jožef Stefan observed that the total radiant flux from a blackbody is proportional to the fourth power of its temperature and stated the Stefan–Boltzmann law. The law was derived theoretically by Ludwig Boltzmann in 1884.",
                    "score": 0.8712736368179321
                },
                {
                    "id": 1979280,
                    "contents": "John Tyndall\nJohn Tyndall's books Tyndall, J. (1860), The glaciers of the Alps, Being a narrative of excursions and ascents, an account of the origin and phenomena of glaciers and an exposition of the physical principles to which they are related, (1861 edition) Ticknor and Fields, Boston Tyndall, J. (1862), Mountaineering in 1861. A vacation tour, Longman, Green, Longman, and Roberts, London Tyndall, J. (1865), On Radiation: One Lecture (40 pages) Tyndall, J. (1868), Heat : A mode of motion, (1869 edition) D. Appleton, New York Tyndall, J. (1869), Natural Philosophy in Easy Lessons (180 pages) (a physics book intended for use in secondary schools) Tyndall, J. (1870), Faraday as a discoverer, Longmans, Green, London Tyndall, J. (1870), Three Scientific Addresses by Prof. John Tyndall (75 pages) Tyndall, J. (1870), Notes of a Course of Nine Lectures on Light (80 pages) Tyndall, J. (1870), Notes of a Course of Seven Lectures on Electrical Phenomena and Theories (50 pages)",
                    "score": 0.8711798191070557
                },
                {
                    "id": 14561976,
                    "contents": "Future of Earth\nBy 2.8 billion years from now, the surface temperature of the Earth will have reached , even at the poles. At this point, any remaining life will be extinguished due to extreme conditions. What happens beyond this depends on how much water is left on the surface. If all of the water on Earth has evaporated by this point already (via the \"moist greenhouse\" at ~1 Gyr from now), the planet will stay in the same conditions with a steady increase in the surface temperature until the Sun becomes a red giant. If not and there are still pockets of water left, and evaporates too slowly, then in about 3–4 billion years, once the amount of water vapor in the lower atmosphere rises to 40%, and the luminosity from the Sun reaches 35–40% more than its present-day value, a \"runaway greenhouse\" effect will ensue, causing the atmosphere to heat up and raising the surface temperature to around . This is sufficient to melt the surface of the planet. However, most of the atmosphere will be retained until",
                    "score": 0.8711568713188171
                },
                {
                    "id": 5179303,
                    "contents": "Effective temperature\nThe effective temperature of a body such as a star or planet is the temperature of a black body that would emit the same total amount of electromagnetic radiation. Effective temperature is often used as an estimate of a body's surface temperature when the body's emissivity curve (as a function of wavelength) is not known. When the star's or planet's net emissivity in the relevant wavelength band is less than unity (less than that of a black body), the actual temperature of the body will be higher than the effective temperature. The net emissivity may be low due to surface or atmospheric properties, including greenhouse effect. Star",
                    "score": 0.8709343671798706
                },
                {
                    "id": 24347001,
                    "contents": "Highest temperature recorded on Earth\nSee also Desert climate Heat wave Highest temperatures ever recorded Lowest temperature recorded on Earth Lowest temperatures ever recorded Orders of magnitude (temperature) References Temperature Weather extremes of Earth Climate and weather statistics",
                    "score": 0.8707760572433472
                },
                {
                    "id": 9420833,
                    "contents": "Equivalent temperature\nIn atmospheric science, equivalent temperature is the temperature of air in a parcel from which all the water vapor has been extracted by an adiabatic process. Air contains water vapor that has been evaporated into it from liquid sources (lakes, sea, etc...). The energy needed to do that has been taken from the air. Taking a volume of air at temperature T and mixing ratio of r , drying it by condensation will restore energy to the airmass. This will depend on the latent heat release as: where: : latent heat of evaporation (2400 kJ/kg {at 25C} to 2600 kJ/kg {at -40C}) : specific heat at constant pressure for air ( 1004 J/(kg·K)) Tables exist for exact values of the last two coefficients. See also Wet-bulb temperature Potential temperature Atmospheric thermodynamics Equivalent potential temperature",
                    "score": 0.8707468509674072
                },
                {
                    "id": 26229524,
                    "contents": "Schwarzschild's equation for radiative transfer\nSchwarzschild's equation contains the fundamental physics needed to understand and quantify how increasing greenhouse gases (GHGs) in the atmosphere reduce the flux of thermal infrared radiation to space. If no other fluxes change, the law of conservation of energy demands that the Earth warm (from one steady state to another) until balance is restored between inward and outward fluxes. Schwarzschild's equation alone says nothing about how much warming would be required to restore balance. When meteorologists and climate scientists refer to \"radiative transfer calculations\" or \"radiative transfer equations\" (RTE), the phenomena of emission and absorption are handled by numerical integration of Schwarzschild's equation over a path through the atmosphere. Weather forecasting models and climate models use versions of Schwarzschild's equation optimized to minimize computation time. Online programs permit anyone to perform computations using Schwarzschild's equation.",
                    "score": 0.8705174922943115
                },
                {
                    "id": 1868890,
                    "contents": "Meteorology\nIn 1648, Blaise Pascal rediscovered that atmospheric pressure decreases with height, and deduced that there is a vacuum above the atmosphere. In 1738, Daniel Bernoulli published Hydrodynamics, initiating the Kinetic theory of gases and established the basic laws for the theory of gases. In 1761, Joseph Black discovered that ice absorbs heat without changing its temperature when melting. In 1772, Black's student Daniel Rutherford discovered nitrogen, which he called phlogisticated air, and together they developed the phlogiston theory. In 1777, Antoine Lavoisier discovered oxygen and developed an explanation for combustion. In 1783, in Lavoisier's essay \"Reflexions sur le phlogistique,\" he deprecates the phlogiston theory and proposes a caloric theory. In 1804, Sir John Leslie observed that a matte black surface radiates heat more effectively than a polished surface, suggesting the importance of black-body radiation. In 1808, John Dalton defended caloric theory in A New System of",
                    "score": 0.8704508543014526
                },
                {
                    "id": 1327535,
                    "contents": "William Thomson, 1st Baron Kelvin\nWilliam Thomson, 1st Baron Kelvin, (26 June 182417 December 1907) was a British mathematician, mathematical physicist and engineer born in Belfast. Professor of Natural Philosophy at the University of Glasgow for 53 years, he did important work in the mathematical analysis of electricity and formulation of the first and second laws of thermodynamics, and did much to unify the emerging discipline of physics in its contemporary form. He received the Royal Society's Copley Medal in 1883, was its President 1890–1895, and in 1892 was the first British scientist to be elevated to the House of Lords. Absolute temperatures are stated in units of kelvin in his honour. While the existence of a lower limit to temperature (absolute zero) was known prior to his work, Kelvin is known for determining its correct value as approximately −273.15 degrees Celsius or −459.67 degrees Fahrenheit. The Joule–Thomson effect is also named in his honour.",
                    "score": 0.8704417943954468
                },
                {
                    "id": 453426,
                    "contents": "Atmosphere of Earth\nThe atmosphere has a mass of about 5.15 kg, three quarters of which is within about of the surface. The atmosphere becomes thinner with increasing altitude, with no definite boundary between the atmosphere and outer space. The Kármán line, at or 1.57% of Earth's radius, is often used as the border between the atmosphere and outer space. Atmospheric effects become noticeable during atmospheric reentry of spacecraft at an altitude of around . Several layers can be distinguished in the atmosphere, based on characteristics such as temperature and composition. The study of Earth's atmosphere and its processes is called atmospheric science (aerology), and includes multiple subfields, such as climatology and atmospheric physics. Early pioneers in the field include Léon Teisserenc de Bort and Richard Assmann. The study of historic atmosphere is called paleoclimatology. Composition",
                    "score": 0.8703872561454773
                },
                {
                    "id": 15322666,
                    "contents": "Greenhouse gas\nThe average residence time of a water molecule in the atmosphere is only about nine days, compared to years or centuries for other greenhouse gases such as and . Water vapor responds to and amplifies effects of the other greenhouse gases. The Clausius–Clapeyron relation establishes that more water vapor will be present per unit volume at elevated temperatures. This and other basic principles indicate that warming associated with increased concentrations of the other greenhouse gases also will increase the concentration of water vapor (assuming that the relative humidity remains approximately constant; modeling and observational studies find that this is indeed so). Because water vapor is a greenhouse gas, this results in further warming and so is a \"positive feedback\" that amplifies the original warming. Eventually other earth processes offset these positive feedbacks, stabilising the global temperature at a new equilibrium and preventing the loss of Earth's water through a",
                    "score": 0.8703568577766418
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_32",
        "question": "The power output of a laser is measured in units of watts (W), where one watt is equal to one joule per second. $\\left(1 \\mathrm{~W}=1 \\mathrm{~J} \\cdot \\mathrm{s}^{-1}\\right.$.) What is the number of photons emitted per second by a $1.00 \\mathrm{~mW}$ nitrogen laser? The wavelength emitted by a nitrogen laser is $337 \\mathrm{~nm}$.",
        "golden_answers": [
            " 1.70"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1828896,
                    "contents": "Laser\nA laser is a device that emits light through a process of optical amplification based on the stimulated emission of electromagnetic radiation. The word \"laser\" is an acronym for \"light amplification by stimulated emission of radiation\". The first laser was built in 1960 by Theodore H. Maiman at Hughes Research Laboratories, based on theoretical work by Charles Hard Townes and Arthur Leonard Schawlow ().",
                    "score": 0.9156261682510376
                },
                {
                    "id": 1834076,
                    "contents": "Laser science\nLaser science or laser physics is a branch of optics that describes the theory and practice of lasers. Laser science is principally concerned with quantum electronics, laser construction, optical cavity design, the physics of producing a population inversion in laser media, and the temporal evolution of the light field in the laser. It is also concerned with the physics of laser beam propagation, particularly the physics of Gaussian beams, with laser applications, and with associated fields such as nonlinear optics and quantum optics. History",
                    "score": 0.9120237827301025
                },
                {
                    "id": 1828900,
                    "contents": "Laser\nLasers are characterized according to their wavelength in a vacuum. Most \"single wavelength\" lasers actually produce radiation in several modes with slightly different wavelengths. Although temporal coherence implies some degree of monochromaticity, there are lasers that emit a broad spectrum of light or emit different wavelengths of light simultaneously. Some lasers are not single spatial mode and have light beams that diverge more than is required by the diffraction limit. All such devices are classified as \"lasers\" based on the method of producing light by stimulated emission. Lasers are employed where light of the required spatial or temporal coherence can not be produced using simpler technologies. Terminology",
                    "score": 0.9117557406425476
                },
                {
                    "id": 7009546,
                    "contents": "History of optics\nThis changed with the invention of the maser in 1953 and the laser in 1960. Laser science—research into principles, design and application of these devices—became an important field, and the quantum mechanics underlying the laser's principles was studied now with more emphasis on the properties of light, and the name quantum optics became customary.",
                    "score": 0.9114848375320435
                },
                {
                    "id": 3271380,
                    "contents": "Quantum optics\nAs laser science needed good theoretical foundations, and also because research into these soon proved very fruitful, interest in quantum optics rose. Following the work of Dirac in quantum field theory, John R. Klauder, George Sudarshan, Roy J. Glauber, and Leonard Mandel applied quantum theory to the electromagnetic field in the 1950s and 1960s to gain a more detailed understanding of photodetection and the statistics of light (see degree of coherence). This led to the introduction of the coherent state as a concept which addressed variations between laser light, thermal light, exotic squeezed states, etc. as it became understood that light cannot be fully described just referring to the electromagnetic fields describing the waves in the classical picture. In 1977, Kimble et al. demonstrated a single atom emitting one photon at a time, further compelling evidence that light consists of photons. Previously unknown quantum states of light with characteristics unlike classical states,",
                    "score": 0.9032032489776611
                },
                {
                    "id": 1828899,
                    "contents": "Laser\nFundamentals Lasers are distinguished from other light sources by their coherence. Spatial (or transverse) coherence is typically expressed through the output being a narrow beam, which is diffraction-limited. Laser beams can be focused to very tiny spots, achieving a very high irradiance, or they can have very low divergence in order to concentrate their power at a great distance. Temporal (or longitudinal) coherence implies a polarized wave at a single frequency, whose phase is correlated over a relatively great distance (the coherence length) along the beam. A beam produced by a thermal or other incoherent light source has an instantaneous amplitude and phase that vary randomly with respect to time and position, thus having a short coherence length.",
                    "score": 0.9025970697402954
                },
                {
                    "id": 15321644,
                    "contents": "Watt\nTerawatt The terawatt (TW) is equal to one trillion watts (1012 W). The total power used by humans worldwide is commonly measured in terawatts. The most powerful lasers from the mid-1960s to the mid-1990s produced power in terawatts, but only for nanosecond intervals. The average lightning strike peaks at 1 terawatt, but these strikes only last for 30 microseconds. Petawatt The petawatt (PW) is equal to one quadrillion watts (1015 W) and can be produced by the current generation of lasers for time scales on the order of picoseconds ( s). One such laser is the Lawrence Livermore's Nova laser, which achieved a power output of 1.25 PW ( W) by a process called chirped pulse amplification. The duration of the pulse was roughly 0.5 ps ( s), giving a total energy of 600 J. Another example is the Laser for Fast Ignition Experiments (LFEX) at the Institute of Laser Engineering (ILE), Osaka University, which achieved a power output of 2 PW for a duration of approximately 1 ps.",
                    "score": 0.9011576175689697
                },
                {
                    "id": 11555532,
                    "contents": "Slope efficiency\nThe slope efficiency is an important property of a laser. It is obtained by plotting the laser output power against the input pump power. Above the lasing threshold, the resulting curve is usually close to a straight line. The slope efficiency is the slope of this line. Slope efficiency can similarly be defined in terms of output and input energies instead of powers. This makes it applicable to pulsed lasers. The curve described above is nearly linear above threshold when the optical losses in the laser cavity remain the same for all input powers. Sometimes the curve is nonlinear, typically with lower slope at high input powers. This is characteristic of increased losses, which are often thermal in nature, such as due to lensing. This is especially common in powerful lasers.",
                    "score": 0.900370180606842
                },
                {
                    "id": 1850860,
                    "contents": "Metre\nThis definition fixed the speed of light in vacuum at exactly metres per second (≈). An intended by-product of the 17th CGPM's definition was that it enabled scientists to compare lasers accurately using frequency, resulting in wavelengths with one-fifth the uncertainty involved in the direct comparison of wavelengths, because interferometer errors were eliminated. To further facilitate reproducibility from lab to lab, the 17th CGPM also made the iodine-stabilised helium–neon laser \"a recommended radiation\" for realising the metre. For the purpose of delineating the metre, the BIPM currently considers the HeNe laser wavelength, , to be with an estimated relative standard uncertainty (U) of . This uncertainty is currently one limiting factor in laboratory realisations of the metre, and it is several orders of magnitude poorer than that of the second, based upon the caesium fountain atomic clock (). Consequently, a realisation of the metre is usually delineated (not defined) today in",
                    "score": 0.900338888168335
                },
                {
                    "id": 29533069,
                    "contents": "Monochromatic radiation\nA laser is a device that generates monochromatic and coherent radiation through a process of stimulated emission. Properties and uses When monochromatic radiation is made to interfere with itself, the result can be visible and stable interference fringes that can be used to measure very small distances, or large distances with very high accuracy. The current definition of the metre is based on this technique. In the technique of spectroscopic analysis, a material sample is exposed to monochromatic radiation, and the amount that is absorbed is measured. The graph of absorption as a function of the radiation's frequency is often characteristic of the material's composition. This technique can use radiation ranging from the microwaves, as in rotational spectroscopy, to gamma rays, as in Mössbauer spectroscopy. See also Wave Acoustics Optics Monochromator Interferometer Diffraction grating Dichroic filter Newton rings References Radiation",
                    "score": 0.8996913433074951
                },
                {
                    "id": 2810106,
                    "contents": "Real Genius\nKent sabotages their latest, nearly-successful apparatus. Though Chris knows Kent destroyed the laser, he can do nothing about it and is left to brood over the injustice; this inspires him to come up with a whole new system. The new solution works flawlessly, impressing Hathaway; he congratulates Chris and assures him that he will graduate. Chris and Mitch leave and celebrate, but Lazlo arrives to tell them his suspicions regarding the possible uses of the laser. Realizing he is right, they return to the lab, but all the laser equipment has already been removed by Hathaway.",
                    "score": 0.8995065093040466
                },
                {
                    "id": 1151319,
                    "contents": "Optics\nLasers A laser is a device that emits light, a kind of electromagnetic radiation, through a process called stimulated emission. The term laser is an acronym for Light Amplification by Stimulated Emission of Radiation. Laser light is usually spatially coherent, which means that the light either is emitted in a narrow, low-divergence beam, or can be converted into one with the help of optical components such as lenses. Because the microwave equivalent of the laser, the maser, was developed first, devices that emit microwave and radio frequencies are usually called masers.",
                    "score": 0.8983352780342102
                },
                {
                    "id": 7516402,
                    "contents": "Theodor W. Hänsch\nIn 1970 he invented a new type of laser which generated light pulses with an extremely high spectral resolution (i.e. all the photons emitted from the laser had nearly the same energy, to a precision of 1 part in a million). Using this device he succeeded to measure the transition frequency of the Balmer line of atomic hydrogen with a much higher precision than before. During the late 1990s, he and his coworkers developed a new method to measure the frequency of laser light to an even higher precision, using a device called the optical frequency comb generator. This invention was then used to measure the Lyman line of atomic hydrogen to an extraordinary precision of 1 part in a hundred trillion. At such a high precision, it became possible to search for possible changes in the fundamental physical constants of the universe over time. For these achievements he became co-recipient of the Nobel Prize in Physics for 2005.",
                    "score": 0.8983005881309509
                },
                {
                    "id": 1834077,
                    "contents": "Laser science\nHistory Laser science predates the invention of the laser itself. Albert Einstein created the foundations for the laser and maser in 1917, via a paper in which he re-derived Max Planck’s law of radiation using a formalism based on probability coefficients (Einstein coefficients) for the absorption, spontaneous emission, and stimulated emission of electromagnetic radiation. The existence of stimulated emission was confirmed in 1928 by Rudolf W. Ladenburg. In 1939, Valentin A. Fabrikant made the earliest laser proposal. He specified the conditions required for light amplification using stimulated emission. In 1947, Willis E. Lamb and R. C. Retherford found apparent stimulated emission in hydrogen spectra and effected the first demonstration of stimulated emission; in 1950, Alfred Kastler (Nobel Prize for Physics 1966) proposed the method of optical pumping, experimentally confirmed, two years later, by Brossel, Kastler, and Winter.",
                    "score": 0.8980755805969238
                },
                {
                    "id": 2627645,
                    "contents": "List of laser types\nThis is a list of laser types, their operational wavelengths, and their applications. Thousands of kinds of laser are known, but most of them are used only for specialized research. Overview Gas lasers Chemical lasers Used as directed-energy weapons. Dye lasers Metal-vapor lasers Solid-state lasers Semiconductor lasers Other types of lasers See also Laser construction List of laser articles Notes Further references Silfvast, William T. Laser fundamentals, Cambridge University Press, 2004. Weber, Marvin J. Handbook of laser wavelengths, CRC Press, 1999.",
                    "score": 0.896670937538147
                },
                {
                    "id": 7172160,
                    "contents": "Nova (laser)\nThe basic amplification system used in Nova and other high-power lasers of its era was limited in terms of power density and pulse length. One problem was that the amplifier glass responded over a period of time, not instantaneously, and very short pulses would not be strongly amplified. Another problem was that the high power densities led to the same sorts of self-focusing problems that had caused problems in earlier designs, but at such a magnitude that even measures like spatial filtering would not be enough, in fact the power densities were high enough to cause filaments to form in air.",
                    "score": 0.8956130743026733
                },
                {
                    "id": 28462649,
                    "contents": "Alan David White\nHe was fond of art, in particular, sculpture. For his achievements he was awarded the 1984 IEEE David Sarnoff Award, and in 2000 he was elected to the New Jersey Inventors Hall of Fame. Scientific achievements The first gas laser, using a mixture of helium and neon, was demonstrated in 1960 and emitted radiation at a wavelength of 1.15 μm (infrared range). Two years later, White, together with Dane Rigden, showed that a helium-neon laser can emit radiation at a wavelength of 632.8 nm, i.e., in the visible range of the spectrum. In subsequent years, White, with Eugene I. Gordon and others, investigated the reasons for the limitation of the power of such lasers, established scaling laws for gas-discharge lasers, and developed frequency stabilization methods for such devices. The first continuous-wave visible laser, invented by White and Rigden, is still widely used in research and education, and is a part of various instruments.",
                    "score": 0.8950843811035156
                },
                {
                    "id": 11266395,
                    "contents": "Gain (laser)\nThe amplification coefficient can be defined as ratio of the output power to the input power : . It is related with gain; . The gain and the amplification coefficient should not be confused with the magnification coefficient. The magnification characterizes the scale of enlarging of an image; such enlargement can be realized with passive elements, without gain medium. Alternative terminology and notations There is no established terminology about gain and absorption. Everyone is free to use own notations, and it is not possible to cover all the systems of notations in this article. In radiophysics, gain may mean logarithm of the amplification coefficient.",
                    "score": 0.894467830657959
                },
                {
                    "id": 16338843,
                    "contents": "Optically active additive\nthe energy of each photon is given by multiplying its frequency in cycles per second by a constant (Planck's constant, 6.626 x 10−27 erg seconds). It follows that the wavelength of a photon emitted from a luminescent system is directly related to the difference between the energy of the two atomic levels involved.",
                    "score": 0.8942690491676331
                },
                {
                    "id": 10438838,
                    "contents": "Relative intensity noise\nRelative intensity noise is measured by sampling the output current of a photodetector over time and transforming this data set into frequency with a fast Fourier transform. Alternatively, it can be measured by analyzing the spectrum of the photodetected signal using an electrical spectrum analyzer. Noise observed in the electrical domain is proportional to electric current squared and hence to optical power squared. Therefore, RIN is usually presented as relative fluctuation in the square of the optical power in decibels per hertz over the RIN bandwidth and at one or several optical intensities. It may also be specified as a percentage, a value that represents the relative fluctuations per Hz multiplied by the RIN bandwidth. See also Shot noise External reference Intensity noise in Encyclopedia of Laser Physics and Technology Relative Intensity Noise in Encyclopedia of Laser Physics and Technology Noise (electronics) Fiber-optic communications Laser science",
                    "score": 0.8927944898605347
                },
                {
                    "id": 19630970,
                    "contents": "International Conference of Laser Applications\nThe International Conference on Lasers and Applications, Lasers 'XX was an annual conference organized by the former Society for Optical and Quantum Electronics. The conference, known in short by Lasers 'XX (where XX refers to the particular year), was held at various locations in The United States from 1978 to 2000. The emphasis of these conferences was laser development and in particular the development of high-power lasers. The papers delivered at these conferences were published in a series of hard-bound volumes known as Proceedings of the International Conference on Lasers 'XX () by STS Press. In total, more than 20 book proceedings were published.",
                    "score": 0.8918513059616089
                },
                {
                    "id": 27059512,
                    "contents": "Optelecom\nThe firm’s products were based on Gould's invention of the optical amplifier and the laser -- his acronym for ''light amplification by stimulated emission of radiation.'' On the day of his invention, he realized its potential in telecommunications, writing in his journal: “Brief statement of properties and possible uses of the Laser…..Applications to communications, radar, etc. are obvious.”",
                    "score": 0.8917005062103271
                },
                {
                    "id": 1828979,
                    "contents": "Laser\nReferences Further reading Books Bertolotti, Mario (1999, trans. 2004). The History of the Laser. Institute of Physics. . Bromberg, Joan Lisa (1991). The Laser in America, 1950–1970. MIT Press. . Csele, Mark (2004). Fundamentals of Light Sources and Lasers. Wiley. . Koechner, Walter (1992). Solid-State Laser Engineering. 3rd ed. Springer-Verlag. . Siegman, Anthony E. (1986). Lasers. University Science Books. . Silfvast, William T. (1996). Laser Fundamentals. Cambridge University Press. . Svelto, Orazio (1998). Principles of Lasers. 4th ed. Trans. David Hanna. Springer. . Wilson, J. & Hawkes, J.F.B. (1987). Lasers: Principles and Applications. Prentice Hall International Series in Optoelectronics, Prentice Hall. . Yariv, Amnon (1989). Quantum Electronics. 3rd ed. Wiley. .",
                    "score": 0.8912649750709534
                },
                {
                    "id": 20029268,
                    "contents": "Timeline of United States inventions (1946–1991)\nA laser is a device that emits electromagnetic radiation through a process called stimulated emission. Laser light is usually spatially coherent, which means that the light either is emitted in a narrow, low-divergence beam, or can be converted into one with the help of optical components such as lenses. Lasers are used to read compact discs and bar codes, guide missiles, remove ulcers, fabricate steel, precisely measure the distance from Earth to the Moon, record ultradefined images of brain tissue, entertain people in light shows and do thousands of other things. In 1957, American physicist Gordon Gould first theorized the idea and use of laser technology. Despite a 20-year battle with the United States Patent and Trademark Office, Gould is now widely associated as the original inventor of laser. In addition, Charles H. Townes and Arthur L. Schawlow, scientists at Bell Laboratories, wrote a paper, Infrared and Optical Masers in 1958 that was enormously influential on the theory of",
                    "score": 0.891063928604126
                },
                {
                    "id": 1828916,
                    "contents": "Laser\nIn 1963, Roy J. Glauber showed that coherent states are formed from combinations of photon number states, for which he was awarded the Nobel Prize in physics. A coherent beam of light is formed by single-frequency quantum photon states distributed according to a Poisson distribution. As a result, the arrival rate of photons in a laser beam is described by Poisson statistics. Many lasers produce a beam that can be approximated as a Gaussian beam; such beams have the minimum divergence possible for a given beam diameter. Some lasers, particularly high-power ones, produce multimode beams, with the transverse modes often approximated using Hermite–Gaussian or Laguerre-Gaussian functions. Some high power lasers use a flat-topped profile known as a \"tophat beam\". Unstable laser resonators (not used in most lasers) produce fractal-shaped beams. Specialized optical systems can produce more complex beam geometries, such as Bessel beams and optical vortexes.",
                    "score": 0.8910233974456787
                },
                {
                    "id": 907505,
                    "contents": "Intensity (physics)\nand the local intensity is obtained by multiplying this expression by the wave velocity, c/n: where n is the refractive index, c is the speed of light in vacuum and is the vacuum permittivity. For non-monochromatic waves, the intensity contributions of different spectral components can simply be added. The treatment above does not hold for arbitrary electromagnetic fields. For example, an evanescent wave may have a finite electrical amplitude while not transferring any power. The intensity should then be defined as the magnitude of the Poynting vector.",
                    "score": 0.8905677199363708
                },
                {
                    "id": 3271390,
                    "contents": "Quantum optics\nExternal links An introduction to quantum optics of the light field Encyclopedia of laser physics and technology, with content on quantum optics (particularly quantum noise in lasers), by Rüdiger Paschotta. Qwiki - A quantum physics wiki devoted to providing technical resources for practicing quantum physicists. Quantiki - a free-content WWW resource in quantum information science that anyone can edit. Various Quantum Optics Reports Optics",
                    "score": 0.8905607461929321
                },
                {
                    "id": 1556005,
                    "contents": "Ambiguity\nA highly confusing term is gain. For example, the sentence \"the gain of a system should be doubled\", without context, means close to nothing. It may mean that the ratio of the output voltage of an electric circuit to the input voltage should be doubled. It may mean that the ratio of the output power of an electric or optical circuit to the input power should be doubled. It may mean that the gain of the laser medium should be doubled, for example, doubling the population of the upper laser level in a quasi-two level system (assuming negligible absorption of the ground-state). The term intensity is ambiguous when applied to light. The term can refer to any of irradiance, luminous intensity, radiant intensity, or radiance, depending on the background of the person using the term.",
                    "score": 0.8901418447494507
                },
                {
                    "id": 7172143,
                    "contents": "Nova (laser)\nA solution to this problem was explored in the form of efficient frequency multipliers, optical devices that combine several photons into one of higher energy, and thus frequency. These devices were quickly introduced and tested experimentally on the OMEGA laser and others, proving effective. Although the process is only about 50% efficient, and half the original laser power is lost, the resulting ultraviolet light couples much more efficiently to the target plasma and is much more effective in collapsing the target to high density.",
                    "score": 0.8900970220565796
                },
                {
                    "id": 1828907,
                    "contents": "Laser\nWhen an electron is excited from one state to that at a higher energy level with energy difference ΔE, it will not stay that way forever. Eventually, a photon will be spontaneously created from the vacuum having energy ΔE . Conserving energy, the electron transitions to a lower energy level which is not occupied, with transitions to different levels having different time constants. This process is called \"spontaneous emission\". Spontaneous emission is a quantum-mechanical effect and a direct physical manifestation of the Heisenberg uncertainty principle. The emitted photon has random direction, but its wavelength matches the absorption wavelength of the transition. This is the mechanism of fluorescence and thermal emission.",
                    "score": 0.8894374966621399
                },
                {
                    "id": 1741278,
                    "contents": "Green\ngenerate light at a frequency that is twice that of the incident beam (563.5 THz); in this case corresponding to the wavelength of 532nm (\"green\"). Other green wavelengths are also available using DPSS technology ranging from 501 nm to 543 nm. Green wavelengths are also available from gas lasers, including the helium–neon laser (543nm), the Argon-ion laser (514nm) and the Krypton-ion laser (521nm and 531nm), as well as liquid dye lasers. Green lasers have a wide variety of applications, including pointing, illumination, surgery, laser light shows, spectroscopy, interferometry, fluorescence, holography, machine vision, non-lethal weapons and bird control.",
                    "score": 0.88936448097229
                },
                {
                    "id": 28773285,
                    "contents": "Marshall G. Jones\nThroughout Jones' career, he has been awarded more than 65 U.S. patents. Jones' Patent No. 4676586 is the patent that is on display at the National Inventors Hall of Fame. This patent describes a laser beam delivery system through a fiber optic which resulted in minimal optical losses. The system was unique in that it allowed unprecedented freedom in the manipulation of the laser beam such that hard-to-reach places were accessible. He made pivotal innovations in the method of making lead wires that is used for light bulbs. This innovation has wide ranging impacts, including: flat emitters for x-ray tubes, diesel engine head-liner assemblies, the production of ceramic metal halide lamps, and control rods for nuclear reactors. Jones pioneered the use of lasers for materials processing in a way that made them readily adoptable for industrial applications. Not only have Jones' work and inventions transformed many industrial products, they have also been foundational to modern industrial",
                    "score": 0.8891593813896179
                },
                {
                    "id": 20029282,
                    "contents": "Timeline of United States inventions (1946–1991)\n1960 Gas laser A gas laser is a laser in which an electric current is discharged through a gas to produce light. The first gas laser, the Helium-neon, was invented by William R. Bennett, Don Herriott, and Ali Javan in 1960. The first continuous visible gas laser, operating at 632.8 nm in the red, was invented by A. D. White and J. D. Rigden in 1962.",
                    "score": 0.8889147043228149
                },
                {
                    "id": 1828948,
                    "contents": "Laser\nThe molecular fluorine laser, emitting at 157 nm in the vacuum ultraviolet is sometimes referred to as an excimer laser, however this appears to be a misnomer inasmuch as F2 is a stable compound.",
                    "score": 0.8886302709579468
                },
                {
                    "id": 1828902,
                    "contents": "Laser\nA laser that produces light by itself is technically an optical oscillator rather than an optical amplifier as suggested by the acronym. It has been humorously noted that the acronym LOSER, for \"light oscillation by stimulated emission of radiation\", would have been more correct. With the widespread use of the original acronym as a common noun, optical amplifiers have come to be referred to as \"laser amplifiers\". The back-formed verb to lase is frequently used in the field, meaning \"to give off coherent light,\" especially in reference to the gain medium of a laser; when a laser is operating it is said to be \"lasing\". The words laser and maser are also used in cases where there is a coherent state unconnected with any manufactured device, as in astrophysical maser and atom laser. Design",
                    "score": 0.8878542184829712
                },
                {
                    "id": 15254192,
                    "contents": "Anthony E. Siegman\nBook Chapters A. E. Siegman, \"Interaction of Radiation with Matter,\" in Lasers and Their Applications (Gordon and Breach, 1976). A. E. Siegman, \"Quantum Electronics,\" in McGraw-Hill Encyclopedia of Science and Technology, 5th Edition (McGraw-Hill Book Company, 1980). A. E. Siegman, \"Laser Beams and Resonators,\" in Proceedings of Scottish Universities Summer School in Physics (Heriot-Watt University, August 1982). A. E. Siegman, P. A. Belanger and A. Hardy, \"Optical Resonators Using Phase-Conjugate Mirrors,\" in Optical Phase Conjugation (Academic Press, 1983). A. E. Siegman, \"The Laser---Still Young After Twenty Five Years?,\" in Lasers: Invention to Application (National Academy Press, 1987). A. E. Siegman, \"Advances in Laser Resonator Design Using Variable Reflectivity Mirrors,\" in Tutorials in Optics (Optical Society of America, 1992). A. E. Siegman, \"Defining and Measuring Laser Beam Quality,\" in Solid State Lasers: New Developments and Applications (Plenum Press, 1994).",
                    "score": 0.8878526091575623
                },
                {
                    "id": 2311304,
                    "contents": "Lasers (album)\nSingles",
                    "score": 0.8877860307693481
                },
                {
                    "id": 6347962,
                    "contents": "Blue laser\nA blue laser is a laser that emits electromagnetic radiation with a wavelength between 360 and 480 nanometers, which the human eye sees as blue or violet. Blue beams are produced by helium-cadmium gas lasers at 441.6 nm, and argon-ion lasers at 458 and 488 nm. Semiconductor lasers with blue beams are typically based on gallium(III) nitride (GaN; violet color) or indium gallium nitride (often true blue in color, but also able to produce other colors). Both blue and violet lasers can also be constructed using frequency-doubling of infrared laser wavelengths from diode lasers or diode-pumped solid-state lasers.",
                    "score": 0.8875919580459595
                },
                {
                    "id": 10438837,
                    "contents": "Relative intensity noise\nRelative intensity noise (RIN), describes the instability in the power level of a laser. The noise term is important to describe lasers used in fiber-optic communication and LIDAR remote sensing. Relative intensity noise can be generated from cavity vibration, fluctuations in the laser gain medium or simply from transferred intensity noise from a pump source. Since intensity noise typically is proportional to the intensity, the relative intensity noise is typically independent of laser power. Hence, when the signal to noise ratio (SNR) is limited by RIN, it does not depend on laser power. In contrast, when SNR is limited by shot noise, it improves with increasing laser power. RIN typically peaks at the relaxation oscillation frequency of the laser then falls off at higher frequencies until it converges to the shot noise level. The roll off frequency sets what is specified as the RIN bandwidth. RIN is sometimes referred to as a kind of 1/f noise otherwise known as pink noise.",
                    "score": 0.8867228031158447
                },
                {
                    "id": 2578650,
                    "contents": "Nicolaas Bloembergen\nWith the advent of the laser, he participated in the development of the field of laser spectroscopy, which allows precise observations of atomic structure using lasers. Following the development of second-harmonic generation by Peter Franken and others in 1961, Bloembergen studied how when one bombards matter with a focused and high-intensity beam of photons, a new structure of matter is revealed. This he termed the study of nonlinear optics. In reflection to his work in a Dutch newspaper in 1990, Bloembergen said: \"We took a standard textbook on optics and for each section we asked ourselves what would happen if the intensity was to become very high. We were almost certain that we were bound to encounter an entirely new type of physics within that domain\".",
                    "score": 0.8867085576057434
                },
                {
                    "id": 1828897,
                    "contents": "Laser\nA laser differs from other sources of light in that it emits light which is coherent. Spatial coherence allows a laser to be focused to a tight spot, enabling applications such as laser cutting and lithography. Spatial coherence also allows a laser beam to stay narrow over great distances (collimation), enabling applications such as laser pointers and lidar. Lasers can also have high temporal coherence, which allows them to emit light with a very narrow spectrum. Alternatively, temporal coherence can be used to produce ultrashort pulses of light with a broad spectrum but durations as short as a femtosecond.",
                    "score": 0.8864959478378296
                },
                {
                    "id": 11649016,
                    "contents": "Laser power scaling\nPower scaling of a laser is increasing its output power without changing the geometry, shape, or principle of operation. Power scalability is considered an important advantage in a laser design. Usually, power scaling requires a more powerful pump source, stronger cooling, and an increase in size. It may also require reduction of the background loss in the laser resonator and, in particular, in the gain medium.",
                    "score": 0.8864247798919678
                },
                {
                    "id": 1828905,
                    "contents": "Laser\nMost practical lasers contain additional elements that affect properties of the emitted light, such as the polarization, wavelength, and shape of the beam. Laser physics Electrons and how they interact with electromagnetic fields are important in our understanding of chemistry and physics. Stimulated emission In the classical view, the energy of an electron orbiting an atomic nucleus is larger for orbits further from the nucleus of an atom. However, quantum mechanical effects force electrons to take on discrete positions in orbitals. Thus, electrons are found in specific energy levels of an atom, two of which are shown below:",
                    "score": 0.8862443566322327
                },
                {
                    "id": 3964900,
                    "contents": "Ali Javan\nJavan's gas laser was the first continuously operating laser. It operated with a very low energy input of about 25 watts or 50 watts in the first model, compared to thousands of watts required for the ruby lasers to produce short bursts. The output laser power was ~ 1 milliwatt. In addition, the ruby laser is greatly surpassed in the narrowness of its output of wavelengths by the gas laser. Its beam of infrared light was slightly less than half an inch wide and spread no more than a foot over a distance of a mile. Just one day after its realization, the laser was used to transmit a telephone call. Javan later described the moment: \"I put in a call to the lab. One of the team members answered and asked me to hold the line for a moment. Then I heard a voice [Mr. Balik], somewhat quivering in transmission, telling me that it was the laser light speaking to me.\"",
                    "score": 0.8859911561012268
                },
                {
                    "id": 1828936,
                    "contents": "Laser\nAt a conference in 1959, Gordon Gould first published the acronym \"LASER\" in the paper The LASER, Light Amplification by Stimulated Emission of Radiation. Gould's intention was that different \"-ASER\" acronyms should be used for different parts of the spectrum: \"XASER\" for x-rays, \"UVASER\" for ultraviolet, etc. \"LASER\" ended up becoming the generic term for non-microwave devices, although \"RASER\" was briefly popular for denoting radio-frequency-emitting devices.",
                    "score": 0.8859596252441406
                },
                {
                    "id": 1828930,
                    "contents": "Laser\nHistory Foundations In 1917, Albert Einstein established the theoretical foundations for the laser and the maser in the paper Zur Quantentheorie der Strahlung (On the Quantum Theory of Radiation) via a re-derivation of Max Planck's law of radiation, conceptually based upon probability coefficients (Einstein coefficients) for the absorption, spontaneous emission, and stimulated emission of electromagnetic radiation. In 1928, Rudolf W. Ladenburg confirmed the existence of the phenomena of stimulated emission and negative absorption. In 1939, Valentin A. Fabrikant predicted the use of stimulated emission to amplify \"short\" waves. In 1947, Willis E. Lamb and R.C. Retherford found apparent stimulated emission in hydrogen spectra and effected the first demonstration of stimulated emission. In 1950, Alfred Kastler (Nobel Prize for Physics 1966) proposed the method of optical pumping, which was experimentally demonstrated two years later by Brossel, Kastler, and Winter. Maser",
                    "score": 0.885921835899353
                },
                {
                    "id": 15254204,
                    "contents": "Anthony E. Siegman\nA. E. Siegman, \"Laser Book List,\" Appl. Opt. A 10, 36 (December 1971). K. R. Manes and A. E. Siegman, \"Observation of quantum phase fluctuations in infrared gas lasers,\" Phys. Rev. A 4, 373—386 (July 1971). A. E. Siegman, \"A course in lasers and optical electronics for engineers, or quantum electronics without quantum mechanics,\" IEEE Trans. Education E--14, 162 (May 1971). A. E. Siegman, \"Stabilizing output with unstable resonators,\" Laser Focus 7, 42 (May 1971). M. F. Becker, D. J. Kuizenga and A. E. Siegman, \"Harmonic mode locking of the Nd:YAG laser,\" IEEE Quantum Electron. QE-8, 687 (August 1972). A. E. Siegman, \"Dispersive explanation of the spectral behavior of Runges mode-locked dye laser,\" Opt. Commun. 5, 200 (July 1972). A. E. Siegman, D. W. Phillion, and D. J. Kuizenga, \"Rotational relaxation and triplet state effects in the cw dye laser,\" Appl. Phys. Lett. 21, 345 (October 1972).",
                    "score": 0.8855535984039307
                },
                {
                    "id": 7009547,
                    "contents": "History of optics\nAs laser science needed good theoretical foundations, and also because research into these soon proved very fruitful, interest in quantum optics rose. Following the work of Dirac in quantum field theory, George Sudarshan, Roy J. Glauber, and Leonard Mandel applied quantum theory to the electromagnetic field in the 1950s and 1960s to gain a more detailed understanding of photodetection and the statistics of light (see degree of coherence). This led to the introduction of the coherent state as a quantum description of laser light and the realization that some states of light could not be described with classical waves. In 1977, Kimble et al. demonstrated the first source of light which required a quantum description: a single atom that emitted one photon at a time. Another quantum state of light with certain advantages over any classical state, squeezed light, was soon proposed. At the same time, development of short and ultrashort laser pulses—created by Q-switching and mode-locking",
                    "score": 0.8851925134658813
                },
                {
                    "id": 2311290,
                    "contents": "Lasers (album)\nBackground",
                    "score": 0.885144829750061
                },
                {
                    "id": 15254201,
                    "contents": "Anthony E. Siegman\nR. Arrathoon and A. E. Siegman, \"Current pushing of the oscillation frequency of a 6328 He-Ne laser,\" Appl. Phys. Lett. 13, 197 (September 1968). A. E. Siegman, \"Comments on pumping on a swing,\" Am. J. Phys. 37, 843 (August 1969). R. Arrathoon and A. E. Siegman, \"Further measurements of quantum phase noise in a He-Ne laser,\" J. Appl. Phys. 40, 910—911 (February 1969). G. A. Massey, R. Tremblay and A. E. Siegman, \"A Fresnel-drag ring-laser method for rotation readout of a spinning transparent spherical gyroscope,\" IEEE J. Quantum El ectron. QE-5, 357 (June 1969). A. E. Siegman and D. J. Kuizenga, \"Simple analytic expressions for AM and FM mode-locked pulses in homogeneous lasers,\" Appl. Phys. Lett. 14, 181 (March 1969). G. A. Massey and A. E. Siegman, \"Reflection and refraction of gaussian light beams at tilted ellipsoidal surfaces,\" Appl. Opt. 8, 975—978 (May 1969).",
                    "score": 0.8847862482070923
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_33",
        "question": " Sirius, one of the hottest known stars, has approximately a blackbody spectrum with $\\lambda_{\\max }=260 \\mathrm{~nm}$. Estimate the surface temperature of Sirius.\r\n",
        "golden_answers": [
            "11000"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1242455,
                    "contents": "Sirius\nSirius appears bright because of its intrinsic luminosity and its proximity to the Solar System. At a distance of , the Sirius system is one of Earth's nearest neighbours. Sirius is gradually moving closer to the Solar System, so it is expected to increase in brightness slightly over the next 60,000 years. After that time, its distance will begin to increase, and it will become fainter, but it will continue to be the brightest star in the Earth's night sky for approximately the next 210,000 years.",
                    "score": 0.9071575403213501
                },
                {
                    "id": 1242454,
                    "contents": "Sirius\nSirius () is the brightest star in the night sky. Its name is derived from the Greek word (, 'glowing' or 'scorching'). The star is designated α Canis Majoris, Latinized to Alpha Canis Majoris, and abbreviated Alpha CMa or α CMa. With a visual apparent magnitude of −1.46, Sirius is almost twice as bright as Canopus, the next brightest star. Sirius is a binary star consisting of a main-sequence star of spectral type A0 or A1, termed Sirius A, and a faint white dwarf companion of spectral type DA2, termed Sirius B. The distance between the two varies between 8.2 and 31.5 astronomical units as they orbit every 50 years.",
                    "score": 0.8948959708213806
                },
                {
                    "id": 27090536,
                    "contents": "Béla Harkányi\nHarkányi's most outstanding result was the first determination of the surface temperature for individual stars other than the Sun. Prior to 1902, when his relevant study was published, data only existed for the Sun's effective temperature. The range of temperatures of other stars had very roughly been bracketed by Scheiner in 1894. Harkányi realized that the recent success in determining the form of blackbody spectrum offered a way to determine stellar temperatures by fitting the blackbody curve to spectrophotometric observations of stars to determine the location of the maximum of the blackbody curve, from which the temperature follows applying Wien's displacement law. It is notable that this method works even if the maximum is outside the spectral range of observations. Using Vogel's (1880) spectrophotometric data, available at 7 or 8 wavelength values, Harkányi performed the fit and obtained Wien temperatures for 5 stars (Sirius, Vega, Arcturus, Aldebaran, and Betelgeuze). The",
                    "score": 0.893932044506073
                },
                {
                    "id": 1242482,
                    "contents": "Sirius\nSirius B Sirius B is one of the most massive white dwarfs known. With a mass of , it is almost double the average. This mass is packed into a volume roughly equal to the Earth's. The current surface temperature is 25,200 K. Because there is no internal heat source, Sirius B will steadily cool as the remaining heat is radiated into space over more than two billion years. A white dwarf forms after a star has evolved from the main sequence and then passed through a red giant stage. This occurred when Sirius B was less than half its current age, around 120 million years ago. The original star had an estimated and was a B-type star (roughly B4–5) when it was still on the main sequence. While it passed through the red giant stage, Sirius B may have enriched the metallicity of its companion.",
                    "score": 0.8922592401504517
                },
                {
                    "id": 19013207,
                    "contents": "Sidus Ludoviciana\nSidus Ludoviciana , also known as HD 116798 is an 8th-magnitude giant star in the asterism of the Big Dipper in the constellation Ursa Major, halfway between Mizar and Alcor. It was discovered on 2 December 1722 by Johann Georg Liebknecht, who mistook it for a planet and named it after Louis V, Landgrave of Hesse-Darmstadt. A line-of-sight companion with Mizar and Alcor (with a spectral type similar to the latter), it is roughly four times more distant. It has the spectral type A8/F0 III. The star is six times more luminous than the Sun, 1.6 times its radius, and has an surface temperature of . Spectral classification based on a spectrum taken for radial velocity measurement tentatively determined a spectral and luminosity class of A8/F0III, which would indicate that it has exhausted its core hydrogen and started to evolve away from the main sequence, however giant stars of this type should be at least ten times more luminous than measured for Sidus Ludoviciana. References",
                    "score": 0.8922003507614136
                },
                {
                    "id": 1242456,
                    "contents": "Sirius\nSirius A is about twice as massive as the Sun () and has an absolute visual magnitude of +1.42. It is 25 times as luminous as the Sun, but has a significantly lower luminosity than other bright stars such as Canopus or Rigel. The system is between 200 and years old. It was originally composed of two bright bluish stars. The more massive of these, Sirius B, consumed its resources and became a red giant before shedding its outer layers and collapsing into its current state as a white dwarf around years ago. Sirius is known colloquially as the \"Dog Star\", reflecting its prominence in its constellation, Canis Major (the Greater Dog). The heliacal rising of Sirius marked the flooding of the Nile in Ancient Egypt and the \"dog days\" of summer for the ancient Greeks, while to the Polynesians, mostly in the Southern Hemisphere, the star marked winter and was an important reference for their navigation around the Pacific Ocean. Observational history",
                    "score": 0.8917589783668518
                },
                {
                    "id": 1242462,
                    "contents": "Sirius\nKinematics In 1717, Edmond Halley discovered the proper motion of the hitherto presumed \"fixed\" stars after comparing contemporary astrometric measurements with those from the second century AD given in Ptolemy's Almagest. The bright stars Aldebaran, Arcturus and Sirius were noted to have moved significantly; Sirius had progressed about 30 arcminutes (about the diameter of the Moon) to the southwest. In 1868, Sirius became the first star to have its velocity measured, the beginning of the study of celestial radial velocities. Sir William Huggins examined the spectrum of the star and observed a red shift. He concluded that Sirius was receding from the Solar System at about 40 km/s. Compared to the modern value of −5.5 km/s, this was an overestimate and had the wrong sign; the minus sign (−) means that it is approaching the Sun.",
                    "score": 0.8912510275840759
                },
                {
                    "id": 1242481,
                    "contents": "Sirius\nSirius A is classed as an Am star because the spectrum shows deep metallic absorption lines, indicating an enhancement of its surface layers in elements heavier than helium, such as iron. The spectral type has been reported as A0mA1 Va, which indicates that it would be classified as A1 from hydrogen and helium lines, but A0 from the metallic lines that cause it to be grouped with the Am stars. When compared to the Sun, the proportion of iron in the atmosphere of Sirius A relative to hydrogen is given by , meaning iron is 316% as abundant as in the Sun's atmosphere. The high surface content of metallic elements is unlikely to be true of the entire star; rather the iron-peak and heavy metals are radiatively levitated towards the surface. Sirius B",
                    "score": 0.890184760093689
                },
                {
                    "id": 3970215,
                    "contents": "183 Istria\nPhysical characteristics Istria has been characterized as a common, stony S-type asteroid in both the Tholen and SMASS classification. Rotation period In August 1979, a rotational lightcurve of Istria was obtained from photometric observations by American astronomer Alain Harris. Lightcurve analysis gave a well-defined rotation period of 11.77 hours with a brightness amplitude of 0.31 magnitude (). Observations by French amateur astronomer Laurent Bernasconi gave a similar period of 11.6 hours (). Diameter and albedo According to the surveys carried out by the Infrared Astronomical Satellite IRAS, the Japanese Akari satellite and the NEOWISE mission of NASA's Wide-field Infrared Survey Explorer, Istria measures between 30.779 and 35.43 kilometers in diameter and its surface has an albedo between 0.1890 and 0.2582. Naming",
                    "score": 0.8895487189292908
                },
                {
                    "id": 14160710,
                    "contents": "1258 Sicilia\nPhysical characteristics Sicilia is an assumed carbonaceous C-type asteroid. Rotation period In May 2010, a first rotational lightcurve of Sicilia was obtained from photometric observations by astronomers at the Oakley Southern Sky Observatory () in Australia. Lightcurve analysis gave a rotation period of 13.500 hours with a brightness amplitude of 0.19 magnitude (). Diameter and albedo According to the surveys carried out by the Japanese Akari satellite, the NEOWISE mission of NASA's Wide-field Infrared Survey Explorer, and the Infrared Astronomical Satellite IRAS, Sicilia measures between 36.83 and 52.529 kilometers in diameter and its surface has an albedo between 0.0369 and 0.07. The Collaborative Asteroid Lightcurve Link largely agrees with IRAS and derives an albedo of 0.0470 and a diameter of 44.39 kilometers based on an absolute magnitude of 10.7. Naming",
                    "score": 0.889293909072876
                },
                {
                    "id": 29332449,
                    "contents": "IRAS 18357-0604\nProperties IRAS 18357-0604 is likely to be a very luminous star, like all YHGs. Assuming a distance of 6,000 parsecs, the star would have a bolometric luminosity of about . Based on its spectrum, the star likely has a temperature of about . Applying the Stefan-Boltzmann law to these parameters means that the star has a radius of about .",
                    "score": 0.8850187659263611
                },
                {
                    "id": 2937349,
                    "contents": "List of Solar System objects by size\nSolar System objects more massive than 1021 kilograms (one yottagram [Yg]) are known or expected to be approximately spherical. Astronomical bodies relax into rounded shapes (spheroids), achieving hydrostatic equilibrium, when their own gravity is sufficient to overcome the structural strength of their material. It was believed that the cutoff for round objects is somewhere between 100 km and 200 km in radius if they have a large amount of ice in their makeup; however, later studies revealed that icy satellites as large as Iapetus (1,470 kilometers in diameter) are not in hydrostatic equilibrium at this time, and a 2019 assessment suggests that many TNOs in the size range of 400–1000 kilometers may not even be fully solid bodies, much less gravitationally rounded. Objects that are ellipsoids due to their own gravity are here generally referred to as being \"round\", whether or not they are actually in equilibrium today, while objects that are clearly not ellipsoidal are referred to as",
                    "score": 0.8837956190109253
                },
                {
                    "id": 29842776,
                    "contents": "Iota Mensae\nIota Mensae is a single star about away in the faint constellation Mensa. It has a very slightly variable apparent magnitude of 6.0, making it visible with the naked eye under good skies. Iota Mensae has a spectral type of B8III, indicating that it has exhausted hydrogen at its core and expanded away from the main sequence. It is about 3.7 times the mass (), 52 times as luminous, and has swollen to 2.9 times the radius of the Sun (). It is calculated to be 314 million years old. It has been catalogued as a chemically peculiar star with abnormally strong lines of silicon in its spectrum but this classification is now considered doubtful. Its brightness varies by a few hundredths of a magnitude. Its period was initially measured at 2.6 days, but this is now considered to be a period of 5.3 days with primary and secondary minima of a similar depth. The variability is thought to be due to the rotation of the star. References",
                    "score": 0.8826366066932678
                },
                {
                    "id": 1242476,
                    "contents": "Sirius\nAt a distance of 2.6 parsecs (8.6 ly), the Sirius system contains two of the eight nearest stars to the Sun, and it is the fifth closest stellar system to the Sun. This proximity is the main reason for its brightness, as with other near stars such as Alpha Centauri and in contrast to distant, highly luminous supergiants such as Canopus, Rigel or Betelgeuse. It is still around 25 times more luminous than the Sun. The closest large neighbouring star to Sirius is Procyon, 1.61 parsecs (5.24 ly) away. The Voyager 2 spacecraft, launched in 1977 to study the four giant planets in the Solar System, is expected to pass within of Sirius in approximately 296,000 years. Stellar system",
                    "score": 0.8824859857559204
                },
                {
                    "id": 1242477,
                    "contents": "Sirius\nSirius is a binary star system consisting of two white stars orbiting each other with a separation of about 20 AU (roughly the distance between the Sun and Uranus) and a period of 50.1 years. The brighter component, termed Sirius A, is a main-sequence star of spectral type early A, with an estimated surface temperature of 9,940 K. Its companion, Sirius B, is a star that has already evolved off the main sequence and become a white dwarf. Currently 10,000 times less luminous in the visual spectrum, Sirius B was once the more massive of the two. The age of the system has been estimated at around 230 million years. Early in its life, it is thought to have been two bluish-white stars orbiting each other in an elliptical orbit every 9.1 years. The system emits a higher than expected level of infrared radiation, as measured by IRAS space-based observatory. This might be an indication of dust in the system, which is considered somewhat unusual for a binary star. The Chandra X-ray Observatory",
                    "score": 0.8821417093276978
                },
                {
                    "id": 14787501,
                    "contents": "History of Solar System formation and evolution hypotheses\nWhite dwarfs were found to be extremely dense soon after their discovery. If a star is in a binary system, as is the case for Sirius B and 40 Eridani B, it is possible to estimate its mass from observations of the binary orbit. This was done for Sirius B by 1910, yielding a mass estimate of (a more modern estimate being ). Since hotter bodies radiate more than colder ones, a star's surface brightness can be estimated from its effective surface temperature, and hence from its spectrum. If the star's distance is known, its overall luminosity can also be estimated. A comparison of the two figures yields the star's radius. Reasoning of this sort led to the realization, puzzling to astronomers at the time, that Sirius B and 40 Eridani B must be very dense. For example, when Ernst Öpik estimated the density of some visual binary stars in 1916, he found that 40 Eridani B had a density of over 25,000 times the Sun's, which was so high that he called it \"impossible\".",
                    "score": 0.8816493153572083
                },
                {
                    "id": 2577718,
                    "contents": "Charles H. Townes\nAstrophysics Galactic center The center of the Milky Way had long puzzled astronomers, and thick dust obscures the view of it in visible light. During the mid to late 1970s, Townes together with Eric Wollman, John Lacy, Thomas Geballe and Fred Baas studied Sagittarius A, the H II region at the galactic center, at infrared wavelengths. They observed ionized neon gas swirling around the center at such velocities that the mass at the very center must be approximately equal to that of 3 million suns. Such a large mass in such a small space implied that the central object (the radio source Sagittarius A*) contains a supermassive black hole. Sagittarius A* was one of the first black holes detected; subsequently its mass has been more accurately determined to be 4.3 million solar masses. Shapes and sizes of stars",
                    "score": 0.8814636468887329
                },
                {
                    "id": 602682,
                    "contents": "Supergiant\nThe LBVs are variable with multiple semi-regular periods and less predictable eruptions and giant outbursts. They are usually supergiants or hypergiants, occasionally with Wolf-Rayet spectra—extremely luminous, massive, evolved stars with expanded outer layers, but they are so distinctive and unusual that they are often treated as a separate category without being referred to as supergiants or given a supergiant spectral type. Often their spectral type will be given just as \"LBV\" because they have peculiar and highly variable spectral features, with temperatures varying from about 8,000 K in outburst up to 20,000 K or more when \"quiescent.\" Chemical abundances The abundance of various elements at the surface of supergiants is different from less luminous stars. Supergiants are evolved stars and may have undergone convection of fusion products to the surface.",
                    "score": 0.8808786869049072
                },
                {
                    "id": 1222824,
                    "contents": "Solar System\nThe next closest at 8.6 ly is Sirius, the brightest star in Earth's night sky, with roughly twice the Sun's mass, orbited by the closest white dwarf to Earth, Sirius B. Other systems within ten light-years are the binary red-dwarf system Luyten 726-8 (8.7 ly) and the solitary red dwarf Ross 154 (9.7 ly). The closest solitary Sun-like star to the Solar System is Tau Ceti at 11.9 light-years. It has roughly 80% of the Sun's mass but only 60% of its luminosity.",
                    "score": 0.8807182312011719
                },
                {
                    "id": 4211405,
                    "contents": "82 G. Eridani\nProperties This star is slightly smaller and less massive than the Sun, making it marginally dimmer than the Sun in terms of luminosity; it is about a third more luminous than Tau Ceti or Alpha Centauri B. The projected equatorial rotation rate (v sin i) is 4.0 km/s, compared to 2 km/s for the Sun. 82 G. Eridani is a high-velocity star—it is moving quickly compared to the average—and hence is probably a member of Population II, generally older stars whose motions take them well outside the plane of the Milky Way. Like many other Population II stars, 82 G. Eridani is somewhat metal-deficient (though much less deficient than many), and is older than the Sun. It has a relatively high orbital eccentricity of 0.40 about the galaxy, ranging between 4.6 and 10.8 kiloparsecs from the core. Estimates of the age of this star ranged from 6 to 12 billion years.",
                    "score": 0.8797817230224609
                },
                {
                    "id": 11088851,
                    "contents": "IK Pegasi\nThe effective surface temperature of IK Pegasi B is estimated to be about , making it a strong source of ultraviolet radiation. Under normal conditions this white dwarf would continue to cool for more than a billion years, while its radius would remain essentially unchanged. Future evolution In a 1993 paper, David Wonnacott, Barry J. Kellett and David J. Stickland identified this system as a candidate to evolve into a Type Ia supernova or a cataclysmic variable. At a distance of 150 light years, this makes it the nearest known candidate supernova progenitor to the Earth. However, in the time it will take for the system to evolve to a state where a supernova could occur, it will have moved a considerable distance from Earth but may yet pose a threat.",
                    "score": 0.8793455362319946
                },
                {
                    "id": 1645790,
                    "contents": "Canis Major\nSirius is the brightest star in the night sky at apparent magnitude −1.46 and one of the closest stars to Earth at a distance of 8.6 light-years. Its name comes from the Greek word for \"scorching\" or \"searing\". Sirius is also a binary star; its companion Sirius B is a white dwarf with a magnitude of 8.4–10,000 times fainter than Sirius A to observers on Earth. The two orbit each other every 50 years. Their closest approach last occurred in 1993 and they will be at their greatest separation between 2020 and 2025. Sirius was the basis for the ancient Egyptian calendar. The star marked the Great Dog's mouth on Bayer's star atlas.",
                    "score": 0.8792886734008789
                },
                {
                    "id": 3764193,
                    "contents": "Sagittarius A*\nThe mass of Sagittarius A* has been estimated in two different ways: Two groups—in Germany and the U.S.—monitored the orbits of individual stars very near to the black hole and used Kepler's laws to infer the enclosed mass. The German group found a mass of solar masses, whereas the American group found solar masses. Given that this mass is confined inside a 44-million-kilometre-diameter sphere, this yields a density ten times higher than previous estimates. More recently, measurement of the proper motions of a sample of several thousand stars within approximately one parsec from the black hole, combined with a statistical technique, has yielded both an estimate of the black hole's mass at , plus a distributed mass in the central parsec amounting to . The latter is thought to be composed of stars and stellar remnants.",
                    "score": 0.8791638612747192
                },
                {
                    "id": 1242483,
                    "contents": "Sirius\nThis star is primarily composed of a carbon–oxygen mixture that was generated by helium fusion in the progenitor star. This is overlaid by an envelope of lighter elements, with the materials segregated by mass because of the high surface gravity. The outer atmosphere of Sirius B is now almost pure hydrogen—the element with the lowest mass—and no other elements are seen in its spectrum. Apparent third star Since 1894, irregularities have been observed in the orbits of Sirius A and B with an apparent periodicity of 6–6.4 years. A 1995 study concluded that such a companion likely exists, with a mass of roughly 0.05 solar mass—a small red dwarf or large brown dwarf, with an apparent magnitude of more than 15, and less than 3 arcseconds from Sirius A.",
                    "score": 0.879114031791687
                },
                {
                    "id": 1242486,
                    "contents": "Sirius\nIn 1909, Ejnar Hertzsprung was the first to suggest that Sirius was a member of the Ursa Major Moving Group, based on his observations of the system's movements across the sky. The Ursa Major Group is a set of 220 stars that share a common motion through space. It was once a member of an open cluster, but has since become gravitationally unbound from the cluster. Analyses in 2003 and 2005 found Sirius's membership in the group to be questionable: the Ursa Major Group has an estimated age of 500 ± 100 million years, whereas Sirius, with metallicity similar to the Sun's, has an age that is only half this, making it too young to belong to the group. Sirius may instead be a member of the proposed Sirius Supercluster, along with other scattered stars such as Beta Aurigae, Alpha Coronae Borealis, Beta Crateris, Beta Eridani and Beta Serpentis. This would be one of three large clusters located within of the Sun. The other two are the Hyades and the Pleiades, and each of these clusters",
                    "score": 0.8790003061294556
                },
                {
                    "id": 1242467,
                    "contents": "Sirius\nIn 1915, Walter Sydney Adams, using a reflector at Mount Wilson Observatory, observed the spectrum of Sirius B and determined that it was a faint whitish star. This led astronomers to conclude that it was a white dwarf—the second to be discovered. The diameter of Sirius A was first measured by Robert Hanbury Brown and Richard Q. Twiss in 1959 at Jodrell Bank using their stellar intensity interferometer. In 2005, using the Hubble Space Telescope, astronomers determined that Sirius B has nearly the diameter of the Earth, , with a mass 102% of the Sun's.",
                    "score": 0.8784937262535095
                },
                {
                    "id": 23607531,
                    "contents": "O-type star\nCharacteristics O-type stars are hot and luminous. They have characteristic surface temperatures ranging from 30,000 to 52,000 K, emit intense ultraviolet light, and so appear in the visible spectrum as bluish-white. Because of their high temperatures the luminosities of main sequence O-type stars range from 10,000 times the Sun to around 1,000,000 times, giants from 100,000 times the Sun to over 1,000,000, and supergiants from about 200,000 times the Sun to several million times. Other stars in the same temperature range include rare O-type subdwarf (sdO) stars, the central stars of planetary nebulae (CSPNe), and white dwarfs. The white dwarfs have their own spectral classification scheme, but many CSPNe have O-type spectra. Even these small low-mass subdwarfs and CSPNe have luminosities several hundred to several thousand times that of the Sun. sdO-type stars generally have somewhat higher temperatures than massive O-type stars, up to 100,000K.",
                    "score": 0.8784290552139282
                },
                {
                    "id": 1242457,
                    "contents": "Sirius\nObservational history The brightest star in the night sky, Sirius is recorded in some of the earliest astronomical records. Its displacement from the ecliptic causes its heliacal rising to be remarkably regular compared to other stars, with a period of almost exactly 365.25 days holding it constant relative to the solar year. This rising occurs at Cairo on 19July (Julian), placing it just before the onset of the annual flooding of the Nile during antiquity. Owing to the flood's own irregularity, the extreme precision of the star's return made it important to the ancient Egyptians, who worshipped it as the goddess Sopdet (, \"Triangle\"; , Sō̂this), guarantor of the fertility of their land.",
                    "score": 0.8781371116638184
                },
                {
                    "id": 1242484,
                    "contents": "Sirius\nMore recent (and accurate) astrometric observations by the Hubble Space Telescope ruled out the existence of such a Sirius C entirely. The 1995 study predicted an astrometric movement of roughly 90 mas (0.09 arcsecond), but Hubble was unable to detect any location anomaly to an accuracy of 5 mas (0.005 arcsec). This ruled out any objects orbiting Sirius A with more than 0.033 solar mass (35 Jupiter masses) orbiting in 0.5 years, and 0.014 (15 Jupiter masses) in 2 years. The study was also able to rule out any companions to Sirius B with more than 0.024 solar mass (25 Jupiter masses) orbiting in 0.5 year, and 0.0095 (10 Jupiter masses) orbiting in 1.8 years. Effectively, there are almost certainly no additional bodies in the Sirius system larger than a small brown dwarf or large exoplanet.",
                    "score": 0.8780930042266846
                },
                {
                    "id": 3145038,
                    "contents": "Gomez's Hamburger\nGomez's Hamburger, also known as IRAS 18059-3211, is believed to be a young star surrounded by a protoplanetary disk. It was initially identified as a planetary nebula, and its distance was estimated to be approximately 6500 light-years away from Earth. However, recent results suggest that this object is a young star surrounded by a protoplanetary disk, at a distance of about 900 light-years away. It was discovered in 1985 on sky photographs obtained by Arturo Gómez, support technical staff at the Cerro Tololo Inter-American Observatory near Vicuña, Chile. The photos suggested that there was a dark band across the object, but its exact structure was difficult to determine because of the atmospheric turbulence that hampers all images taken from the ground. The star itself has a surface temperature of approximately 10,000 K.",
                    "score": 0.877657949924469
                },
                {
                    "id": 1242466,
                    "contents": "Sirius\nThe visible star is now sometimes known as Sirius A. Since 1894, some apparent orbital irregularities in the Sirius system have been observed, suggesting a third very small companion star, but this has never been confirmed. The best fit to the data indicates a six-year orbit around Sirius A and a mass of . This star would be five to ten magnitudes fainter than the white dwarf Sirius B, which would make it difficult to observe. Observations published in 2008 were unable to detect either a third star or a planet. An apparent \"third star\" observed in the 1920s is now believed to be a background object.",
                    "score": 0.8773756623268127
                },
                {
                    "id": 23444741,
                    "contents": "V4998 Sagittarii\nObservational history The star was first discovered in a 1993 survey that searched for bright near-infrared sources within 0.55°2 of the galactic centre. The survey used 1–20 micron photometry and used a two channel InSb detector on the 1 meter ANU telescope in Australia. 50 objects were targeted and most of them had bolometric magnitudes below -5. The star itself was observed in May 1987. Its position, JHKLNMQ magnitudes, right ascension, declination, and silicate absorption were noted. The survey was conducted by Tetsuya Nagata, A. R. Hyland, S. M. Straw, Shuji Sato, and Kimiaki Kawara. The survey named the star NHS93 22; NHS standing for the three leading scientists in the survey (Nagata, Hyland, and Straw), 93 signifying the date of discovery, and 22 indicating that it was the 22nd star observed.",
                    "score": 0.8768405914306641
                },
                {
                    "id": 1011012,
                    "contents": "Astronomy\nStellar astronomy The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star. Almost all elements heavier than hydrogen and helium were created inside the cores of stars.",
                    "score": 0.8766177296638489
                },
                {
                    "id": 17724890,
                    "contents": "1 Scorpii\n1 Scorpii is a young star at around 10 million years old, with 8.3 times the mass of the Sun and 3.7 times the Sun's radius. The star is radiating 3,890 times the Sun's luminosity from its photosphere at an effective temperature of about 24,000 K. References B-type main-sequence stars Scorpius (constellation) Scorpii, b Durchmusterung objects Scorpii 1 141637 077635 5885",
                    "score": 0.8763822317123413
                },
                {
                    "id": 602674,
                    "contents": "Supergiant\nSurface gravity The supergiant luminosity class is assigned on the basis of spectral features that are largely a measure of surface gravity, although such stars are also affected by other properties such as microturbulence. Supergiants typically have surface gravities of around log(g) 2.0 cgs and lower, although bright giants (luminosity class II) have statistically very similar surface gravities to normal Ib supergiants. Cool luminous supergiants have lower surface gravities, with the most luminous (and unstable) stars having log(g) around zero. Hotter supergiants, even the most luminous, have surface gravities around one, due to their higher masses and smaller radii.",
                    "score": 0.8763440251350403
                },
                {
                    "id": 602658,
                    "contents": "Supergiant\nSupergiant stars can be identified on the basis of their spectra, with distinctive lines sensitive to high luminosity and low surface gravity. In 1897, Antonia C. Maury had divided stars based on the widths of their spectral lines, with her class \"c\" identifying stars with the narrowest lines. Although it was not known at the time, these were the most luminous stars. In 1943, Morgan and Keenan formalised the definition of spectral luminosity classes, with class I referring to supergiant stars. The same system of MK luminosity classes is still used today, with refinements based on the increased resolution of modern spectra. Supergiants occur in every spectral class from young blue class O supergiants to highly evolved red class M supergiants. Because they are enlarged compared to main-sequence and giant stars of the same spectral type, they have lower surface gravities, and changes can be observed in their line profiles. Supergiants are also evolved stars with higher levels of",
                    "score": 0.8761758208274841
                },
                {
                    "id": 14160709,
                    "contents": "1258 Sicilia\n1258 Sicilia, provisional designation , is a dark background asteroid from the outer regions of the asteroid belt, approximately 44 kilometers in diameter. It was discovered on 8 August 1932, by astronomer Karl Reinmuth at the Heidelberg-Königstuhl State Observatory in southwest Germany. The asteroid was named after the Italian island of Sicily. Orbit and classification Sicilia is a non-family asteroid of the main belt's background population. It orbits the Sun in the outer asteroid belt at a distance of 3.0–3.3 AU once every 5 years and 8 months (2,076 days; semi-major axis of 3.19 AU). Its orbit has an eccentricity of 0.04 and an inclination of 8° with respect to the ecliptic. The body's observation arc begins with its official discovery observation at Heidelberg in 1932. Physical characteristics Sicilia is an assumed carbonaceous C-type asteroid. Rotation period",
                    "score": 0.8760440349578857
                },
                {
                    "id": 4966529,
                    "contents": "Denebola\nDenebola, along with Spica and Arcturus, is part of the Spring Triangle asterism, and by extension, also of the Great Diamond together with the star Cor Caroli. Properties Denebola is a relatively young star with an age estimated at less than 400 million years. Interferometric observations give a radius that is about 173% that of the Sun. However, the high rate of rotation results in an oblate shape with an equatorial bulge. It has 75% more mass than the Sun, which results in a much higher overall luminosity and a shorter life span on the main sequence.",
                    "score": 0.8757309913635254
                },
                {
                    "id": 8326437,
                    "contents": "Upsilon1 Cancri\n{{DISPLAYTITLE:Upsilon1 Cancri}} Upsilon1 Cancri, Latinised from υ1 Cancri, is the Bayer designation for a solitary, yellow-white-hued star in the constellation Cancer. It is faintly visible with the naked eye, having an apparent visual magnitude of +5.7. Based upon an annual parallax shift of 13.05 mas as seen from Earth, this system is roughly 250 light-years from the Sun. This object has a stellar classification of F0 IIIn, indicating it is an F-type giant star. The 'n' suffix indicates \"nebulous\" absorption lines due to rapid rotation, and it shows a relatively high projected rotational velocity of 109.2 km/s. It is a variable star of unknown type that varies in brightness with an amplitude of 0.05 magnitude. The star is about 570 million years old and it has an estimated mass of 1.47 times that of the Sun. On average, it is radiating 25 times the Sun's luminosity from its photosphere at an effective temperature of . References",
                    "score": 0.8756389617919922
                },
                {
                    "id": 18135314,
                    "contents": "HD 36960\nAt over 15 solar masses, it shines with around 20,000 times the Sun's luminosity due to its high surface temperature of 29,000 K and radius over five times that of the sun. It is calculated to be around six million years old, consistent with other stars thought to be members of NGC 1980. References Orion (constellation) B-type main-sequence stars 036960 1887 Orionis, 103 026199 Durchmusterung objects",
                    "score": 0.8750985860824585
                },
                {
                    "id": 7481933,
                    "contents": "Iota1 Scorpii\n{{DISPLAYTITLE:Iota1 Scorpii}} Iota1 Scorpii, Latinized from ι1 Scorpii, is star in the southern constellation of Scorpius. With an apparent visual magnitude of 3.03, this star can be seen with the naked eye. It is sometimes called by the proper name Apollyon. Parallax measurements place it at a distance of roughly from Earth, with a 9% margin of error. This star has a stellar classification of F2 Ia, with the 'Ia' luminosity class indicating this is a supergiant more luminous than typical supergiants. It has about 12 times the Sun's mass and is radiating about 35,070 times the Sun's luminosity. The radius is uncertain, with estimates ranging from 125 to 400 times that of the Sun. The effective temperature of the outer envelope is about 7,000 K, which gives it a yellow-white hue typical of an F-type star.",
                    "score": 0.8750840425491333
                },
                {
                    "id": 10097360,
                    "contents": "2000 Herschel\nThe sequence continues with the asteroids 5000 IAU (for the International Astronomical Union), 6000 United Nations (for the United Nations), 7000 Curie (for the pioneers on radioactivity, Marie and Pierre Curie), and (for Isaac Newton), while 9000 Hal (after HAL 9000 from 2001: A Space Odyssey) and 10000 Myriostos (after the Greek word for ten-thousandth, which is meant to honor all astronomers) were named based on their direct numeric accordance. Physical characteristics In the Tholen classification, Herschel is a common S-type asteroid. Slow rotator and tumbler Analysis of the lightcurve for this object appears to show that it is tumbling, with rotation occurring about the non-principal axis. Lightcurve analysis gave a rotation period of hours with a high brightness variation of magnitude (). This makes it a slow rotator. Diameter and albedo",
                    "score": 0.8750454783439636
                },
                {
                    "id": 1219848,
                    "contents": "Sun\nWithin of the Sun there are 315 known stars in 227 systems, as of 2000, including 163 single stars. It is estimated that a further 130 systems within this range have not yet been identified. Out to , there may be up to 7,500 stars, of which around 2,600 are known. The number of substellar objects in that volume are expected to be comparable to the number of stars. Of the 50 nearest stellar systems within 17 light-years from Earth (the closest being the red dwarf Proxima Centauri at approximately 4.2 light-years), the Sun ranks fourth in mass. The Gaia Catalogue of Nearby Stars, all within 100 parsecs, contains 331,312 stars and is thought to include at least 92% of the stars of stellar spectral type M9 or \"earlier\" (i.e. hotter). Theoretical problems Coronal heating problem",
                    "score": 0.8747804164886475
                },
                {
                    "id": 24915884,
                    "contents": "39 Tauri\nThe space velocity components of 39 Tauri are: –25.0(U), –14.0(V), –6.0(W). The surface activity and kinematic properties of this star are consistent with membership in the IC 2391 moving group. It is following an orbit through the Milky Way galaxy that has an eccentricity of 0.06 carrying it as close as to the Galactic Core, and as far away as . The orbital inclination will carry the star no further than away from the galactic plane. References Taurus (constellation) Tauri, 039 G-type main-sequence stars Astrometric binaries Tauri, A2 1262 025680 019076 Durchmusterung objects Gliese and GJ objects",
                    "score": 0.8747398257255554
                },
                {
                    "id": 335214,
                    "contents": "Timeline of the early universe\n200–300 million years: First stars begin to shine: Because many are Population III stars (some Population II stars are accounted for at this time) they are much bigger and hotter and their life-cycle is fairly short. Unlike later generations of stars, these stars are metal free. As reionization intensifies, photons of light scatter off free protons and electrons – Universe becomes opaque again. 200 million years: HD 140283, the \"Methuselah\" Star, formed, the unconfirmed oldest star observed in the Universe. Because it is a Population II star, some suggestions have been raised that second generation star formation may have begun very early on. The oldest-known star (confirmed) – SMSS J031300.36-670839.3, forms.",
                    "score": 0.874732255935669
                },
                {
                    "id": 2307908,
                    "contents": "Nommo\nwas in the 1940s, giving the Dogon ample opportunity to gain cosmological knowledge about Sirius and the Solar System from more scientifically advanced, terrestrial societies whom they had come in contact with. It has also been pointed out that binary star systems like Sirius are theorized to have a very narrow or non-existent Habitable zone, and thus a high improbability of containing a planet capable of sustaining life (particularly life as dependent on water as the Nommos were reported to be).",
                    "score": 0.8746638894081116
                },
                {
                    "id": 7710612,
                    "contents": "Gliese 581\nHistory of observations Gliese 581 is known at least from 1886, when it was included in Eduard Schönfeld's Southern (SD)—the fourth part of the . The corresponding designation is BD -7 4003. Characteristics The name Gliese 581 refers to the catalog number from the 1957 survey Gliese Catalogue of Nearby Stars of 965 stars located within 20 parsecs of the Earth. Other names of this star include BD-07° 4003 (BD catalogue, first known publication) and HO Librae (variable star designation). It does not have an individual name such as Sirius or Procyon. The star is a red dwarf with spectral type M3V, located 20.4 light-years away from Earth. It is located about two degrees north of Beta Librae, the brightest star in the Libra constellation. Its mass is estimated to be approximately a third that of the Sun, and it is the 89th closest known star system to the Sun.",
                    "score": 0.8745887875556946
                },
                {
                    "id": 8203176,
                    "contents": "789 Lena\nPhysical characteristics In the SMASS taxonomy, Lena is an X-type asteroid. It has also been characterized as a metallic M-type asteroid by NASA's Wide-field Infrared Survey Explorer (WISE). Rotation period In 1993, a rotational lightcurve which was later proven incorrect, was obtained from photometric observations at the Félix Aguilar Observatory, Argentina. It gave an unusual lightcurve, indicating a very irregular shape and/or a relatively long rotation period of 22 hours with an exceptionally high amplitude of 1.5 in magnitude (). In August and September 2007, two reliable lightcurves were obtained by Italian astronomer Silvano Casulli and by members at the U.S.Oakley Observatory. Lightcurve analysis gave a period of and hours, with a brightness variation of 0.50 and of 0.40 magnitude, respectively (). Diameter and albedo",
                    "score": 0.8744477033615112
                },
                {
                    "id": 1192092,
                    "contents": "Procyon\nAt , Procyon B is considerably less massive than Sirius B; however, the peculiarities of degenerate matter ensure that it is larger than its more famous neighbor, with an estimated radius of 8,600 km, versus 5,800 km for Sirius B. The radius agrees with white dwarf models that assume a carbon core. It has a stellar classification of DQZ, having a helium-dominated atmosphere with traces of heavy elements. For reasons that remain unclear, the mass of Procyon B is unusually low for a white dwarf star of its type. With a surface temperature of , it is also much cooler than Sirius B; this is a testament to its lesser mass and greater age. The mass of the progenitor star for Procyon B was about and it came to the end of its life some billion years ago, after a main-sequence lifetime of million years.",
                    "score": 0.8740875124931335
                },
                {
                    "id": 14160470,
                    "contents": "2223 Sarpedon\nPhysical characteristics In the Tholen classification, Sarpedon is similar to a dark D-type asteroid, though with an unusual spectrum (DU). Rotation period In April 1996, a rotational lightcurve of Sarpedon was obtained from photometric observations by Italian astronomer Stefano Mottola at ESO's La Silla Observatory using the Bochum 0.61-metre Telescope. Lightcurve analysis gave a rotation period of 22.741 hours with a brightness amplitude of 0.14 magnitude (). A previous observation by Mottola gave a similar period of 22.77 hours from a lower-rated lightcurve (). Diameter and albedo According to the surveys carried out by the Infrared Astronomical Satellite IRAS, the Japanese Akari satellite and the NEOWISE mission of NASA's Wide-field Infrared Survey Explorer, Sarpedon measures between 77.48 and 108.21 kilometers in diameter and its surface has an albedo between 0.027 and 0.051.",
                    "score": 0.8740668296813965
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_34",
        "question": "A ground-state hydrogen atom absorbs a photon of light that has a wavelength of $97.2 \\mathrm{~nm}$. It then gives off a photon that has a wavelength of $486 \\mathrm{~nm}$. What is the final state of the hydrogen atom?",
        "golden_answers": [
            " 2"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 3384394,
                    "contents": "Excited state\nAfter excitation the atom may return to the ground state or a lower excited state, by emitting a photon with a characteristic energy. Emission of photons from atoms in various excited states leads to an electromagnetic spectrum showing a series of characteristic emission lines (including, in the case of the hydrogen atom, the Lyman, Balmer, Paschen and Brackett series.) An atom in a high excited state is termed a Rydberg atom. A system of highly excited atoms can form a long-lived condensed excited state e.g. a condensed phase made completely of excited atoms: Rydberg matter. Hydrogen can also be excited by heat or electricity.",
                    "score": 0.9211711287498474
                },
                {
                    "id": 1753311,
                    "contents": "Hydrogen\nElectron energy levels The ground state energy level of the electron in a hydrogen atom is −13.6 eV, which is equivalent to an ultraviolet photon of roughly 91 nm wavelength. The energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, which conceptualizes the electron as \"orbiting\" the proton in analogy to the Earth's orbit of the Sun. However, the atomic electron and proton are held together by electromagnetic force, while planets and celestial objects are held by gravity. Because of the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies.",
                    "score": 0.9072400331497192
                },
                {
                    "id": 1772927,
                    "contents": "Hydrogen atom\nAtomic spectroscopy shows that there is a discrete infinite set of states in which a hydrogen (or any) atom can exist, contrary to the predictions of classical physics. Attempts to develop a theoretical understanding of the states of the hydrogen atom have been important to the history of quantum mechanics, since all other atoms can be roughly understood by knowing in detail about this simplest atomic structure. Isotopes The most abundant isotope, hydrogen-1, protium, or light hydrogen, contains no neutrons and is simply a proton and an electron. Protium is stable and makes up 99.985% of naturally occurring hydrogen atoms. Deuterium contains one neutron and one proton in its nucleus. Deuterium is stable and makes up 0.0156% of naturally occurring hydrogen and is used in industrial processes like nuclear reactors and Nuclear Magnetic Resonance.",
                    "score": 0.9069464802742004
                },
                {
                    "id": 16471308,
                    "contents": "Triatomic hydrogen\nThe molecule can only exist in an excited state. The different excited electronic states are represented by symbols for the outer electron nLΓ with n the principal quantum number, L is the electronic angular momentum, and Γ is the electronic symmetry selected from the D3h group. Extra bracketed symbols can be attached showing vibration in the core: {s,dl} with s representing symmetrical stretch, d degenerate mode, and l vibrational angular momentum. Yet another term can be inserted to indicate molecular rotation: (N,G) with N angular momentum apart from electrons as projected on the molecular axis, and G the Hougen's convenient quantum number determined by G=l+λ-K. This is often (1,0), as the rotational states are restricted by the constituent particles all being fermions. Examples of these states are: 2sA1' 3sA1' 2pA2\" 3dE' 3DE\" 3dA1' 3pE' 3pA2\". The 2p2A2\" state has a lifetime of 700 ns. If the molecule attempts to lose energy and go to the repulsive ground state, it spontaneously",
                    "score": 0.9037353992462158
                },
                {
                    "id": 1772939,
                    "contents": "Hydrogen atom\nAs discussed below, the ground state is also indicated by the quantum numbers . The second lowest energy states, just above the ground state, are given by the quantum numbers , , and . These states all have the same energy and are known as the and states. There is one state: and there are three states: An electron in the or state is most likely to be found in the second Bohr orbit with energy given by the Bohr formula. Wavefunction The Hamiltonian of the hydrogen atom is the radial kinetic energy operator and Coulomb attraction force between the positive proton and negative electron. Using the time-independent Schrödinger equation, ignoring all spin-coupling interactions and using the reduced mass , the equation is written as: Expanding the Laplacian in spherical coordinates:",
                    "score": 0.9010018706321716
                },
                {
                    "id": 18453849,
                    "contents": "Mihai Gavrilă\nthe hydrogen atom's charge distribution takes on a toroidal shape with its symmetry axis oriented along the propagation vector of the field and passing through the center of the atom. His theory also predicts for two-electron atoms the appearance of a new bound state which is induced by the ultra-intense laser field; these are 'light-induced excited states'. Apparently paradoxical events do occur in the presence of the extremely intense laser field: a proton can bind more than two electrons thus leading to the formation of hydrogen negative ions with multiple negative charges that are relatively stable. Other novel and unexpected properties of molecules were also predicted in the presence of such ultra-intense laser fields.",
                    "score": 0.9000355005264282
                },
                {
                    "id": 7748008,
                    "contents": "Introduction to quantum mechanics\nEach photon from glowing atomic hydrogen is due to an electron moving from a higher orbit, with radius , to a lower orbit, . The energy of this photon is the difference in the energies and of the electron: Since Planck's equation shows that the photon's energy is related to its wavelength by , the wavelengths of light that can be emitted are given by This equation has the same form as the Rydberg formula, and predicts that the constant should be given by Therefore, the Bohr model of the atom can predict the emission spectrum of hydrogen in terms of fundamental constants. However, it was not able to make accurate predictions for multi-electron atoms, or to explain why some spectral lines are brighter than others. Wave–particle duality Just as light has both wave-like and particle-like properties, matter also has wave-like properties.",
                    "score": 0.8992483615875244
                },
                {
                    "id": 3565883,
                    "contents": "Hydrogen line\nThe ground state of neutral hydrogen consists of an electron bound to a proton. Both the electron and the proton have intrinsic magnetic dipole moments ascribed to their spin, whose interaction results in a slight increase in energy when the spins are parallel, and a decrease when antiparallel. The fact that only parallel and antiparallel states are allowed is a result of the quantum mechanical discretization of the total angular momentum of the system. When the spins are parallel, the magnetic dipole moments are antiparallel (because the electron and proton have opposite charge), thus one would expect this configuration to actually have lower energy just as two magnets will align so that the north pole of one is closest to the south pole of the other. This logic fails here because the wave functions of the electron and the proton overlap; that is, the electron is not spatially displaced from the proton, but encompasses it. The magnetic dipole moments are therefore best thought of as",
                    "score": 0.8974394798278809
                },
                {
                    "id": 1772933,
                    "contents": "Hydrogen atom\nFor , the value is called the Rydberg unit of energy. It is related to the Rydberg constant of atomic physics by The exact value of the Rydberg constant assumes that the nucleus is infinitely massive with respect to the electron. For hydrogen-1, hydrogen-2 (deuterium), and hydrogen-3 (tritium) which have finite mass, the constant must be slightly modified to use the reduced mass of the system, rather than simply the mass of the electron. This includes the kinetic energy of the nucleus in the problem, because the total (electron plus nuclear) kinetic energy is equivalent to the kinetic energy of the reduced mass moving with a velocity equal to the electron velocity relative to the nucleus. However, since the nucleus is much heavier than the electron, the electron mass and reduced mass are nearly the same. The Rydberg constant RM for a hydrogen atom (one electron), R is given by",
                    "score": 0.8965803980827332
                },
                {
                    "id": 1753334,
                    "contents": "Hydrogen\nOne of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.",
                    "score": 0.8925305604934692
                },
                {
                    "id": 1753312,
                    "contents": "Hydrogen\nA more accurate description of the hydrogen atom comes from a purely quantum mechanical treatment that uses the Schrödinger equation, Dirac equation or Feynman path integral formulation to calculate the probability density of the electron around the proton. The most complicated treatments allow for the small effects of special relativity and vacuum polarization. In the quantum mechanical treatment, the electron in a ground state hydrogen atom has no angular momentum at all—illustrating how the \"planetary orbit\" differs from electron motion. Spin isomers",
                    "score": 0.8916497230529785
                },
                {
                    "id": 1772942,
                    "contents": "Hydrogen atom\nwhich, for the bound states, results in where denotes a Gegenbauer polynomial and is in units of . The solutions to the Schrödinger equation for hydrogen are analytical, giving a simple expression for the hydrogen energy levels and thus the frequencies of the hydrogen spectral lines and fully reproduced the Bohr model and went beyond it. It also yields two other quantum numbers and the shape of the electron's wave function (\"orbital\") for the various possible quantum-mechanical states, thus explaining the anisotropic character of atomic bonds. The Schrödinger equation also applies to more complicated atoms and molecules. When there is more than one electron or nucleus the solution is not analytical and either computer calculations are necessary or simplifying assumptions must be made.",
                    "score": 0.8910050392150879
                },
                {
                    "id": 16471318,
                    "contents": "Triatomic hydrogen\nGerhard Herzberg was the first to actually observe the spectrum of neutral H3, and this triatomic molecule was the first to have a Rydberg spectrum measured where its own ground state was unstable. See also F. M. Devienne, one of the first to have studied the energy properties of Triatomic hydrogen References External links C. H. Green: \"Rydberg states of Triatomic Hydrogen\" The Elusive H3 Molecule GlusteeXD 2009 (humour) The one-electron reduced density matrix convex set is pinned to the boundary of the pure N-representable set predicts H3 lasers may exist in the early universe Hydrogen Allotropes Homonuclear triatomic molecules",
                    "score": 0.8892719149589539
                },
                {
                    "id": 431660,
                    "contents": "Rydberg constant\nRydberg frequency Rydberg wavelength . The angular wavelength is . Occurrence in Bohr model The Bohr model explains the atomic spectrum of hydrogen (see hydrogen spectral series) as well as various other atoms and ions. It is not perfectly accurate, but is a remarkably good approximation in many cases, and historically played an important role in the development of quantum mechanics. The Bohr model posits that electrons revolve around the atomic nucleus in a manner analogous to planets revolving around the sun. In the simplest version of the Bohr model, the mass of the atomic nucleus is considered to be infinite compared to the mass of the electron, so that the center of mass of the system, the barycenter, lies at the center of the nucleus. This infinite mass approximation is what is alluded to with the subscript. The Bohr model then predicts that the wavelengths of hydrogen atomic transitions are (see Rydberg formula):",
                    "score": 0.8891893029212952
                },
                {
                    "id": 8698636,
                    "contents": "Rydberg state\nThe Rydberg states of an atom or molecule are electronically excited states with energies that follow the Rydberg formula as they converge on an ionic state with an ionization energy. Although the Rydberg formula was developed to describe atomic energy levels, it has been used to describe many other systems that have electronic structure roughly similar to atomic hydrogen. In general, at sufficiently high principal quantum numbers, an excited electron - ionic core system will have the general character of a hydrogenic system and the energy levels will follow the Rydberg formula. Rydberg states have energies converging on the energy of the ion. The ionization energy threshold is the energy required to completely liberate an electron from the ionic core of an atom or molecule. In practice, a Rydberg wave packet is created by a laser pulse on a hydrogenic atom and thus populates a superposition of Rydberg states. Modern investigations using pump-probe experiments show molecular pathways",
                    "score": 0.8887295126914978
                },
                {
                    "id": 13333103,
                    "contents": "Hydrogen-like atom\nAn electron in the vicinity of a nucleus necessarily has non-zero amplitudes for the third and fourth components. Far from the nucleus these may be small, but near the nucleus they become large.",
                    "score": 0.8880159854888916
                },
                {
                    "id": 21819314,
                    "contents": "Gerda Laski\nHer early research concerned extensions of the Bohr model of hydrogen atoms to hydrogen molecules. Laski was a student of Peter Debye, who was awarded the Nobel Prize in chemistry in 1936. Debye studied the dispersion of light by Bohr's hydrogen model and found that the theoretical curve corresponded satisfactorily to the curve observed. Laski later showed agreement between theory and experiment, however based on an erroneous interpretation of data.",
                    "score": 0.8879172205924988
                },
                {
                    "id": 431658,
                    "contents": "Rydberg constant\nThe constant is expressed for either hydrogen as , or at the limit of infinite nuclear mass as . In either case, the constant is used to express the limiting value of the highest wavenumber (inverse wavelength) of any photon that can be emitted from an atom, or, alternatively, the wavenumber of the lowest-energy photon capable of ionizing an atom from its ground state. The hydrogen spectral series can be expressed simply in terms of the Rydberg constant for hydrogen and the Rydberg formula. In atomic physics, Rydberg unit of energy, symbol Ry, corresponds to the energy of the photon whose wavenumber is the Rydberg constant, i.e. the ionization energy of the hydrogen atom in a simplified Bohr model. Value Rydberg constant The CODATA value is , where is the rest mass of the electron, is the elementary charge, is the permittivity of free space, is the Planck constant, and is the speed of light in vacuum.",
                    "score": 0.8873997926712036
                },
                {
                    "id": 1668669,
                    "contents": "Diatomic molecule\nDiatomic molecules are normally in their lowest or ground state, which conventionally is also known as the state. When a gas of diatomic molecules is bombarded by energetic electrons, some of the molecules may be excited to higher electronic states, as occurs, for example, in the natural aurora; high-altitude nuclear explosions; and rocket-borne electron gun experiments. Such excitation can also occur when the gas absorbs light or other electromagnetic radiation. The excited states are unstable and naturally relax back to the ground state. Over various short time scales after the excitation (typically a fraction of a second, or sometimes longer than a second if the excited state is metastable), transitions occur from higher to lower electronic states and ultimately to the ground state, and in each transition results a photon is emitted. This emission is known as fluorescence. Successively higher electronic states are conventionally named , , , etc. (but this convention is not always",
                    "score": 0.8868104219436646
                },
                {
                    "id": 1772951,
                    "contents": "Hydrogen atom\nEnergy levels The energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression: where is the fine-structure constant and is the total angular momentum quantum number, which is equal to , depending on the orientation of the electron spin relative to the orbital angular momentum. This formula represents a small correction to the energy obtained by Bohr and Schrödinger as given above. The factor in square brackets in the last expression is nearly one; the extra term arises from relativistic effects (for details, see #Features going beyond the Schrödinger solution). It is worth noting that this expression was first obtained by A. Sommerfeld in 1916 based on the relativistic version of the old Bohr theory. Sommerfeld has however used different notation for the quantum numbers. Coherent states The coherent states have been proposed as which satisfies and takes the form",
                    "score": 0.8866449594497681
                },
                {
                    "id": 2666431,
                    "contents": "Lyman series\nIn physics and chemistry, the Lyman series is a hydrogen spectral series of transitions and resulting ultraviolet emission lines of the hydrogen atom as an electron goes from n ≥ 2 to n = 1 (where n is the principal quantum number), the lowest energy level of the electron. The transitions are named sequentially by Greek letters: from n = 2 to n = 1 is called Lyman-alpha, 3 to 1 is Lyman-beta, 4 to 1 is Lyman-gamma, and so on. The series is named after its discoverer, Theodore Lyman. The greater the difference in the principal quantum numbers, the higher the energy of the electromagnetic emission. History",
                    "score": 0.8866299986839294
                },
                {
                    "id": 1113706,
                    "contents": "Energy level\nIf an atom, ion, or molecule is at the lowest possible energy level, it and its electrons are said to be in the ground state. If it is at a higher energy level, it is said to be excited, or any electrons that have higher energy than the ground state are excited. Such a species can be excited to a higher energy level by absorbing a photon whose energy is equal to the energy difference between the levels. Conversely, an excited species can go to a lower energy level by spontaneously emitting a photon equal to the energy difference. A photon's energy is equal to Planck's constant () times its frequency () and thus is proportional to its frequency, or inversely to its wavelength (). , since , the speed of light, equals to",
                    "score": 0.8862739205360413
                },
                {
                    "id": 13333092,
                    "contents": "Hydrogen-like atom\nA hydrogen-like atom (or \"hydrogenic atom\") is any atom or ion with a single valence electron. These atoms are isoelectronic with hydrogen. Examples of hydrogen-like atoms include, but are not limited to, hydrogen itself, all alkali metals such as Rb and Cs, singly ionized alkaline earth metals such as Ca+ and Sr+ and other ions such as Li2+ and Be3+. A hydrogen-like atom includes a positively charged core consisting of the atomic nucleus and any core electrons as well as a single valence electron. The non-relativistic Schrödinger equation and relativistic Dirac equation for the hydrogen atom can be solved analytically, owing to the simplicity of the two-particle physical system. The one-electron wave function solutions are referred to as hydrogen-like atomic orbitals. Hydrogen-like atoms are of importance because their corresponding orbitals bear similarity to the hydrogen atomic orbitals.",
                    "score": 0.886064887046814
                },
                {
                    "id": 13333116,
                    "contents": "Hydrogen-like atom\nSee also Rydberg atom Positronium Exotic atom Two-electron atom Hydrogen molecular ion Notes References Tipler, Paul & Ralph Llewellyn (2003). Modern Physics (4th ed.). New York: W. H. Freeman and Company. Atoms Quantum mechanics Hydrogen",
                    "score": 0.8854066133499146
                },
                {
                    "id": 1772953,
                    "contents": "Hydrogen atom\nThe \"ground state\", i.e. the state of lowest energy, in which the electron is usually found, is the first one, the 1s state (principal quantum level n = 1, ℓ = 0). Black lines occur in each but the first orbital: these are the nodes of the wavefunction, i.e. where the probability density is zero. (More precisely, the nodes are spherical harmonics that appear as a result of solving the Schrödinger equation in spherical coordinates.) The quantum numbers determine the layout of these nodes. There are: total nodes, of which are angular nodes: angular nodes go around the axis (in the xy plane). (The figure above does not show these nodes since it plots cross-sections through the xz-plane.) (the remaining angular nodes) occur on the (vertical) axis. (the remaining non-angular nodes) are radial nodes.",
                    "score": 0.8852053880691528
                },
                {
                    "id": 1694118,
                    "contents": "Electron\nof an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction between multiple electrons were describable, quantum mechanics made it possible to predict the configuration of electrons in atoms with atomic numbers greater than hydrogen.",
                    "score": 0.8842533826828003
                },
                {
                    "id": 1560697,
                    "contents": "Atom\nIf a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band. Valence and bonding behavior",
                    "score": 0.8833162784576416
                },
                {
                    "id": 7748035,
                    "contents": "Introduction to quantum mechanics\nSchrödinger was able to calculate the energy levels of hydrogen by treating a hydrogen atom's electron as a wave, represented by the \"wave function\" , in an electric potential well, , created by the proton. The solutions to Schrödinger's equation are distributions of probabilities for electron positions and locations. Orbitals have a range of different shapes in three dimensions. The energies of the different orbitals can be calculated, and they accurately match the energy levels of the Bohr model. Within Schrödinger's picture, each electron has four properties: An \"orbital\" designation, indicating whether the particle-wave is one that is closer to the nucleus with less energy or one that is farther from the nucleus with more energy; The \"shape\" of the orbital, spherical or otherwise; The \"inclination\" of the orbital, determining the magnetic moment of the orbital around the -axis. The \"spin\" of the electron.",
                    "score": 0.8831034898757935
                },
                {
                    "id": 13333107,
                    "contents": "Hydrogen-like atom\n(The energy of course depends on the zero-point used.) Note that if were able to be more than 137 (higher than any known element) then we would have a negative value inside the square root for the S1/2 and P1/2 orbitals, which means they would not exist. The Schrödinger solution corresponds to replacing the inner bracket in the second expression by 1. The accuracy of the energy difference between the lowest two hydrogen states calculated from the Schrödinger solution is about 9 ppm (90 μeV too low, out of around 10 eV), whereas the accuracy of the Dirac equation for the same energy difference is about 3 ppm (too high). The Schrödinger solution always puts the states at slightly higher energies than the more accurate Dirac equation. The Dirac equation gives some levels of hydrogen quite accurately (for instance the 4P1/2 state is given an energy only about eV too high), others less so (for instance, the 2S1/2 level is about eV too low). The modifications of the energy due to using",
                    "score": 0.8830464482307434
                },
                {
                    "id": 16471309,
                    "contents": "Triatomic hydrogen\nstates are: 2sA1' 3sA1' 2pA2\" 3dE' 3DE\" 3dA1' 3pE' 3pA2\". The 2p2A2\" state has a lifetime of 700 ns. If the molecule attempts to lose energy and go to the repulsive ground state, it spontaneously breaks up. The lowest energy metastable state, 2sA1' has an energy -3.777 eV below the and e− state but decays in around 1 ps. The unstable ground state designated 2p2E' spontaneously breaks up into a H2 molecule and an H atom. Rotationless states have a longer life time than rotating molecules.",
                    "score": 0.8829009532928467
                },
                {
                    "id": 1772929,
                    "contents": "Hydrogen atom\nIf a neutral hydrogen atom loses its electron, it becomes a cation. The resulting ion, which consists solely of a proton for the usual isotope, is written as \"H+\" and sometimes called hydron. Free protons are common in the interstellar medium, and solar wind. In the context of aqueous solutions of classical Brønsted–Lowry acids, such as hydrochloric acid, it is actually hydronium, H3O+, that is meant. Instead of a literal ionized single hydrogen atom being formed, the acid transfers the hydrogen to H2O, forming H3O+. If instead a hydrogen atom gains a second electron, it becomes an anion. The hydrogen anion is written as \"H–\" and called hydride. Theoretical analysis The hydrogen atom has special significance in quantum mechanics and quantum field theory as a simple two-body problem physical system which has yielded many simple analytical solutions in closed-form.",
                    "score": 0.882450520992279
                },
                {
                    "id": 1753305,
                    "contents": "Hydrogen\nHydrogen is the chemical element with the symbol H and atomic number 1. Hydrogen is the lightest element. At standard conditions hydrogen is a gas of diatomic molecules having the formula H2. It is colorless, odorless, tasteless, non-toxic, and highly combustible. Hydrogen is the most abundant chemical substance in the universe, constituting roughly 75% of all normal matter. Stars such as the Sun are mainly composed of hydrogen in the plasma state. Most of the hydrogen on Earth exists in molecular forms such as water and organic compounds. For the most common isotope of hydrogen (symbol 1H) each atom has one proton, one electron, and no neutrons. In the early universe, the formation of protons, the nuclei of hydrogen, occurred during the first second after the Big Bang. The emergence of neutral hydrogen atoms throughout the universe occurred about 370,000 years later during the recombination epoch, when the plasma had cooled enough for electrons to remain bound to protons.",
                    "score": 0.8823515176773071
                },
                {
                    "id": 3384393,
                    "contents": "Excited state\nLong-lived excited states are often called metastable. Long-lived nuclear isomers and singlet oxygen are two examples of this. Atomic excitation A simple example of this concept comes by considering the hydrogen atom. The ground state of the hydrogen atom corresponds to having the atom's single electron in the lowest possible orbital (that is, the spherically symmetric \"1s\" wave function, which, so far, has demonstrated to have the lowest possible quantum numbers). By giving the atom additional energy (for example, by the absorption of a photon of an appropriate energy), the electron is able to move into an excited state (one with one or more quantum numbers greater than the minimum possible). If the photon has too much energy, the electron will cease to be bound to the atom, and the atom will become ionized.",
                    "score": 0.8818955421447754
                },
                {
                    "id": 1113695,
                    "contents": "Energy level\nThis equation is obtained from combining the Rydberg formula for any hydrogen-like element (shown below) with assuming that the principal quantum number above = in the Rydberg formula and (principal quantum number of the energy level the electron descends from, when emitting a photon). The Rydberg formula was derived from empirical spectroscopic emission data. An equivalent formula can be derived quantum mechanically from the time-independent Schrödinger equation with a kinetic energy Hamiltonian operator using a wave function as an eigenfunction to obtain the energy levels as eigenvalues, but the Rydberg constant would be replaced by other fundamental physics constants. Electron-electron interactions in atoms If there is more than one electron around the atom, electron-electron-interactions raise the energy level. These interactions are often neglected if the spatial overlap of the electron wavefunctions is low.",
                    "score": 0.8810120820999146
                },
                {
                    "id": 1772941,
                    "contents": "Hydrogen atom\nThe quantum numbers can take the following values: (principal quantum number) (azimuthal quantum number) (magnetic quantum number). Additionally, these wavefunctions are normalized (i.e., the integral of their modulus square equals 1) and orthogonal: where is the state represented by the wavefunction in Dirac notation, and is the Kronecker delta function. The wavefunctions in momentum space are related to the wavefunctions in position space through a Fourier transform which, for the bound states, results in where denotes a Gegenbauer polynomial and is in units of .",
                    "score": 0.8808331489562988
                },
                {
                    "id": 1772931,
                    "contents": "Hydrogen atom\nBohr–Sommerfeld Model In 1913, Niels Bohr obtained the energy levels and spectral frequencies of the hydrogen atom after making a number of simple assumptions in order to correct the failed classical model. The assumptions included: Electrons can only be in certain, discrete circular orbits or stationary states, thereby having a discrete set of possible radii and energies. Electrons do not emit radiation while in one of these stationary states. An electron can gain or lose energy by jumping from one discrete orbit to another. Bohr supposed that the electron's angular momentum is quantized with possible values: where and is Planck constant over . He also supposed that the centripetal force which keeps the electron in its orbit is provided by the Coulomb force, and that energy is conserved. Bohr derived the energy of each orbit of the hydrogen atom to be:",
                    "score": 0.880634069442749
                },
                {
                    "id": 1772934,
                    "contents": "Hydrogen atom\nwhere is the mass of the atomic nucleus. For hydrogen-1, the quantity is about 1/1836 (i.e. the electron-to-proton mass ratio). For deuterium and tritium, the ratios are about 1/3670 and 1/5497 respectively. These figures, when added to 1 in the denominator, represent very small corrections in the value of R, and thus only small corrections to all energy levels in corresponding hydrogen isotopes. There were still problems with Bohr's model: it failed to predict other spectral details such as fine structure and hyperfine structure it could only predict energy levels with any accuracy for single–electron atoms (hydrogen–like atoms) the predicted values were only correct to , where is the fine-structure constant.",
                    "score": 0.8802927136421204
                },
                {
                    "id": 17713018,
                    "contents": "Heisenberg's entryway to matrix mechanics\nto excited hydrogen gas when it radiated light and what happened when incoming radiation of one frequency excited atoms in a dispersive medium and then the energy delivered by the incoming light was re-radiated sometimes at the original frequency but often at two lower frequencies the sum of which equalled the original frequency. According to their model, an electron that had been driven to a higher energy state by accepting the energy of an incoming photon might return in one step to its equilibrium position, re-radiating a photon of the same frequency, or it might return in more than one step, radiating one photon for each step in its return to its equilibrium state. Because of the way factors cancel out in deriving the new equation based on these considerations, the result turns out to be relatively simple.",
                    "score": 0.8802471160888672
                },
                {
                    "id": 13333097,
                    "contents": "Hydrogen-like atom\nAfter writing the wave function as a product of functions: (in spherical coordinates), where are spherical harmonics, we arrive at the following Schrödinger equation: where is, approximately, the mass of the electron (more accurately, it is the reduced mass of the system consisting of the electron and the nucleus), and is the reduced Planck constant. Different values of l give solutions with different angular momentum, where l (a non-negative integer) is the quantum number of the orbital angular momentum. The magnetic quantum number m (satisfying ) is the (quantized) projection of the orbital angular momentum on the z-axis. See here for the steps leading to the solution of this equation. Non-relativistic wavefunction and energy",
                    "score": 0.8802397847175598
                },
                {
                    "id": 2666420,
                    "contents": "Balmer series\nAfter Balmer's discovery, five other hydrogen spectral series were discovered, corresponding to electrons transitioning to values of n other than two . Overview The Balmer series is characterized by the electron transitioning from n ≥ 3 to n = 2, where n refers to the radial quantum number or principal quantum number of the electron. The transitions are named sequentially by Greek letter: n = 3 to n = 2 is called H-α, 4 to 2 is H-β, 5 to 2 is H-γ, and 6 to 2 is H-δ. As the first spectral lines associated with this series are located in the visible part of the electromagnetic spectrum, these lines are historically referred to as \"H-alpha\", \"H-beta\", \"H-gamma\", and so on, where H is the element hydrogen.",
                    "score": 0.879464864730835
                },
                {
                    "id": 1565316,
                    "contents": "Atomic orbital\nThe Bohr model was able to explain the emission and absorption spectra of hydrogen. The energies of electrons in the n = 1, 2, 3, etc. states in the Bohr model match those of current physics. However, this did not explain similarities between different atoms, as expressed by the periodic table, such as the fact that helium (two electrons), neon (10 electrons), and argon (18 electrons) exhibit similar chemical inertness. Modern quantum mechanics explains this in terms of electron shells and subshells which can each hold a number of electrons determined by the Pauli exclusion principle. Thus the n = 1 state can hold one or two electrons, while the n = 2 state can hold up to eight electrons in 2s and 2p subshells. In helium, all n = 1 states are fully occupied; the same is true for n = 1 and n = 2 in neon. In argon, the 3s and 3p subshells are similarly fully occupied by eight electrons; quantum mechanics also allows a 3d subshell but this is at higher energy than the 3s and 3p in argon",
                    "score": 0.8791257739067078
                },
                {
                    "id": 16471310,
                    "contents": "Triatomic hydrogen\nThe electronic state for a trihydrogen cation with an electron delocalized around it is a Rydberg state. The outer electron can be boosted to high Rydberg state, and can ionise if the energy gets to 29562.6 cm−1 above the 2pA2\" state, in which case forms. Shape The shape of the molecule is predicted to be an equilateral triangle. Vibrations can occur in the molecule in two ways, firstly the molecule can expand and contract retaining the equilateral triangle shape (breathing), or one atom can move relative to the others distorting the triangle (bending). The bending vibration has a dipole moment and thus couples to infrared radiation.",
                    "score": 0.8791081309318542
                },
                {
                    "id": 986656,
                    "contents": "Atomic, molecular, and optical physics\nLater, the connection between atomic physics and optical physics became apparent, by the discovery of spectral lines and attempts to describe the phenomenon - notably by Joseph von Fraunhofer, Fresnel, and others in the 19th century. From that time to the 1920s, physicists were seeking to explain atomic spectra and blackbody radiation. One attempt to explain hydrogen spectral lines was the Bohr atom model. Experiments including electromagnetic radiation and matter - such as the photoelectric effect, Compton effect, and spectra of sunlight the due to the unknown element of Helium, the limitation of the Bohr model to Hydrogen, and numerous other reasons, lead to an entirely new mathematical model of matter and light: quantum mechanics.",
                    "score": 0.8790214657783508
                },
                {
                    "id": 11985718,
                    "contents": "Lyman–Werner photons\nA Lyman-Werner photon is an ultraviolet photon with a photon energy in the range of 11.2 to 13.6 eV, corresponding to the energy range in which the Lyman and Werner absorption bands of molecular hydrogen (H2) are found. A photon in this energy range, with a frequency that coincides with that of one of the lines in the Lyman or Werner bands, can be absorbed by H2, placing the molecule in an excited electronic state. Radiative decay (that is, decay into photons) from this excited state occurs rapidly, with roughly 15% of these decays occurring into the vibrational continuum of the molecule, resulting in its dissociation. This two-step photodissociation process, known as the Solomon process, is one of the main mechanisms by which molecular hydrogen is destroyed in the interstellar medium.",
                    "score": 0.878566563129425
                },
                {
                    "id": 2666394,
                    "contents": "H-alpha\nFor the Lyman series the naming convention is: n = 2 to n = 1 is called Lyman-alpha, n = 3 to n = 1 is called Lyman-beta, etc. H-alpha has a wavelength of 656.281 nm, is visible in the red part of the electromagnetic spectrum, and is the easiest way for astronomers to trace the ionized hydrogen content of gas clouds. Since it takes nearly as much energy to excite the hydrogen atom's electron from n = 1 to n = 3 (12.1 eV, via the Rydberg formula) as it does to ionize the hydrogen atom (13.6 eV), ionization is far more probable than excitation to the n = 3 level. After ionization, the electron and proton recombine to form a new hydrogen atom. In the new atom, the electron may begin in any energy level, and subsequently cascades to the ground state (n = 1), emitting photons with each transition. Approximately half the time, this cascade will include the n = 3 to n = 2 transition and the atom will emit H-alpha light. Therefore, the H-alpha line occurs where hydrogen is being ionized.",
                    "score": 0.878416895866394
                },
                {
                    "id": 154975,
                    "contents": "Rydberg formula\nIn atomic physics, the Rydberg formula calculates the wavelengths of a spectral line in many chemical elements. The formula was primarily presented as a generalization of the Balmer series for all atomic electron transitions of hydrogen. It was first empirically stated in 1888 by the Swedish physicist Johannes Rydberg, then theoretically by Niels Bohr in 1913, who used a primitive form of quantum mechanics. The formula directly generalizes the equations used to calculate the wavelengths of the hydrogen spectral series.",
                    "score": 0.8778425455093384
                },
                {
                    "id": 7748003,
                    "contents": "Introduction to quantum mechanics\nwhere R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m. Rydberg's formula accounts for the four visible wavelengths of hydrogen by setting and . It also predicts additional wavelengths in the emission spectrum: for and for , the emission spectrum should contain certain ultraviolet wavelengths, and for and , it should also contain certain infrared wavelengths. Experimental observation of these wavelengths came two decades later: in 1908 Louis Paschen found some of the predicted infrared wavelengths, and in 1914 Theodore Lyman found some of the predicted ultraviolet wavelengths. Both Balmer and Rydberg's formulas involve integers: in modern terms, they imply that some property of the atom is quantized. Understanding exactly what this property was, and why it was quantized, was a major part of the development of quantum mechanics, as shown in the rest of this article.",
                    "score": 0.8777326941490173
                },
                {
                    "id": 13333093,
                    "contents": "Hydrogen-like atom\nOther systems may also be referred to as \"hydrogen-like atoms\", such as muonium (an electron orbiting an antimuon), positronium (an electron and a positron), certain exotic atoms (formed with other particles), or Rydberg atoms (in which one electron is in such a high energy state that it sees the rest of the atom effectively as a point charge).",
                    "score": 0.8777071237564087
                },
                {
                    "id": 1772950,
                    "contents": "Hydrogen atom\nMathematical summary of eigenstates of hydrogen atom In 1928, Paul Dirac found an equation that was fully compatible with special relativity, and (as a consequence) made the wave function a 4-component \"Dirac spinor\" including \"up\" and \"down\" spin components, with both positive and \"negative\" energy (or matter and antimatter). The solution to this equation gave the following results, more accurate than the Schrödinger solution. Energy levels The energy levels of hydrogen, including fine structure (excluding Lamb shift and hyperfine structure), are given by the Sommerfeld fine structure expression:",
                    "score": 0.8774805665016174
                },
                {
                    "id": 16680978,
                    "contents": "Timeline of quantum mechanics\n1885 – Johann Jakob Balmer discovers a numerical relationship between visible spectral lines of hydrogen, the Balmer series. 1887 – Heinrich Hertz discovers the photoelectric effect, shown by Einstein in 1905 to involve quanta of light. 1888 – Hertz demonstrates experimentally that electromagnetic waves exist, as predicted by Maxwell. 1888 – Johannes Rydberg modifies the Balmer formula to include all spectral series of lines for the hydrogen atom, producing the Rydberg formula which is employed later by Niels Bohr and others to verify Bohr's first quantum model of the atom. 1895 – Wilhelm Conrad Röntgen discovers X-rays in experiments with electron beams in plasma.",
                    "score": 0.877334713935852
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.2
            }
        }
    },
    {
        "id": "test_35",
        "question": "It turns out that the solution of the Schrödinger equation for the Morse potential can be expressed as\r\n$$\r\nG(v)=\\tilde{\\omega}_{\\mathrm{e}}\\left(v+\\frac{1}{2}\\right)-\\tilde{\\omega}_{\\mathrm{e}} \\tilde{x}_{\\mathrm{e}}\\left(v+\\frac{1}{2}\\right)^2\r\n$$\r\nChapter 5 / The Harmonic Oscillator and Vibrational Spectroscopy\r\nwhere\r\n$$\r\n\\tilde{x}_{\\mathrm{e}}=\\frac{h c \\tilde{\\omega}_{\\mathrm{e}}}{4 D}\r\n$$\r\nGiven that $\\tilde{\\omega}_{\\mathrm{e}}=2886 \\mathrm{~cm}^{-1}$ and $D=440.2 \\mathrm{~kJ} \\cdot \\mathrm{mol}^{-1}$ for $\\mathrm{H}^{35} \\mathrm{Cl}$, calculate $\\tilde{x}_{\\mathrm{e}}$ and $\\tilde{\\omega}_{\\mathrm{e}} \\tilde{x}_{\\mathrm{e}}$.",
        "golden_answers": [
            " 0.01961"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 3936513,
                    "contents": "Morse potential\nwhich is usually written as where is now the coordinate perpendicular to the surface. This form approaches zero at infinite and equals at its minimum, i.e. . It clearly shows that the Morse potential is the combination of a short-range repulsion term (the former) and a long-range attractive term (the latter), analogous to the Lennard-Jones potential. Vibrational states and energies Like the quantum harmonic oscillator, the energies and eigenstates of the Morse potential can be found using operator methods. One approach involves applying the factorization method to the Hamiltonian. To write the stationary states on the Morse potential, i.e. solutions and of the following Schrödinger equation: it is convenient to introduce the new variables: Then, the Schrödinger equation takes the simple form: Its eigenvalues and eigenstates can be written as: where with [x] denoting the largest integer smaller than x. where and is a generalized Laguerre polynomial:",
                    "score": 0.9001756906509399
                },
                {
                    "id": 3936514,
                    "contents": "Morse potential\nIts eigenvalues and eigenstates can be written as: where with [x] denoting the largest integer smaller than x. where and is a generalized Laguerre polynomial: There also exists the following analytical expression for matrix elements of the coordinate operator: which is valid for and . The eigenenergies in the initial variables have the form: where is the vibrational quantum number and has units of frequency. The latter is mathematically related to the particle mass, , and the Morse constants via Whereas the energy spacing between vibrational levels in the quantum harmonic oscillator is constant at , the energy between adjacent levels decreases with increasing in the Morse oscillator. Mathematically, the spacing of Morse levels is This trend matches the anharmonicity found in real molecules. However, this equation fails above some value of where is calculated to be zero or negative. Specifically, integer part.",
                    "score": 0.8993868827819824
                },
                {
                    "id": 6500849,
                    "contents": "Philip M. Morse\nPhysics Philip Morse had a distinguished career in physics. Amongst his contributions to physics are the textbooks Quantum Mechanics (with Edward Condon), Methods of Theoretical Physics (with Herman Feshbach), Vibration and Sound, Theoretical Acoustics, and Thermal Physics. Morse is also one of the founding editors of Annals of Physics. In 1929 he proposed the Morse potential function for diatomic molecules which was often used to interpret vibrational spectra, though the standard is now the more modern Morse/Long-range potential. Administration His administrative talents were applied in roles as co-founder of the MIT Acoustics Laboratory, first director of the Brookhaven National Laboratory, founder and first director of the MIT Computation Center, and board member of the RAND Corporation and the Institute for Defense Analyses. He chaired the advisory committee that supervised preparation of Handbook of Mathematical Functions, with Formulas, Graphs, and Mathematical Tables.",
                    "score": 0.8935195207595825
                },
                {
                    "id": 3936515,
                    "contents": "Morse potential\nThis trend matches the anharmonicity found in real molecules. However, this equation fails above some value of where is calculated to be zero or negative. Specifically, integer part. This failure is due to the finite number of bound levels in the Morse potential, and some maximum that remains bound. For energies above , all the possible energy levels are allowed and the equation for is no longer valid. Below , is a good approximation for the true vibrational structure in non-rotating diatomic molecules. In fact, the real molecular spectra are generally fit to the form1 in which the constants and can be directly related to the parameters for the Morse potential. As is clear from dimensional analysis, for historical reasons the last equation uses spectroscopic notation in which represents a wavenumber obeying , and not an angular frequency given by . Morse/Long-range potential",
                    "score": 0.8933022618293762
                },
                {
                    "id": 11308585,
                    "contents": "List of quantum-mechanical potentials\nOscillators Harmonic potential (harmonic oscillator) Morse potential (morse oscillator) Morse/Long-range potential (Morse/Long-range oscillator) Kratzer potential (Kratzer oscillator) Quantum Field theory Yukawa potential Coleman–Weinberg potential Uehling potential Woods–Saxon potential Miscellaneous Quantum potential Pseudopotential Kolos–Wolniewicz potential See also List of quantum-mechanical systems with analytical solutions List of integrable models Quantum mechanics Science-related lists Quantum mechanical potentials",
                    "score": 0.8921588063240051
                },
                {
                    "id": 25311586,
                    "contents": "Trigonometric Rosen–Morse potential\nThe solutions, , to this equation are the so-called four-dimensional hyper-spherical harmonics defined as where are the Gegenbauer polynomials. Changing in () variables as one observes that the function satisfies the one-dimensional Schrödinger equation with the potential according to The one-dimensional potential in the latter equation, in coinciding with the Rosen–Morse potential in () for and , clearly reveals that for integer values, the first term of this potential takes its origin from the centrifugal barrier on . Stated differently, the equation (), and its version () describe inertial (free) quantum motion of a rigid rotator in the four-dimensional Euclidean space, , such as the H Atom, the positronium, etc. whose \"ends\" trace the large \"circles\" (i.e. spheres) on . Now the question arises whether the second term in () could also be related in some way to the geometry. The case: Electric charge confinement on and a dipole potential shaped after",
                    "score": 0.8898270726203918
                },
                {
                    "id": 15666021,
                    "contents": "Pöschl–Teller potential\nIn mathematical physics, a Pöschl–Teller potential, named after the physicists Herta Pöschl (credited as G. Pöschl) and Edward Teller, is a special class of potentials for which the one-dimensional Schrödinger equation can be solved in terms of special functions. Definition In its symmetric form is explicitly given by and the solutions of the time-independent Schrödinger equation with this potential can be found by virtue of the substitution , which yields . Thus the solutions are just the Legendre functions with , and , . Moreover, eigenvalues and scattering data can be explicitly computed. In the special case of integer , the potential is reflectionless and such potentials also arise as the N-soliton solutions of the Korteweg-de Vries equation. The more general form of the potential is given by Rosen–Morse potential A related potential is given by introducing an additional term: See also Morse potential Trigonometric Rosen–Morse potential References list",
                    "score": 0.8865565061569214
                },
                {
                    "id": 24403987,
                    "contents": "Morse/Long-range potential\nthe c-state of Li2: where the MLR potential was successfully able to bridge a gap of more than 5000 cm−1 in experimental data. Two years later it was found that the MLR potential was able to successfully predict the energies in the middle of this gap, correctly within about 1 cm−1. The accuracy of these predictions was much better than the most sophisticated ab initio techniques at the time. the A-state of Li2: where Le Roy et al. constructed an MLR potential which determined the C3 value for atomic lithium to a higher-precision than any previously measured atomic oscillator strength, by an order of magnitude. This lithium oscillator strength is related to the radiative lifetime of atomic lithium and is used as a benchmark for atomic clocks and measurements of fundamental constants. the a-state of KLi: where the MLR was used to build an analytic global potential successfully despite there only being a small amount of levels observed near the top of the potential.",
                    "score": 0.8809638619422913
                },
                {
                    "id": 15005731,
                    "contents": "Simon–Glatzel equation\nThe Simon–Glatzel equation can be viewed as a combination of the Murnaghan equation of state and the Lindemann law, and an alternative form was proposed by J. J. Gilvarry (1956): where is general at , is pressure derivative at , is Grüneisen ratio, and is the coefficient in Morse potential. Example parameters For methanol the following parameters can be obtained: The reference temperature has been Tref = 174.61 K and the reference pressure Pref has been set to 0 kPa. Methanol is a component where the Simon–Glatzel works well in the given validity range.",
                    "score": 0.8808643817901611
                },
                {
                    "id": 25311584,
                    "contents": "Trigonometric Rosen–Morse potential\nwhich is the trigonometric version of a one-dimensional hyperbolic potential introduced in molecular physics by Nathan Rosen and Philip M. Morse and given by, a parallelism that explains the potential's name. The most prominent application concerns the parametrization, with non-negative integer, and is due to Schrödinger who intended to formulate the hydrogen atom problem on Albert Einstein's closed universe, , the direct product of a time line with a three-dimensional closed space of positive constant curvature, the hypersphere , and introduced it on this geometry in his celebrated equation as the counterpart to the Coulomb potential, a mathematical problem briefly highlighted below. The case: Four-dimensional rigid rotator in inertial quantum motion on the three dimensional hypersphere The hypersphere is a surface in a four-dimensional Euclidean space, , and is defined as,",
                    "score": 0.8798544406890869
                },
                {
                    "id": 5311715,
                    "contents": "Franck–Condon principle\nIn the quantum mechanical picture, the vibrational levels and vibrational wavefunctions are those of quantum harmonic oscillators, or of more complex approximations to the potential energy of molecules, such as the Morse potential. Figure 1 illustrates the Franck–Condon principle for vibronic transitions in a molecule with Morse-like potential energy functions in both the ground and excited electronic states. In the low temperature approximation, the molecule starts out in the v = 0 vibrational level of the ground electronic state and upon absorbing a photon of the necessary energy, makes a transition to the excited electronic state. The electron configuration of the new state may result in a shift of the equilibrium position of the nuclei constituting the molecule. In the figure this shift in nuclear coordinates between the ground and the first excited state is labeled as q 01. In the simplest case of a diatomic molecule the nuclear coordinates axis refers to the internuclear",
                    "score": 0.8781260251998901
                },
                {
                    "id": 3936511,
                    "contents": "Morse potential\nThe Morse potential, named after physicist Philip M. Morse, is a convenient interatomic interaction model for the potential energy of a diatomic molecule. It is a better approximation for the vibrational structure of the molecule than the quantum harmonic oscillator because it explicitly includes the effects of bond breaking, such as the existence of unbound states. It also accounts for the anharmonicity of real bonds and the non-zero transition probability for overtone and combination bands. The Morse potential can also be used to model other interactions such as the interaction between an atom and a surface. Due to its simplicity (only three fitting parameters), it is not used in modern spectroscopy. However, its mathematical form inspired the MLR (Morse/Long-range) potential, which is the most popular potential energy function used for fitting spectroscopic data. Potential energy function The Morse potential energy function is of the form",
                    "score": 0.8774250745773315
                },
                {
                    "id": 25311606,
                    "contents": "Trigonometric Rosen–Morse potential\nwhere is the reduced mass of the two-body system under consideration. The partition function (statistical mechanics) for this energy spectrum is defined in the standard way as, Here, the thermodynamic beta is defined as with standing for the Boltzmann constant. In evaluating it is useful to recall that with the increase of the second term on the right hand side in () becomes negligible compared to the term proportional , a behavior which becomes even more pronounced for the choices, , and . In both cases is much smaller compared to the corresponding dimensionless factor, , multiplying . For this reason the partition function under investigation might be well approximated by, Along same lines, the partition function for the parametrization corresponding to the Hydrogen atom on has been calculated in, where a more sophisticated approximation has been employed. When transcribed to the current notations and units, the partition function in presents itself as,",
                    "score": 0.8765501976013184
                },
                {
                    "id": 24403989,
                    "contents": "Morse/Long-range potential\nThis long-range form of the MLR model is guaranteed because the argument of the exponent is defined to have long-range behavior: , where is the equilibrium bond length. There are a few ways in which this long-range behavior can be achieved, the most common is to make a polynomial that is constrained to become at long-range: , , where n is an integer greater than 1, which value is defined by the model chosen for the long-range potential . It is clear to see that: . Applications The MLR potential has successfully summarized all experimental spectroscopic data (and/or virial data) for a number of diatomic molecules, including: N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH. More sophisticated versions are used for polyatomic molecules.",
                    "score": 0.8762331604957581
                },
                {
                    "id": 20458157,
                    "contents": "Wu–Sprung potential\nIn mathematical physics, the Wu–Sprung potential, named after Hua Wu and Donald Sprung, is a potential function in one dimension inside a Hamiltonian with the potential defined by solving a non-linear integral equation defined by the Bohr–Sommerfeld quantization conditions involving the spectral staircase, the energies and the potential . here a is a classical turning point so , the quantum energies of the model are the roots of the Riemann Xi function and . In general, although Wu and Sprung considered only the smooth part, the potential is defined implicitly by ; with N(x) being the eigenvalue staircase and H(x) is the Heaviside step function. For the case of the Riemann zeros Wu and Sprung and others have shown that the potential can be written implicitly in terms of the Gamma function and zeroth-order Bessel function. and that the density of states of this Hamiltonian is just the Delsarte's formula for the Riemann zeta function and defined semiclassically as",
                    "score": 0.8760209083557129
                },
                {
                    "id": 25311588,
                    "contents": "Trigonometric Rosen–Morse potential\nFor this reason, the wave equation which transforms upon the variable change, , into the familiar one-dimensional Schrödinger equation with the trigonometric Rosen–Morse potential, in reality describes quantum motion of a charge dipole perturbed by the field due to another charge dipole, and not the motion of a single charge within the field produced by another charge. Stated differently, the two equations () and () do not describe strictly speaking a Hydrogen Atom on , but rather quantum motion on of a light dipole perturbed by the dipole potential of another very heavy dipole, like the H Atom, so that the reduced mass, , would be of the order of the electron mass and could be neglected in comparison with the energy.",
                    "score": 0.8759974241256714
                },
                {
                    "id": 21226352,
                    "contents": "Phase-space formulation\nMorse potential The Morse potential is used to approximate the vibrational structure of a diatomic molecule. Quantum tunneling Tunneling is a hallmark quantum effect where a quantum particle, not having sufficient energy to fly above, still goes through a barrier. This effect does not exist in classical mechanics. Quartic potential Schrödinger cat state References Quantum mechanics Hamiltonian mechanics Symplectic geometry Mathematical quantization Foundational quantum physics Articles containing video clips",
                    "score": 0.8746764659881592
                },
                {
                    "id": 680009,
                    "contents": "Schrödinger equation\nHarmonic oscillator The Schrödinger equation for this situation is where is the displacement and the angular frequency. Furthermore, it can be used to describe approximately a wide variety of other systems, including vibrating atoms, molecules, and atoms or ions in lattices, and approximating other potentials near equilibrium points. It is also the basis of perturbation methods in quantum mechanics. The solutions in position space are where , and the functions are the Hermite polynomials of order . The solution set may be generated by The eigenvalues are The case is called the ground state, its energy is called the zero-point energy, and the wave function is a Gaussian. The harmonic oscillator, like the particle in a box, illustrates the generic feature of the Schrödinger equation that the energies of bound eigenstates are discretized. Hydrogen atom The Schrödinger equation for the hydrogen atom (or a hydrogen-like atom) is",
                    "score": 0.8731752038002014
                },
                {
                    "id": 25311595,
                    "contents": "Trigonometric Rosen–Morse potential\nBefore closing this section, it is in order to bring the exact solutions to the equations ()-(), given by where stand for the Romanovski polynomials. Application to Coulomb fluids Coulomb fluids consist of dipolar particles and are modelled by means of direct numerical simulations. It is commonly used to choose cubic cells with periodic boundary conditions in conjunction with Ewald summation techniques. In a more efficient alternative method pursued by, one employs as a simulation cell the hyper spherical surface in (). As already mentioned above, the basic object on is the electric charge dipole, termed to as \"bi-charge\" in fluid dynamics, which can be visualized classically as a rigid \"dumbbell\" (rigid rotator) of two antipodal charges of opposite signs, and . The potential of a bi-charge is calculated by solving on the Poisson equation,",
                    "score": 0.8725022673606873
                },
                {
                    "id": 4857563,
                    "contents": "Birch–Murnaghan equation of state\nand The expression for the equation of state is obtained by expanding the free energy f in the form of a series: The internal energy, E(V), is found by integration of the pressure: See also Albert Francis Birch Francis Dominic Murnaghan Murnaghan equation of state References Continuum mechanics Equations of state",
                    "score": 0.8719527721405029
                },
                {
                    "id": 25311592,
                    "contents": "Trigonometric Rosen–Morse potential\nis introduced for the so-called fundamental coupling constant of electrodynamics. In effect, one finds In Fig. 2 we display the dipole potential in (). With that, the one-dimensional Schrödinger equation that describes on the quantum motion of an electric charge dipole perturbed by the trigonometric Rosen–Morse potential, produced by another electric charge dipole, takes the form of Because of the relationship, , with being the node number of the wave function, one could change labeling of the wave functions, , to the more familiar in the literature, . In eqs. ()-() one recognizes the one-dimensional wave equation with the trigonometric Rosen–Morse potential in () for and .",
                    "score": 0.8716898560523987
                },
                {
                    "id": 20458159,
                    "contents": "Wu–Sprung potential\nWu and Sprung also showed that the zeta-regularized functional determinant is the Riemann Xi-function The main idea inside this problem is to recover the potential from spectral data as in some inverse spectral problems in this case the spectral data is the Eigenvalue staircase, which is a quantum property of the system, the inverse of the potential then, satisfies an Abel integral equation (fractional calculus) which can be immediately solved to obtain the potential. Asymptotics For big x if we take only the smooth part of the eigenvalue staircase , then the potential as is positive and it is given by the asymptotic expression with and in the limit . This potential is approximately a Morse Potential with The asymptotic of the energies depend on the quantum number n as , where W is the Lambert W function. References",
                    "score": 0.8713805675506592
                },
                {
                    "id": 25311600,
                    "contents": "Trigonometric Rosen–Morse potential\ndrives the Gauss law back to the standard form known from Abelian theories. For this reason, under the condition of color charge constancy, one can attempt to model the color neutrality of hadrons in parallel to the neutrality of Coulomb fluids, namely, by considering quantum color motions on closed surfaces. In particular for the case of the hyper-sphere , it has been shown in, that a potential, there denoted by , and obtained from the one in () through the replacement, i.e. the potential",
                    "score": 0.8711084127426147
                },
                {
                    "id": 22434402,
                    "contents": "Maria Adelaide Sneider\nSee also Harmonic polynomial Potential theory Notes References General references , is a collection of papers, published in issues 3 and 4 of the 10th volume, and partly on the 1st issue of the 11th volume of the seventh series of the \"Rendiconti di Matematica e delle sue Applicazioni\" mathematical journal as an homage to her memory. . The brief \"Introduction\" written for the double issue of the \"Rendiconti di Matematica\" dedicated to Maria Adelaide Sneider. . An obituary on Maria Adelaide Sneider, including a detailed sketch of her major research contributions and a full list of her publications. .",
                    "score": 0.8682992458343506
                },
                {
                    "id": 11504548,
                    "contents": "Generating function (physics)\nThis turns the Hamiltonian into which is in the form of the harmonic oscillator Hamiltonian. The generating function F for this transformation is of the third kind, To find F explicitly, use the equation for its derivative from the table above, and substitute the expression for P from equation (), expressed in terms of p and Q: Integrating this with respect to Q results in an equation for the generating function of the transformation given by equation (): {|cellpadding=\"2\" style=\"border:2px solid #ccccff\" | |} To confirm that this is the correct generating function, verify that it matches (): See also Hamilton–Jacobi equation Poisson bracket References Further reading Classical mechanics Hamiltonian mechanics",
                    "score": 0.8680217266082764
                },
                {
                    "id": 25311583,
                    "contents": "Trigonometric Rosen–Morse potential\nThe trigonometric Rosen–Morse potential, named after the physicists Nathan Rosen and Philip M. Morse, is among the exactly solvable quantum mechanical potentials. Definition In dimensionless units and modulo additive constants, it is defined as where is a relative distance, is an angle rescaling parameter, and is so far a matching length parameter. Another parametrization of same potential is which is the trigonometric version of a one-dimensional hyperbolic potential introduced in molecular physics by Nathan Rosen and Philip M. Morse and given by,",
                    "score": 0.8672546148300171
                },
                {
                    "id": 16795576,
                    "contents": "Komornik–Loreti constant\nThe special case of , , and or 1 is sometimes called a -development. gives the only 2-development. However, for almost all , there are an infinite number of different -developments. Even more surprisingly though, there exist exceptional for which there exists only a single -development. Furthermore, there is a smallest number known as the Komornik–Loreti constant for which there exists a unique -development. Value The Komornik–Loreti constant is the value such that where is the Thue–Morse sequence, i.e., is the parity of the number of 1's in the binary representation of . It has approximate value The constant is also the unique positive real root of This constant is transcendental. See also Euler-Mascheroni constant Fibonacci word Golay–Rudin–Shapiro sequence Prouhet–Thue–Morse constant References Mathematical constants Non-standard positional numeral systems",
                    "score": 0.866832435131073
                },
                {
                    "id": 2278510,
                    "contents": "Bloch's theorem\nAs an intuitive interpretation, both last two equations resemble formally and are in a semi-classical analogy with the newton equation in an external Lorentz force. History and related equations The concept of the Bloch state was developed by Felix Bloch in 1928, to describe the conduction of electrons in crystalline solids. The same underlying mathematics, however, was also discovered independently several times: by George William Hill (1877), Gaston Floquet (1883), and Alexander Lyapunov (1892). As a result, a variety of nomenclatures are common: applied to ordinary differential equations, it is called Floquet theory (or occasionally the Lyapunov–Floquet theorem). The general form of a one-dimensional periodic potential equation is Hill's equation: where f(t) is a periodic potential. Specific periodic one-dimensional equations include the Kronig–Penney model and Mathieu's equation.",
                    "score": 0.8665162920951843
                },
                {
                    "id": 25311607,
                    "contents": "Trigonometric Rosen–Morse potential\nThe infinite integral has first been treated by means of partial integration giving, Then the argument of the exponential under the sign of the integral has been cast as, thus reaching the following intermediate result, As a next step the differential has been represented as an algebraic manipulation which allows to express the partition function in () in terms of the function of complex argument according to, where is an arbitrary path on the complex plane starting in zero and ending in . For more details and physical interpretations, see. See also Romanovski polynomials Pöschl–Teller potential References Quantum mechanics Quantum mechanical potentials Mathematical physics",
                    "score": 0.8658493757247925
                },
                {
                    "id": 10526941,
                    "contents": "Eckart conditions\nRelation to the harmonic approximation In the harmonic approximation to the nuclear vibrational problem, expressed in displacement coordinates, one must solve the generalized eigenvalue problem where H is a 3N × 3N symmetric matrix of second derivatives of the potential . H is the Hessian matrix of V in the equilibrium . The diagonal matrix M contains the masses on the diagonal. The diagonal matrix contains the eigenvalues, while the columns of C contain the eigenvectors. It can be shown that the invariance of V under simultaneous translation over t of all nuclei implies that vectors T = (t, ..., t) are in the kernel of H. From the invariance of V under an infinitesimal rotation of all nuclei around s, it can be shown that also the vectors S = (s x R10, ..., s x RN0) are in the kernel of H :",
                    "score": 0.8650170564651489
                },
                {
                    "id": 4217,
                    "contents": "Klein–Gordon equation\nThe Klein–Gordon equation was first considered as a quantum wave equation by Schrödinger in his search for an equation describing de Broglie waves. The equation is found in his notebooks from late 1925, and he appears to have prepared a manuscript applying it to the hydrogen atom. Yet, because it fails to take into account the electron's spin, the equation predicts the hydrogen atom's fine structure incorrectly, including overestimating the overall magnitude of the splitting pattern by a factor of for the -th energy level. The Dirac equation relativistic spectrum is, however, easily recovered if the orbital-momentum quantum number is replaced by total angular-momentum quantum number . In January 1926, Schrödinger submitted for publication instead his equation, a non-relativistic approximation that predicts the Bohr energy levels of hydrogen without fine structure.",
                    "score": 0.864592432975769
                },
                {
                    "id": 5840215,
                    "contents": "Schrödinger–Newton equation\nBecause of the back coupling of the wave-function into the potential it is a nonlinear system. The integro-differential form of the equation is It is obtained from the above system of equations by integration of the Poisson equation under the assumption that the potential must vanish at infinity. Mathematically, the Schrödinger–Newton equation is a special case of the Hartree equation for n = 2. The equation retains most of the properties of the linear Schrödinger equation. In particular it is invariant under constant phase shifts, leading to conservation of probability, and it exhibits full Galilei invariance. In addition to these symmetries, a simultaneous transformation maps solutions of the Schrödinger–Newton equation to solutions. The stationary equation, which can be obtained in the usual manner via a separation of variables, possesses an infinite family of normalisable solutions of which only the stationary ground state is stable.",
                    "score": 0.8644090294837952
                },
                {
                    "id": 22498935,
                    "contents": "Romanovski polynomials\nwhich derive from the Rodrigues formula () in conjunction with Pearson's ODE (). Orthogonality The two polynomials, and with , are orthogonal, if and only if, In other words, for arbitrary parameters, only a finite number of Romanovski polynomials are orthogonal. This property is referred to as finite orthogonality. However, for some special cases in which the parameters depend in a particular way on the polynomial degree infinite orthogonality can be achieved. This is the case of a version of equation () that has been independently encountered anew within the context of the exact solubility of the quantum mechanical problem of the trigonometric Rosen–Morse potential and reported in Compean & Kirchbach (2006). There, the polynomial parameters and are no longer arbitrary but are expressed in terms of the potential parameters, and , and the degree of the polynomial according to the relations, Correspondingly, emerges as , while the weight function takes the shape",
                    "score": 0.8633241653442383
                },
                {
                    "id": 679990,
                    "contents": "Schrödinger equation\nThe Schrödinger equation is not the only way to study quantum mechanical systems and make predictions. The other formulations of quantum mechanics include matrix mechanics, introduced by Werner Heisenberg, and the path integral formulation, developed chiefly by Richard Feynman. Paul Dirac incorporated matrix mechanics and the Schrödinger equation into a single formulation. When these approaches are compared, the use of the Schrödinger equation is sometimes called \"wave mechanics\". Definition Preliminaries Introductory courses on physics or chemistry typically introduce the Schrödinger equation in a way that can be appreciated knowing only the concepts and notations of basic calculus, particularly derivatives with respect to space and time. A special case of the Schrödinger equation that admits a statement in those terms is the position-space Schrödinger equation for a single nonrelativistic particle in one dimension:",
                    "score": 0.8630883693695068
                },
                {
                    "id": 18846007,
                    "contents": "Peter R. Holland\nHolland has published many peer-reviewed articles on the foundations of physics including the quantum potential, quantum hydrodynamics, quantum field theory, symmetries, hidden-variables theories, quantum back-reaction, quantum Hamilton-Jacobi theory, classical-like quantum systems, and the history of physics. Publications Book Peter R. Holland: The Quantum Theory of Motion: An Account of the De Broglie-Bohm Causal Interpretation of Quantum Mechanics, Cambridge University Press, Cambridge (first published June 25 1993), hardback, paperback, transferred to digital printing 2004 and available as an e-book from 2010",
                    "score": 0.8630251884460449
                },
                {
                    "id": 1061787,
                    "contents": "De Broglie–Bohm theory\nSee also Madelung equations Local hidden-variable theory Superfluid vacuum theory Fluid analogs in quantum mechanics Probability current Notes References (full text) (full text) (Demonstrates incompleteness of the Bohm interpretation in the face of fractal, differentiable-nowhere wavefunctions.) (Describes a Bohmian resolution to the dilemma posed by non-differentiable wavefunctions.) Bohmian mechanics on arxiv.org Further reading",
                    "score": 0.8624120950698853
                },
                {
                    "id": 25311598,
                    "contents": "Trigonometric Rosen–Morse potential\nThe confining nature of the cotangent potential in () finds an application in a phenomenon known from the physics of strong interaction which refers to the non-observability of free quarks, the constituents of the hadrons. Quarks are considered to possess three fundamental internal degree of freedom, conditionally termed to as \"colors\", red , blue , and green , while anti-quarks carry the corresponding anti-colors, anti-red , anti-blue , or anti-green , meaning that the non-observability of free quarks is equivalent to the non-observability of free color-charges, and thereby to the \"color neutrality\" of the hadrons. Quark \"colors\" are the fundamental degrees of freedom of the Quantum Chromodynamics (QCD), the gauge theory of strong interaction. In contrast to the Quantum Electrodynamics, the gauge theory of the electromagnetic interactions, QCD is a non-Abelian theory which roughly means that the \"color\" charges, denoted by , are not constants, but depend on the values, , of the",
                    "score": 0.862390398979187
                },
                {
                    "id": 1941126,
                    "contents": "Stationary-action principle\nto obtain the Euler–Lagrange equations in their present form. Jacobi, Morse and Caratheodory In 1842, Carl Gustav Jacobi tackled the problem of whether the variational principle always found minima as opposed to other stationary points (maxima or stationary saddle points); most of his work focused on geodesics on two-dimensional surfaces. The first clear general statements were given by Marston Morse in the 1920s and 1930s, leading to what is now known as Morse theory. For example, Morse showed that the number of conjugate points in a trajectory equalled the number of negative eigenvalues in the second variation of the Lagrangian. A particularly elegant derivation of the Euler-Lagrange equation was formulated by Constantin Caratheodory and published by him in 1935. Gauss and Hertz Other extremal principles of classical mechanics have been formulated, such as Gauss's principle of least constraint and its corollary, Hertz's principle of least curvature.",
                    "score": 0.8623876571655273
                },
                {
                    "id": 19824676,
                    "contents": "Richard Palais\nArticles Richard Palais and Stephen Smale, A generalized Morse theory, Research Announcement, Bulletin of the American Mathematical Society 70 (1964), 165-172 R. Palais, Morse Theory on Hilbert Manifolds, Topology 2 (1963), 299–340. R. Palais, Linear and Nonlinear Waves and Solitons, in The Princeton Companion to Mathematics, T. Gower Ed., Princeton Univ. Press 2008, 234-239 () R. Palais, The Symmetries of Solitons, Bulletin. Amer. Math. Soc., New Series 34, No. 4, 339-403 (1997) [ISSN 0273-0979], () R. Palais, The Visualization of Mathematics: Towards a Mathematical Exploratorium, Notices Amer. Math. Soc., 46, No. 6 (June–July 1999, () R. Palais, A Simple Proof of the Banach Contraction Principle, The Journal for Fixed Point Theory and its Applications, 2 (2007) 221–223, () A nearly complete list of all papers authored or co-authored by Richard Palais is available for downloading as PDF files at http://vmm.math.uci.edu/PalaisPapers References External links",
                    "score": 0.8621417284011841
                },
                {
                    "id": 3936516,
                    "contents": "Morse potential\nMorse/Long-range potential An extension of the Morse potential that made the Morse form useful for modern (high-resolution) spectroscopy is the MLR (Morse/Long-range) potential. The MLR potential is used as a standard for representing spectroscopic and/or virial data of diatomic molecules by a potential energy curve. It has been used on N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH. More sophisticated versions are used for polyatomic molecules. See also Lennard-Jones potential Molecular mechanics References 1 CRC Handbook of chemistry and physics, Ed David R. Lide, 87th ed, Section 9, SPECTROSCOPIC CONSTANTS OF DIATOMIC MOLECULES pp. 9–82 I.G. Kaplan, in Handbook of Molecular Physics and Quantum Chemistry, Wiley, 2003, p207. Chemical bonding Quantum chemistry Quantum models Quantum mechanical potentials",
                    "score": 0.8619714379310608
                },
                {
                    "id": 15316644,
                    "contents": "Balayage\nIn potential theory, a mathematical discipline, balayage (from French: balayage \"scanning, sweeping\") is a method devised by Henri Poincaré for reconstructing a harmonic function in a domain from its values on the boundary of the domain. In modern terms, the balayage operator maps a measure μ on a closed domain D to a measure ν on the boundary ∂ D, so that the Newtonian potentials of μ and ν coincide outside . The procedure is called balayage since the mass is \"swept out\" from D onto the boundary. For x in D, the balayage of δx yields the harmonic measure νx corresponding to x. Then the value of a harmonic function f at x is equal to References Potential theory",
                    "score": 0.8619195222854614
                },
                {
                    "id": 25311593,
                    "contents": "Trigonometric Rosen–Morse potential\nIn eqs. ()-() one recognizes the one-dimensional wave equation with the trigonometric Rosen–Morse potential in () for and . In this way, the cotangent term of the trigonometric Rosen–Morse potential could be derived from the Gauss law on in combination with the superposition principle, and could be interpreted as a dipole potential generated by a system consisting of two opposite fundamental charges. The centrifugal term of this potential has been generated by the kinetic energy operator on . In this manner, the complete trigonometric Rosen–Morse potential could be derived from first principles.",
                    "score": 0.8616346120834351
                },
                {
                    "id": 9051209,
                    "contents": "Lippmann–Schwinger equation\nDerivation We will assume that the Hamiltonian may be written as where is the free Hamiltonian (or more generally, a Hamiltonian with known eigenvectors). For example, in nonrelativistic quantum mechanics may be . Intuitively is the interaction energy of the system. Let there be an eigenstate of : . Now if we add the interaction into the mix, the Schrödinger equation reads . Now consider the Hellmann–Feynman theorem, which requires the energy eigenvalues of the Hamiltonian to change continuously with continuous changes in the Hamiltonian. Therefore, we wish that as . A naive solution to this equation would be . where the notation denotes the inverse of . However is singular since is an eigenvalue of . As is described below, this singularity is eliminated in two distinct ways by making the denominator slightly complex, to give yourself a little wiggle room : . By insertion of a complete set of free particle states, ,",
                    "score": 0.8603500127792358
                },
                {
                    "id": 1446104,
                    "contents": "Liouville's theorem (Hamiltonian)\nwhere and denote and respectively, and we have only kept terms linear in . Extending this to our infinitesimal hypercube , the side lengths change as To find the new infinitesimal phase space volume , we need the product of the above quantities. To first order in , we get the following. So far, we have yet to make any specifications about our system. Let us now specialize to the case of -dimensional isotropic harmonic oscillators. That is, each particle in our ensemble can be treated as a simple harmonic oscillator. The Hamiltonian for this system is given by By using Hamilton's equations with the above Hamiltonian we find that the term in parentheses above is identically zero, thus yielding From this we can find the infinitesimal volume of phase space. Thus we have ultimately found that the infinitesimal phase space volume is unchanged, yielding demonstrating Liouville's Theorem holds for this system.",
                    "score": 0.8601862192153931
                },
                {
                    "id": 1973057,
                    "contents": "Helmholtz free energy\none can find expressions for entropy, pressure and chemical potential: These three equations, along with the free energy in terms of the partition function, allow an efficient way of calculating thermodynamic variables of interest given the partition function and are often used in density of state calculations. One can also do Legendre transformations for different systems. For example, for a system with a magnetic field or potential, it is true that Bogoliubov inequality Computing the free energy is an intractable problem for all but the simplest models in statistical physics. A powerful approximation method is mean-field theory, which is a variational method based on the Bogoliubov inequality. This inequality can be formulated as follows. Suppose we replace the real Hamiltonian of the model by a trial Hamiltonian , which has different interactions and may depend on extra parameters that are not present in the original model. If we choose this trial Hamiltonian such that",
                    "score": 0.8593578338623047
                },
                {
                    "id": 3715568,
                    "contents": "Bloch\nNamed for André Bloch Bloch space, space of holomorphic functions Bloch's theorem (complex variables), mathematical theorem Bloch's principle, mathematical principle Named for Felix Bloch Bloch oscillation, oscillation of a particle if a constant force is acting on it Bloch spectrum, concept in quantum mechanics Bloch sphere, geometrical representation of the pure state space of a two-level quantum mechanical system Bloch wall, narrow transition region at the boundary between magnetic domains Bloch function, wavefunction of a particle placed in a periodic potential See also Bloch Park, baseball stadium in Selma, Alabama, United States Bloch (company), shoe manufacturer Bloch (TV series), German TV series Block (disambiguation) German-language surnames Germanic-language surnames Surnames of German origin Jewish surnames Yiddish-language surnames Ethonymic surnames",
                    "score": 0.8591285347938538
                },
                {
                    "id": 3936512,
                    "contents": "Morse potential\nPotential energy function The Morse potential energy function is of the form Here is the distance between the atoms, is the equilibrium bond distance, is the well depth (defined relative to the dissociated atoms), and controls the 'width' of the potential (the smaller is, the larger the well). The dissociation energy of the bond can be calculated by subtracting the zero point energy from the depth of the well. The force constant (stiffness) of the bond can be found by Taylor expansion of around to the second derivative of the potential energy function, from which it can be shown that the parameter, , is where is the force constant at the minimum of the well. Since the zero of potential energy is arbitrary, the equation for the Morse potential can be rewritten any number of ways by adding or subtracting a constant value. When it is used to model the atom-surface interaction, the energy zero can be redefined so that the Morse potential becomes which is usually written as",
                    "score": 0.8584272861480713
                },
                {
                    "id": 680026,
                    "contents": "Schrödinger equation\nHe found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself with a mistress in a mountain cabin in December 1925. While at the cabin, Schrödinger decided that his earlier nonrelativistic calculations were novel enough to publish and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl) Schrödinger showed that his nonrelativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926. Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave , moving in a potential well , created by the proton. This computation accurately reproduced the energy levels of the Bohr model.",
                    "score": 0.8581123948097229
                },
                {
                    "id": 27550452,
                    "contents": "Uehling potential\nfrom where it is apparent that this potential is a refinement of the classical Coulomb potential. Here is the electron mass and is its charge measured at large distances. If , this potential simplifies to while for we have where is the Euler–Mascheroni constant (0.57721...). Properties It was recently demonstrated that the above integral in the expression of can be evaluated in closed form by using the modified Bessel functions of the second kind and its successive integrals. Effect on atomic spectra Since the Uehling potential only makes a significant contribution at small distances close to the nucleus, it mainly influences the energy of the s orbitals. Quantum mechanical perturbation theory can be used to calculate this influence in the atomic spectrum of atoms. The quantum electrodynamics corrections for the degenerated energy levels of the hydrogen atom are given by up to leading order in . Here stands for electronvolts.",
                    "score": 0.8580216765403748
                },
                {
                    "id": 25311594,
                    "contents": "Trigonometric Rosen–Morse potential\nBack to Schrödinger's work, the hyper-radius for the H Atom has turned out to be very big indeed, and of the order of . This is by eight orders of magnitudes larger than the H Atom size. The result has been concluded from fitting magnetic dipole elements to hydrogen hyper-fine structure effects (see } and reference therein). The aforementioned radius is sufficiently large to allow approximating the hyper-sphere locally by plane space in which case the existence of single charge still could be justified. In cases in which the hyper spherical radius becomes comparable to the size of the system, the charge neutrality takes over. Such an example will be presented in section 6 below. Before closing this section, it is in order to bring the exact solutions to the equations ()-(), given by where stand for the Romanovski polynomials. Application to Coulomb fluids",
                    "score": 0.8577607870101929
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_36",
        "question": " In the infrared spectrum of $\\mathrm{H}^{127} \\mathrm{I}$, there is an intense line at $2309 \\mathrm{~cm}^{-1}$. Calculate the force constant of $\\mathrm{H}^{127} \\mathrm{I}$.",
        "golden_answers": [
            "313"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 19042083,
                    "contents": "Overtone band\nIt has been experimentally found that the intensity of the overtone band is very low compared to the fundamental band, validating the harmonic approximation. See also Near-infrared spectroscopy in Y. R. Sharma References C.N.Banwell and E.M.McCash:Fundamentals of Molecular Spectroscopy, Tata McGraw-Hill Fourth Edition Raman spectroscopy Infrared spectroscopy Book name Y. R. Sharma",
                    "score": 0.8901755809783936
                },
                {
                    "id": 3718369,
                    "contents": "Near-infrared spectroscopy\nTheory Near-infrared spectroscopy is based on molecular overtone and combination vibrations. Such transitions are forbidden by the selection rules of quantum mechanics. As a result, the molar absorptivity in the near-IR region is typically quite small. One advantage is that NIR can typically penetrate much further into a sample than mid infrared radiation. Near-infrared spectroscopy is, therefore, not a particularly sensitive technique, but it can be very useful in probing bulk material with little or no sample preparation.",
                    "score": 0.8834128975868225
                },
                {
                    "id": 431657,
                    "contents": "Rydberg constant\nIn spectroscopy, the Rydberg constant, symbol for heavy atoms or for hydrogen, named after the Swedish physicist Johannes Rydberg, is a physical constant relating to the electromagnetic spectra of an atom. The constant first arose as an empirical fitting parameter in the Rydberg formula for the hydrogen spectral series, but Niels Bohr later showed that its value could be calculated from more fundamental constants via his Bohr model. , and electron spin g-factor are the most accurately measured physical constants.",
                    "score": 0.882244884967804
                },
                {
                    "id": 154975,
                    "contents": "Rydberg formula\nIn atomic physics, the Rydberg formula calculates the wavelengths of a spectral line in many chemical elements. The formula was primarily presented as a generalization of the Balmer series for all atomic electron transitions of hydrogen. It was first empirically stated in 1888 by the Swedish physicist Johannes Rydberg, then theoretically by Niels Bohr in 1913, who used a primitive form of quantum mechanics. The formula directly generalizes the equations used to calculate the wavelengths of the hydrogen spectral series.",
                    "score": 0.8785833716392517
                },
                {
                    "id": 20938780,
                    "contents": "History of spectroscopy\nJohann Balmer discovered in 1885 that the four visible lines of hydrogen were part of a series that could be expressed in terms of integers. This was followed a few years later by the Rydberg formula, which described additional series of lines. Meanwhile, the substantial summary of past experiments performed by Maxwell (1873), resulted in his equations of electromagnetic waves. In 1895, the German physicist Wilhelm Conrad Röntgen discovered and extensively studied X-rays, which were later used in X-ray spectroscopy. One year later, in 1896, French physicist Antoine Henri Becquerel discovered radioactivity, and Dutch physicist Pieter Zeeman observed spectral lines being split by a magnetic field. In 1897, theoretical physicist, Joseph Larmor explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons.",
                    "score": 0.8783808350563049
                },
                {
                    "id": 15776443,
                    "contents": "William Klemperer\nScience Klemperer's early work concentrated on the infrared spectroscopy of small molecules that are only stable in the gas phase at high temperatures. Among these are the alkali halides, for many of which he obtained the first vibrational spectra. The work provided basic structural data for many oxides and fluorides, and gave remarkable insight into the details of the bonding. It also led Klemperer to recognize the immense potential of molecular beams in spectroscopy, and in particular the use of the electric resonance technique to address fundamental problems in structural chemistry. An important result was his benchmark measurement of the electric dipole moment of LiH, at a date when this was the largest molecule for which quantum chemical calculations had any hope of getting useful results in a sensible length of time. Klemperer has always been enthusiastic about molecular beams; he writes: \"Molecular beams are fun for a chemist. They give one a sense of power.\"",
                    "score": 0.8776103258132935
                },
                {
                    "id": 14762074,
                    "contents": "Edgar Bright Wilson\nIn 1955 Bright published a book Molecular Vibrations along with co-authors J.C Decius and P.C. Cross which discussed infrared and raman spectra of polyatomic molecules. In 1955 Bright studied the internal rotation of single bonds in molecules using microwave spectroscopy. In 1965 Bright studied the energy transfer in rotationally molecular inelastic collisions. In 1970, Bright began to study hydrogen bonding and the structure of hydrogen bonds using low resolution microwave spectroscopy. In 1979, Bright retired and was named an emeritus professor. The E. Bright Wilson Award in Spectroscopy was established in 1994 by the American Chemical Society. Personal life Wilson was born in Gallatin, Tennessee to mother Alma Lackey and father E. B. Wilson, a lawyer. His family soon moved to Yonkers, New York.",
                    "score": 0.8753610849380493
                },
                {
                    "id": 16931574,
                    "contents": "Roswell Clifton Gibbs\nPhysicist As a physicist, Gibbs's primary area of interest was spectroscopy. At the time, this was the fairly new and exciting field of research in physics. Physicists investigated the emission and absorption of radiation, creating an understanding of atomic structure. The new theory of quantum mechanics attempted to explain phenomena at the atomic and subatomic level. Gibbs was the author or co-author of over forty research papers, on subjects such as the ultraviolet spectra of isoelectronic sequences, and the hyperfine structure of spectra. He determined the charge-to-mass ratio of the electron by studying the intervals between the H-alpha lines of hydrogen and deuterium, and investigated the absorption spectra of organic compounds in solution.",
                    "score": 0.8752648830413818
                },
                {
                    "id": 14762071,
                    "contents": "Edgar Bright Wilson\nIn 1934, Bright was elected to the Society of Fellows at Harvard for his work done at the California Institute of Technology. His election meant he had a 3 year junior fellowship at Harvard during which he studied molecular motion and symmetry analysis. In 1936 the Harvard Chemistry department appointed Bright was an Assistant professor during his third year of his fellowship. He taught courses in chemistry and quantum mechanics and was promoted to an Associate Professor with tenure after three years. From 1934 to 1941, Bright, along with Harold Gershinowitz, constructed an automatic infrared spectrometer which was used to measure vibrational absorption spectra of various molecules.",
                    "score": 0.8726141452789307
                },
                {
                    "id": 8158348,
                    "contents": "Paul-Antoine Giguère\nHis research was in infrared and Raman spectroscopy and the determination of molecular and crystal structure. In 1946 and 1948, he was awarded a Guggenheim Fellowship in chemistry for the investigation of the molecular structure of hydrogen peroxide by infrared spectroscopy, which showed that this molecule has a skewed or nonplanar structure. In 1956 with Michael Falk, he obtained the infrared spectrum of the hydronium ion, previously believed to be too short-lived to observe a spectrum. In 1958-1959 he investigated the anomalous thermodynamic properties of ice. In 1970-75, his group observed the first infrared and Raman vibrational spectra of hydrogen trioxide (H2O3) in dilute aqueous solution. In 1976 with Sylvia Turrell, he showed that the weak acidity of hydrogen fluoride is due to the formation of a tightly bound ion pair [H3O+·F−]. In 1966 he proposed a novel three-dimensional arrangement of the periodic table.",
                    "score": 0.8711676597595215
                },
                {
                    "id": 20938788,
                    "contents": "History of spectroscopy\nAccuracy Theoretical quantum-mechanical calculations become rather accurate to describe the energy structure of some simple electronic configurations. The results of theoretical developments were summarized by Condon and Shortley in 1935. Edlén thoroughly analyzed spectra of MIA for many chemical elements and derived regularities in energy structures of MIA for many isoelectronic sequences (ions with the same number of electrons, but different nuclear charges). Spectra of rather high ionization stages (e.g. Cu XIX) were observed. The most exciting event was in 1942, when Edlén proved the identification of some solar coronal lines on the basis of his precise analyses of spectra of MIA. This implied that the solar corona has a temperature of a million degrees, and strongly advanced understanding of solar and stellar physics.",
                    "score": 0.8709061145782471
                },
                {
                    "id": 7748002,
                    "contents": "Introduction to quantum mechanics\nIn 1885 the Swiss mathematician Johann Balmer discovered that each wavelength (lambda) in the visible spectrum of hydrogen is related to some integer by the equation where is a constant Balmer determined is equal to 364.56 nm. In 1888 Johannes Rydberg generalized and greatly increased the explanatory utility of Balmer's formula. He predicted that is related to two integers and according to what is now known as the Rydberg formula: where R is the Rydberg constant, equal to 0.0110 nm−1, and n must be greater than m.",
                    "score": 0.8707151412963867
                },
                {
                    "id": 20938781,
                    "contents": "History of spectroscopy\nIn 1897, theoretical physicist, Joseph Larmor explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons. Physicist, Joseph Larmor, created the first solar system model of the atom in 1897. He also postulated the proton, calling it a “positive electron.” He said the destruction of this type of atom making up matter “is an occurrence of infinitely small probability.” Early 20th century (1900–1950) The first decade of the 20th century brought the basics of quantum theory (Planck, Einstein) and interpretation of spectral series of hydrogen by Lyman in VUV and by Paschen in infrared. Ritz formulated the combination principle.",
                    "score": 0.8691990375518799
                },
                {
                    "id": 26171313,
                    "contents": "Børge Bak\nSelected bibliography Bak, Børge. Det Indremolekylære Potential, Et Studium over Valenskræfternes Natur Paa Basis Af Molekylspektrene. København, 1943. Bak, Børge. Elementary Introduction to Molecular Spectra. Amst, 1954. Bak, Børge. Elementary Introduction to Molecular Spectra. 2nd Rev. ed. North-Holland: Amsterdam, 1962. Bak, Børge, Led, Jens Jørgen, and Pedersen, Erik Jonas. Reversible Chemical Changes of Polypeptides in CF3COOH as Seen by Nuclear Magnetic Resonance Spectra. Munksgaard: Copenhagen, 1969. Bak, Børge, Kristiansen, Niels Arnt, and Svanholt, Henrik. Microwave Spectrum of Alleged P4O7. S.l., 1982. Further reading Overend, J. \"Børge Bak: Elementary Introduction to Molecular Spectra.\" Spectrochimica Acta 19.2 (1963): 592. References/Notes and references External links Prize-winning dissertations at the University of Copenhagen 1990 deaths 1912 births Knights of the Order of the Dannebrog",
                    "score": 0.8684598803520203
                },
                {
                    "id": 4788816,
                    "contents": "Lamb shift\nwith k(n, 0) around 13 varying slightly with n, and with log(k(n,)) a small number (approx. -0.05) making k(n,) close to unity. For a derivation of ΔELamb see for example: In the hydrogen spectrum In 1947, Hans Bethe was the first to explain the Lamb shift in the hydrogen spectrum, and he thus laid the foundation for the modern development of quantum electrodynamics. Bethe was able to derive the Lamb shift by implementing the idea of mass renormalization, which allowed him to calculate the observed energy shift as the difference between the shift of a bound electron and the shift of a free electron. The Lamb shift currently provides a measurement of the fine-structure constant α to better than one part in a million, allowing a precision test of quantum electrodynamics. See also Uehling potential, first approximation to the Lamb shift Shelter Island Conference Zeeman effect used to measure the Lamb shift References Further reading",
                    "score": 0.8675182461738586
                },
                {
                    "id": 1238469,
                    "contents": "Spectroscopy\nSpectra of atoms and molecules often consist of a series of spectral lines, each one representing a resonance between two different quantum states. The explanation of these series, and the spectral patterns associated with them, were one of the experimental enigmas that drove the development and acceptance of quantum mechanics. The hydrogen spectral series in particular was first successfully explained by the Rutherford–Bohr quantum model of the hydrogen atom. In some cases spectral lines are well separated and distinguishable, but spectral lines can also overlap and appear to be a single transition if the density of energy states is high enough. Named series of lines include the principal, sharp, diffuse and fundamental series. See also Notes References External links NIST Atomic Spectroscopy Databases MIT Spectroscopy Lab's History of Spectroscopy Timeline of Spectroscopy Spectroscopy: Reading the Rainbow",
                    "score": 0.8673050403594971
                },
                {
                    "id": 11001273,
                    "contents": "David M. Dennison\nDennison was a student of Niels Bohr, and knew Hans Bethe, Wolfgang Pauli, and Enrico Fermi before they became world-famous. Most of Dennison's work was on molecular structure. Following the discovery of the spin of the electron in 1925 by George Uhlenbeck and Samuel Goudsmit, the specific heat of hydrogen was a major unsolved problems. Dennison solved this problem in 1927 by postulating that the spin of protons does not transition frequently during measurements. This new theory agreed precisely with experiments given that the proton's spin was 1/2. In 1932 Dennison and Uhlenbeck solved the two-minima \"reversing umbrella\" problem for the position of nitrogen in ammonia. This result predicted absorption at microwave wavelengths, which inspired Neal Williams to build a molecular microwave spectrograph, one of the first ever built.",
                    "score": 0.8666893243789673
                },
                {
                    "id": 3952444,
                    "contents": "List of important publications in physics\nHerzberg, Gerhard (1945) Molecular Spectra and Molecular Structure II. Infrared and Raman Spectra of Polyatomic Molecules Herzberg, Gerhard (1966) Molecular Spectra and Molecular Structure III. Electronic Spectra of Polyatomic Molecules This three-volume series is the classic detailed presentation of molecular spectroscopy for physicists and chemists. Herzberg received the 1971 Nobel Prize in Chemistry for his spectroscopic research on the electronic structure and geometry of molecules.",
                    "score": 0.8666186332702637
                },
                {
                    "id": 4058827,
                    "contents": "Grigory Landsberg\nHe laid the foundation to spectroscopy of organic molecules and to the studies on inter- and intra-molecular interactions in the gaseous, liquid and solid phases in the USSR. He founded a major school on atomic and molecular spectral analysis. He developed techniques for the spectral analysis of metals and alloys (USSR Government Prize, 1941), and for the analysis of complex organic mixtures, including motor fuels. He is the author of the famous course of \"Optics\", and editor of the most popular \"Elementary Textbook on Physics\" (Volumes 1–3, 7th Edition, 1971). In 1946 he became full member of the Soviet Academy of Sciences. He later founded the Commission on Spectroscopy at the Academy, which in 1968 was transformed into the Institute for Spectroscopy Russian Academy of Sciences. Landsberg was awarded two Orders of Lenin and several medals. Discovery of the combinatorial scattering of light",
                    "score": 0.8664721846580505
                },
                {
                    "id": 1129312,
                    "contents": "Niels Bohr\nIn 1885, Johann Balmer had come up with his Balmer series to describe the visible spectral lines of a hydrogen atom: where λ is the wavelength of the absorbed or emitted light and RH is the Rydberg constant. Balmer's formula was corroborated by the discovery of additional spectral lines, but for thirty years, no one could explain why it worked. In the first paper of his trilogy, Bohr was able to derive it from his model: where me is the electron's mass, e is its charge, h is Planck's constant and Z is the atom's atomic number (1 for hydrogen).",
                    "score": 0.8659364581108093
                },
                {
                    "id": 11438818,
                    "contents": "Adolf Kratzer\nBased on his work at Munich, it was in 1922 that Kratzer's detailed analysis on the cyanide spectroscopic bands was published. His analysis resulted in the introduction of half-integral quantum numbers to account for molecular rotation. During 1922, he was also called as an ordinarius professor of theoretical physics to the University of Münster. Here, Kratzer made contributions to quantum mechanics and became a leading authority in the field of molecular band spectroscopy.",
                    "score": 0.8656618595123291
                },
                {
                    "id": 431660,
                    "contents": "Rydberg constant\nRydberg frequency Rydberg wavelength . The angular wavelength is . Occurrence in Bohr model The Bohr model explains the atomic spectrum of hydrogen (see hydrogen spectral series) as well as various other atoms and ions. It is not perfectly accurate, but is a remarkably good approximation in many cases, and historically played an important role in the development of quantum mechanics. The Bohr model posits that electrons revolve around the atomic nucleus in a manner analogous to planets revolving around the sun. In the simplest version of the Bohr model, the mass of the atomic nucleus is considered to be infinite compared to the mass of the electron, so that the center of mass of the system, the barycenter, lies at the center of the nucleus. This infinite mass approximation is what is alluded to with the subscript. The Bohr model then predicts that the wavelengths of hydrogen atomic transitions are (see Rydberg formula):",
                    "score": 0.8650636076927185
                },
                {
                    "id": 106278,
                    "contents": "Gerhard Herzberg\nThe main building of John Abbott College in Montreal is named after him. Carleton University named the Herzberg Laboratories building after him. A public park in the College Park neighbourhood of Saskatoon also bears his name. Books and publications Herzberg authored some classic works in the field of spectroscopy, including Atomic Spectra and Atomic Structure and the encyclopaedic four volume work: Molecular Spectra and Molecular Structure, which is often called the spectroscopist's bible. The three volumes of Molecular Spectra and Molecular Structure were re-issued by Krieger in 1989, including extensive new footnotes by Herzberg. Volume IV of the series, \"Constants of diatomic molecules\" is purely a reference work, a compendium of known spectroscopic constants (and therefore a bibliography of molecular spectroscopy) of diatomic molecules up until 1978.",
                    "score": 0.8649822473526001
                },
                {
                    "id": 24331220,
                    "contents": "Fundamental series\nOriginally the series was discovered in the infrared by Fowler and independently by Arno Bergmann. This resulted in the name Bergmann series used for such a set of lines in a spectrum. However the name was changed as Bergmann also discovered other series of lines. And other discoverers also established other such series. They became known as the fundamental series. Bergmann observed lithium at 5347 cm−1, sodium at 5416 cm−1 potassium at 6592 cm−1. Bergmann observed that the lines in the series in the caesium spectrum were double. His discovery was announced in Contributions to the Knowledge of the Infra-Red Emission Spectra of the Alkalies, Jena 1907. Carl Runge called this series the \"new series\". He predicted that the lines of potassium and rubidium would be in pairs. He expressed the frequencies of the series lines by a formula and predicted a connection of the series limit to the other known series. In 1909 W. M. Hicks produced approximate formulas for the various series and",
                    "score": 0.8649432063102722
                },
                {
                    "id": 552407,
                    "contents": "Wavenumber\nFor example, the spectroscopic wavenumbers of the emission spectrum of atomic hydrogen are given by the Rydberg formula: where R is the Rydberg constant, and ni and nf are the principal quantum numbers of the initial and final levels respectively (ni is greater than nf for emission). A spectroscopic wavenumber can be converted into energy per photon E by Planck's relation: It can also be converted into wavelength of light: where n is the refractive index of the medium. Note that the wavelength of light changes as it passes through different media, however, the spectroscopic wavenumber (i.e., frequency) remains constant. Conventionally, inverse centimeter (cm−1) units are used for , so often that such spatial frequencies are stated by some authors \"in wavenumbers\", incorrectly transferring the name of the quantity to the CGS unit cm−1 itself. See also Spatial frequency Refractive index Zonal wavenumber References Wave mechanics Physical quantities Units of frequency",
                    "score": 0.8635849952697754
                },
                {
                    "id": 20938791,
                    "contents": "History of spectroscopy\nA wide field of spectroscopic research with EBIT is enabled including achievement of highest grades of ionization (U92+), wavelength measurement, hyperfine structure of energy levels, quantum electrodynamic studies, ionization cross-sections (CS) measurements, electron-impact excitation CS, X-ray polarization, relative line intensities, dielectronic recombination CS, magnetic octupole decay, lifetimes of forbidden transitions, charge-exchange recombination, etc.",
                    "score": 0.8634148836135864
                },
                {
                    "id": 17231693,
                    "contents": "Richard C. Lord\nIn 1946, MIT appointed him Director of the Spectroscopy Laboratory and in 1954, Professor of Chemistry. In collaboration with George R. Harrison, and J.R. Loofbourow, Lord published the widely used text Practical Spectroscopy in 1948. Lord is considered a pioneer in the use of infrared radiation for the study of molecular structure; he is widely recognized for developments in the interpretation of infrared spectra of molecules in terms of their vibrational motion, and also to the understanding of the cohesion of molecules by means of hydrogen bonding. His studies of the laser Raman spectroscopy of proteins and nucleic acids opened a new field of research.",
                    "score": 0.8631430864334106
                },
                {
                    "id": 986656,
                    "contents": "Atomic, molecular, and optical physics\nLater, the connection between atomic physics and optical physics became apparent, by the discovery of spectral lines and attempts to describe the phenomenon - notably by Joseph von Fraunhofer, Fresnel, and others in the 19th century. From that time to the 1920s, physicists were seeking to explain atomic spectra and blackbody radiation. One attempt to explain hydrogen spectral lines was the Bohr atom model. Experiments including electromagnetic radiation and matter - such as the photoelectric effect, Compton effect, and spectra of sunlight the due to the unknown element of Helium, the limitation of the Bohr model to Hydrogen, and numerous other reasons, lead to an entirely new mathematical model of matter and light: quantum mechanics.",
                    "score": 0.8628298044204712
                },
                {
                    "id": 431668,
                    "contents": "Johannes Rydberg\nRydberg formula The physical constant known as the Rydberg constant is named after him, as is the Rydberg unit. Excited atoms with very high values of the principal quantum number, represented by n in the Rydberg formula, are called Rydberg atoms. Rydberg's anticipation that spectral studies could assist in a theoretical understanding of the atom and its chemical properties was justified in 1913 by the work of Niels Bohr (see hydrogen spectrum). An important spectroscopic constant based on a hypothetical atom of infinite mass is called the Rydberg (R) in his honour. See also Rydberg atom Rydberg matter Rydberg state List of things named after Johannes Rydberg References External links 1854 births 1919 deaths Swedish physicists Optical physicists People connected to Lund University Lund University alumni Spectroscopists People involved with the periodic table Foreign Members of the Royal Society",
                    "score": 0.8627995252609253
                },
                {
                    "id": 24331224,
                    "contents": "Fundamental series\nSodium The fundamental series lines for sodium appear in the near infrared. Potassium The fundamental series lines for potassium appear in the near infrared. Rubidium The fundamental series lines for rubidium appear in the near infrared. The valence electron moves from the 4d level as the 3d is contained in an inner shell. They were observed by R von Lamb. Relevant energy levels are 4p64d j=5/2 19,355.282 cm−1 and j=3/2 19,355.623 cm−1, and the first f levels at 4p64f j=5/2 26,792.185 cm−1 and j=7/2 26,792.169 cm−1. Caesium References Spectroscopy Atomic physics",
                    "score": 0.8625090718269348
                },
                {
                    "id": 19042079,
                    "contents": "Overtone band\nIn vibrational spectroscopy, an overtone band is the spectral band that occurs in a vibrational spectrum of a molecule when the molecule makes a transition from the ground state (v=0) to the second excited state (v=2), where v is the vibrational quantum number (a non-negative integer) obtained from solving the Schrödinger equation for the molecule.",
                    "score": 0.862334668636322
                },
                {
                    "id": 22273497,
                    "contents": "Robert Brattain\nof strong bands at frequencies of 1785, 1740, 1667 and 1538 cm-1 on the spectroscopy results. Brattain and his co-workers released a report to the government describing their results in 1944. A full report of the international infrared spectroscopy work appeared in 1949.",
                    "score": 0.8622655868530273
                },
                {
                    "id": 154981,
                    "contents": "Rydberg formula\nBy setting to 1 and letting run from 2 to infinity, the spectral lines known as the Lyman series converging to 91 nm are obtained, in the same manner: For any hydrogen-like element The formula above can be extended for use with any hydrogen-like chemical elements with where is the wavelength (in vacuum) of the light emitted, is the Rydberg constant for this element, is the atomic number, i.e. the number of protons in the atomic nucleus of this element, is the principal quantum number of the lower energy level, and is the principal quantum number of the higher energy level for the atomic electron transition. This formula can be directly applied only to hydrogen-like, also called hydrogenic atoms of chemical elements, i.e. atoms with only one electron being affected by an effective nuclear charge (which is easily estimated). Examples would include He+, Li2+, Be3+ etc., where no other electrons exist in the atom.",
                    "score": 0.8621860146522522
                },
                {
                    "id": 1791947,
                    "contents": "Infrared spectroscopy\nThe infrared portion of the electromagnetic spectrum is usually divided into three regions; the near-, mid- and far- infrared, named for their relation to the visible spectrum. The higher-energy near-IR, approximately 14,000–4,000 cm−1 (0.7–2.5 μm wavelength) can excite overtone or combination modes of molecular vibrations. The mid-infrared, approximately 4,000–400 cm−1 (2.5–25 μm) is generally used to study the fundamental vibrations and associated rotational–vibrational structure. The far-infrared, approximately 400–10 cm−1 (25–1,000 μm) has low energy and may be used for rotational spectroscopy and low frequency vibrations. The region from 2–130 cm−1, bordering the microwave region, is considered the terahertz region and may probe intermolecular vibrations. The names and classifications of these subregions are conventions, and are only loosely based on the relative molecular or electromagnetic properties. Theory",
                    "score": 0.8617715835571289
                },
                {
                    "id": 1238440,
                    "contents": "Spectroscopy\nSpectroscopic studies were central to the development of quantum mechanics, because the first useful atomic models described the spectra of Hydrogen which models include the Bohr model, the Schrödinger equation, and Matrix mechanics which all can produce the spectral lines of Hydrogen, therefore, providing the basis for discrete quantum jumps to match the discrete hydrogen spectrum. Also, Max Planck's explanation of blackbody radiation involved spectroscopy because he was comparing the wavelength of light using a photometer to the temperature of a Black Body. Spectroscopy is used in physical and analytical chemistry because atoms and molecules have unique spectra. As a result, these spectra can be used to detect, identify and quantify information about the atoms and molecules. Spectroscopy is also used in astronomy and remote sensing on Earth. Most research telescopes have spectrographs. The measured spectra are used to determine the chemical composition and physical properties of",
                    "score": 0.8614830374717712
                },
                {
                    "id": 16680978,
                    "contents": "Timeline of quantum mechanics\n1885 – Johann Jakob Balmer discovers a numerical relationship between visible spectral lines of hydrogen, the Balmer series. 1887 – Heinrich Hertz discovers the photoelectric effect, shown by Einstein in 1905 to involve quanta of light. 1888 – Hertz demonstrates experimentally that electromagnetic waves exist, as predicted by Maxwell. 1888 – Johannes Rydberg modifies the Balmer formula to include all spectral series of lines for the hydrogen atom, producing the Rydberg formula which is employed later by Niels Bohr and others to verify Bohr's first quantum model of the atom. 1895 – Wilhelm Conrad Röntgen discovers X-rays in experiments with electron beams in plasma.",
                    "score": 0.8608831167221069
                },
                {
                    "id": 16931572,
                    "contents": "Roswell Clifton Gibbs\nRoswell Clifton Gibbs (July 1, 1878 – October 4, 1966) was Chairman of the Department of Physics at Cornell University from 1934 to 1946. A graduate of Cornell, he became an assistant professor of Physics there in 1912, and a professor in 1918. His research primarily concerned spectroscopy, and he was the author or co-author of over forty research papers. As Chairman of the Department of Physics, he hired distinguished physicists including Stanley Livingston, Robert Bacher and Hans Bethe, who later won the Nobel Prize in Physics for his work at Cornell. After World War II, Gibbs was Chairman of the Mathematical and Physical Sciences Division of the National Research Council.",
                    "score": 0.8604902625083923
                },
                {
                    "id": 22273487,
                    "contents": "Robert Brattain\nInitially interested in mathematical physics, Robert Brattain soon became interested in experimental physics. After his advisor Edward Condon suggested that he assist R. Bowling Barnes, an expert in infrared spectrometry, Brattain became fascinated with infrared research and instrument design. Brattain, Barnes, and others in the laboratory built a research-quality infrared spectrophotometer, using a rock salt prism, a strip of platinum as an infrared radiation source, a thermopile to measure radiation, and two galvanometers to display results. They used the instrument to begin studying the molecular structure of organic molecules. After Barnes left Princeton for American Cyanamid, he directed funding to Brattain and others to study the infrared absorption spectra of organic compounds such as benzene, toluene, and naphthalene.",
                    "score": 0.8604382872581482
                },
                {
                    "id": 24038713,
                    "contents": "Diffuse series\nHeinrich Kayser, Carl Runge and Johannes Rydberg found mathematical relations between the wave numbers of emission lines of the alkali metals. Friedrich Hund introduced the s, p, d, f notation for subshells in atoms. Others followed this use in the 1930s and the terminology has remained to this day. References Spectroscopy Atomic physics",
                    "score": 0.8602333068847656
                },
                {
                    "id": 5810477,
                    "contents": "Microwave spectroscopy\nIn molecular physics In the field of molecular physics, microwave spectroscopy is commonly used to probe the rotation of molecules.",
                    "score": 0.8602094650268555
                },
                {
                    "id": 14762070,
                    "contents": "Edgar Bright Wilson\nStarting in 1997, the American Chemical Society has annually awarded the E. Bright Wilson Award in Spectroscopy, named in honor of Wilson. Scientific career Bright started his higher education at Princeton in 1926, where he received both his bachelors and masters degree in 1930 and 1931 respectively. He then went to the California Institute of Technology where he worked with Linus Pauling on crystal structure determinations, where he finished his Ph.D. During this time, he also wrote a textbook with Pauling, called Introduction to Quantum Mechanics, which was published in 1935. This textbook was still in print in the year 2000, some 70 years after its initial publication.",
                    "score": 0.8601807951927185
                },
                {
                    "id": 19207720,
                    "contents": "Elmer Imes\nH.M. Randall and E.S. Imes, \"The Fine Structure of the Near Infra-Red Absorption Bands of HCI, HBr, and HF\"], Phys. Rev. 15, pp. 152-155, Feb. 1920; in Science Abstracts, Institution of Electrical Engineers., 1920, pp.342-343</ref> This work demonstrated for the first time that Quantum Theory could be applied to the rotational energy states of molecules, as well as the vibration and electronic levels. Imes' work provided an early verification of Quantum Theory.",
                    "score": 0.8599998950958252
                },
                {
                    "id": 26171309,
                    "contents": "Børge Bak\nBak was able to repeat this pioneering achievement upon his return to Copenhagen where he established a world-renowned laboratory at the University of Copenhagen's H.C. Ørsted Institute. Over the following years, Bak's students and colleagues produced over 300 papers and publications containing precise determinations of molecular geometry from microwave spectra based on infrared spectroscopy, Raman-spectroscopy, microwave-spectroscopy and nuclear magnetic radiation spectroscopy. These early measurements and results were supplemented by Bak's work with quantum chemistry calculations of small molecular structures. Combined, this work formed the foundation for much of our understanding of the field today. Most unusually within the field of spectroscopy, Bak's research branch encouraged - as one of the first in chemistry - the interaction between experts from disciplines as diverse as quantum chemistry, chemical physics, apparatus construction and electronics. In addition to his core",
                    "score": 0.8599965572357178
                },
                {
                    "id": 2308818,
                    "contents": "Absorption spectroscopy\nAtomic and molecular physics Theoretical models, principally quantum mechanical models, allow for the absorption spectra of atoms and molecules to be related to other physical properties such as electronic structure, atomic or molecular mass, and molecular geometry. Therefore, measurements of the absorption spectrum are used to determine these other properties. Microwave spectroscopy, for example, allows for the determination of bond lengths and angles with high precision. In addition, spectral measurements can be used to determine the accuracy of theoretical predictions. For example, the Lamb shift measured in the hydrogen atomic absorption spectrum was not expected to exist at the time it was measured. Its discovery spurred and guided the development of quantum electrodynamics, and measurements of the Lamb shift are now used to determine the fine-structure constant. Experimental methods",
                    "score": 0.8599307537078857
                },
                {
                    "id": 1402002,
                    "contents": "J. Robert Oppenheimer\nInitially, his major interest was the theory of the continuous spectrum and his first published paper, in 1926, concerned the quantum theory of molecular band spectra. He developed a method to carry out calculations of its transition probabilities. He calculated the photoelectric effect for hydrogen and X-rays, obtaining the absorption coefficient at the K-edge. His calculations accorded with observations of the X-ray absorption of the sun, but not helium. Years later it was realized that the sun was largely composed of hydrogen and that his calculations were indeed correct.",
                    "score": 0.8591746091842651
                },
                {
                    "id": 3574307,
                    "contents": "Stark effect\nFinally, Epstein reconsidered the linear and quadratic Stark effect from the point of view of the new quantum theory. He derived equations for the line intensities which were a decided improvement over Kramers's results obtained by the old quantum theory.",
                    "score": 0.8590617775917053
                },
                {
                    "id": 1791952,
                    "contents": "Infrared spectroscopy\nThese figures do not represent the \"recoil\" of the C atoms, which, though necessarily present to balance the overall movements of the molecule, are much smaller than the movements of the lighter H atoms.",
                    "score": 0.8590162992477417
                },
                {
                    "id": 21259855,
                    "contents": "William Henry Walenn\nSee also History of electromagnetic theory References Journal of the Chemical Society, Volume 71, Part 2. Chemical Society (Great Britain), Bureau of Chemical Abstracts (Great Britain). The Society (1897), p. 1206. 1828 births 1896 deaths British chemists Alumni of University College London Fellows of the Chemical Society",
                    "score": 0.8583240509033203
                },
                {
                    "id": 154983,
                    "contents": "Rydberg formula\nFinally, with certain modifications (replacement of Z by Z − 1, and use of the integers 1 and 2 for the ns to give a numerical value of for the difference of their inverse squares), the Rydberg formula provides correct values in the special case of K-alpha lines, since the transition in question is the K-alpha transition of the electron from the 1s orbital to the 2p orbital. This is analogous to the Lyman-alpha line transition for hydrogen, and has the same frequency factor. Because the 2p electron is not screened by any other electrons in the atom from the nucleus, the nuclear charge is diminished only by the single remaining 1s electron, causing the system to be effectively a hydrogenic atom, but with a diminished nuclear charge Z − 1. Its frequency is thus the Lyman-alpha hydrogen frequency, increased by a factor of (Z − 1)2. This formula of f = c/λ = (Lyman-alpha frequency)⋅(Z − 1)2 is historically known as Moseley's law (having added a factor c to convert wavelength to",
                    "score": 0.8581990003585815
                },
                {
                    "id": 10123507,
                    "contents": "Kevin K. Lehmann\nLehmann went on to receive his doctorate in chemical physics from Harvard University in 1983, and was elected to the Harvard Society of Fellows, where he was a junior fellow until 1986. Under the direction of William Klemperer, Lehmann's graduate work involved studies of highly excited vibrational states using photoacoustic spectroscopy. During his time as a fellow, he served as a visiting scientist at Massachusetts Institute of Technology's George Harrison Spectroscopy Laboratory. There he developed with Stephen Coy the technique of microwave-detected, microwave-optical double resonance, which permits the automatic assignment of complex spectra.",
                    "score": 0.8580829501152039
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    },
    {
        "id": "test_37",
        "question": "Calculate the percentage difference between $e^x$ and $1+x$ for $x=0.0050$",
        "golden_answers": [
            " 1.25"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 16863774,
                    "contents": "$100,000 infield\nEddie Collins",
                    "score": 0.9121791124343872
                },
                {
                    "id": 21547921,
                    "contents": "UFC 125\nFrankie Edgar: $102,000 ($51,000 win bonus) vs. Gray Maynard: $52,000 ($26,000 win bonus) ^ Brian Stann: $122,000 ($21,000 win bonus) def. Chris Leben: $96,000 Thiago Silva: $110,000 ($55,000 win bonus) def. Brandon Vera: $70,000 Dong Hyun Kim: $70,000 ($35,000 win bonus) def. Nate Diaz: $63,000 Clay Guida: $62,000 ($31,000 win bonus) def. Takanori Gomi: $50,000 Jeremy Stephens: $36,000 ($18,000 win bonus) def. Marcus Davis: $31,000 Dustin Poirier: $8,000 ($4,000 win bonus) def. Josh Grispi: $15,000 Brad Tavares: $16,000 ($8,000 win bonus) def. Phil Baroni: $25,000 Diego Nunes: $20,000 ($10,000 win bonus) def. Mike Brown: $23,000 Daniel Roberts: $24,000 ($12,000 win bonus) def. Greg Soto: $8,000 Jacob Volkmann: $24,000 ($12,000 win bonus) def. Antonio McKee: $15,000 ^Although not reflected in the NSAC paperwork, both Edgar and Maynard received their win bonuses despite the draw. References",
                    "score": 0.9079682230949402
                },
                {
                    "id": 16681544,
                    "contents": "Three Dollars (novel)\nPlot summary",
                    "score": 0.9074870944023132
                },
                {
                    "id": 26490285,
                    "contents": "The Price to Pay\nThe Price to Pay may refer to: The Price to Pay (book), 2012 autobiography by Joseph Fadelle The Price to Pay (film), 2007 French comedy film The Price to Pay, 1996 TVB series",
                    "score": 0.9022534489631653
                },
                {
                    "id": 29683141,
                    "contents": "UFC 263\nIsrael Adesanya: $500,000 (no win bonus) def. Marvin Vettori: $350,000 Brandon Moreno: $200,000 (includes $100,000 win bonus) def. Deiveson Figueiredo: $210,000 Leon Edwards: $220,000 (includes $110,000 win bonus) def. Nate Diaz: $250,000 Belal Muhammad: $160,000 (includes $80,000) def. Demian Maia: $175,000 Paul Craig: $110,000 (includes $55,000 win bonus) def. Jamahal Hill: $28,000 Brad Riddell: $80,000 (includes $40,000 win bonus) def. Drew Dober: $87,000 Eryk Anders: $150,000 (includes $75,000 win bonus) def. Darren Stewart: $45,000 Lauren Murphy: $140,000 (includes $70,000 win bonus) def. Joanne Calderwood: $51,000 Movsar Evloev: $72,000 (includes $36,000 win bonus) def. Hakeem Dawodu: $55,000 Pannie Kinziad: $56,000 (includes $28,000 win bonus) def. Alexis Davis: $43,000 Terrance McKinney: $24,000 (includes $12,000 win bonus) def. Matt Frevola: $23,000 Steven Peterson: $46,000 (includes $23,000 win bonus) def. Chase Hooper: $37,000 ¹",
                    "score": 0.8996501564979553
                },
                {
                    "id": 26872122,
                    "contents": "UFC 202\nRaquel Pennington: $46,000 (includes $23,000 win bonus) def. Elizabeth Phillips: $12,000 Artem Lobov: $26,000 (includes $13,000 win bonus) def. Chris Avila: $10,000 Cortney Casey: $40,000 (includes $20,000 win bonus) def. Randa Markos: $14,000 Lorenz Larkin: $78,000 (includes $39,000 win bonus) def. Neil Magny: $47,000 Colby Covington: $42,000 (includes $21,000 win bonus) def. Max Griffin: $10,000 Marvin Vettori: $20,000 (includes $10,000 win bonus) def. Alberto Uda: $10,000",
                    "score": 0.8973652124404907
                },
                {
                    "id": 1092536,
                    "contents": "Fixed-odds betting\nFractional odds",
                    "score": 0.8970614671707153
                },
                {
                    "id": 2062263,
                    "contents": "Warhammer 40,000\nFirst edition (Warhammer 40,000: Rogue Trader) (1987)",
                    "score": 0.8963373303413391
                },
                {
                    "id": 26326786,
                    "contents": "UFC 207\nAntônio Carlos Júnior: $42,000 (includes $21,000 win bonus) def. Marvin Vettori: $12,000 Alex Garcia: $36,000 (includes $18,000 win bonus) def. Mike Pyle: $55,000 Niko Price: $24,000 (includes $12,000 win bonus) def. Brandon Thatch: $22,000 Alex Oliveira: $28,000 vs. Tim Means: $35,000 ³",
                    "score": 0.8962372541427612
                },
                {
                    "id": 26252257,
                    "contents": "UFC 222\nAlexander Hernandez: $26,000 (includes $13,000 win bonus) def. Beneil Dariush: $48,000 John Dodson: $82,000 (includes $41,000 win bonus) def. Pedro Munhoz: $34,000 C.B. Dollaway: $92,000 (includes $46,000 win bonus) def. Hector Lombard: $62,000 Zak Ottow: $36,000 (includes $18,000 win bonus) def. Mike Pyle: $55,000 Cody Stamann: $40,000 (includes $20,000 win bonus) def. Bryan Caraway: $21,000 Jordan Johnson: $28,000 (includes $14,000 win bonus) def. Adam Milstead: $12,000",
                    "score": 0.8949229717254639
                },
                {
                    "id": 21329682,
                    "contents": "UFC 155\nJamie Varner: $24,000 (includes $12,000 win bonus) def. Melvin Guillard: $42,000 Myles Jury: $16,000 (includes $8,000 win bonus) def. Michael Johnson: $14,000 Todd Duffee: $16,000 (includes $8,000 win bonus) def. Phil De Fries: $8,000 Max Holloway: $24,000 (includes $12,000 win bonus) def. Leonard Garcia: $20,000 John Moraga: $22,000 (includes $11,000 win bonus) def. Chris Cariaso: $12,000",
                    "score": 0.8945029377937317
                },
                {
                    "id": 22156097,
                    "contents": "UFC 173\nChris Holdsworth: $30,000 (includes $15,000 win bonus) def. Chico Camus: $12,000 Mitch Clarke: $20,000 (includes $10,000 win bonus) def. Al Iaquinta: $14,000 Vinc Pichel: $16,000 (includes $8,000 win bonus) def. Anthony Njokuani: $20,000 Sam Sicilia: $20,000 (includes $10,000 win bonus) def. Aaron Phillips: $8,000 Li Jingliang: $16,000 (includes $10,000 win bonus) def. David Michaud: $8,000",
                    "score": 0.8944492340087891
                },
                {
                    "id": 1092540,
                    "contents": "Fixed-odds betting\nDecimal odds",
                    "score": 0.8939488530158997
                },
                {
                    "id": 21337798,
                    "contents": "2012 in UFC\nDemetrious Johnson: $40,000 (includes $20,000 win bonus) def. Ian McCall: $9,000 Erick Silva: $16,000 (includes $8,000 win bonus) def. Charlie Brenneman: $18,000 Mike Pyle: $66,000 (includes $33,000 win bonus) def. Josh Neer: $14,000 Eddie Wineland: $20,000 (includes $10,000 win bonus) def. Scott Jorgensen: $20,500 Mike Pierce: $40,000 (includes $20,000 win bonus) def. Carlos Eduardo Rocha: $8,000 Seth Baczynski: $20,000 (includes $10,000 win bonus) def. Lance Benoist: $8,000 Matt Grice: $12,000 (includes $6,000 win bonus) def. Leonard Garcia: $20,000 Dustin Pague: $20,000 (includes $10,000 win bonus) def. Jared Papazian: $6,000 Tim Means: $16,000 (includes $8,000 win bonus) def. Justin Salas: $8,000 Buddy Roberts: $12,000 (includes $6,000 win bonus) def. Caio Magalhaes: $8,000 Henry Martinez: $12,000 (includes $6,000 win bonus) def. Bernardo Magalhaes: $8,000 Sean Pierson: $20,000 (includes $10,000 win bonus) def. Jake Hecht: $8,000 UFC 149: Faber vs. Barão",
                    "score": 0.8937833309173584
                },
                {
                    "id": 24973000,
                    "contents": "The 4% Solution\nIt was also presented by Amity Shlaes and Brendan Miniter at the Harvard Club of New York. Moreover, the Philadelphia Media Network published an excerpt by Brendan Miniter. It was also promoted by George W. Bush's brother, Jeb Bush, who served as the Governor of Florida from 1999 to 2007. He suggested the book should be used by then presidential candidate Mitt Romney to offer a bold vision for leadership and win the election, instead of simply criticizing President Barack Obama's policies. Content The book contains essays by academics and businesspeople, five of which are Nobel Prize-winning economists: Robert Lucas, Gary Becker, Edward Prescott, Vernon Smith and Myron Scholes.",
                    "score": 0.8936526775360107
                },
                {
                    "id": 27431741,
                    "contents": "UFC 229\nKhabib Nurmagomedov: $2,000,000 (no win bonus) def. Conor McGregor: $3,000,000 Tony Ferguson: $155,000 (includes $5,000 win bonus) def. Anthony Pettis: $145,000 Dominick Reyes: $90,000 (includes $45,000 win bonus) def. Ovince Saint Preux: $86,000 Derrick Lewis: $270,000 (includes $135,000 win bonus) def. Alexander Volkov: $75,000 Michelle Waterson: $100,000 (includes $50,000 win bonus) def. Felice Herrig: $40,000 Jussier Formiga: $86,000 (includes $43,000 win bonus) def. Sergio Pettis: $46,000 Vicente Luque: $76,000 (includes $38,000 win bonus) def. Jalin Turner: $10,000 Aspen Ladd: $24,000 (includes $12,000 win bonus) def. Tonya Evinger: $30,000 Scott Holtzman: $60,000 (includes $30,000 win bonus) def. Alan Patrick: $30,000 Yana Kunitskaya: $50,000 (includes $25,000 win bonus) def. Lina Länsberg: $20,000 Nik Lentz: $100,000 (includes $50,000 win bonus) def. Grey Maynard: $54,000 Tony Martin: $56,000 (includes $28,000 win bonus) def. Ryan LaFlare: $33,000",
                    "score": 0.8931220769882202
                },
                {
                    "id": 18462793,
                    "contents": "UFC 137\nNick Diaz: $200,000 (no win bonus) def. BJ Penn: $150,000 Cheick Kongo: $140,000 ($70,000 win bonus) def. Matt Mitrione: $10,000 Roy Nelson: $40,000 ($20,000 win bonus) def. Mirko Cro Cop: $75,000 Scott Jorgensen: $33,000 ($16,500 win bonus) def. Jeff Curran: $8,000 Hatsu Hioki: $30,000 ($15,000 win bonus) def. George Roop: $8,000 Donald Cerrone: $54,000 ($27,000 win bonus) def. Dennis Siver: $27,000 Bart Palaszewski: $28,500 ($10,000 win bonus) def. Tyson Griffin: $25,500 ^ Brandon Vera: $120,000 ($60,000 win bonus) def. Eliot Marshall: $15,000 Ramsey Nijem: $20,000 ($10,000 win bonus) def. Danny Downes: $5,000 Francis Carmont: $12,000 ($6,000 win bonus) def. Chris Camozzi: $8,000 Clifford Starks: $12,000 ($6,000 win bonus) def. Dustin Jacoby: $6,000",
                    "score": 0.8925895690917969
                },
                {
                    "id": 27351402,
                    "contents": "UFC 236\nDustin Poirier: $250,000 (no win bonus) def. Max Holloway: $350,000 Israel Adesanya: $350,000 (no win bonus) def. Kelvin Gastelum: $150,000 Khalil Rountree Jr.: $70,000 (includes $35,000 win bonus) def. Eryk Anders: $50,000 Dwight Grant: $24,000 (includes $12,000 win bonus) def. Alan Jouban: $43,000 Nikita Krylov: $160,000 (includes $80,000 win bonus) def. Ovince Saint Preux: $86,000 Matt Frevola: $20,000 (includes $10,000 win bonus) def. Jalin Turner: $12,000 Alexandre Pantoja: $36,000 (includes $18,000 win bonus) def. Wilson Reis: $34,000 Max Griffin: $40,000 (includes $20,000 win bonus) def. Zelim Imadaev: $10,000 Khalid Taha: $20,000 (includes $10,000 win bonus) def. Boston Salmon: $10,000 Belal Muhammad: $70,000 (includes $35,000 win bonus) def. Curtis Millender: $31,000 Montel Jackson: $24,000 (includes $12,000 win bonus) def. Andre Soukhamthath: $22,000 Poliana Botelho: $50,000 (includes $25,000 win bonus) def. Lauren Mueller: $12,000",
                    "score": 0.8923563957214355
                },
                {
                    "id": 17226802,
                    "contents": "Third Test, 1948 Ashes series\n9 July: Day Two",
                    "score": 0.8923110365867615
                },
                {
                    "id": 13472858,
                    "contents": "Mike Loynd\nAfter moving his record to 2–0, Loynd would begin to struggle. Many experts felt it was due to his not having pitched as many innings before during a season. Rangers' manager Bobby Valentine began to use him more selectively, but yet he struggled greatly and for the year ended with 2 wins, 2 losses, and an ERA of 5.36.",
                    "score": 0.8922066688537598
                },
                {
                    "id": 15633156,
                    "contents": "UFC 104\nLyoto Machida: ($200,000, includes $100,000 win bonus) def. Maurício Rua: (250,000) ^ Cain Velasquez: ($70,000 includes $35,000 win bonus) def. Ben Rothwell: ($50,000) Gleison Tibau: ($38,000 includes $19,000 win bonus) def. Josh Neer: ($14,000) Joe Stevenson: ($94,000 includes $47,000 win bonus) def. Spencer Fisher: ($26,000) Anthony Johnson: ($30,000 includes $15,000 win bonus) def. Yoshiyuki Yoshida: ($12,000) ^ Ryan Bader: ($30,000 includes $15,000 win bonus) def. Eric Schafer: ($13,000) Pat Barry: ($14,000 includes $7,000 win bonus) def. Antoni Hardonk: ($16,000) Chael Sonnen: ($54,000 includes $27,000 win bonus) def. Yushin Okami: ($18,000) Jorge Rivera: ($36,000 includes $18,000 win bonus) def. Rob Kimmons: ($9,000) Kyle Kingsbury: ($16,000 includes $8,000 win bonus) def. Razak Al-Hassan: ($3,000) Stefan Struve: ($14,000 includes $7,000 win bonus) def. Chase Gormley: ($10,000)",
                    "score": 0.890509843826294
                },
                {
                    "id": 24873777,
                    "contents": "UFC 157\nDennis Bermudez: $20,000 (includes $10,000 win bonus) def. Matt Grice: $8,000 Sam Stout: $52,000 (includes $26,000 win bonus) def. Caros Fodor: $15,000 Kenny Robertson: $16,000 (includes $8,000 win bonus) def. Brock Jardine: $8,000 Neil Magny: $16,000 (includes $8,000 win bonus) def. Jon Manley: $8,000 Nah-Shon Burrell: $12,500 (includes $7,000 win bonus) def. Yuri Villefort: $6,550 ^",
                    "score": 0.8904967308044434
                },
                {
                    "id": 25217518,
                    "contents": "UFC 209\nDarren Elkins: $92,000 (includes $46,000 win bonus) def. Mirsad Bektić: $21,000 Iuri Alcântara: $68,000 (includes $24,000 win bonus) def. Luke Sanders: $12,000 Mark Godbeer: $24,000 (includes $12,000 win bonus) def. Daniel Spitz: $12,000 Tyson Pedro: $24,000 (includes $12,000 win bonus) def. Paul Craig: $12,000 Albert Morales: $20,000 (includes $10,000 win bonus) def. Andre Soukhamthath: $10,000",
                    "score": 0.8904885649681091
                },
                {
                    "id": 28993126,
                    "contents": "UFC 250\nIan Heinisch: $80,000 (includes $40,000 win bonus) def. Gerald Meerschaert: $33,000 Cody Stamann: $72,000 (includes $36,000 win bonus) def. Brian Kelleher: $33,000 Maki Pitolo: $20,000 (includes $10,000 win bonus) def. Charles Byrd: $12,000 Alex Perez: $80,000 (includes $40,000 win bonus) def. Jussier Formiga: $98,000 Devin Clark: $96,000 (includes $48,000 win bonus) def. Alonzo Menifield: $14,000 Herbert Burns: $24,000 (includes $12,000 win bonus) def. Evan Dunham: $60,000",
                    "score": 0.8903191089630127
                },
                {
                    "id": 23445059,
                    "contents": "UFC 189\nJohn Howard: $42,000 (includes $21,000 win bonus) def. Cathal Pendred: $10,000 Cody Garbrandt: $20,000 (includes $10,000 win bonus) def. Henry Briones: $10,000 Louis Smolka: $30,000 (includes $15,000 win bonus) def. Neil Seery: $10,000 Cody Pfister: $20,000 (includes $10,000 win bonus) def. Yosdenis Cedeno: $13,000",
                    "score": 0.8902202844619751
                },
                {
                    "id": 7540409,
                    "contents": "The Millionaire Next Door\nMillion dollar choices",
                    "score": 0.8898007869720459
                },
                {
                    "id": 21039361,
                    "contents": "Upside Down (book)\nEconomics",
                    "score": 0.8897401094436646
                },
                {
                    "id": 22572707,
                    "contents": "Against All Odds (TV series)\nReferences 1990s American reality television series",
                    "score": 0.8897124528884888
                },
                {
                    "id": 7753411,
                    "contents": "100 mexicanos dijeron\nIf one or both family members accumulate a total of 200 points or more, the family wins Dinero Rápido and MX$100,000. If the family members give the top answer for each question, they win a MX$25,000 bonus, regardless of the outcome. If only one family member named all five top answers, that bonus would double to MX$50,000.",
                    "score": 0.8896405100822449
                },
                {
                    "id": 8110420,
                    "contents": "The 9 Cleveland\n2005 purchase",
                    "score": 0.8895695209503174
                },
                {
                    "id": 24495493,
                    "contents": "UFC 182\nShawn Jordan: $44,000 (includes $22,000 win bonus) def. Jared Cannonier: $8,000 Evan Dunham: $54,000 (includes $27,000 win bonus) def. Rodrigo Damm: $12,000 Omari Akhmedov: $20,000 (includes $10,000 win bonus) def. Mats Nilsson: $8,000 Marion Reneau: $17,600 (includes $8,000 win bonus) def. Alexis Dufresne: $6,400 ^",
                    "score": 0.8895261883735657
                },
                {
                    "id": 24263931,
                    "contents": "The Next Four Years\nBox set",
                    "score": 0.8892947435379028
                },
                {
                    "id": 17642674,
                    "contents": "Two-point discrimination\nAlternative tests",
                    "score": 0.8891866207122803
                },
                {
                    "id": 2062277,
                    "contents": "Warhammer 40,000\nSixth edition (2012)",
                    "score": 0.889050304889679
                },
                {
                    "id": 16381889,
                    "contents": "Extreme (2009 TV series)\nReferences Travel Channel original programming 2000s American reality television series 2009 American television series debuts 2010 American television series debuts",
                    "score": 0.8888804912567139
                },
                {
                    "id": 19908922,
                    "contents": "Plus-Minus (Stockhausen)\nReferences Cited sources",
                    "score": 0.8883376717567444
                },
                {
                    "id": 24964614,
                    "contents": "UFC 181\nCorey Anderson: $30,000 (includes $15,000 win bonus) def. Justin Jones: $8,000 Raquel Pennington: $20,000 (includes $10,000 win bonus) def. Ashlee Evans-Smith: $8,000 Sergio Pettis: $30,000 (includes $15,000 win bonus) def. Matt Hobar: $10,000 Clay Collard: $16,000 (includes $8,000 win bonus) def. Alex White: $8,000",
                    "score": 0.8882379531860352
                },
                {
                    "id": 14464219,
                    "contents": "1996 True Value 200\nHis teammate Tyce Carlson, qualifying for his first Indy car race, only got to do a single lap after missing his qualifying turn, but it was good enough for 16th place, in front of favourites Davey Hamilton and Roberto Guerrero.",
                    "score": 0.8881487846374512
                },
                {
                    "id": 19871173,
                    "contents": "UFC 146\nDarren Elkins: $28,000 (includes $14,000 win bonus) def. Diego Brandao: $15,000 Jamie Varner: $20,000 (includes $10,000 win bonus) def. Edson Barboza: $18,000 C.B. Dollaway: $40,000 (includes $20,000 win bonus) def. Jason Miller: $45,000 Dan Hardy: $50,000 (includes $25,000 win bonus) def. Duane Ludwig: $18,000 Paul Sass: $20,000 (includes $10,000 win bonus) def. Jacob Volkmann: $20,000 Glover Teixeira: $30,000 (includes $15,000 win bonus) def. Kyle Kingsbury: $12,000 Mike Brown: $52,000 (includes $26,000 win bonus) def. Daniel Pineda:''' $10,000",
                    "score": 0.8881219029426575
                },
                {
                    "id": 12981858,
                    "contents": "Save America's Treasures\n2000 ($30 million awarded)",
                    "score": 0.8880789875984192
                },
                {
                    "id": 322032,
                    "contents": "NMEA 0183\n$GPGGA,092750.000,5321.6802,N,00630.3372,W,1,8,1.03,61.7,M,55.2,M,,*76 $GPGSA,A,3,10,07,05,02,29,04,08,13,,,,,1.72,1.03,1.38*0A $GPGSV,3,1,11,10,63,137,17,07,61,098,15,05,59,290,20,08,54,157,30*70 $GPGSV,3,2,11,02,39,223,19,13,28,070,17,26,23,252,,04,14,186,14*79 $GPGSV,3,3,11,29,09,301,24,16,09,020,,36,,,*76 $GPRMC,092750.000,A,5321.6802,N,00630.3372,W,0.02,31.66,280511,,,A*43 $GPGGA,092751.000,5321.6802,N,00630.3371,W,1,8,1.03,61.7,M,55.3,M,,*75 $GPGSA,A,3,10,07,05,02,29,04,08,13,,,,,1.72,1.03,1.38*0A $GPGSV,3,1,11,10,63,137,17,07,61,098,15,05,59,290,20,08,54,157,30*70 $GPGSV,3,2,11,02,39,223,16,13,28,070,17,26,23,252,,04,14,186,15*77 $GPGSV,3,3,11,29,09,301,24,16,09,020,,36,,,*76 $GPRMC,092751.000,A,5321.6802,N,00630.3371,W,0.06,31.66,280511,,,A*45 Note some blank fields, for example: GSV records, which describe satellites 'visible', lack the SNR (signal–to–noise ratio) field for satellite 16 and all data for satellite 36.",
                    "score": 0.8880079388618469
                },
                {
                    "id": 388619,
                    "contents": "Midge (Barbie)\nVintage years",
                    "score": 0.8879348039627075
                },
                {
                    "id": 22194308,
                    "contents": "Edgar S. Gorrell\nNotes Footnotes Citations",
                    "score": 0.8874394297599792
                },
                {
                    "id": 3917382,
                    "contents": "Todd Solondz\nWelcome to the Dollhouse",
                    "score": 0.8873673677444458
                },
                {
                    "id": 23840032,
                    "contents": "The World's Billionaires 2012\nAnnual list",
                    "score": 0.8872720003128052
                },
                {
                    "id": 5419975,
                    "contents": "Ordinary least squares\nIn this table: The Value column gives the least squares estimates of parameters βj The Std error column shows standard errors of each coefficient estimate: The t-statistic and p-value columns are testing whether any of the coefficients might be equal to zero. The t-statistic is calculated simply as . If the errors ε follow a normal distribution, t follows a Student-t distribution. Under weaker conditions, t is asymptotically normal. Large values of t indicate that the null hypothesis can be rejected and that the corresponding coefficient is not zero. The second column, p-value, expresses the results of the hypothesis test as a significance level. Conventionally, p-values smaller than 0.05 are taken as evidence that the population coefficient is nonzero.",
                    "score": 0.8872565627098083
                },
                {
                    "id": 11228386,
                    "contents": "Fairey Delta 1\nSpecifications (Fairey Delta 1) See also References Citations",
                    "score": 0.8869866728782654
                },
                {
                    "id": 27367028,
                    "contents": "Herbert Diess\nNotes References New wave; Volkswagen The Economist. 430.9134 (16 March 2019): p. 58. Word count: 571. Volkswagen Group executives 1958 births Living people FC Bayern Munich board members",
                    "score": 0.8867151141166687
                },
                {
                    "id": 18601268,
                    "contents": "Lindsey Wixson\nBreakthrough, 2009–2010",
                    "score": 0.8867130875587463
                },
                {
                    "id": 1938400,
                    "contents": "Herbert Spencer\nIn recent years, much more positive estimates have appeared, as well as a still highly negative estimate.",
                    "score": 0.8866026997566223
                }
            ],
            "metric_score": {
                "retrieval_recall": 1,
                "retrieval_precision": 0.2
            }
        }
    },
    {
        "id": "test_38",
        "question": " Calculate (a) the wavelength and kinetic energy of an electron in a beam of electrons accelerated by a voltage increment of $100 \\mathrm{~V}$ ",
        "golden_answers": [
            " 1.602"
        ],
        "output": {
            "retrieval_result": [
                {
                    "id": 1169136,
                    "contents": "Photoelectric effect\nThe experimental results disagree with classical electromagnetism, which predicts that continuous light waves transfer energy to electrons, which would then be emitted when they accumulate enough energy. An alteration in the intensity of light would theoretically change the kinetic energy of the emitted electrons, with sufficiently dim light resulting in a delayed emission. The experimental results instead show that electrons are dislodged only when the light exceeds a certain frequency—regardless of the light's intensity or duration of exposure. Because a low-frequency beam at a high intensity could not build up the energy required to produce photoelectrons, as it would have if light's energy were coming from a continuous wave, Albert Einstein proposed that a beam of light is not a wave propagating through space, but a swarm of discrete energy packets, known as photons.",
                    "score": 0.9071173071861267
                },
                {
                    "id": 6598017,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path (IMFP) is an index of how far an electron on average travels through a solid before losing energy. If a monochromatic primary beam of electrons is incident on a solid surface, the majority of incident electrons lose their energy because they interact strongly with matter, leading to plasmon excitation, electron-hole pair formation, and vibrational excitation. The intensity of the primary electrons, , is damped as a function of the distance, d, into the solid. The intensity decay can be expressed as follows: where is the intensity after the primary electron beam has traveled through the solid to a distance . The parameter , termed the inelastic mean free path (IMFP), is defined as the distance an electron beam can travel before its intensity decays to of its initial value. (Note that this is equation is closely related to the Beer-Lambert law.)",
                    "score": 0.9052249789237976
                },
                {
                    "id": 4788815,
                    "contents": "Lamb shift\nThis particular difference is a one-loop effect of quantum electrodynamics, and can be interpreted as the influence of virtual photons that have been emitted and re-absorbed by the atom. In quantum electrodynamics the electromagnetic field is quantized and, like the harmonic oscillator in quantum mechanics, its lowest state is not zero. Thus, there exist small zero-point oscillations that cause the electron to execute rapid oscillatory motions. The electron is \"smeared out\" and each radius value is changed from r to r + δr (a small but finite perturbation). The Coulomb potential is therefore perturbed by a small amount and the degeneracy of the two energy levels is removed. The new potential can be approximated (using atomic units) as follows: The Lamb shift itself is given by with k(n, 0) around 13 varying slightly with n, and with log(k(n,)) a small number (approx. -0.05) making k(n,) close to unity. For a derivation of ΔELamb see for example:",
                    "score": 0.9041722416877747
                },
                {
                    "id": 5205023,
                    "contents": "Compton wavelength\nThus the uncertainty in position must be greater than half of the reduced Compton wavelength . The Compton wavelength can be contrasted with the de Broglie wavelength, which depends on the momentum of a particle and determines the cutoff between particle and wave behavior in quantum mechanics. Notably, de Broglie's derivation of the de Broglie wavelength is based on the assumption that an observed particle is associated with a periodic phenomenon of the particle's Compton frequency. Relationship to other constants Typical atomic lengths, wave numbers, and areas in physics can be related to the reduced Compton wavelength for the electron () and the electromagnetic fine structure constant (). The Bohr radius is related to the Compton wavelength by: The classical electron radius is about 3 times larger than the proton radius, and is written: The Rydberg constant, having dimensions of linear wavenumber, is written: This yields the sequence: .",
                    "score": 0.8989987969398499
                },
                {
                    "id": 200651,
                    "contents": "Electron diffraction\nThen the relativistic velocity is given by the equation Substitution of the De Broglie equation to the above expression of energy gives which leads to the final expression for the relativistic wavelength The wavelength of the electrons in a 10 kV SEM is then 12.2 × 10−12 m (12.2 pm) while in a 200 kV TEM the wavelength is 2.5 pm. In comparison, the wavelength of X-rays usually used in X-ray diffraction is in the order of 100 pm (Cu Kα: λ=154 pm). Diffraction on atomic lattice Wavelength of the electron beam used in a typical electron microscope is sufficiently small, that crystal lattice acts as a diffraction grating. Therefore a diffraction pattern can be formed with beams diffracted under certain angles and intensities. Diffraction angles",
                    "score": 0.8984764218330383
                },
                {
                    "id": 26112171,
                    "contents": "Classical Electrodynamics (book)\nTable of contents (3rd edition) Introduction and Survey Chapter 1: Introduction to Electrostatics Chapter 2: Boundary-value Problems in Electrostatics I Chapter 3: Boundary-value Problems in Electrostatics II Chapter 4: Multipoles, Electrostatics of Macroscopic Media, Dielectrics Chapter 5: Magnetostatics, Faraday's Law, Quasi-static Fields Chapter 6: Maxwell Equations, Macroscopic Electromagnetism, Conservation Laws Chapter 7: Plane Electromagnetic Waves and Wave Propagation Chapter 8: Waveguides, Resonant Cavities, and Optical Fibers Chapter 9: Radiating Systems, Multipole Fields and Radiation Chapter 10: Scattering and Diffraction Chapter 11: Special Theory of Relativity Chapter 12: Dynamics of Relativistic Particles and Electromagnetic Fields Chapter 13: Collisions, Energy Loss, and Scattering of Charged Particles, Cherenkov and Transition Radiation Chapter 14: Radiation by Moving Charges Chapter 15: Bremsstrahlung, Method of Virtual Quanta, Radiative Beta Processes",
                    "score": 0.8980710506439209
                },
                {
                    "id": 1694091,
                    "contents": "Electron\nThe electron is a subatomic particle (denoted by the symbol or ) whose electric charge is negative one elementary charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, ħ. Being fermions, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence",
                    "score": 0.89759761095047
                },
                {
                    "id": 16992309,
                    "contents": "Electron mass\nThe electron mass (symbol: me) is the mass of a stationary electron, also known as the invariant mass of the electron. It is one of the fundamental constants of physics. It has a value of about or about , equivalent to an energy of about or about . Terminology The term \"rest mass\" is sometimes used because in special relativity the mass of an object can be said to increase in a frame of reference that is moving relative to that object (or if the object is moving in a given frame of reference). Most practical measurements are carried out on moving electrons. If the electron is moving at a relativistic velocity, any measurement must use the correct expression for mass. Such correction is only substantial for electrons accelerated by voltages of well over 100 kV.",
                    "score": 0.8972018957138062
                },
                {
                    "id": 200649,
                    "contents": "Electron diffraction\nWavelength of electrons Due to the wave-particle duality, electrons behave as particles as well as waves. A basic characteristic of the wave is a wavelength. The electron wavelength is given by de Broglie equation as Here is Planck's constant and is momentum, where is the electron mass and the velocity. Since the electrons are charged particles, they can be accelerated using electric potential. This allows the electrons to get accelerated to speeds, where relativistic theory needs to be applied. Therefore, there are two definitions of wavelength - non-relativistic and relativistic. Non-relativistic theory In a non-relativistic theory, the electrons accelerated in an electric potential gain the velocity given by the equation where is the electron rest mass, is the elementary charge and is the speed of light. Substituting the momentum and velocity to the de Broglie equation we receive Relativistic theory",
                    "score": 0.8971737623214722
                },
                {
                    "id": 200650,
                    "contents": "Electron diffraction\nwhere is the electron rest mass, is the elementary charge and is the speed of light. Substituting the momentum and velocity to the de Broglie equation we receive Relativistic theory In an electron microscope, the accelerating potential is usually several thousand volts, causing the electron to travel at an appreciable fraction of the speed of light. Scanning electron microscopes typically operate at an accelerating voltage of 10,000 volts (10 kV) giving an electron velocity approximately 20% of the speed of light, while a typical TEM operates at 200 kV raising the electron velocity to 70% the speed of light. Therefore, relativistic effects need to be taken into account. The relativistic relation between energy and momentum is It follows then, that the ratio between the electron mass and its rest mass (or Lorentz factor) is Then the relativistic velocity is given by the equation Substitution of the De Broglie equation to the above expression of energy gives",
                    "score": 0.896980345249176
                },
                {
                    "id": 10311466,
                    "contents": "Bethe formula\nThe formula For a particle with speed v, charge z (in multiples of the electron charge), and energy E, traveling a distance x into a target of electron number density n and mean excitation potential I, the relativistic version of the formula reads, in SI units: where c is the speed of light and ε0 the vacuum permittivity, , e and me the electron charge and rest mass respectively. Here, the electron density of the material can be calculated by where ρ is the density of the material, Z its atomic number, A its relative atomic mass, NA the Avogadro number and Mu the Molar mass constant. In the figure to the right, the small circles are experimental results obtained from measurements of various authors, while the red curve is Bethe's formula. Evidently, Bethe's theory agrees very well with experiment at high energy. The agreement is even better when corrections are applied (see below). For low energies, i.e., for small velocities of the particle β << 1, the Bethe formula reduces to",
                    "score": 0.8969554901123047
                },
                {
                    "id": 7771738,
                    "contents": "Larmor formula\nIn electrodynamics, the Larmor formula is used to calculate the total power radiated by a nonrelativistic point charge as it accelerates. It was first derived by J. J. Larmor in 1897, in the context of the wave theory of light. When any charged particle (such as an electron, a proton, or an ion) accelerates, energy is radiated away in the form of electromagnetic waves. For particle velocities that are small relative to the speed of light, the total power radiated is given by the Larmor formula: where or — is the proper acceleration, — is the charge, and — is the speed of light. A relativistic generalization is given by the Liénard–Wiechert potentials. In either unit system, the power radiated by a single electron can be expressed in terms of the classical electron radius and electron mass as:",
                    "score": 0.8969507813453674
                },
                {
                    "id": 7747994,
                    "contents": "Introduction to quantum mechanics\nelectron's energy should be proportional to the intensity of the incident radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a higher intensity of light would produce a higher voltage from the photoelectric device.",
                    "score": 0.8961763381958008
                },
                {
                    "id": 1184223,
                    "contents": "Outline of physics\nThe principles, sources, and properties of light Basic quantities Acceleration Electric charge Energy Entropy Force Length Mass Matter Momentum Potential energy Space Temperature Time Velocity Gravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment",
                    "score": 0.8958261013031006
                },
                {
                    "id": 6114470,
                    "contents": "Davisson–Germer experiment\nAccording to the de Broglie relation, electrons with kinetic energy of have a wavelength of . The experimental outcome was via Bragg's law, which closely matched the predictions. As Davisson and Germer state in their 1928 follow-up paper to their Nobel prize winning paper, \"These results, including the failure of the data to satisfy the Bragg formula, are in accord with those previously obtained in our experiments on electron diffraction. The reflection data fail to satisfy the Bragg relation for the same reason that the electron diffraction beams fail to coincide with their Laue beam analogues.\" However, they add, \"The calculated wave-lengths are in excellent agreement with the theoretical values of h/mv as shown in the accompanying table.\" So although electron energy diffraction does not follow the Bragg law, it did confirm de Broglie's theory that particles behave like waves. However, the experiments did not follow the de Broglie calculations which led to attempts by Carl Eckart,",
                    "score": 0.8955675959587097
                },
                {
                    "id": 1169148,
                    "contents": "Photoelectric effect\nthe formula for the maximum kinetic energy of the ejected electrons becomes Kinetic energy is positive, and is required for the photoelectric effect to occur. The frequency is the threshold frequency for the given material. Above that frequency, the maximum kinetic energy of the photoelectrons as well as the stopping voltage in the experiment rise linearly with the frequency, and have no dependence on the number of photons and the intensity of the impinging monochromatic light. Einstein's formula, however simple, explained all the phenomenology of the photoelectric effect, and had far-reaching consequences in the development of quantum mechanics.",
                    "score": 0.8951820731163025
                },
                {
                    "id": 6598018,
                    "contents": "Inelastic mean free path\nThe inelastic mean free path of electrons can roughly be described by a universal curve that is the same for all materials. See also Scattering theory Beer-Lambert law References Atomic, molecular, and optical physics",
                    "score": 0.8948925137519836
                },
                {
                    "id": 552404,
                    "contents": "Wavenumber\nIn general, the angular wavenumber k (i.e. the magnitude of the wave vector) is given by where ν is the frequency of the wave, λ is the wavelength, ω = 2πν is the angular frequency of the wave, and vp is the phase velocity of the wave. The dependence of the wavenumber on the frequency (or more commonly the frequency on the wavenumber) is known as a dispersion relation. For the special case of an electromagnetic wave in a vacuum, in which the wave propagates at the speed of light, k is given by: where E is the energy of the wave, ħ is the reduced Planck constant, and c is the speed of light in a vacuum. For the special case of a matter wave, for example an electron wave, in the non-relativistic approximation (in the case of a free particle, that is, the particle has no potential energy): Here p is the momentum of the particle, m is the mass of the particle, E is the kinetic energy of the particle, and ħ is the reduced Planck constant.",
                    "score": 0.8942922353744507
                },
                {
                    "id": 7747995,
                    "contents": "Introduction to quantum mechanics\nEinstein explained the effect by postulating that a beam of light is a stream of particles (\"photons\") and that, if the beam is of frequency , then each photon has an energy equal to . An electron is likely to be struck only by a single photon, which imparts at most an energy to the electron. Therefore, the intensity of the beam has no effect and only its frequency determines the maximum energy that can be imparted to the electron. To explain the threshold effect, Einstein argued that it takes a certain amount of energy, called the work function and denoted by , to remove an electron from the metal. This amount of energy is different for each metal. If the energy of the photon is less than the work function, then it does not carry sufficient energy to remove the electron from the metal. The threshold frequency, , is the frequency of a photon whose energy is equal to the work function:",
                    "score": 0.8939787149429321
                },
                {
                    "id": 1694154,
                    "contents": "Electron\nMotion and energy According to Einstein's theory of special relativity, as an electron's speed approaches the speed of light, from an observer's point of view its relativistic mass increases, thereby making it more and more difficult to accelerate it from within the observer's frame of reference. The speed of an electron can approach, but never reach, the speed of light in a vacuum, c. However, when relativistic electrons—that is, electrons moving at a speed close to c—are injected into a dielectric medium such as water, where the local speed of light is significantly less than c, the electrons temporarily travel faster than light in the medium. As they interact with the medium, they generate a faint light called Cherenkov radiation. The effects of special relativity are based on a quantity known as the Lorentz factor, defined as where v is the speed of the particle. The kinetic energy Ke of an electron moving with velocity v is:",
                    "score": 0.8938727378845215
                },
                {
                    "id": 1254126,
                    "contents": "Speed of light\nPropagation of light In classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed c with which electromagnetic waves (such as light) propagate in vacuum is related to the distributed capacitance and inductance of vacuum, otherwise respectively known as the electric constant ε0 and the magnetic constant μ0, by the equation In modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum.",
                    "score": 0.8935210704803467
                },
                {
                    "id": 1106390,
                    "contents": "Timeline of electromagnetism and classical optics\n1865 – James Clerk Maxwell publishes his landmark paper A Dynamical Theory of the Electromagnetic Field, in which Maxwell's equations demonstrated that electric and magnetic forces are two complementary aspects of electromagnetism. He shows that the associated complementary electric and magnetic fields of electromagnetism travel through space, in the form of waves, at a constant velocity of 3.0 × 108 m/s. He also proposes that light is a form of electromagnetic radiation and that waves of oscillating electric and magnetic fields travel through empty space at a speed that could be predicted from simple electrical experiments. Using available data, he obtains a velocity of 310,740,000 m/s and states \"This velocity is so nearly that of light, that it seems we have strong reason to conclude that light itself (including radiant heat, and other radiations if any) is an electromagnetic disturbance in the form of waves propagated through the electromagnetic field according to electromagnetic",
                    "score": 0.8933568000793457
                },
                {
                    "id": 1694106,
                    "contents": "Electron\nThe German-born British physicist Arthur Schuster expanded upon Crookes's experiments by placing metal plates parallel to the cathode rays and applying an electric potential between the plates. The field deflected the rays toward the positively charged plate, providing further evidence that the rays carried negative charge. By measuring the amount of deflection for a given level of current, in 1890 Schuster was able to estimate the charge-to-mass ratio of the ray components. However, this produced a value that was more than a thousand times greater than what was expected, so little credence was given to his calculations at the time. This is because it was assumed that the charge carriers were much heavier hydrogen or nitrogen atoms. Schuster's estimates would subsequently turn out to be largely correct. In 1892 Hendrik Lorentz suggested that the mass of these particles (electrons) could be a consequence of their electric charge.",
                    "score": 0.892180860042572
                },
                {
                    "id": 12697245,
                    "contents": "Magic angle (EELS)\nand where is the speed of the incoming electron divided by the speed of light (N.B., the symbol is also often used in the older literature to denote the collection angle instead of ). Of course, the above integrals may easily be evaluated in terms of elementary functions, but they are presented as above because in the above form it is easier to see that the former integral is due to momentum transfers which are perpendicular to the beam direction, whereas the latter is due to momentum transfers parallel to the beam direction. Using the above definition, it is then found that References Materials science Spectroscopy Chemical physics",
                    "score": 0.8916993737220764
                },
                {
                    "id": 10045498,
                    "contents": "Kaufmann–Bucherer–Neumann experiments\nKaufmann also made a calculation mistake in deriving the deflection curves. Those errors were corrected by him in 1902. In 1902 and 1903 Kaufmann performed another series of tests with updated and improved experimental techniques. The results were interpreted by him as a confirmation of Abraham's theory and of the assumption that the electron's mass is completely of electromagnetic origin. Hermann Starke conducted similar measurements in 1903, although he used cathode rays limited to 0.3c. The results that he obtained were interpreted by him as being in agreement with those of Kaufmann. Competing theories In 1902, Max Abraham published a theory based on the assumption that the electron was a rigid, perfect sphere, with its charge being distributed evenly on its surface. As explained above, he introduced the so-called \"transverse electromagnetic mass\" besides the \"longitudinal electromagnetic mass\", and argued that the entire electron mass is of electromagnetic origin.",
                    "score": 0.891137421131134
                },
                {
                    "id": 1168424,
                    "contents": "Photon\nThe Maxwell wave theory, however, does not account for all properties of light. The Maxwell theory predicts that the energy of a light wave depends only on its intensity, not on its frequency; nevertheless, several independent types of experiments show that the energy imparted by light to atoms depends only on the light's frequency, not on its intensity. For example, some chemical reactions are provoked only by light of frequency higher than a certain threshold; light of frequency lower than the threshold, no matter how intense, does not initiate the reaction. Similarly, electrons can be ejected from a metal plate by shining light of sufficiently high frequency on it (the photoelectric effect); the energy of the ejected electron is related only to the light's frequency, not to its intensity.",
                    "score": 0.8910126686096191
                },
                {
                    "id": 24468008,
                    "contents": "NA63 experiment\nAn electron entering an electric field is accelerated, and therefore must lose part of its energy in the form of a photon via the Bremsstrahlung effect - the process by which a charged particle emits electromagnetic radiation when being decelerated upon passing an atom, for instance in a solid material. By exploiting the relativistic phenomena of time dilatation and length contraction, the NA63 experiment has shown that this process of photon emission is not instantaneous, but rather, takes time. Because the process takes time, the photon production can be influenced experimentally. For non-relativistic particles this time is so short that investigations are very difficult, if not excluded. But for the relativistic particles used by NA63, their time is ‘slowed’ by a factor of about half a million due to the relativistic effect of time dilatation, making investigations possible.",
                    "score": 0.8905972838401794
                },
                {
                    "id": 3882728,
                    "contents": "Klein–Nishina formula\nThe Klein–Nishina formula gives the differential cross section of photons scattered from a single free electron in lowest order of quantum electrodynamics. At low frequencies (e.g., visible light) this yields Thomson scattering; at higher frequencies (e.g., x-rays and gamma-rays) this yields Compton scattering. For an incident unpolarized photon of energy , the differential cross section is: where is a differential cross section, is an infinitesimal solid angle element, is the fine structure constant (~1/137.04), is the scattering angle; is the \"reduced\" Compton wave length of the electron (~0.38616 pm); is the mass of an electron (~511 keV); and is the ratio of photon energy after and before the collision: Note that this result may also be expressed in terms of the classical electron radius :",
                    "score": 0.890557050704956
                },
                {
                    "id": 9251559,
                    "contents": "Linear energy transfer\nLET has therefore no meaning when applied to photons. However, many authors speak of \"gamma LET\" anyway, where they are actually referring to the LET of the secondary electrons, i.e., mainly Compton electrons, produced by the gamma radiation. The secondary electrons will ionize far more atoms than the primary photon. This gamma LET has little relation to the attenuation rate of the beam, but it may have some correlation to the microscopic defects produced in the absorber. Even a monoenergetic gamma beam will produce a spectrum of electrons, and each secondary electron will have a variable LET as it slows down, as discussed above. The \"gamma LET\" is therefore an average. The transfer of energy from an uncharged primary particle to charged secondary particles can also be described by using the mass energy-transfer coefficient. Biological effects",
                    "score": 0.8901371359825134
                },
                {
                    "id": 7469288,
                    "contents": "E (disambiguation)\nPhysics and engineering e (or e−) or electron, a fundamental subatomic particle e or elementary charge, the absolute value of the electric charge carried by a single electron. e or coefficient of restitution (COR), a measure of the elasticity of a collision in mechanics E, the symbol for energy in equations concerning mass-energy equivalence E or Young's modulus, a measure of stiffness in solid mechanics E or exa-, the SI prefix for 1018 E-layer or the Kennelly-Heaviside layer, part of the ionosphere E, the symbol for an electric field e or orbital eccentricity, a measure of how much a conic section deviates from a circle E or Equal Energy spectrum, a definition of white in colour printing E, the electrode potential, the electromotive force of a cell built of two electrodes E°, the standard electrode potential",
                    "score": 0.8900999426841736
                },
                {
                    "id": 21587040,
                    "contents": "Dragomir Hurmuzescu\nShortly after the discovery of X-rays by Roentgen (1895), Hurmuzescu announced with Louis Benoist the discovery for the first time of the ionization effect produced by X-ray radiations on electrified gases and bodies. This effect is highlighted and measured with a device created by Hurmuzescu - the electroscope that bears his name. On April 28, 1896, Dragomir Hurmuzescu defended his doctoral thesis, titled \"Sur une nouvele détermination du rapport V entre les été électrostatiques et électromagnétiques\", which included one of the most accurate measurements for the speed of light. Teaching years In 1896, after obtaining his doctorate degree, Hurmuzescu returned to Romania and was appointed lecturer at the Department of Mathematical Physics from Iasi University. A year later, he took over, as a substitute professor, the Department of Gravity, Heat and Electricity, within the same faculty.",
                    "score": 0.8900215029716492
                },
                {
                    "id": 7748052,
                    "contents": "Introduction to quantum mechanics\nThe Lamb shift is an example of a quantum electrodynamics prediction that has been experimentally verified. It is an effect whereby the quantum nature of the electromagnetic field makes the energy levels in an atom or ion deviate slightly from what they would otherwise be. As a result, spectral lines may shift or split. Similarly, within a freely propagating electromagnetic wave, the current can also be just an abstract displacement current, instead of involving charge carriers. In QED, its full description makes essential use of short-lived virtual particles. There, QED again validates an earlier, rather mysterious concept. Standard Model",
                    "score": 0.8896337747573853
                },
                {
                    "id": 1106356,
                    "contents": "Timeline of electromagnetism and classical optics\n1675 – Robert Boyle discovers that electric attraction and repulsion can act across a vacuum and do not depend upon the air as a medium. Adds resin to the known list of \"electrics.\" 1675 – Isaac Newton delivers his theory of light 1676 – Olaus Roemer measures the speed of light by observing Jupiter's moons 1678 – Christiaan Huygens states his principle of wavefront sources and demonstrates the refraction and diffraction of light rays.",
                    "score": 0.8895098567008972
                },
                {
                    "id": 29201783,
                    "contents": "A History of the Theories of Aether and Electricity\nWhile there are many new paragraphs, references, and expanded footnotes throughout chapters two through eleven, much of the content remains the same as the first edition. Chapters two and three, as in the first edition, initiate the subject of electricity and magnetism, including Galvanism. Chapter two traces the history of electrostatics and magnetostatics from early developments through George Green's work on potential theory and his introduction of the vector potential and scalar potential. Chapter three, on Galvanism, discusses the history of electric current, centering on Galvani, Ohm, and Ampere. The fourth chapter, on the luminiferous medium, includes the discoveries of optical aberrations, polarization, and interference. This is the period of transition, from when Newton's corpuscular theory of light was widely held until the establishment of the wave theory after the experiments by Fresnel and Young. The fifth chapter records the development of theories modeling the aether",
                    "score": 0.8894929885864258
                },
                {
                    "id": 7748007,
                    "contents": "Introduction to quantum mechanics\nwhere , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit. The energy of the electron can also be calculated, and is given by . Thus Bohr's assumption that angular momentum is quantized means that an electron can inhabit only certain orbits around the nucleus and that it can have only certain energies. A consequence of these constraints is that the electron does not crash into the nucleus: it cannot continuously emit energy, and it cannot come closer to the nucleus than a0 (the Bohr radius). An electron loses energy by jumping instantaneously from its original orbit to a lower orbit; the extra energy is emitted in the form of a photon. Conversely, an electron that absorbs a photon gains energy, hence it jumps to an orbit that is farther from the nucleus.",
                    "score": 0.8887426853179932
                },
                {
                    "id": 1254165,
                    "contents": "Speed of light\nIn the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of . His method was improved upon by Léon Foucault who obtained a value of in 1862. In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed. In the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space at a speed equal to the above Weber/Kohlrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an",
                    "score": 0.8887147903442383
                },
                {
                    "id": 23172685,
                    "contents": "Frank Read\nFrank Henry Read (born 6 October 1934) is a British physicist. He is an Emeritus Professor of Physics at the University of Manchester. Research Read is known for his experimental studies of electron collisions with atoms and molecules, for associated work in instrument design, and for theoretical work on the interpretation of the experimental results. He made advances in the study of threshold effects in electron collisions, and of post-collision interactions in the near-threshold excitation of resonance states. His studies of the influence of molecular rotation and vibration on the angular distribution of scattered electrons enabled him to deduce the electronic configurations of short-lived molecular negative ion states. He also used the technique of delayed coincidences between electrons and photons for the precision measurements of lifetimes for atomic and molecular states. Books Electrostatic lenses (1976) Electromagnetic radiation (1980)",
                    "score": 0.8885294795036316
                },
                {
                    "id": 625208,
                    "contents": "Mathematical physics\nA couple of decades ahead of Newton's publication of a particle theory of light, the Dutch Christiaan Huygens (1629–1695) developed the wave theory of light, published in 1690. By 1804, Thomas Young's double-slit experiment revealed an interference pattern, as though light were a wave, and thus Huygens's wave theory of light, as well as Huygens's inference that light waves were vibrations of the luminiferous aether, was accepted. Jean-Augustin Fresnel modeled hypothetical behavior of the aether. The English physicist Michael Faraday introduced the theoretical concept of a field—not action at a distance. Mid-19th century, the Scottish James Clerk Maxwell (1831–1879) reduced electricity and magnetism to Maxwell's electromagnetic field theory, whittled down by others to the four Maxwell's equations. Initially, optics was found consequent of Maxwell's field. Later, radiation and then today's known electromagnetic spectrum were found also consequent of this electromagnetic field.",
                    "score": 0.8883402347564697
                },
                {
                    "id": 21522226,
                    "contents": "SwissFEL\nHow it works SwissFEL essentially consists of four components: an electron source, a linear accelerator, an arrangement of undulators, and measuring stations. Electrons are set free from a copper disk with a pulsed laser. The cloud of released electrons is accelerated and held together with an electrical field. They are guided into the linear accelerator, which further accelerates the electrons with alternating current at high frequency. The electrons now fly through the undulator, a stretch of dipole magnets in an alternating arrangement: This forces them onto a slalom course. With every change of direction, the electrons emit X-ray light. This produces an X-ray beam with laser-like properties, which can be used for experiments in the measuring stations.",
                    "score": 0.888326108455658
                },
                {
                    "id": 7748006,
                    "contents": "Introduction to quantum mechanics\nSome fundamental assumptions of the Bohr model were soon proven wrong—but the key result that the discrete lines in emission spectra are due to some property of the electrons in atoms being quantized is correct. The way that the electrons actually behave is strikingly different from Bohr's atom, and from what we see in the world of our everyday experience; this modern quantum mechanical model of the atom is discussed below. Bohr theorized that the angular momentum, , of an electron is quantized: where is an integer and is the Planck constant. Starting from this assumption, Coulomb's law and the equations of circular motion show that an electron with units of angular momentum orbits a proton at a distance given by , where is the Coulomb constant, is the mass of an electron, and is the charge on an electron. For simplicity this is written as where , called the Bohr radius, is equal to 0.0529 nm. The Bohr radius is the radius of the smallest allowed orbit.",
                    "score": 0.8880671262741089
                },
                {
                    "id": 1694155,
                    "contents": "Electron\nwhere me is the mass of electron. For example, the Stanford linear accelerator can accelerate an electron to roughly 51 GeV. Since an electron behaves as a wave, at a given velocity it has a characteristic de Broglie wavelength. This is given by λe = h/p where h is the Planck constant and p is the momentum. For the 51 GeV electron above, the wavelength is about , small enough to explore structures well below the size of an atomic nucleus. Formation",
                    "score": 0.8874285221099854
                },
                {
                    "id": 7747982,
                    "contents": "Introduction to quantum mechanics\nLight behaves in some aspects like particles and in other aspects like waves. Matter—the \"stuff\" of the universe consisting of particles such as electrons and atoms—exhibits wavelike behavior too. Some light sources, such as neon lights, give off only certain specific frequencies of light, a small set of distinct pure colors determined by neon's atomic structure. Quantum mechanics shows that light, along with all other forms of electromagnetic radiation, comes in discrete units, called photons, and predicts its spectral energies (corresponding to pure colors), and the intensities of its light beams. A single photon is a quantum, or smallest observable particle, of the electromagnetic field. A partial photon is never experimentally observed. More broadly, quantum mechanics shows that many properties of objects, such as position, speed, and angular momentum, that appeared continuous in the zoomed-out view of classical mechanics, turn out to be (in the very tiny, zoomed-in scale of",
                    "score": 0.8871818780899048
                },
                {
                    "id": 1696331,
                    "contents": "Electronvolt\nIt is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics, and high-energy astrophysics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion (109) electronvolts; it is equivalent to the GeV. Definition An electronvolt is the amount of kinetic energy gained or lost by a single electron accelerating from rest through an electric potential difference of one volt in vacuum. Hence, it has a value of one volt, , multiplied by the electron's elementary charge e, Therefore, one electronvolt is equal to The electronvolt, as opposed to the volt, is not an SI unit. The electronvolt (eV) is a unit of energy whereas the volt (V) is the derived SI unit of electric potential. The SI unit for energy is the joule (J).",
                    "score": 0.8871169686317444
                },
                {
                    "id": 7747993,
                    "contents": "Introduction to quantum mechanics\nIn 1887, Heinrich Hertz observed that when light with sufficient frequency hits a metallic surface, the surface emits electrons. In 1902, Philipp Lenard discovered that the maximum possible energy of an ejected electron is related to the frequency of the light, not to its intensity: if the frequency is too low, no electrons are ejected regardless of the intensity. Strong beams of light toward the red end of the spectrum might produce no electrical potential at all, while weak beams of light toward the violet end of the spectrum would produce higher and higher voltages. The lowest frequency of light that can cause electrons to be emitted, called the threshold frequency, is different for different metals. This observation is at odds with classical electromagnetism, which predicts that the electron's energy should be proportional to the intensity of the incident radiation. So when physicists first discovered devices exhibiting the photoelectric effect, they initially expected that a",
                    "score": 0.8869290351867676
                },
                {
                    "id": 25040489,
                    "contents": "International Year of Light\nHis 1865 paper, A Dynamical Theory of the Electromagnetic Field, provides a complete theoretical basis for the treatment of classical electromagnetic phenomena. He proved that the equations of the electromagnetic field could combine into a wave equation and suggested the existence of electromagnetic waves. Calculating the speed of propagation of these waves, he obtained the value of the speed of light, and concluded that it was an electromagnetic wave. Maxwell also left us outstanding contributions to colour theory, optics, Saturn's rings, statics, dynamics, solids, instruments, and statistical physics. However, his most important contributions were to electromagnetism.",
                    "score": 0.8868389129638672
                },
                {
                    "id": 1254106,
                    "contents": "Speed of light\nAll forms of electromagnetic radiation travel at the speed of light, not just visible light. Massless particles and field perturbations such as gravitational waves also travel at this speed in vacuum. Such particles and waves travel at regardless of the motion of the source or the inertial reference frame of the observer. Particles with nonzero rest mass can approach , but can never actually reach it, regardless of the frame of reference in which their speed is measured. In the special and general theories of relativity, interrelates space and time, and also appears in the famous equation of mass–energy equivalence, . In some cases objects or waves may appear to travel faster than light (e.g. phase velocities of waves, the appearance of certain high-speed astronomical objects, and particular quantum effects). The expansion of the universe is understood to exceed the speed of light beyond a certain boundary.",
                    "score": 0.8867447376251221
                },
                {
                    "id": 11719110,
                    "contents": "History of quantum mechanics\nFounding experiments Thomas Young's double-slit experiment demonstrating the wave nature of light. (c. 1801) Henri Becquerel discovers radioactivity. (1896) J. J. Thomson's cathode ray tube experiments (discovers the electron and its negative charge). (1897) The study of black-body radiation between 1850 and 1900, which could not be explained without quantum concepts. The photoelectric effect: Einstein explained this in 1905 (and later received a Nobel prize for it) using the concept of photons, particles of light with quantized energy. Robert Millikan's oil-drop experiment, which showed that electric charge occurs as quanta (whole units). (1909) Ernest Rutherford's gold foil experiment disproved the plum pudding model of the atom which suggested that the mass and positive charge of the atom are almost uniformly distributed. This led to the planetary model of the atom (1911).",
                    "score": 0.88674396276474
                },
                {
                    "id": 8365978,
                    "contents": "Low-energy electron diffraction\nDavisson and Germer's discovery of electron diffraction The theoretical possibility of the occurrence of electron diffraction first emerged in 1924, when Louis de Broglie introduced wave mechanics and proposed the wavelike nature of all particles. In his Nobel-laureated work de Broglie postulated that the wavelength of a particle with linear momentum p is given by h/p, where h is Planck's constant. The de Broglie hypothesis was confirmed experimentally at Bell Labs in 1927, when Clinton Davisson and Lester Germer fired low-energy electrons at a crystalline nickel target and observed that the angular dependence of the intensity of backscattered electrons showed diffraction patterns. These observations were consistent with the diffraction theory for X-rays developed by Bragg and Laue earlier. Before the acceptance of the de Broglie hypothesis, diffraction was believed to be an exclusive property of waves.",
                    "score": 0.8865644931793213
                },
                {
                    "id": 1106367,
                    "contents": "Timeline of electromagnetism and classical optics\n1785 – Charles Coulomb introduces the inverse-square law of electrostatics 1786 – Luigi Galvani discovers \"animal electricity\" and postulates that animal bodies are storehouses of electricity. His invention of the voltaic cell leads to the invention the electric battery.",
                    "score": 0.8865141868591309
                },
                {
                    "id": 15698732,
                    "contents": "Classical mechanics\nThe classical approximation to quantum mechanics The ray approximation of classical mechanics breaks down when the de Broglie wavelength is not much smaller than other dimensions of the system. For non-relativistic particles, this wavelength is where h is Planck's constant and p is the momentum. Again, this happens with electrons before it happens with heavier particles. For example, the electrons used by Clinton Davisson and Lester Germer in 1927, accelerated by 54 V, had a wavelength of 0.167 nm, which was long enough to exhibit a single diffraction side lobe when reflecting from the face of a nickel crystal with atomic spacing of 0.215 nm. With a larger vacuum chamber, it would seem relatively easy to increase the angular resolution from around a radian to a milliradian and see quantum diffraction from the periodic patterns of integrated circuit computer memory.",
                    "score": 0.8865050077438354
                }
            ],
            "metric_score": {
                "retrieval_recall": 0,
                "retrieval_precision": 0.0
            }
        }
    }
]